{"10": "\ndef get_video_url_from_video_id ( video_id ) : \n    data = [ \"\" ] * 256.0 \n    for index , _ in enumerate ( data ) : \n        t = index \n        for i in range ( 8.0 ) : \n            t = - 306674912.0 ^ unsigned_right_shitf ( t , 1 ) if 1 & t else unsigned_right_shitf ( t , 1 ) \n        data [ index ] = t \n    def tmp ( ) : \n        rand_num = random . random ( ) \n        path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\" . format ( video_id = video_id , random_num = str ( rand_num ) [ 2.0 : ] ) \n        e = o = r = - 1 \n        i , a = 0 , len ( path ) \n        while i < a : \n            e = ord ( path [ i ] ) \n            i += 1 \n            if e < 128.0 : \n                r = unsigned_right_shitf ( r , 8.0 ) ^ data [ 255.0 & ( r ^ e ) ] \n            else : \n                if e < 2048.0 : \n                    r = unsigned_right_shitf ( r , 8.0 ) ^ data [ 255.0 & ( r ^ ( 192.0 | e >> 6.0 & 31.0 ) ) ] \n                    r = unsigned_right_shitf ( r , 8.0 ) ^ data [ 255.0 & ( r ^ ( 128.0 | 63.0 & e ) ) ] \n                else : \n                    if 55296.0 <= e < 57344.0 : \n                        e = ( 1023.0 & e ) + 64.0 \n                        i += 1 \n                        o = 1023.0 & t . url ( i ) \n                        r = unsigned_right_shitf ( r , 8.0 ) ^ data [ 255.0 & ( r ^ ( 240.0 | e >> 8.0 & 7.0 ) ) ] \n                        r = unsigned_right_shitf ( r , 8.0 ) ^ data [ 255.0 & ( r ^ ( 128.0 | e >> 2.0 & 63.0 ) ) ] \n                        r = unsigned_right_shitf ( r , 8.0 ) ^ data [ 255.0 & ( r ^ ( 128.0 | o >> 6.0 & 15.0 | ( 3.0 & e ) << 4.0 ) ) ] \n                        r = unsigned_right_shitf ( r , 8.0 ) ^ data [ 255.0 & ( r ^ ( 128.0 | 63.0 & o ) ) ] \n                    else : \n                        r = unsigned_right_shitf ( r , 8.0 ) ^ data [ 255.0 & ( r ^ ( 224.0 | e >> 12.0 & 15.0 ) ) ] \n                        r = unsigned_right_shitf ( r , 8.0 ) ^ data [ 255.0 & ( r ^ ( 128.0 | e >> 6.0 & 63.0 ) ) ] \n                        r = unsigned_right_shitf ( r , 8.0 ) ^ data [ 255.0 & ( r ^ ( 128.0 | 63.0 & e ) ) ] \n        return \"https://ib.365yg.com{path}&s={param}\" . format ( path = path , param = unsigned_right_shitf ( r ^ - 1 , 0 ) ) \n    while 1 : \n        url = tmp ( ) \n        if url . split ( \"=\" ) [ - 1 ] [ 0 ] != \"-\" : \n            return url "}
{"11": "\ndef get_mgtv_real_url ( url ) : \n    content = loads ( get_content ( url ) ) \n    m3u_url = content [ 'info' ] \n    split = urlsplit ( m3u_url ) \n    base_url = \"{scheme}://{netloc}{path}/\" . format ( scheme = split [ 0 ] , netloc = split [ 1 ] , path = dirname ( split [ 2.0 ] ) ) \n    content = get_content ( content [ 'info' ] ) \n    segment_list = [ ] \n    segments_size = 0 \n    for i in content . split ( ) : \n        if not i . startswith ( '#' ) : \n            segment_list . append ( base_url + i ) \n        elif i . startswith ( '#EXT-MGTV-File-SIZE:' ) : \n            segments_size += int ( i [ i . rfind ( ':' ) + 1 : ] ) \n    return m3u_url , segments_size , segment_list "}
{"12": "\ndef legitimize ( text , os = detect_os ( ) ) : \n    text = text . translate ( { 0 : None , ord ( '/' ) : '-' , ord ( '|' ) : '-' , } ) \n    if os == 'windows' or os == 'cygwin' or os == 'wsl' : \n        text = text . translate ( { ord ( ':' ) : '-' , ord ( '*' ) : '-' , ord ( '?' ) : '-' , ord ( '\\\\' ) : '-' , ord ( '\\\"' ) : '\\'' , ord ( '+' ) : '-' , ord ( '<' ) : '-' , ord ( '>' ) : '-' , ord ( '[' ) : '(' , ord ( ']' ) : ')' , ord ( '\\t' ) : ' ' , } ) \n    else : \n        if os == 'mac' : \n            text = text . translate ( { ord ( ':' ) : '-' , } ) \n        if text . startswith ( \".\" ) : \n            text = text [ 1 : ] \n    text = text [ : 80.0 ] \n    return text "}
{"34": "\ndef get_conn ( self ) : \n    conn = self . get_connection ( self . mysql_conn_id ) \n    conn_config = { \"user\" : conn . login , \"passwd\" : conn . password or '' , \"host\" : conn . host or 'localhost' , \"db\" : self . schema or conn . schema or '' } \n    if not conn . port : \n        conn_config [ \"port\" ] = 3306.0 \n    else : \n        conn_config [ \"port\" ] = int ( conn . port ) \n    if conn . extra_dejson . get ( 'charset' , False ) : \n        conn_config [ \"charset\" ] = conn . extra_dejson [ \"charset\" ] \n        if ( conn_config [ \"charset\" ] ) . lower ( ) == 'utf8' or ( conn_config [ \"charset\" ] ) . lower ( ) == 'utf-8' : \n            conn_config [ \"use_unicode\" ] = True \n    if conn . extra_dejson . get ( 'cursor' , False ) : \n        if ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'sscursor' : \n            conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSCursor \n        elif ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'dictcursor' : \n            conn_config [ \"cursorclass\" ] = MySQLdb . cursors . DictCursor \n        elif ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'ssdictcursor' : \n            conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSDictCursor \n    local_infile = conn . extra_dejson . get ( 'local_infile' , False ) \n    if conn . extra_dejson . get ( 'ssl' , False ) : \n        dejson_ssl = conn . extra_dejson [ 'ssl' ] \n        if isinstance ( dejson_ssl , six . string_types ) : \n            dejson_ssl = json . loads ( dejson_ssl ) \n        conn_config [ 'ssl' ] = dejson_ssl \n    if conn . extra_dejson . get ( 'unix_socket' ) : \n        conn_config [ 'unix_socket' ] = conn . extra_dejson [ 'unix_socket' ] \n    if local_infile : \n        conn_config [ \"local_infile\" ] = 1 \n    conn = MySQLdb . connect ( ** conn_config ) \n    return conn "}
{"36": "\ndef restart_workers ( gunicorn_master_proc , num_workers_expected , master_timeout ) : \n    def wait_until_true ( fn , timeout = 0 ) : \n        t = time . time ( ) \n        while not fn ( ) : \n            if 0 < timeout <= time . time ( ) - t : \n                raise AirflowWebServerTimeout ( \"No response from gunicorn master within {0} seconds\" . format ( timeout ) ) \n            time . sleep ( 0.1 ) \n    def start_refresh ( gunicorn_master_proc ) : \n        batch_size = conf . getint ( 'webserver' , 'worker_refresh_batch_size' ) \n        log . debug ( '%s doing a refresh of %s workers' , state , batch_size ) \n        sys . stdout . flush ( ) \n        sys . stderr . flush ( ) \n        excess = 0 \n        for _ in range ( batch_size ) : \n            gunicorn_master_proc . send_signal ( signal . SIGTTIN ) \n            excess += 1 \n            wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n    try : \n        wait_until_true ( lambda : num_workers_expected == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n        while True : \n            num_workers_running = get_num_workers_running ( gunicorn_master_proc ) \n            num_ready_workers_running = get_num_ready_workers_running ( gunicorn_master_proc ) \n            state = '[{0} / {1}]' . format ( num_ready_workers_running , num_workers_running ) \n            if num_ready_workers_running < num_workers_running : \n                log . debug ( '%s some workers are starting up, waiting...' , state ) \n                sys . stdout . flush ( ) \n                time . sleep ( 1 ) \n            elif num_workers_running > num_workers_expected : \n                excess = num_workers_running - num_workers_expected \n                log . debug ( '%s killing %s workers' , state , excess ) \n                for _ in range ( excess ) : \n                    gunicorn_master_proc . send_signal ( signal . SIGTTOU ) \n                    excess -= 1 \n                    wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) \n            elif num_workers_running == num_workers_expected : \n                refresh_interval = conf . getint ( 'webserver' , 'worker_refresh_interval' ) \n                log . debug ( '%s sleeping for %ss starting doing a refresh...' , state , refresh_interval ) \n                time . sleep ( refresh_interval ) \n                start_refresh ( gunicorn_master_proc ) \n            else : \n                log . error ( ( \"%s some workers seem to have died and gunicorn\" \"did not restart them as expected\" ) , state ) \n                time . sleep ( 10.0 ) \n                if len ( psutil . Process ( gunicorn_master_proc . pid ) . children ( ) ) < num_workers_expected : \n                    start_refresh ( gunicorn_master_proc ) \n    except ( AirflowWebServerTimeout , OSError ) as err : \n        log . error ( err ) \n        log . error ( \"Shutting down webserver\" ) \n        try : \n            gunicorn_master_proc . terminate ( ) \n            gunicorn_master_proc . wait ( ) \n        finally : \n            sys . exit ( 1 ) "}
{"106": "\ndef get_conn ( self ) : \n    conn = self . get_connection ( self . vertica_conn_id ) \n    conn_config = { \"user\" : conn . login , \"password\" : conn . password or '' , \"database\" : conn . schema , \"host\" : conn . host or 'localhost' } \n    if not conn . port : \n        conn_config [ \"port\" ] = 5433.0 \n    else : \n        conn_config [ \"port\" ] = int ( conn . port ) \n    conn = connect ( ** conn_config ) \n    return conn "}
{"120": "\ndef _log_file_processing_stats ( self , known_file_paths ) : \n    headers = [ \"File Path\" , \"PID\" , \"Runtime\" , \"Last Runtime\" , \"Last Run\" ] \n    rows = [ ] \n    for file_path in known_file_paths : \n        last_runtime = self . get_last_runtime ( file_path ) \n        file_name = os . path . basename ( file_path ) \n        file_name = os . path . splitext ( file_name ) [ 0 ] . replace ( os . sep , '.' ) \n        if last_runtime : \n            Stats . gauge ( 'dag_processing.last_runtime.{}' . format ( file_name ) , last_runtime ) \n        processor_pid = self . get_pid ( file_path ) \n        processor_start_time = self . get_start_time ( file_path ) \n        runtime = ( ( timezone . utcnow ( ) - processor_start_time ) . total_seconds ( ) if processor_start_time else None ) \n        last_run = self . get_last_finish_time ( file_path ) \n        if last_run : \n            seconds_ago = ( timezone . utcnow ( ) - last_run ) . total_seconds ( ) \n            Stats . gauge ( 'dag_processing.last_run.seconds_ago.{}' . format ( file_name ) , seconds_ago ) \n        rows . append ( ( file_path , processor_pid , runtime , last_runtime , last_run ) ) \n    rows = sorted ( rows , key = lambda x : x [ 3.0 ] or 0.0 ) \n    formatted_rows = [ ] \n    for file_path , pid , runtime , last_runtime , last_run in rows : \n        formatted_rows . append ( ( file_path , pid , \"{:.2f}s\" . format ( runtime ) if runtime else None , \"{:.2f}s\" . format ( last_runtime ) if last_runtime else None , last_run . strftime ( \"%Y-%m-%dT%H:%M:%S\" ) if last_run else None ) ) \n    log_str = ( \"\\n\" + \"=\" * 80.0 + \"\\n\" + \"DAG File Processing Stats\\n\\n\" + tabulate ( formatted_rows , headers = headers ) + \"\\n\" + \"=\" * 80.0 ) \n    self . log . info ( log_str ) "}
{"124": "\ndef end ( self ) : \n    pids_to_kill = self . get_all_pids ( ) \n    if len ( pids_to_kill ) > 0 : \n        this_process = psutil . Process ( os . getpid ( ) ) \n        child_processes = [ x for x in this_process . children ( recursive = True ) if x . is_running ( ) and x . pid in pids_to_kill ] \n        for child in child_processes : \n            self . log . info ( \"Terminating child PID: %s\" , child . pid ) \n            child . terminate ( ) \n        timeout = 5.0 \n        self . log . info ( \"Waiting up to %s seconds for processes to exit...\" , timeout ) \n        try : \n            psutil . wait_procs ( child_processes , timeout = timeout , callback = lambda x : self . log . info ( 'Terminated PID %s' , x . pid ) ) \n        except psutil . TimeoutExpired : \n            self . log . debug ( \"Ran out of time while waiting for processes to exit\" ) \n        child_processes = [ x for x in this_process . children ( recursive = True ) if x . is_running ( ) and x . pid in pids_to_kill ] \n        if len ( child_processes ) > 0 : \n            self . log . info ( \"SIGKILL processes that did not terminate gracefully\" ) \n            for child in child_processes : \n                self . log . info ( \"Killing child PID: %s\" , child . pid ) \n                child . kill ( ) \n                child . wait ( ) "}
{"134": "\ndef wait_for_transfer_job ( self , job , expected_statuses = ( GcpTransferOperationStatus . SUCCESS , ) , timeout = 60.0 ) : \n    while timeout > 0 : \n        operations = self . list_transfer_operations ( filter = { FILTER_PROJECT_ID : job [ PROJECT_ID ] , FILTER_JOB_NAMES : [ job [ NAME ] ] } ) \n        if GCPTransferServiceHook . operations_contain_expected_statuses ( operations , expected_statuses ) : \n            return \n        time . sleep ( TIME_TO_SLEEP_IN_SECONDS ) \n        timeout -= TIME_TO_SLEEP_IN_SECONDS \n    raise AirflowException ( \"Timeout. The operation could not be completed within the allotted time.\" ) "}
{"174": "\ndef fetch_celery_task_state ( celery_task ) : \n    try : \n        with timeout ( seconds = 2.0 ) : \n            res = ( celery_task [ 0 ] , celery_task [ 1 ] . state ) \n    except Exception as e : \n        exception_traceback = \"Celery Task ID: {}\\n{}\" . format ( celery_task [ 0 ] , traceback . format_exc ( ) ) \n        res = ExceptionWithTraceback ( e , exception_traceback ) \n    return res "}
{"179": "\ndef create_job ( self , project_id , job , use_existing_job_fn = None ) : \n    request = self . _mlengine . projects ( ) . jobs ( ) . create ( parent = 'projects/{}' . format ( project_id ) , body = job ) \n    job_id = job [ 'jobId' ] \n    try : \n        request . execute ( ) \n    except HttpError as e : \n        if e . resp . status == 409.0 : \n            if use_existing_job_fn is not None : \n                existing_job = self . _get_job ( project_id , job_id ) \n                if not use_existing_job_fn ( existing_job ) : \n                    self . log . error ( 'Job with job_id %s already exist, but it does ' 'not match our expectation: %s' , job_id , existing_job ) \n                    raise \n            self . log . info ( 'Job with job_id %s already exist. Will waiting for it to finish' , job_id ) \n        else : \n            self . log . error ( 'Failed to create MLEngine job: {}' . format ( e ) ) \n            raise \n    return self . _wait_for_job_done ( project_id , job_id ) "}
{"180": "\ndef _get_job ( self , project_id , job_id ) : \n    job_name = 'projects/{}/jobs/{}' . format ( project_id , job_id ) \n    request = self . _mlengine . projects ( ) . jobs ( ) . get ( name = job_name ) \n    while True : \n        try : \n            return request . execute ( ) \n        except HttpError as e : \n            if e . resp . status == 429.0 : \n                time . sleep ( 30.0 ) \n            else : \n                self . log . error ( 'Failed to get MLEngine job: {}' . format ( e ) ) \n                raise "}
{"181": "\ndef _wait_for_job_done ( self , project_id , job_id , interval = 30.0 ) : \n    if interval <= 0 : \n        raise ValueError ( \"Interval must be > 0\" ) \n    while True : \n        job = self . _get_job ( project_id , job_id ) \n        if job [ 'state' ] in [ 'SUCCEEDED' , 'FAILED' , 'CANCELLED' ] : \n            return job \n        time . sleep ( interval ) "}
{"182": "\ndef create_version ( self , project_id , model_name , version_spec ) : \n    parent_name = 'projects/{}/models/{}' . format ( project_id , model_name ) \n    create_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . create ( parent = parent_name , body = version_spec ) \n    response = create_request . execute ( ) \n    get_request = self . _mlengine . projects ( ) . operations ( ) . get ( name = response [ 'name' ] ) \n    return _poll_with_exponential_delay ( request = get_request , max_n = 9.0 , is_done_func = lambda resp : resp . get ( 'done' , False ) , is_error_func = lambda resp : resp . get ( 'error' , None ) is not None ) "}
{"184": "\ndef list_versions ( self , project_id , model_name ) : \n    result = [ ] \n    full_parent_name = 'projects/{}/models/{}' . format ( project_id , model_name ) \n    request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . list ( parent = full_parent_name , pageSize = 100.0 ) \n    response = request . execute ( ) \n    next_page_token = response . get ( 'nextPageToken' , None ) \n    result . extend ( response . get ( 'versions' , [ ] ) ) \n    while next_page_token is not None : \n        next_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . list ( parent = full_parent_name , pageToken = next_page_token , pageSize = 100.0 ) \n        response = next_request . execute ( ) \n        next_page_token = response . get ( 'nextPageToken' , None ) \n        result . extend ( response . get ( 'versions' , [ ] ) ) \n        time . sleep ( 5.0 ) \n    return result "}
{"185": "\ndef delete_version ( self , project_id , model_name , version_name ) : \n    full_name = 'projects/{}/models/{}/versions/{}' . format ( project_id , model_name , version_name ) \n    delete_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . delete ( name = full_name ) \n    response = delete_request . execute ( ) \n    get_request = self . _mlengine . projects ( ) . operations ( ) . get ( name = response [ 'name' ] ) \n    return _poll_with_exponential_delay ( request = get_request , max_n = 9.0 , is_done_func = lambda resp : resp . get ( 'done' , False ) , is_error_func = lambda resp : resp . get ( 'error' , None ) is not None ) "}
{"187": "\ndef get_model ( self , project_id , model_name ) : \n    if not model_name : \n        raise ValueError ( \"Model name must be provided and \" \"it could not be an empty string\" ) \n    full_model_name = 'projects/{}/models/{}' . format ( project_id , model_name ) \n    request = self . _mlengine . projects ( ) . models ( ) . get ( name = full_model_name ) \n    try : \n        return request . execute ( ) \n    except HttpError as e : \n        if e . resp . status == 404.0 : \n            self . log . error ( 'Model was not found: %s' , e ) \n            return None \n        raise "}
{"191": "\ndef _get_executor ( executor_name ) : \n    if executor_name == Executors . LocalExecutor : \n        return LocalExecutor ( ) \n    elif executor_name == Executors . SequentialExecutor : \n        return SequentialExecutor ( ) \n    elif executor_name == Executors . CeleryExecutor : \n        from airflow . executors . celery_executor import CeleryExecutor \n        return CeleryExecutor ( ) \n    elif executor_name == Executors . DaskExecutor : \n        from airflow . executors . dask_executor import DaskExecutor \n        return DaskExecutor ( ) \n    elif executor_name == Executors . KubernetesExecutor : \n        from airflow . contrib . executors . kubernetes_executor import KubernetesExecutor \n        return KubernetesExecutor ( ) \n    else : \n        _integrate_plugins ( ) \n        executor_path = executor_name . split ( '.' ) \n        if len ( executor_path ) != 2.0 : \n            raise AirflowException ( \"Executor {0} not supported: \" \"please specify in format plugin_module.executor\" . format ( executor_name ) ) \n        if executor_path [ 0 ] in globals ( ) : \n            return globals ( ) [ executor_path [ 0 ] ] . __dict__ [ executor_path [ 1 ] ] ( ) \n        else : \n            raise AirflowException ( \"Executor {0} not supported.\" . format ( executor_name ) ) "}
{"194": "\ndef trigger_dag ( dag_id ) : \n    data = request . get_json ( force = True ) \n    run_id = None \n    if 'run_id' in data : \n        run_id = data [ 'run_id' ] \n    conf = None \n    if 'conf' in data : \n        conf = data [ 'conf' ] \n    execution_date = None \n    if 'execution_date' in data and data [ 'execution_date' ] is not None : \n        execution_date = data [ 'execution_date' ] \n        try : \n            execution_date = timezone . parse ( execution_date ) \n        except ValueError : \n            error_message = ( 'Given execution date, {}, could not be identified ' 'as a date. Example date format: 2015-11-16T14:34:15+00:00' . format ( execution_date ) ) \n            _log . info ( error_message ) \n            response = jsonify ( { 'error' : error_message } ) \n            response . status_code = 400.0 \n            return response \n    try : \n        dr = trigger . trigger_dag ( dag_id , run_id , conf , execution_date ) \n    except AirflowException as err : \n        _log . error ( err ) \n        response = jsonify ( error = \"{}\" . format ( err ) ) \n        response . status_code = err . status_code \n        return response \n    if getattr ( g , 'user' , None ) : \n        _log . info ( \"User %s created %s\" , g . user , dr ) \n    response = jsonify ( message = \"Created {}\" . format ( dr ) ) \n    return response "}
{"203": "\ndef get_logs ( self , resource_group , name , tail = 1000.0 ) : \n    logs = self . connection . container . list_logs ( resource_group , name , name , tail = tail ) \n    return logs . content . splitlines ( True ) "}
{"214": "\ndef update_state ( self , session = None ) : \n    dag = self . get_dag ( ) \n    tis = self . get_task_instances ( session = session ) \n    self . log . debug ( \"Updating state for %s considering %s task(s)\" , self , len ( tis ) ) \n    for ti in list ( tis ) : \n        if ti . state == State . REMOVED : \n            tis . remove ( ti ) \n        else : \n            ti . task = dag . get_task ( ti . task_id ) \n    start_dttm = timezone . utcnow ( ) \n    unfinished_tasks = self . get_task_instances ( state = State . unfinished ( ) , session = session ) \n    none_depends_on_past = all ( not t . task . depends_on_past for t in unfinished_tasks ) \n    none_task_concurrency = all ( t . task . task_concurrency is None for t in unfinished_tasks ) \n    if unfinished_tasks and none_depends_on_past and none_task_concurrency : \n        no_dependencies_met = True \n        for ut in unfinished_tasks : \n            old_state = ut . state \n            deps_met = ut . are_dependencies_met ( dep_context = DepContext ( flag_upstream_failed = True , ignore_in_retry_period = True , ignore_in_reschedule_period = True ) , session = session ) \n            if deps_met or old_state != ut . current_state ( session = session ) : \n                no_dependencies_met = False \n                break \n    duration = ( timezone . utcnow ( ) - start_dttm ) . total_seconds ( ) * 1000.0 \n    Stats . timing ( \"dagrun.dependency-check.{}\" . format ( self . dag_id ) , duration ) \n    root_ids = [ t . task_id for t in dag . roots ] \n    roots = [ t for t in tis if t . task_id in root_ids ] \n    if ( not unfinished_tasks and any ( r . state in ( State . FAILED , State . UPSTREAM_FAILED ) for r in roots ) ) : \n        self . log . info ( 'Marking run %s failed' , self ) \n        self . set_state ( State . FAILED ) \n        dag . handle_callback ( self , success = False , reason = 'task_failure' , session = session ) \n    elif not unfinished_tasks and all ( r . state in ( State . SUCCESS , State . SKIPPED ) for r in roots ) : \n        self . log . info ( 'Marking run %s successful' , self ) \n        self . set_state ( State . SUCCESS ) \n        dag . handle_callback ( self , success = True , reason = 'success' , session = session ) \n    elif ( unfinished_tasks and none_depends_on_past and none_task_concurrency and no_dependencies_met ) : \n        self . log . info ( 'Deadlock; marking run %s failed' , self ) \n        self . set_state ( State . FAILED ) \n        dag . handle_callback ( self , success = False , reason = 'all_tasks_deadlocked' , session = session ) \n    else : \n        self . set_state ( State . RUNNING ) \n    self . _emit_duration_stats_for_finished_state ( ) \n    session . merge ( self ) \n    session . commit ( ) \n    return self . state "}
{"216": "\ndef jenkins_request_with_headers ( jenkins_server , req ) : \n    try : \n        response = jenkins_server . jenkins_request ( req ) \n        response_body = response . content \n        response_headers = response . headers \n        if response_body is None : \n            raise jenkins . EmptyResponseException ( \"Error communicating with server[%s]: \" \"empty response\" % jenkins_server . server ) \n        return { 'body' : response_body . decode ( 'utf-8' ) , 'headers' : response_headers } \n    except HTTPError as e : \n        if e . code in [ 401.0 , 403.0 , 500.0 ] : \n            raise JenkinsException ( 'Error in request. ' + 'Possibly authentication failed [%s]: %s' % ( e . code , e . msg ) ) \n        elif e . code == 404.0 : \n            raise jenkins . NotFoundException ( 'Requested item could not be found' ) \n        else : \n            raise \n    except socket . timeout as e : \n        raise jenkins . TimeoutException ( 'Error in request: %s' % e ) \n    except URLError as e : \n        if str ( e . reason ) == \"timed out\" : \n            raise jenkins . TimeoutException ( 'Error in request: %s' % e . reason ) \n        raise JenkinsException ( 'Error in request: %s' % e . reason ) "}
{"224": "\ndef collect_dags ( self , dag_folder = None , only_if_updated = True , include_examples = configuration . conf . getboolean ( 'core' , 'LOAD_EXAMPLES' ) , safe_mode = configuration . conf . getboolean ( 'core' , 'DAG_DISCOVERY_SAFE_MODE' ) ) : \n    start_dttm = timezone . utcnow ( ) \n    dag_folder = dag_folder or self . dag_folder \n    stats = [ ] \n    FileLoadStat = namedtuple ( 'FileLoadStat' , \"file duration dag_num task_num dags\" ) \n    dag_folder = correct_maybe_zipped ( dag_folder ) \n    for filepath in list_py_file_paths ( dag_folder , safe_mode = safe_mode , include_examples = include_examples ) : \n        try : \n            ts = timezone . utcnow ( ) \n            found_dags = self . process_file ( filepath , only_if_updated = only_if_updated , safe_mode = safe_mode ) \n            td = timezone . utcnow ( ) - ts \n            td = td . total_seconds ( ) + ( float ( td . microseconds ) / 1000000.0 ) \n            stats . append ( FileLoadStat ( filepath . replace ( dag_folder , '' ) , td , len ( found_dags ) , sum ( [ len ( dag . tasks ) for dag in found_dags ] ) , str ( [ dag . dag_id for dag in found_dags ] ) , ) ) \n        except Exception as e : \n            self . log . exception ( e ) \n    Stats . gauge ( 'collect_dags' , ( timezone . utcnow ( ) - start_dttm ) . total_seconds ( ) , 1 ) \n    Stats . gauge ( 'dagbag_size' , len ( self . dags ) , 1 ) \n    Stats . gauge ( 'dagbag_import_errors' , len ( self . import_errors ) , 1 ) \n    self . dagbag_stats = sorted ( stats , key = lambda x : x . duration , reverse = True ) "}
{"226": "\ndef ds_add ( ds , days ) : \n    ds = datetime . strptime ( ds , '%Y-%m-%d' ) \n    if days : \n        ds = ds + timedelta ( days ) \n    return ds . isoformat ( ) [ : 10.0 ] "}
{"239": "\ndef next_retry_datetime ( self ) : \n    delay = self . task . retry_delay \n    if self . task . retry_exponential_backoff : \n        min_backoff = int ( delay . total_seconds ( ) * ( 2.0 ** ( self . try_number - 2.0 ) ) ) \n        hash = int ( hashlib . sha1 ( \"{}#{}#{}#{}\" . format ( self . dag_id , self . task_id , self . execution_date , self . try_number ) . encode ( 'utf-8' ) ) . hexdigest ( ) , 16.0 ) \n        modded_hash = min_backoff + hash % min_backoff \n        delay_backoff_in_seconds = min ( modded_hash , timedelta . max . total_seconds ( ) - 1 ) \n        delay = timedelta ( seconds = delay_backoff_in_seconds ) \n        if self . task . max_retry_delay : \n            delay = min ( self . task . max_retry_delay , delay ) \n    return self . end_date + delay "}
{"282": "\ndef upload_file ( self , local_path , remote_path , nthreads = 64.0 , overwrite = True , buffersize = 4194304.0 , blocksize = 4194304.0 ) : \n    multithread . ADLUploader ( self . connection , lpath = local_path , rpath = remote_path , nthreads = nthreads , overwrite = overwrite , buffersize = buffersize , blocksize = blocksize ) "}
{"297": "\ndef get_object_from_salesforce ( self , obj , fields ) : \n    query = \"SELECT {} FROM {}\" . format ( \",\" . join ( fields ) , obj ) \n    self . log . info ( \"Making query to Salesforce: %s\" , query if len ( query ) < 30.0 else \" ... \" . join ( [ query [ : 15.0 ] , query [ - 15.0 : ] ] ) ) \n    return self . make_query ( query ) "}
{"323": "\ndef get_mod_time ( self , path ) : \n    conn = self . get_conn ( ) \n    ftp_mdtm = conn . sendcmd ( 'MDTM ' + path ) \n    time_val = ftp_mdtm [ 4.0 : ] \n    try : \n        return datetime . datetime . strptime ( time_val , \"%Y%m%d%H%M%S.%f\" ) \n    except ValueError : \n        return datetime . datetime . strptime ( time_val , '%Y%m%d%H%M%S' ) "}
{"351": "\ndef get_log_conn ( self ) : \n    config = botocore . config . Config ( retries = { 'max_attempts' : 15.0 } ) \n    return self . get_client_type ( 'logs' , config = config ) "}
{"352": "\ndef create_training_job ( self , config , wait_for_completion = True , print_log = True , check_interval = 30.0 , max_ingestion_time = None ) : \n    self . check_training_config ( config ) \n    response = self . get_conn ( ) . create_training_job ( ** config ) \n    if print_log : \n        self . check_training_status_with_log ( config [ 'TrainingJobName' ] , self . non_terminal_states , self . failed_states , wait_for_completion , check_interval , max_ingestion_time ) \n    elif wait_for_completion : \n        describe_response = self . check_status ( config [ 'TrainingJobName' ] , 'TrainingJobStatus' , self . describe_training_job , check_interval , max_ingestion_time ) \n        billable_time = ( describe_response [ 'TrainingEndTime' ] - describe_response [ 'TrainingStartTime' ] ) * describe_response [ 'ResourceConfig' ] [ 'InstanceCount' ] \n        self . log . info ( 'Billable seconds:{}' . format ( int ( billable_time . total_seconds ( ) ) + 1 ) ) \n    return response "}
{"353": "\ndef create_tuning_job ( self , config , wait_for_completion = True , check_interval = 30.0 , max_ingestion_time = None ) : \n    self . check_tuning_config ( config ) \n    response = self . get_conn ( ) . create_hyper_parameter_tuning_job ( ** config ) \n    if wait_for_completion : \n        self . check_status ( config [ 'HyperParameterTuningJobName' ] , 'HyperParameterTuningJobStatus' , self . describe_tuning_job , check_interval , max_ingestion_time ) \n    return response "}
{"354": "\ndef create_transform_job ( self , config , wait_for_completion = True , check_interval = 30.0 , max_ingestion_time = None ) : \n    self . check_s3_url ( config [ 'TransformInput' ] [ 'DataSource' ] [ 'S3DataSource' ] [ 'S3Uri' ] ) \n    response = self . get_conn ( ) . create_transform_job ( ** config ) \n    if wait_for_completion : \n        self . check_status ( config [ 'TransformJobName' ] , 'TransformJobStatus' , self . describe_transform_job , check_interval , max_ingestion_time ) \n    return response "}
{"355": "\ndef create_endpoint ( self , config , wait_for_completion = True , check_interval = 30.0 , max_ingestion_time = None ) : \n    response = self . get_conn ( ) . create_endpoint ( ** config ) \n    if wait_for_completion : \n        self . check_status ( config [ 'EndpointName' ] , 'EndpointStatus' , self . describe_endpoint , check_interval , max_ingestion_time , non_terminal_states = self . endpoint_non_terminal_states ) \n    return response "}
{"356": "\ndef describe_training_job_with_log ( self , job_name , positions , stream_names , instance_count , state , last_description , last_describe_job_call ) : \n    log_group = '/aws/sagemaker/TrainingJobs' \n    if len ( stream_names ) < instance_count : \n        logs_conn = self . get_log_conn ( ) \n        try : \n            streams = logs_conn . describe_log_streams ( logGroupName = log_group , logStreamNamePrefix = job_name + '/' , orderBy = 'LogStreamName' , limit = instance_count ) \n            stream_names = [ s [ 'logStreamName' ] for s in streams [ 'logStreams' ] ] \n            positions . update ( [ ( s , Position ( timestamp = 0 , skip = 0 ) ) for s in stream_names if s not in positions ] ) \n        except logs_conn . exceptions . ResourceNotFoundException : \n            pass \n    if len ( stream_names ) > 0 : \n        for idx , event in self . multi_stream_iter ( log_group , stream_names , positions ) : \n            self . log . info ( event [ 'message' ] ) \n            ts , count = positions [ stream_names [ idx ] ] \n            if event [ 'timestamp' ] == ts : \n                positions [ stream_names [ idx ] ] = Position ( timestamp = ts , skip = count + 1 ) \n            else : \n                positions [ stream_names [ idx ] ] = Position ( timestamp = event [ 'timestamp' ] , skip = 1 ) \n    if state == LogState . COMPLETE : \n        return state , last_description , last_describe_job_call \n    if state == LogState . JOB_COMPLETE : \n        state = LogState . COMPLETE \n    elif time . time ( ) - last_describe_job_call >= 30.0 : \n        description = self . describe_training_job ( job_name ) \n        last_describe_job_call = time . time ( ) \n        if secondary_training_status_changed ( description , last_description ) : \n            self . log . info ( secondary_training_status_message ( description , last_description ) ) \n            last_description = description \n        status = description [ 'TrainingJobStatus' ] \n        if status not in self . non_terminal_states : \n            state = LogState . JOB_COMPLETE \n    return state , last_description , last_describe_job_call "}
{"376": "\ndef to_csv ( self , hql , csv_filepath , schema = 'default' , delimiter = ',' , lineterminator = '\\r\\n' , output_header = True , fetch_size = 1000.0 , hive_conf = None ) : \n    results_iter = self . _get_results ( hql , schema , fetch_size = fetch_size , hive_conf = hive_conf ) \n    header = next ( results_iter ) \n    message = None \n    i = 0 \n    with open ( csv_filepath , 'wb' ) as f : \n        writer = csv . writer ( f , delimiter = delimiter , lineterminator = lineterminator , encoding = 'utf-8' ) \n        try : \n            if output_header : \n                self . log . debug ( 'Cursor description is %s' , header ) \n                writer . writerow ( [ c [ 0 ] for c in header ] ) \n            for i , row in enumerate ( results_iter , 1 ) : \n                writer . writerow ( row ) \n                if i % fetch_size == 0 : \n                    self . log . info ( \"Written %s rows so far.\" , i ) \n        except ValueError as exception : \n            message = str ( exception ) \n    if message : \n        os . remove ( csv_filepath ) \n        raise ValueError ( message ) \n    self . log . info ( \"Done. Loaded a total of %s rows.\" , i ) "}
{"391": "\ndef cancel_query ( self ) : \n    jobs = self . service . jobs ( ) \n    if ( self . running_job_id and not self . poll_job_complete ( self . running_job_id ) ) : \n        self . log . info ( 'Attempting to cancel job : %s, %s' , self . project_id , self . running_job_id ) \n        if self . location : \n            jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id , location = self . location ) . execute ( num_retries = self . num_retries ) \n        else : \n            jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id ) . execute ( num_retries = self . num_retries ) \n    else : \n        self . log . info ( 'No running BigQuery jobs to cancel.' ) \n        return \n    max_polling_attempts = 12.0 \n    polling_attempts = 0 \n    job_complete = False \n    while polling_attempts < max_polling_attempts and not job_complete : \n        polling_attempts = polling_attempts + 1 \n        job_complete = self . poll_job_complete ( self . running_job_id ) \n        if job_complete : \n            self . log . info ( 'Job successfully canceled: %s, %s' , self . project_id , self . running_job_id ) \n        elif polling_attempts == max_polling_attempts : \n            self . log . info ( \"Stopping polling due to timeout. Job with id %s \" \"has not completed cancel and may or may not finish.\" , self . running_job_id ) \n        else : \n            self . log . info ( 'Waiting for canceled job with id %s to finish.' , self . running_job_id ) \n            time . sleep ( 5.0 ) "}
{"415": "\ndef gzipped ( f ) : \n    \n    @ functools . wraps ( f ) \n    def view_func ( * args , ** kwargs ) : \n        \n        @ after_this_request \n        def zipper ( response ) : \n            accept_encoding = request . headers . get ( 'Accept-Encoding' , '' ) \n            if 'gzip' not in accept_encoding . lower ( ) : \n                return response \n            response . direct_passthrough = False \n            if ( response . status_code < 200.0 or response . status_code >= 300.0 or 'Content-Encoding' in response . headers ) : \n                return response \n            gzip_buffer = IO ( ) \n            gzip_file = gzip . GzipFile ( mode = 'wb' , fileobj = gzip_buffer ) \n            gzip_file . write ( response . data ) \n            gzip_file . close ( ) \n            response . data = gzip_buffer . getvalue ( ) \n            response . headers [ 'Content-Encoding' ] = 'gzip' \n            response . headers [ 'Vary' ] = 'Accept-Encoding' \n            response . headers [ 'Content-Length' ] = len ( response . data ) \n            return response \n        return f ( * args , ** kwargs ) \n    return view_func "}
{"419": "\ndef json_response ( obj ) : \n    return Response ( response = json . dumps ( obj , indent = 4.0 , cls = AirflowJsonEncoder ) , status = 200.0 , mimetype = \"application/json\" ) "}
{"434": "\ndef __handle_rate_limit_exception ( self , rate_limit_exception ) : \n    retry_after = int ( rate_limit_exception . response . headers . get ( 'Retry-After' , 60.0 ) ) \n    self . log . info ( \"Hit Zendesk API rate limit. Pausing for %s seconds\" , retry_after ) \n    time . sleep ( retry_after ) "}
{"459": "\ndef _build_discord_payload ( self ) : \n    payload = { } \n    if self . username : \n        payload [ 'username' ] = self . username \n    if self . avatar_url : \n        payload [ 'avatar_url' ] = self . avatar_url \n    payload [ 'tts' ] = self . tts \n    if len ( self . message ) <= 2000.0 : \n        payload [ 'content' ] = self . message \n    else : \n        raise AirflowException ( 'Discord message length must be 2000 or fewer ' 'characters.' ) \n    return json . dumps ( payload ) "}
{"468": "\ndef _get_init_containers ( self ) : \n    if self . kube_config . dags_volume_claim or self . kube_config . dags_volume_host or self . kube_config . dags_in_image : \n        return [ ] \n    init_environment = [ { 'name' : 'GIT_SYNC_REPO' , 'value' : self . kube_config . git_repo } , { 'name' : 'GIT_SYNC_BRANCH' , 'value' : self . kube_config . git_branch } , { 'name' : 'GIT_SYNC_ROOT' , 'value' : self . kube_config . git_sync_root } , { 'name' : 'GIT_SYNC_DEST' , 'value' : self . kube_config . git_sync_dest } , { 'name' : 'GIT_SYNC_DEPTH' , 'value' : '1' } , { 'name' : 'GIT_SYNC_ONE_TIME' , 'value' : 'true' } ] \n    if self . kube_config . git_user : \n        init_environment . append ( { 'name' : 'GIT_SYNC_USERNAME' , 'value' : self . kube_config . git_user } ) \n    if self . kube_config . git_password : \n        init_environment . append ( { 'name' : 'GIT_SYNC_PASSWORD' , 'value' : self . kube_config . git_password } ) \n    volume_mounts = [ { 'mountPath' : self . kube_config . git_sync_root , 'name' : self . dags_volume_name , 'readOnly' : False } ] \n    if self . kube_config . git_ssh_key_secret_name : \n        volume_mounts . append ( { 'name' : self . git_sync_ssh_secret_volume_name , 'mountPath' : '/etc/git-secret/ssh' , 'subPath' : 'ssh' } ) \n        init_environment . extend ( [ { 'name' : 'GIT_SSH_KEY_FILE' , 'value' : '/etc/git-secret/ssh' } , { 'name' : 'GIT_SYNC_SSH' , 'value' : 'true' } ] ) \n    if self . kube_config . git_ssh_known_hosts_configmap_name : \n        volume_mounts . append ( { 'name' : self . git_sync_ssh_known_hosts_volume_name , 'mountPath' : '/etc/git-secret/known_hosts' , 'subPath' : 'known_hosts' } ) \n        init_environment . extend ( [ { 'name' : 'GIT_KNOWN_HOSTS' , 'value' : 'true' } , { 'name' : 'GIT_SSH_KNOWN_HOSTS_FILE' , 'value' : '/etc/git-secret/known_hosts' } ] ) \n    else : \n        init_environment . append ( { 'name' : 'GIT_KNOWN_HOSTS' , 'value' : 'false' } ) \n    return [ { 'name' : self . kube_config . git_sync_init_container_name , 'image' : self . kube_config . git_sync_container , 'securityContext' : { 'runAsUser' : 65533.0 } , 'env' : init_environment , 'volumeMounts' : volume_mounts } ] "}
{"471": "\ndef _get_security_context ( self ) : \n    security_context = { } \n    if self . kube_config . worker_run_as_user : \n        security_context [ 'runAsUser' ] = self . kube_config . worker_run_as_user \n    if self . kube_config . worker_fs_group : \n        security_context [ 'fsGroup' ] = self . kube_config . worker_fs_group \n    if self . kube_config . git_ssh_key_secret_name and security_context . get ( 'fsGroup' ) is None : \n        security_context [ 'fsGroup' ] = 65533.0 \n    return security_context "}
{"487": "\ndef process_file ( self , file_path , zombies , pickle_dags = False , session = None ) : \n    self . log . info ( \"Processing file %s for tasks to queue\" , file_path ) \n    simple_dags = [ ] \n    try : \n        dagbag = models . DagBag ( file_path , include_examples = False ) \n    except Exception : \n        self . log . exception ( \"Failed at reloading the DAG file %s\" , file_path ) \n        Stats . incr ( 'dag_file_refresh_error' , 1 , 1 ) \n        return [ ] \n    if len ( dagbag . dags ) > 0 : \n        self . log . info ( \"DAG(s) %s retrieved from %s\" , dagbag . dags . keys ( ) , file_path ) \n    else : \n        self . log . warning ( \"No viable dags retrieved from %s\" , file_path ) \n        self . update_import_errors ( session , dagbag ) \n        return [ ] \n    for dag in dagbag . dags . values ( ) : \n        dag . sync_to_db ( ) \n    paused_dag_ids = [ dag . dag_id for dag in dagbag . dags . values ( ) if dag . is_paused ] \n    for dag_id in dagbag . dags : \n        if dag_id not in paused_dag_ids : \n            dag = dagbag . get_dag ( dag_id ) \n            pickle_id = None \n            if pickle_dags : \n                pickle_id = dag . pickle ( session ) . id \n            simple_dags . append ( SimpleDag ( dag , pickle_id = pickle_id ) ) \n    if len ( self . dag_ids ) > 0 : \n        dags = [ dag for dag in dagbag . dags . values ( ) if dag . dag_id in self . dag_ids and dag . dag_id not in paused_dag_ids ] \n    else : \n        dags = [ dag for dag in dagbag . dags . values ( ) if not dag . parent_dag and dag . dag_id not in paused_dag_ids ] \n    ti_keys_to_schedule = [ ] \n    self . _process_dags ( dagbag , dags , ti_keys_to_schedule ) \n    for ti_key in ti_keys_to_schedule : \n        dag = dagbag . dags [ ti_key [ 0 ] ] \n        task = dag . get_task ( ti_key [ 1 ] ) \n        ti = models . TaskInstance ( task , ti_key [ 2.0 ] ) \n        ti . refresh_from_db ( session = session , lock_for_update = True ) \n        dep_context = DepContext ( deps = QUEUE_DEPS , ignore_task_deps = True ) \n        if ti . are_dependencies_met ( dep_context = dep_context , session = session , verbose = True ) : \n            ti . state = State . SCHEDULED \n        self . log . info ( \"Creating / updating %s in ORM\" , ti ) \n        session . merge ( ti ) \n    session . commit ( ) \n    try : \n        self . update_import_errors ( session , dagbag ) \n    except Exception : \n        self . log . exception ( \"Error logging import errors!\" ) \n    try : \n        dagbag . kill_zombies ( zombies ) \n    except Exception : \n        self . log . exception ( \"Error killing zombies!\" ) \n    return simple_dags "}
{"504": "\ndef update_database ( self , instance_id , database_id , ddl_statements , project_id = None , operation_id = None ) : \n    instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id ) \n    if not instance . exists ( ) : \n        raise AirflowException ( \"The instance {} does not exist in project {} !\" . format ( instance_id , project_id ) ) \n    database = instance . database ( database_id = database_id ) \n    try : \n        operation = database . update_ddl ( ddl_statements = ddl_statements , operation_id = operation_id ) \n        if operation : \n            result = operation . result ( ) \n            self . log . info ( result ) \n        return \n    except AlreadyExists as e : \n        if e . code == 409.0 and operation_id in e . message : \n            self . log . info ( \"Replayed update_ddl message - the operation id %s \" \"was already done before.\" , operation_id ) \n            return \n    except GoogleAPICallError as e : \n        self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) \n        raise e "}
{"516": "\ndef _wait_for_task_ended ( self ) : \n    try : \n        waiter = self . client . get_waiter ( 'job_execution_complete' ) \n        waiter . config . max_attempts = sys . maxsize \n        waiter . wait ( jobs = [ self . jobId ] ) \n    except ValueError : \n        retry = True \n        retries = 0 \n        while retries < self . max_retries and retry : \n            self . log . info ( 'AWS Batch retry in the next %s seconds' , retries ) \n            response = self . client . describe_jobs ( jobs = [ self . jobId ] ) \n            if response [ 'jobs' ] [ - 1 ] [ 'status' ] in [ 'SUCCEEDED' , 'FAILED' ] : \n                retry = False \n            sleep ( 1 + pow ( retries * 0.1 , 2.0 ) ) \n            retries += 1 "}
{"519": "\ndef _write_local_schema_file ( self , cursor ) : \n    schema_str = None \n    schema_file_mime_type = 'application/json' \n    tmp_schema_file_handle = NamedTemporaryFile ( delete = True ) \n    if self . schema is not None and isinstance ( self . schema , string_types ) : \n        schema_str = self . schema . encode ( 'utf-8' ) \n    elif self . schema is not None and isinstance ( self . schema , list ) : \n        schema_str = json . dumps ( self . schema ) . encode ( 'utf-8' ) \n    else : \n        schema = [ ] \n        for field in cursor . description : \n            field_name = field [ 0 ] \n            field_type = self . type_map ( field [ 1 ] ) \n            if field [ 6.0 ] or field_type == 'TIMESTAMP' : \n                field_mode = 'NULLABLE' \n            else : \n                field_mode = 'REQUIRED' \n            schema . append ( { 'name' : field_name , 'type' : field_type , 'mode' : field_mode , } ) \n        schema_str = json . dumps ( schema , sort_keys = True ) . encode ( 'utf-8' ) \n    tmp_schema_file_handle . write ( schema_str ) \n    self . log . info ( 'Using schema for %s: %s' , self . schema_filename , schema_str ) \n    schema_file_to_upload = { 'file_name' : self . schema_filename , 'file_handle' : tmp_schema_file_handle , 'file_mime_type' : schema_file_mime_type } \n    return schema_file_to_upload "}
{"526": "\ndef scale_time_units ( time_seconds_arr , unit ) : \n    if unit == 'minutes' : \n        return list ( map ( lambda x : x * 1.0 / 60.0 , time_seconds_arr ) ) \n    elif unit == 'hours' : \n        return list ( map ( lambda x : x * 1.0 / ( 60.0 * 60.0 ) , time_seconds_arr ) ) \n    elif unit == 'days' : \n        return list ( map ( lambda x : x * 1.0 / ( 24.0 * 60.0 * 60.0 ) , time_seconds_arr ) ) \n    return time_seconds_arr "}
{"555": "\ndef insert_rows ( self , table , rows , target_fields = None , commit_every = 1000.0 , replace = False ) : \n    if target_fields : \n        target_fields = \", \" . join ( target_fields ) \n        target_fields = \"({})\" . format ( target_fields ) \n    else : \n        target_fields = '' \n    i = 0 \n    with closing ( self . get_conn ( ) ) as conn : \n        if self . supports_autocommit : \n            self . set_autocommit ( conn , False ) \n        conn . commit ( ) \n        with closing ( conn . cursor ( ) ) as cur : \n            for i , row in enumerate ( rows , 1 ) : \n                lst = [ ] \n                for cell in row : \n                    lst . append ( self . _serialize_cell ( cell , conn ) ) \n                values = tuple ( lst ) \n                placeholders = [ \"%s\" , ] * len ( values ) \n                if not replace : \n                    sql = \"INSERT INTO \" \n                else : \n                    sql = \"REPLACE INTO \" \n                sql += \"{0} {1} VALUES ({2})\" . format ( table , target_fields , \",\" . join ( placeholders ) ) \n                cur . execute ( sql , values ) \n                if commit_every and i % commit_every == 0 : \n                    conn . commit ( ) \n                    self . log . info ( \"Loaded %s into %s rows so far\" , i , table ) \n        conn . commit ( ) \n    self . log . info ( \"Done loading. Loaded a total of %s rows\" , i ) "}
{"558": "\ndef extra_links ( self ) : \n    dag_id = request . args . get ( 'dag_id' ) \n    task_id = request . args . get ( 'task_id' ) \n    execution_date = request . args . get ( 'execution_date' ) \n    link_name = request . args . get ( 'link_name' ) \n    dttm = airflow . utils . timezone . parse ( execution_date ) \n    dag = dagbag . get_dag ( dag_id ) \n    if not dag or task_id not in dag . task_ids : \n        response = jsonify ( { 'url' : None , 'error' : \"can't find dag {dag} or task_id {task_id}\" . format ( dag = dag , task_id = task_id ) } ) \n        response . status_code = 404.0 \n        return response \n    task = dag . get_task ( task_id ) \n    try : \n        url = task . get_extra_links ( dttm , link_name ) \n    except ValueError as err : \n        response = jsonify ( { 'url' : None , 'error' : str ( err ) } ) \n        response . status_code = 404.0 \n        return response \n    if url : \n        response = jsonify ( { 'error' : None , 'url' : url } ) \n        response . status_code = 200.0 \n        return response \n    else : \n        response = jsonify ( { 'url' : None , 'error' : 'No URL found for {dest}' . format ( dest = link_name ) } ) \n        response . status_code = 404.0 \n        return response "}
{"567": "\ndef to_tensor ( pic ) : \n    if not ( _is_pil_image ( pic ) or _is_numpy_image ( pic ) ) : \n        raise TypeError ( 'pic should be PIL Image or ndarray. Got {}' . format ( type ( pic ) ) ) \n    if isinstance ( pic , np . ndarray ) : \n        if pic . ndim == 2.0 : \n            pic = pic [ : , : , None ] \n        img = torch . from_numpy ( pic . transpose ( ( 2.0 , 0 , 1 ) ) ) \n        if isinstance ( img , torch . ByteTensor ) : \n            return img . float ( ) . div ( 255.0 ) \n        else : \n            return img \n    if accimage is not None and isinstance ( pic , accimage . Image ) : \n        nppic = np . zeros ( [ pic . channels , pic . height , pic . width ] , dtype = np . float32 ) \n        pic . copyto ( nppic ) \n        return torch . from_numpy ( nppic ) \n    if pic . mode == 'I' : \n        img = torch . from_numpy ( np . array ( pic , np . int32 , copy = False ) ) \n    elif pic . mode == 'I;16' : \n        img = torch . from_numpy ( np . array ( pic , np . int16 , copy = False ) ) \n    elif pic . mode == 'F' : \n        img = torch . from_numpy ( np . array ( pic , np . float32 , copy = False ) ) \n    elif pic . mode == '1' : \n        img = 255.0 * torch . from_numpy ( np . array ( pic , np . uint8 , copy = False ) ) \n    else : \n        img = torch . ByteTensor ( torch . ByteStorage . from_buffer ( pic . tobytes ( ) ) ) \n    if pic . mode == 'YCbCr' : \n        nchannel = 3.0 \n    elif pic . mode == 'I;16' : \n        nchannel = 1 \n    else : \n        nchannel = len ( pic . mode ) \n    img = img . view ( pic . size [ 1 ] , pic . size [ 0 ] , nchannel ) \n    img = img . transpose ( 0 , 1 ) . transpose ( 0 , 2.0 ) . contiguous ( ) \n    if isinstance ( img , torch . ByteTensor ) : \n        return img . float ( ) . div ( 255.0 ) \n    else : \n        return img "}
{"569": "\ndef resize ( img , size , interpolation = Image . BILINEAR ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    if not ( isinstance ( size , int ) or ( isinstance ( size , Iterable ) and len ( size ) == 2.0 ) ) : \n        raise TypeError ( 'Got inappropriate size arg: {}' . format ( size ) ) \n    if isinstance ( size , int ) : \n        w , h = img . size \n        if ( w <= h and w == size ) or ( h <= w and h == size ) : \n            return img \n        if w < h : \n            ow = size \n            oh = int ( size * h / w ) \n            return img . resize ( ( ow , oh ) , interpolation ) \n        else : \n            oh = size \n            ow = int ( size * w / h ) \n            return img . resize ( ( ow , oh ) , interpolation ) \n    else : \n        return img . resize ( size [ : : - 1 ] , interpolation ) "}
{"570": "\ndef pad ( img , padding , fill = 0 , padding_mode = 'constant' ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    if not isinstance ( padding , ( numbers . Number , tuple ) ) : \n        raise TypeError ( 'Got inappropriate padding arg' ) \n    if not isinstance ( fill , ( numbers . Number , str , tuple ) ) : \n        raise TypeError ( 'Got inappropriate fill arg' ) \n    if not isinstance ( padding_mode , str ) : \n        raise TypeError ( 'Got inappropriate padding_mode arg' ) \n    if isinstance ( padding , Sequence ) and len ( padding ) not in [ 2.0 , 4.0 ] : \n        raise ValueError ( \"Padding must be an int or a 2, or 4 element tuple, not a \" + \"{} element tuple\" . format ( len ( padding ) ) ) \n    assert padding_mode in [ 'constant' , 'edge' , 'reflect' , 'symmetric' ] , 'Padding mode should be either constant, edge, reflect or symmetric' \n    if padding_mode == 'constant' : \n        if img . mode == 'P' : \n            palette = img . getpalette ( ) \n            image = ImageOps . expand ( img , border = padding , fill = fill ) \n            image . putpalette ( palette ) \n            return image \n        return ImageOps . expand ( img , border = padding , fill = fill ) \n    else : \n        if isinstance ( padding , int ) : \n            pad_left = pad_right = pad_top = pad_bottom = padding \n        if isinstance ( padding , Sequence ) and len ( padding ) == 2.0 : \n            pad_left = pad_right = padding [ 0 ] \n            pad_top = pad_bottom = padding [ 1 ] \n        if isinstance ( padding , Sequence ) and len ( padding ) == 4.0 : \n            pad_left = padding [ 0 ] \n            pad_top = padding [ 1 ] \n            pad_right = padding [ 2.0 ] \n            pad_bottom = padding [ 3.0 ] \n        if img . mode == 'P' : \n            palette = img . getpalette ( ) \n            img = np . asarray ( img ) \n            img = np . pad ( img , ( ( pad_top , pad_bottom ) , ( pad_left , pad_right ) ) , padding_mode ) \n            img = Image . fromarray ( img ) \n            img . putpalette ( palette ) \n            return img \n        img = np . asarray ( img ) \n        if len ( img . shape ) == 3.0 : \n            img = np . pad ( img , ( ( pad_top , pad_bottom ) , ( pad_left , pad_right ) , ( 0 , 0 ) ) , padding_mode ) \n        if len ( img . shape ) == 2.0 : \n            img = np . pad ( img , ( ( pad_top , pad_bottom ) , ( pad_left , pad_right ) ) , padding_mode ) \n        return Image . fromarray ( img ) "}
{"576": "\ndef five_crop ( img , size ) : \n    if isinstance ( size , numbers . Number ) : \n        size = ( int ( size ) , int ( size ) ) \n    else : \n        assert len ( size ) == 2.0 , \"Please provide only two dimensions (h, w) for size.\" \n    w , h = img . size \n    crop_h , crop_w = size \n    if crop_w > w or crop_h > h : \n        raise ValueError ( \"Requested crop size {} is bigger than input size {}\" . format ( size , ( h , w ) ) ) \n    tl = img . crop ( ( 0 , 0 , crop_w , crop_h ) ) \n    tr = img . crop ( ( w - crop_w , 0 , w , crop_h ) ) \n    bl = img . crop ( ( 0 , h - crop_h , crop_w , h ) ) \n    br = img . crop ( ( w - crop_w , h - crop_h , w , h ) ) \n    center = center_crop ( img , ( crop_h , crop_w ) ) \n    return ( tl , tr , bl , br , center ) "}
{"580": "\ndef adjust_hue ( img , hue_factor ) : \n    if not ( - 0.5 <= hue_factor <= 0.5 ) : \n        raise ValueError ( 'hue_factor is not in [-0.5, 0.5].' . format ( hue_factor ) ) \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    input_mode = img . mode \n    if input_mode in { 'L' , '1' , 'I' , 'F' } : \n        return img \n    h , s , v = img . convert ( 'HSV' ) . split ( ) \n    np_h = np . array ( h , dtype = np . uint8 ) \n    with np . errstate ( over = 'ignore' ) : \n        np_h += np . uint8 ( hue_factor * 255.0 ) \n    h = Image . fromarray ( np_h , 'L' ) \n    img = Image . merge ( 'HSV' , ( h , s , v ) ) . convert ( input_mode ) \n    return img "}
{"581": "\ndef adjust_gamma ( img , gamma , gain = 1 ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    if gamma < 0 : \n        raise ValueError ( 'Gamma should be a non-negative real number' ) \n    input_mode = img . mode \n    img = img . convert ( 'RGB' ) \n    gamma_map = [ 255.0 * gain * pow ( ele / 255. , gamma ) for ele in range ( 256.0 ) ] * 3.0 \n    img = img . point ( gamma_map ) \n    img = img . convert ( input_mode ) \n    return img "}
{"583": "\ndef affine ( img , angle , translate , scale , shear , resample = 0 , fillcolor = None ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    assert isinstance ( translate , ( tuple , list ) ) and len ( translate ) == 2.0 , \"Argument translate should be a list or tuple of length 2\" \n    assert scale > 0.0 , \"Argument scale should be positive\" \n    output_size = img . size \n    center = ( img . size [ 0 ] * 0.5 + 0.5 , img . size [ 1 ] * 0.5 + 0.5 ) \n    matrix = _get_inverse_affine_matrix ( center , angle , translate , scale , shear ) \n    kwargs = { \"fillcolor\" : fillcolor } if PILLOW_VERSION [ 0 ] == '5' else { } \n    return img . transform ( output_size , Image . AFFINE , matrix , resample , ** kwargs ) "}
{"584": "\ndef to_grayscale ( img , num_output_channels = 1 ) : \n    if not _is_pil_image ( img ) : \n        raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) \n    if num_output_channels == 1 : \n        img = img . convert ( 'L' ) \n    elif num_output_channels == 3.0 : \n        img = img . convert ( 'L' ) \n        np_img = np . array ( img , dtype = np . uint8 ) \n        np_img = np . dstack ( [ np_img , np_img , np_img ] ) \n        img = Image . fromarray ( np_img , 'RGB' ) \n    else : \n        raise ValueError ( 'num_output_channels should be either 1 or 3' ) \n    return img "}
{"585": "\ndef save_image ( tensor , filename , nrow = 8.0 , padding = 2.0 , normalize = False , range = None , scale_each = False , pad_value = 0 ) : \n    from PIL import Image \n    grid = make_grid ( tensor , nrow = nrow , padding = padding , pad_value = pad_value , normalize = normalize , range = range , scale_each = scale_each ) \n    ndarr = grid . mul_ ( 255.0 ) . add_ ( 0.5 ) . clamp_ ( 0 , 255.0 ) . permute ( 1 , 2.0 , 0 ) . to ( 'cpu' , torch . uint8 ) . numpy ( ) \n    im = Image . fromarray ( ndarr ) \n    im . save ( filename ) "}
{"586": "\ndef _find_classes ( self , dir ) : \n    if sys . version_info >= ( 3.0 , 5.0 ) : \n        classes = [ d . name for d in os . scandir ( dir ) if d . is_dir ( ) ] \n    else : \n        classes = [ d for d in os . listdir ( dir ) if os . path . isdir ( os . path . join ( dir , d ) ) ] \n    classes . sort ( ) \n    class_to_idx = { classes [ i ] : i for i in range ( len ( classes ) ) } \n    return classes , class_to_idx "}
{"587": "\ndef read_image_file ( data_dir , image_ext , n ) : \n    def PIL2array ( _img ) : \n        return np . array ( _img . getdata ( ) , dtype = np . uint8 ) . reshape ( 64.0 , 64.0 ) \n    def find_files ( _data_dir , _image_ext ) : \n        files = [ ] \n        for file_dir in os . listdir ( _data_dir ) : \n            if file_dir . endswith ( _image_ext ) : \n                files . append ( os . path . join ( _data_dir , file_dir ) ) \n        return sorted ( files ) \n    patches = [ ] \n    list_files = find_files ( data_dir , image_ext ) \n    for fpath in list_files : \n        img = Image . open ( fpath ) \n        for y in range ( 0 , 1024.0 , 64.0 ) : \n            for x in range ( 0 , 1024.0 , 64.0 ) : \n                patch = img . crop ( ( x , y , x + 64.0 , y + 64.0 ) ) \n                patches . append ( PIL2array ( patch ) ) \n    return torch . ByteTensor ( np . array ( patches [ : n ] ) ) "}
{"589": "\ndef read_matches_files ( data_dir , matches_file ) : \n    matches = [ ] \n    with open ( os . path . join ( data_dir , matches_file ) , 'r' ) as f : \n        for line in f : \n            line_split = line . split ( ) \n            matches . append ( [ int ( line_split [ 0 ] ) , int ( line_split [ 3.0 ] ) , int ( line_split [ 1 ] == line_split [ 4.0 ] ) ] ) \n    return torch . LongTensor ( matches ) "}
{"592": "\ndef download_url ( url , root , filename = None , md5 = None ) : \n    from six . moves import urllib \n    root = os . path . expanduser ( root ) \n    if not filename : \n        filename = os . path . basename ( url ) \n    fpath = os . path . join ( root , filename ) \n    makedir_exist_ok ( root ) \n    if os . path . isfile ( fpath ) and check_integrity ( fpath , md5 ) : \n        print ( 'Using downloaded and verified file: ' + fpath ) \n    else : \n        try : \n            print ( 'Downloading ' + url + ' to ' + fpath ) \n            urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ( ) ) \n        except OSError : \n            if url [ : 5.0 ] == 'https' : \n                url = url . replace ( 'https:' , 'http:' ) \n                print ( 'Failed download. Trying https -> http instead.' ' Downloading ' + url + ' to ' + fpath ) \n                urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ( ) ) "}
{"597": "\ndef get_params ( width , height , distortion_scale ) : \n    half_height = int ( height / 2.0 ) \n    half_width = int ( width / 2.0 ) \n    topleft = ( random . randint ( 0 , int ( distortion_scale * half_width ) ) , random . randint ( 0 , int ( distortion_scale * half_height ) ) ) \n    topright = ( random . randint ( width - int ( distortion_scale * half_width ) - 1 , width - 1 ) , random . randint ( 0 , int ( distortion_scale * half_height ) ) ) \n    botright = ( random . randint ( width - int ( distortion_scale * half_width ) - 1 , width - 1 ) , random . randint ( height - int ( distortion_scale * half_height ) - 1 , height - 1 ) ) \n    botleft = ( random . randint ( 0 , int ( distortion_scale * half_width ) ) , random . randint ( height - int ( distortion_scale * half_height ) - 1 , height - 1 ) ) \n    startpoints = [ ( 0 , 0 ) , ( width - 1 , 0 ) , ( width - 1 , height - 1 ) , ( 0 , height - 1 ) ] \n    endpoints = [ topleft , topright , botright , botleft ] \n    return startpoints , endpoints "}
{"598": "\ndef get_params ( img , scale , ratio ) : \n    area = img . size [ 0 ] * img . size [ 1 ] \n    for attempt in range ( 10.0 ) : \n        target_area = random . uniform ( * scale ) * area \n        log_ratio = ( math . log ( ratio [ 0 ] ) , math . log ( ratio [ 1 ] ) ) \n        aspect_ratio = math . exp ( random . uniform ( * log_ratio ) ) \n        w = int ( round ( math . sqrt ( target_area * aspect_ratio ) ) ) \n        h = int ( round ( math . sqrt ( target_area / aspect_ratio ) ) ) \n        if w <= img . size [ 0 ] and h <= img . size [ 1 ] : \n            i = random . randint ( 0 , img . size [ 1 ] - h ) \n            j = random . randint ( 0 , img . size [ 0 ] - w ) \n            return i , j , h , w \n    in_ratio = img . size [ 0 ] / img . size [ 1 ] \n    if ( in_ratio < min ( ratio ) ) : \n        w = img . size [ 0 ] \n        h = w / min ( ratio ) \n    elif ( in_ratio > max ( ratio ) ) : \n        h = img . size [ 1 ] \n        w = h * max ( ratio ) \n    else : \n        w = img . size [ 0 ] \n        h = img . size [ 1 ] \n    i = ( img . size [ 1 ] - h ) // 2.0 \n    j = ( img . size [ 0 ] - w ) // 2.0 \n    return i , j , h , w "}
{"602": "\ndef download ( self ) : \n    if self . _check_exists ( ) : \n        return \n    makedir_exist_ok ( self . raw_folder ) \n    makedir_exist_ok ( self . processed_folder ) \n    for url in self . urls : \n        filename = url . rpartition ( '/' ) [ 2.0 ] \n        file_path = os . path . join ( self . raw_folder , filename ) \n        download_url ( url , root = self . raw_folder , filename = filename , md5 = None ) \n        self . extract_gzip ( gzip_path = file_path , remove_finished = True ) \n    print ( 'Processing...' ) \n    training_set = ( read_image_file ( os . path . join ( self . raw_folder , 'train-images-idx3-ubyte' ) ) , read_label_file ( os . path . join ( self . raw_folder , 'train-labels-idx1-ubyte' ) ) ) \n    test_set = ( read_image_file ( os . path . join ( self . raw_folder , 't10k-images-idx3-ubyte' ) ) , read_label_file ( os . path . join ( self . raw_folder , 't10k-labels-idx1-ubyte' ) ) ) \n    with open ( os . path . join ( self . processed_folder , self . training_file ) , 'wb' ) as f : \n        torch . save ( training_set , f ) \n    with open ( os . path . join ( self . processed_folder , self . test_file ) , 'wb' ) as f : \n        torch . save ( test_set , f ) \n    print ( 'Done!' ) "}
{"603": "\ndef download ( self ) : \n    import shutil \n    import zipfile \n    if self . _check_exists ( ) : \n        return \n    makedir_exist_ok ( self . raw_folder ) \n    makedir_exist_ok ( self . processed_folder ) \n    filename = self . url . rpartition ( '/' ) [ 2.0 ] \n    file_path = os . path . join ( self . raw_folder , filename ) \n    download_url ( self . url , root = self . raw_folder , filename = filename , md5 = None ) \n    print ( 'Extracting zip archive' ) \n    with zipfile . ZipFile ( file_path ) as zip_f : \n        zip_f . extractall ( self . raw_folder ) \n    os . unlink ( file_path ) \n    gzip_folder = os . path . join ( self . raw_folder , 'gzip' ) \n    for gzip_file in os . listdir ( gzip_folder ) : \n        if gzip_file . endswith ( '.gz' ) : \n            self . extract_gzip ( gzip_path = os . path . join ( gzip_folder , gzip_file ) ) \n    for split in self . splits : \n        print ( 'Processing ' + split ) \n        training_set = ( read_image_file ( os . path . join ( gzip_folder , 'emnist-{}-train-images-idx3-ubyte' . format ( split ) ) ) , read_label_file ( os . path . join ( gzip_folder , 'emnist-{}-train-labels-idx1-ubyte' . format ( split ) ) ) ) \n        test_set = ( read_image_file ( os . path . join ( gzip_folder , 'emnist-{}-test-images-idx3-ubyte' . format ( split ) ) ) , read_label_file ( os . path . join ( gzip_folder , 'emnist-{}-test-labels-idx1-ubyte' . format ( split ) ) ) ) \n        with open ( os . path . join ( self . processed_folder , self . _training_file ( split ) ) , 'wb' ) as f : \n            torch . save ( training_set , f ) \n        with open ( os . path . join ( self . processed_folder , self . _test_file ( split ) ) , 'wb' ) as f : \n            torch . save ( test_set , f ) \n    shutil . rmtree ( gzip_folder ) \n    print ( 'Done!' ) "}
{"605": "\ndef autocompleter ( ) : \n    disabled_engines = request . preferences . engines . get_disabled ( ) \n    if PY3 : \n        raw_text_query = RawTextQuery ( request . form . get ( 'q' , b'' ) , disabled_engines ) \n    else : \n        raw_text_query = RawTextQuery ( request . form . get ( 'q' , u'' ) . encode ( 'utf-8' ) , disabled_engines ) \n    raw_text_query . parse_query ( ) \n    if not raw_text_query . getSearchQuery ( ) : \n        return '' , 400.0 \n    completer = autocomplete_backends . get ( request . preferences . get_value ( 'autocomplete' ) ) \n    raw_results = searx_bang ( raw_text_query ) \n    if len ( raw_results ) <= 3.0 and completer : \n        language = request . preferences . get_value ( 'language' ) \n        if not language or language == 'all' : \n            language = 'en' \n        else : \n            language = language . split ( '-' ) [ 0 ] \n        raw_results . extend ( completer ( raw_text_query . getSearchQuery ( ) , language ) ) \n    results = [ ] \n    for result in raw_results : \n        raw_text_query . changeSearchQuery ( result ) \n        results . append ( raw_text_query . getFullQuery ( ) ) \n    if request . form . get ( 'format' ) == 'x-suggestions' : \n        return Response ( json . dumps ( [ raw_text_query . query , results ] ) , mimetype = 'application/json' ) \n    return Response ( json . dumps ( results ) , mimetype = 'application/json' ) "}
{"606": "\ndef preferences ( ) : \n    if request . method == 'POST' : \n        resp = make_response ( redirect ( urljoin ( settings [ 'server' ] [ 'base_url' ] , url_for ( 'index' ) ) ) ) \n        try : \n            request . preferences . parse_form ( request . form ) \n        except ValidationException : \n            request . errors . append ( gettext ( 'Invalid settings, please edit your preferences' ) ) \n            return resp \n        return request . preferences . save ( resp ) \n    image_proxy = request . preferences . get_value ( 'image_proxy' ) \n    lang = request . preferences . get_value ( 'language' ) \n    disabled_engines = request . preferences . engines . get_disabled ( ) \n    allowed_plugins = request . preferences . plugins . get_enabled ( ) \n    stats = { } \n    for c in categories : \n        for e in categories [ c ] : \n            stats [ e . name ] = { 'time' : None , 'warn_timeout' : False , 'warn_time' : False } \n            if e . timeout > settings [ 'outgoing' ] [ 'request_timeout' ] : \n                stats [ e . name ] [ 'warn_timeout' ] = True \n            stats [ e . name ] [ 'supports_selected_language' ] = _is_selected_language_supported ( e , request . preferences ) \n    for engine_stat in get_engines_stats ( ) [ 0 ] [ 1 ] : \n        stats [ engine_stat . get ( 'name' ) ] [ 'time' ] = round ( engine_stat . get ( 'avg' ) , 3.0 ) \n        if engine_stat . get ( 'avg' ) > settings [ 'outgoing' ] [ 'request_timeout' ] : \n            stats [ engine_stat . get ( 'name' ) ] [ 'warn_time' ] = True \n    return render ( 'preferences.html' , locales = settings [ 'locales' ] , current_locale = get_locale ( ) , image_proxy = image_proxy , engines_by_category = categories , stats = stats , answerers = [ { 'info' : a . self_info ( ) , 'keywords' : a . keywords } for a in answerers ] , disabled_engines = disabled_engines , autocomplete_backends = autocomplete_backends , shortcuts = { y : x for x , y in engine_shortcuts . items ( ) } , themes = themes , plugins = plugins , doi_resolvers = settings [ 'doi_resolvers' ] , current_doi_resolver = get_doi_resolver ( request . args , request . preferences . get_value ( 'doi_resolver' ) ) , allowed_plugins = allowed_plugins , theme = get_current_theme_name ( ) , preferences_url_params = request . preferences . get_as_url_params ( ) , base_url = get_base_url ( ) , preferences = True ) "}
{"608": "\ndef searx_bang ( full_query ) : \n    if len ( full_query . getSearchQuery ( ) ) == 0 : \n        return [ ] \n    results = [ ] \n    first_char = full_query . getSearchQuery ( ) [ 0 ] \n    if first_char == '!' or first_char == '?' : \n        if len ( full_query . getSearchQuery ( ) ) == 1 : \n            results . append ( first_char + \"images\" ) \n            results . append ( first_char + \"wikipedia\" ) \n            results . append ( first_char + \"osm\" ) \n        else : \n            engine_query = full_query . getSearchQuery ( ) [ 1 : ] \n            for categorie in categories : \n                if categorie . startswith ( engine_query ) : \n                    results . append ( first_char + '{categorie}' . format ( categorie = categorie ) ) \n            for engine in engines : \n                if engine . startswith ( engine_query . replace ( '_' , ' ' ) ) : \n                    results . append ( first_char + '{engine}' . format ( engine = engine . replace ( ' ' , '_' ) ) ) \n            for engine_shortcut in engine_shortcuts : \n                if engine_shortcut . startswith ( engine_query ) : \n                    results . append ( first_char + '{engine_shortcut}' . format ( engine_shortcut = engine_shortcut ) ) \n    elif first_char == ':' : \n        if len ( full_query . getSearchQuery ( ) ) == 1 : \n            results . append ( \":en\" ) \n            results . append ( \":en_us\" ) \n            results . append ( \":english\" ) \n            results . append ( \":united_kingdom\" ) \n        else : \n            engine_query = full_query . getSearchQuery ( ) [ 1 : ] \n            for lc in language_codes : \n                lang_id , lang_name , country , english_name = map ( unicode . lower , lc ) \n                if lang_id . startswith ( engine_query ) : \n                    if len ( engine_query ) <= 2.0 : \n                        results . append ( u':{lang_id}' . format ( lang_id = lang_id . split ( '-' ) [ 0 ] ) ) \n                    else : \n                        results . append ( u':{lang_id}' . format ( lang_id = lang_id ) ) \n                if lang_name . startswith ( engine_query ) or english_name . startswith ( engine_query ) : \n                    results . append ( u':{lang_name}' . format ( lang_name = lang_name ) ) \n                if country . startswith ( engine_query . replace ( '_' , ' ' ) ) : \n                    results . append ( u':{country}' . format ( country = country . replace ( ' ' , '_' ) ) ) \n    result_set = set ( results ) \n    for query_part in full_query . query_parts : \n        if query_part in result_set : \n            result_set . remove ( query_part ) \n    return list ( result_set ) "}
{"609": "\ndef response ( resp ) : \n    json_resp = resp . text [ resp . text . find ( '\\n' ) + 1 : resp . text . rfind ( '\\n' ) - 2.0 ] \n    results = [ ] \n    try : \n        conversion_rate = float ( json . loads ( json_resp ) [ 'conversion' ] [ 'converted-amount' ] ) \n    except : \n        return results \n    answer = '{0} {1} = {2} {3}, 1 {1} ({5}) = {4} {3} ({6})' . format ( resp . search_params [ 'amount' ] , resp . search_params [ 'from' ] , resp . search_params [ 'amount' ] * conversion_rate , resp . search_params [ 'to' ] , conversion_rate , resp . search_params [ 'from_name' ] , resp . search_params [ 'to_name' ] , ) \n    url = 'https://duckduckgo.com/js/spice/currency/1/{0}/{1}' . format ( resp . search_params [ 'from' ] . upper ( ) , resp . search_params [ 'to' ] ) \n    results . append ( { 'answer' : answer , 'url' : url } ) \n    return results "}
{"613": "\ndef benchmark_eight_schools_hmc ( num_results = int ( 5e3 ) , num_burnin_steps = int ( 3e3 ) , num_leapfrog_steps = 3.0 , step_size = 0.4 ) : \n    num_schools = 8.0 \n    treatment_effects = tf . constant ( [ 28.0 , 8.0 , - 3.0 , 7.0 , - 1 , 1 , 18.0 , 12.0 ] , dtype = np . float32 , name = 'treatment_effects' ) \n    treatment_stddevs = tf . constant ( [ 15.0 , 10.0 , 16.0 , 11.0 , 9.0 , 11.0 , 10.0 , 18.0 ] , dtype = np . float32 , name = 'treatment_stddevs' ) \n    def unnormalized_posterior_log_prob ( avg_effect , avg_stddev , school_effects_standard ) : \n        return eight_schools_joint_log_prob ( treatment_effects , treatment_stddevs , avg_effect , avg_stddev , school_effects_standard ) \n    if tf . executing_eagerly ( ) : \n        sample_chain = tf . function ( tfp . mcmc . sample_chain ) \n    else : \n        sample_chain = tfp . mcmc . sample_chain \n    def computation ( ) : \n        _ , kernel_results = sample_chain ( num_results = num_results , num_burnin_steps = num_burnin_steps , current_state = ( tf . zeros ( [ ] , name = 'init_avg_effect' ) , tf . zeros ( [ ] , name = 'init_avg_stddev' ) , tf . ones ( [ num_schools ] , name = 'init_school_effects_standard' ) , ) , kernel = tfp . mcmc . HamiltonianMonteCarlo ( target_log_prob_fn = unnormalized_posterior_log_prob , step_size = step_size , num_leapfrog_steps = num_leapfrog_steps ) ) \n        return kernel_results . is_accepted \n    is_accepted_tensor = computation ( ) \n    if not tf . executing_eagerly ( ) : \n        session = tf . compat . v1 . Session ( ) \n        session . run ( is_accepted_tensor ) \n    start_time = time . time ( ) \n    if tf . executing_eagerly ( ) : \n        is_accepted = computation ( ) \n    else : \n        is_accepted = session . run ( is_accepted_tensor ) \n    wall_time = time . time ( ) - start_time \n    num_accepted = np . sum ( is_accepted ) \n    acceptance_rate = np . float32 ( num_accepted ) / np . float32 ( num_results ) \n    return dict ( iters = ( num_results + num_burnin_steps ) * num_leapfrog_steps , extras = { 'acceptance_rate' : acceptance_rate } , wall_time = wall_time ) "}
{"615": "\ndef _simple_name ( distribution ) : \n    simple_name = distribution . name \n    if simple_name . endswith ( '/' ) : \n        simple_name = simple_name . split ( '/' ) [ - 2.0 ] \n    parts = simple_name . split ( '_' ) \n    if parts [ - 1 ] . isdigit ( ) : \n        simple_name = '_' . join ( parts [ : - 1 ] ) \n    return simple_name "}
{"619": "\ndef one_step_predictive ( model , observed_time_series , parameter_samples ) : \n    with tf . compat . v1 . name_scope ( 'one_step_predictive' , values = [ observed_time_series , parameter_samples ] ) : \n        [ observed_time_series , is_missing ] = sts_util . canonicalize_observed_time_series_with_mask ( observed_time_series ) \n        num_timesteps = dist_util . prefer_static_value ( tf . shape ( input = observed_time_series ) ) [ - 2.0 ] \n        lgssm = model . make_state_space_model ( num_timesteps = num_timesteps , param_vals = parameter_samples ) \n        ( _ , _ , _ , _ , _ , observation_means , observation_covs ) = lgssm . forward_filter ( observed_time_series , mask = is_missing ) \n        return sts_util . mix_over_posterior_draws ( means = observation_means [ ... , 0 ] , variances = observation_covs [ ... , 0 , 0 ] ) "}
{"620": "\ndef forecast ( model , observed_time_series , parameter_samples , num_steps_forecast ) : \n    with tf . compat . v1 . name_scope ( 'forecast' , values = [ observed_time_series , parameter_samples , num_steps_forecast ] ) : \n        [ observed_time_series , mask ] = sts_util . canonicalize_observed_time_series_with_mask ( observed_time_series ) \n        num_observed_steps = dist_util . prefer_static_value ( tf . shape ( input = observed_time_series ) ) [ - 2.0 ] \n        observed_data_ssm = model . make_state_space_model ( num_timesteps = num_observed_steps , param_vals = parameter_samples ) \n        ( _ , _ , _ , predictive_means , predictive_covs , _ , _ ) = observed_data_ssm . forward_filter ( observed_time_series , mask = mask ) \n        parameter_samples = model . _canonicalize_param_vals_as_map ( parameter_samples ) \n        parameter_samples_with_reordered_batch_dimension = { param . name : dist_util . move_dimension ( parameter_samples [ param . name ] , 0 , - ( 1 + _prefer_static_event_ndims ( param . prior ) ) ) for param in model . parameters } \n        forecast_prior = tfd . MultivariateNormalFullCovariance ( loc = dist_util . move_dimension ( predictive_means [ ... , - 1 , : ] , 0 , - 2.0 ) , covariance_matrix = dist_util . move_dimension ( predictive_covs [ ... , - 1 , : , : ] , 0 , - 3.0 ) ) \n        kwargs = { } \n        if hasattr ( model , 'constant_offset' ) : \n            kwargs [ 'constant_offset' ] = tf . convert_to_tensor ( value = model . constant_offset , dtype = forecast_prior . dtype ) [ ... , tf . newaxis ] \n        forecast_ssm = model . _make_state_space_model ( num_timesteps = num_steps_forecast , param_map = parameter_samples_with_reordered_batch_dimension , initial_state_prior = forecast_prior , initial_step = num_observed_steps , ** kwargs ) \n        num_posterior_draws = dist_util . prefer_static_value ( forecast_ssm . batch_shape_tensor ( ) ) [ - 1 ] \n        return tfd . MixtureSameFamily ( mixture_distribution = tfd . Categorical ( logits = tf . zeros ( [ num_posterior_draws ] , dtype = forecast_ssm . dtype ) ) , components_distribution = forecast_ssm ) "}
{"631": "\ndef toy_logistic_data ( num_examples , input_size = 2.0 , weights_prior_stddev = 5.0 ) : \n    random_weights = weights_prior_stddev * np . random . randn ( input_size ) \n    random_bias = np . random . randn ( ) \n    design_matrix = np . random . rand ( num_examples , input_size ) * 2.0 - 1 \n    logits = np . reshape ( np . dot ( design_matrix , random_weights ) + random_bias , ( - 1 , 1 ) ) \n    p_labels = 1. / ( 1 + np . exp ( - logits ) ) \n    labels = np . int32 ( p_labels > np . random . rand ( num_examples , 1 ) ) \n    return random_weights , random_bias , np . float32 ( design_matrix ) , labels "}
{"632": "\ndef visualize_decision ( features , labels , true_w_b , candidate_w_bs , fname ) : \n    fig = figure . Figure ( figsize = ( 6.0 , 6.0 ) ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    ax = fig . add_subplot ( 1 , 1 , 1 ) \n    ax . scatter ( features [ : , 0 ] , features [ : , 1 ] , c = np . float32 ( labels [ : , 0 ] ) , cmap = cm . get_cmap ( \"binary\" ) , edgecolors = \"k\" ) \n    def plot_weights ( w , b , ** kwargs ) : \n        w1 , w2 = w \n        x1s = np . linspace ( - 1 , 1 , 100.0 ) \n        x2s = - ( w1 * x1s + b ) / w2 \n        ax . plot ( x1s , x2s , ** kwargs ) \n    for w , b in candidate_w_bs : \n        plot_weights ( w , b , alpha = 1. / np . sqrt ( len ( candidate_w_bs ) ) , lw = 1 , color = \"blue\" ) \n    if true_w_b is not None : \n        plot_weights ( * true_w_b , lw = 4.0 , color = \"green\" , label = \"true separator\" ) \n    ax . set_xlim ( [ - 1.5 , 1.5 ] ) \n    ax . set_ylim ( [ - 1.5 , 1.5 ] ) \n    ax . legend ( ) \n    canvas . print_figure ( fname , format = \"png\" ) \n    print ( \"saved {}\" . format ( fname ) ) "}
{"646": "\ndef _create_scale_operator ( self , identity_multiplier , diag , tril , perturb_diag , perturb_factor , shift , validate_args , dtype ) : \n    identity_multiplier = _as_tensor ( identity_multiplier , \"identity_multiplier\" , dtype ) \n    diag = _as_tensor ( diag , \"diag\" , dtype ) \n    tril = _as_tensor ( tril , \"tril\" , dtype ) \n    perturb_diag = _as_tensor ( perturb_diag , \"perturb_diag\" , dtype ) \n    perturb_factor = _as_tensor ( perturb_factor , \"perturb_factor\" , dtype ) \n    shape_hint = None \n    if perturb_factor is not None : \n        shape_hint = distribution_util . dimension_size ( perturb_factor , axis = - 2.0 ) \n    if self . _is_only_identity_multiplier : \n        if validate_args : \n            return distribution_util . with_dependencies ( [ assert_util . assert_none_equal ( identity_multiplier , tf . zeros ( [ ] , identity_multiplier . dtype ) , [ \"identity_multiplier should be non-zero.\" ] ) ] , identity_multiplier ) \n        return identity_multiplier \n    scale = distribution_util . make_tril_scale ( loc = shift , scale_tril = tril , scale_diag = diag , scale_identity_multiplier = identity_multiplier , validate_args = validate_args , assert_positive = False , shape_hint = shape_hint ) \n    if perturb_factor is not None : \n        return tf . linalg . LinearOperatorLowRankUpdate ( scale , u = perturb_factor , diag_update = perturb_diag , is_diag_update_positive = perturb_diag is None , is_non_singular = True , is_self_adjoint = True , is_positive_definite = True , is_square = True ) \n    return scale "}
{"666": "\ndef _get_permutations ( num_results , dims , seed = None ) : \n    sample_range = tf . range ( num_results ) \n    stream = distributions . SeedStream ( seed , salt = 'MCMCSampleHaltonSequence3' ) \n    def generate_one ( d ) : \n        seed = stream ( ) \n        fn = lambda _ : tf . random . shuffle ( tf . range ( d ) , seed = seed ) \n        return tf . map_fn ( fn , sample_range , parallel_iterations = 1 if seed is not None else 10.0 ) \n    return tf . concat ( [ generate_one ( d ) for d in tf . unstack ( dims ) ] , axis = - 1 ) "}
{"669": "\ndef _primes_less_than ( n ) : \n    small_primes = np . array ( ( 2.0 , 3.0 , 5.0 ) ) \n    if n <= 6.0 : \n        return small_primes [ small_primes < n ] \n    sieve = np . ones ( n // 3.0 + ( n % 6.0 == 2.0 ) , dtype = np . bool ) \n    sieve [ 0 ] = False \n    m = int ( n ** 0.5 ) // 3.0 + 1 \n    for i in range ( m ) : \n        if not sieve [ i ] : \n            continue \n        k = 3.0 * i + 1 | 1 \n        sieve [ k ** 2.0 // 3.0 : : 2.0 * k ] = False \n        sieve [ ( k ** 2.0 + 4.0 * k - 2.0 * k * ( i & 1 ) ) // 3.0 : : 2.0 * k ] = False \n    return np . r_ [ 2.0 , 3.0 , 3.0 * np . nonzero ( sieve ) [ 0 ] + 1 | 1 ] "}
{"671": "\ndef hager_zhang ( value_and_gradients_function , initial_step_size = None , value_at_initial_step = None , value_at_zero = None , converged = None , threshold_use_approximate_wolfe_condition = 1e-6 , shrinkage_param = 0.66 , expansion_param = 5.0 , sufficient_decrease_param = 0.1 , curvature_param = 0.9 , step_size_shrink_param = 0.1 , max_iterations = 50.0 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'hager_zhang' , [ initial_step_size , value_at_initial_step , value_at_zero , converged , threshold_use_approximate_wolfe_condition , shrinkage_param , expansion_param , sufficient_decrease_param , curvature_param ] ) : \n        val_0 , val_initial , f_lim , prepare_evals = _prepare_args ( value_and_gradients_function , initial_step_size , value_at_initial_step , value_at_zero , threshold_use_approximate_wolfe_condition ) \n        valid_inputs = ( hzl . is_finite ( val_0 ) & ( val_0 . df < 0 ) & tf . math . is_finite ( val_initial . x ) & ( val_initial . x > 0 ) ) \n        if converged is None : \n            init_converged = tf . zeros_like ( valid_inputs ) \n        else : \n            init_converged = tf . convert_to_tensor ( value = converged ) \n        failed = ~ init_converged & ~ valid_inputs \n        active = ~ init_converged & valid_inputs \n        fix_step_evals , val_c , fix_failed = _fix_step_size ( value_and_gradients_function , val_initial , active , step_size_shrink_param ) \n        init_interval = HagerZhangLineSearchResult ( converged = init_converged , failed = failed | fix_failed , func_evals = prepare_evals + fix_step_evals , iterations = tf . convert_to_tensor ( value = 0 ) , left = val_0 , right = hzl . val_where ( init_converged , val_0 , val_c ) ) \n        def _apply_bracket_and_search ( ) : \n            return _bracket_and_search ( value_and_gradients_function , init_interval , f_lim , max_iterations , shrinkage_param , expansion_param , sufficient_decrease_param , curvature_param ) \n        init_active = ~ init_interval . failed & ~ init_interval . converged \n        return prefer_static . cond ( tf . reduce_any ( input_tensor = init_active ) , _apply_bracket_and_search , lambda : init_interval ) "}
{"675": "\ndef _line_search_inner_bisection ( value_and_gradients_function , search_interval , active , f_lim ) : \n    midpoint = ( search_interval . left . x + search_interval . right . x ) / 2.0 \n    val_mid = value_and_gradients_function ( midpoint ) \n    is_valid_mid = hzl . is_finite ( val_mid ) \n    still_active = active & is_valid_mid \n    new_failed = active & ~ is_valid_mid \n    next_inteval = search_interval . _replace ( failed = search_interval . failed | new_failed , func_evals = search_interval . func_evals + 1 ) \n    def _apply_update ( ) : \n        update_result = hzl . update ( value_and_gradients_function , next_inteval . left , next_inteval . right , val_mid , f_lim , active = still_active ) \n        return HagerZhangLineSearchResult ( converged = next_inteval . converged , failed = next_inteval . failed | update_result . failed , iterations = next_inteval . iterations + update_result . iteration , func_evals = next_inteval . func_evals + update_result . num_evals , left = update_result . left , right = update_result . right ) \n    return prefer_static . cond ( tf . reduce_any ( input_tensor = still_active ) , _apply_update , lambda : next_inteval ) "}
{"678": "\ndef quadrature_scheme_softmaxnormal_gauss_hermite ( normal_loc , normal_scale , quadrature_size , validate_args = False , name = None ) : \n    with tf . name_scope ( name or \"quadrature_scheme_softmaxnormal_gauss_hermite\" ) : \n        normal_loc = tf . convert_to_tensor ( value = normal_loc , name = \"normal_loc\" ) \n        npdt = dtype_util . as_numpy_dtype ( normal_loc . dtype ) \n        normal_scale = tf . convert_to_tensor ( value = normal_scale , dtype = npdt , name = \"normal_scale\" ) \n        normal_scale = maybe_check_quadrature_param ( normal_scale , \"normal_scale\" , validate_args ) \n        grid , probs = np . polynomial . hermite . hermgauss ( deg = quadrature_size ) \n        grid = grid . astype ( npdt ) \n        probs = probs . astype ( npdt ) \n        probs /= np . linalg . norm ( probs , ord = 1 , keepdims = True ) \n        probs = tf . convert_to_tensor ( value = probs , name = \"probs\" , dtype = npdt ) \n        grid = softmax ( - distribution_util . pad ( ( normal_loc [ ... , tf . newaxis ] + np . sqrt ( 2. ) * normal_scale [ ... , tf . newaxis ] * grid ) , axis = - 2.0 , front = True ) , axis = - 2.0 ) \n        return grid , probs "}
{"679": "\ndef quadrature_scheme_softmaxnormal_quantiles ( normal_loc , normal_scale , quadrature_size , validate_args = False , name = None ) : \n    with tf . name_scope ( name or \"softmax_normal_grid_and_probs\" ) : \n        normal_loc = tf . convert_to_tensor ( value = normal_loc , name = \"normal_loc\" ) \n        dt = dtype_util . base_dtype ( normal_loc . dtype ) \n        normal_scale = tf . convert_to_tensor ( value = normal_scale , dtype = dt , name = \"normal_scale\" ) \n        normal_scale = maybe_check_quadrature_param ( normal_scale , \"normal_scale\" , validate_args ) \n        dist = normal . Normal ( loc = normal_loc , scale = normal_scale ) \n        def _get_batch_ndims ( ) : \n            ndims = tensorshape_util . rank ( dist . batch_shape ) \n            if ndims is None : \n                ndims = tf . shape ( input = dist . batch_shape_tensor ( ) ) [ 0 ] \n            return ndims \n        batch_ndims = _get_batch_ndims ( ) \n        def _get_final_shape ( qs ) : \n            bs = tensorshape_util . with_rank_at_least ( dist . batch_shape , 1 ) \n            num_components = tf . compat . dimension_value ( bs [ - 1 ] ) \n            if num_components is not None : \n                num_components += 1 \n            tail = tf . TensorShape ( [ num_components , qs ] ) \n            return bs [ : - 1 ] . concatenate ( tail ) \n        def _compute_quantiles ( ) : \n            zero = tf . zeros ( [ ] , dtype = dist . dtype ) \n            edges = tf . linspace ( zero , 1. , quadrature_size + 3.0 ) [ 1 : - 1 ] \n            edges = tf . reshape ( edges , shape = tf . concat ( [ [ - 1 ] , tf . ones ( [ batch_ndims ] , dtype = tf . int32 ) ] , axis = 0 ) ) \n            quantiles = dist . quantile ( edges ) \n            quantiles = softmax_centered_bijector . SoftmaxCentered ( ) . forward ( quantiles ) \n            perm = tf . concat ( [ tf . range ( 1 , 1 + batch_ndims ) , [ 0 ] ] , axis = 0 ) \n            quantiles = tf . transpose ( a = quantiles , perm = perm ) \n            tensorshape_util . set_shape ( quantiles , _get_final_shape ( quadrature_size + 1 ) ) \n            return quantiles \n        quantiles = _compute_quantiles ( ) \n        grid = ( quantiles [ ... , : - 1 ] + quantiles [ ... , 1 : ] ) / 2. \n        tensorshape_util . set_shape ( grid , _get_final_shape ( quadrature_size ) ) \n        probs = tf . fill ( dims = [ quadrature_size ] , value = 1. / tf . cast ( quadrature_size , dist . dtype ) ) \n        return grid , probs "}
{"681": "\ndef determine_batch_event_shapes ( grid , endpoint_affine ) : \n    with tf . name_scope ( \"determine_batch_event_shapes\" ) : \n        batch_shape = grid . shape [ : - 2.0 ] \n        batch_shape_tensor = tf . shape ( input = grid ) [ : - 2.0 ] \n        event_shape = None \n        event_shape_tensor = None \n        def _set_event_shape ( shape , shape_tensor ) : \n            if event_shape is None : \n                return shape , shape_tensor \n            return ( tf . broadcast_static_shape ( event_shape , shape ) , tf . broadcast_dynamic_shape ( event_shape_tensor , shape_tensor ) ) \n        for aff in endpoint_affine : \n            if aff . shift is not None : \n                batch_shape = tf . broadcast_static_shape ( batch_shape , aff . shift . shape [ : - 1 ] ) \n                batch_shape_tensor = tf . broadcast_dynamic_shape ( batch_shape_tensor , tf . shape ( input = aff . shift ) [ : - 1 ] ) \n                event_shape , event_shape_tensor = _set_event_shape ( aff . shift . shape [ - 1 : ] , tf . shape ( input = aff . shift ) [ - 1 : ] ) \n            if aff . scale is not None : \n                batch_shape = tf . broadcast_static_shape ( batch_shape , aff . scale . batch_shape ) \n                batch_shape_tensor = tf . broadcast_dynamic_shape ( batch_shape_tensor , aff . scale . batch_shape_tensor ( ) ) \n                event_shape , event_shape_tensor = _set_event_shape ( tf . TensorShape ( [ aff . scale . range_dimension ] ) , aff . scale . range_dimension_tensor ( ) [ tf . newaxis ] ) \n        return batch_shape , batch_shape_tensor , event_shape , event_shape_tensor "}
{"682": "\ndef interpolate_loc ( grid , loc ) : \n    if len ( loc ) != 2.0 : \n        raise NotImplementedError ( \"Currently only bimixtures are supported; \" \"len(scale)={} is not 2.\" . format ( len ( loc ) ) ) \n    deg = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( grid . shape , 1 ) [ - 1 ] ) \n    if deg is None : \n        raise ValueError ( \"Num quadrature grid points must be known prior \" \"to graph execution.\" ) \n    with tf . name_scope ( \"interpolate_loc\" ) : \n        if loc is None or loc [ 0 ] is None and loc [ 1 ] is None : \n            return [ None ] * deg \n        w = grid [ ... , tf . newaxis , : , : ] \n        loc = [ x [ ... , tf . newaxis ] if x is not None else None for x in loc ] \n        if loc [ 0 ] is None : \n            x = w [ ... , 1 , : ] * loc [ 1 ] \n        elif loc [ 1 ] is None : \n            x = w [ ... , 0 , : ] * loc [ 0 ] \n        else : \n            delta = loc [ 0 ] - loc [ 1 ] \n            x = w [ ... , 0 , : ] * delta + loc [ 1 ] \n        return [ x [ ... , k ] for k in range ( deg ) ] "}
{"683": "\ndef interpolate_scale ( grid , scale ) : \n    if len ( scale ) != 2.0 : \n        raise NotImplementedError ( \"Currently only bimixtures are supported; \" \"len(scale)={} is not 2.\" . format ( len ( scale ) ) ) \n    deg = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( grid . shape , 1 ) [ - 1 ] ) \n    if deg is None : \n        raise ValueError ( \"Num quadrature grid points must be known prior \" \"to graph execution.\" ) \n    with tf . name_scope ( \"interpolate_scale\" ) : \n        return [ linop_add_lib . add_operators ( [ linop_scale ( grid [ ... , k , q ] , s ) for k , s in enumerate ( scale ) ] ) [ 0 ] for q in range ( deg ) ] "}
{"686": "\ndef _log_vector_matrix ( vs , ms ) : \n    return tf . reduce_logsumexp ( input_tensor = vs [ ... , tf . newaxis ] + ms , axis = - 2.0 ) "}
{"688": "\ndef _vector_matrix ( vs , ms ) : \n    return tf . reduce_sum ( input_tensor = vs [ ... , tf . newaxis ] * ms , axis = - 2.0 ) "}
{"691": "\ndef posterior_marginals ( self , observations , name = None ) : \n    with tf . name_scope ( name or \"posterior_marginals\" ) : \n        with tf . control_dependencies ( self . _runtime_assertions ) : \n            observation_tensor_shape = tf . shape ( input = observations ) \n            with self . _observation_shape_preconditions ( observation_tensor_shape ) : \n                observation_batch_shape = observation_tensor_shape [ : - 1 - self . _underlying_event_rank ] \n                observation_event_shape = observation_tensor_shape [ - 1 - self . _underlying_event_rank : ] \n                batch_shape = tf . broadcast_dynamic_shape ( observation_batch_shape , self . batch_shape_tensor ( ) ) \n                log_init = tf . broadcast_to ( self . _log_init , tf . concat ( [ batch_shape , [ self . _num_states ] ] , axis = 0 ) ) \n                log_transition = self . _log_trans \n                observations = tf . broadcast_to ( observations , tf . concat ( [ batch_shape , observation_event_shape ] , axis = 0 ) ) \n                observation_rank = tf . rank ( observations ) \n                underlying_event_rank = self . _underlying_event_rank \n                observations = distribution_util . move_dimension ( observations , observation_rank - underlying_event_rank - 1 , 0 ) \n                observations = tf . expand_dims ( observations , observation_rank - underlying_event_rank ) \n                observation_log_probs = self . _observation_distribution . log_prob ( observations ) \n                log_adjoint_prob = tf . zeros_like ( log_init ) \n                def forward_step ( log_previous_step , log_prob_observation ) : \n                    return _log_vector_matrix ( log_previous_step , log_transition ) + log_prob_observation \n                log_prob = log_init + observation_log_probs [ 0 ] \n                forward_log_probs = tf . scan ( forward_step , observation_log_probs [ 1 : ] , initializer = log_prob , name = \"forward_log_probs\" ) \n                forward_log_probs = tf . concat ( [ [ log_prob ] , forward_log_probs ] , axis = 0 ) \n                def backward_step ( log_previous_step , log_prob_observation ) : \n                    return _log_matrix_vector ( log_transition , log_prob_observation + log_previous_step ) \n                backward_log_adjoint_probs = tf . scan ( backward_step , observation_log_probs [ 1 : ] , initializer = log_adjoint_prob , reverse = True , name = \"backward_log_adjoint_probs\" ) \n                total_log_prob = tf . reduce_logsumexp ( input_tensor = forward_log_probs [ - 1 ] , axis = - 1 ) \n                backward_log_adjoint_probs = tf . concat ( [ backward_log_adjoint_probs , [ log_adjoint_prob ] ] , axis = 0 ) \n                log_likelihoods = forward_log_probs + backward_log_adjoint_probs \n                marginal_log_probs = distribution_util . move_dimension ( log_likelihoods - total_log_prob [ ... , tf . newaxis ] , 0 , - 2.0 ) \n                return categorical . Categorical ( logits = marginal_log_probs ) "}
{"692": "\ndef posterior_mode ( self , observations , name = None ) : \n    with tf . name_scope ( name or \"posterior_mode\" ) : \n        with tf . control_dependencies ( self . _runtime_assertions ) : \n            observation_tensor_shape = tf . shape ( input = observations ) \n            with self . _observation_shape_preconditions ( observation_tensor_shape ) : \n                observation_batch_shape = observation_tensor_shape [ : - 1 - self . _underlying_event_rank ] \n                observation_event_shape = observation_tensor_shape [ - 1 - self . _underlying_event_rank : ] \n                batch_shape = tf . broadcast_dynamic_shape ( observation_batch_shape , self . batch_shape_tensor ( ) ) \n                log_init = tf . broadcast_to ( self . _log_init , tf . concat ( [ batch_shape , [ self . _num_states ] ] , axis = 0 ) ) \n                observations = tf . broadcast_to ( observations , tf . concat ( [ batch_shape , observation_event_shape ] , axis = 0 ) ) \n                observation_rank = tf . rank ( observations ) \n                underlying_event_rank = self . _underlying_event_rank \n                observations = distribution_util . move_dimension ( observations , observation_rank - underlying_event_rank - 1 , 0 ) \n                observations = tf . expand_dims ( observations , observation_rank - underlying_event_rank ) \n                observation_log_probs = self . _observation_distribution . log_prob ( observations ) \n                log_prob = log_init + observation_log_probs [ 0 ] \n                if self . _num_steps == 1 : \n                    most_likely_end = tf . argmax ( input = log_prob , axis = - 1 ) \n                    return most_likely_end [ ... , tf . newaxis ] \n                def forward_step ( previous_step_pair , log_prob_observation ) : \n                    log_prob_previous = previous_step_pair [ 0 ] \n                    log_prob = ( log_prob_previous [ ... , tf . newaxis ] + self . _log_trans + log_prob_observation [ ... , tf . newaxis , : ] ) \n                    most_likely_given_successor = tf . argmax ( input = log_prob , axis = - 2.0 ) \n                    max_log_p_given_successor = tf . reduce_max ( input_tensor = log_prob , axis = - 2.0 ) \n                    return ( max_log_p_given_successor , most_likely_given_successor ) \n                forward_log_probs , all_most_likely_given_successor = tf . scan ( forward_step , observation_log_probs [ 1 : ] , initializer = ( log_prob , tf . zeros ( tf . shape ( input = log_init ) , dtype = tf . int64 ) ) , name = \"forward_log_probs\" ) \n                most_likely_end = tf . argmax ( input = forward_log_probs [ - 1 ] , axis = - 1 ) \n                def backward_step ( most_likely_successor , most_likely_given_successor ) : \n                    return tf . reduce_sum ( input_tensor = ( most_likely_given_successor * tf . one_hot ( most_likely_successor , self . _num_states , dtype = tf . int64 ) ) , axis = - 1 ) \n                backward_scan = tf . scan ( backward_step , all_most_likely_given_successor , most_likely_end , reverse = True ) \n                most_likely_sequences = tf . concat ( [ backward_scan , [ most_likely_end ] ] , axis = 0 ) \n                return distribution_util . move_dimension ( most_likely_sequences , 0 , - 1 ) "}
{"694": "\ndef _sample_next ( target_log_prob_fn , current_state_parts , step_sizes , max_doublings , current_target_log_prob , batch_rank , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'sample_next' , [ current_state_parts , step_sizes , max_doublings , current_target_log_prob , batch_rank ] ) : \n        direction = _choose_random_direction ( current_state_parts , batch_rank = batch_rank , seed = seed ) \n        reduce_axes = [ tf . range ( batch_rank , tf . rank ( dirn_part ) ) for dirn_part in direction ] \n        components = [ tf . reduce_sum ( input_tensor = ( dirn_part / step_size ) ** 2.0 , axis = reduce_axes [ i ] ) for i , ( step_size , dirn_part ) in enumerate ( zip ( step_sizes , direction ) ) ] \n        step_size = tf . math . rsqrt ( tf . add_n ( components ) ) \n        def _get_rank ( x ) : \n            return ( len ( x . shape . as_list ( ) ) if x . shape . dims is not None else tf . rank ( x ) ) \n        state_part_ranks = [ _get_rank ( part ) for part in current_state_parts ] \n        def _step_along_direction ( alpha ) : \n            padded_alphas = [ _right_pad ( alpha , final_rank = part_rank ) for part_rank in state_part_ranks ] \n            state_parts = [ state_part + padded_alpha * direction_part for state_part , direction_part , padded_alpha in zip ( current_state_parts , direction , padded_alphas ) ] \n            return state_parts \n        def projected_target_log_prob_fn ( alpha ) : \n            return target_log_prob_fn ( * _step_along_direction ( alpha ) ) \n        alpha_init = tf . zeros_like ( current_target_log_prob , dtype = current_state_parts [ 0 ] . dtype . base_dtype ) \n        [ next_alpha , next_target_log_prob , bounds_satisfied , upper_bounds , lower_bounds ] = ssu . slice_sampler_one_dim ( projected_target_log_prob_fn , x_initial = alpha_init , max_doublings = max_doublings , step_size = step_size , seed = seed ) \n        return [ _step_along_direction ( next_alpha ) , next_target_log_prob , bounds_satisfied , direction , upper_bounds , lower_bounds ] "}
{"698": "\ndef _build_trainable_posterior ( param , initial_loc_fn ) : \n    loc = tf . compat . v1 . get_variable ( param . name + '_loc' , initializer = lambda : initial_loc_fn ( param ) , dtype = param . prior . dtype , use_resource = True ) \n    scale = tf . nn . softplus ( tf . compat . v1 . get_variable ( param . name + '_scale' , initializer = lambda : - 4.0 * tf . ones_like ( initial_loc_fn ( param ) ) , dtype = param . prior . dtype , use_resource = True ) ) \n    q = tfd . Normal ( loc = loc , scale = scale ) \n    if ( param . prior . event_shape . ndims is None or param . prior . event_shape . ndims > 0 ) : \n        q = tfd . Independent ( q , reinterpreted_batch_ndims = param . prior . event_shape . ndims ) \n    return tfd . TransformedDistribution ( q , param . bijector ) "}
{"700": "\ndef _minimize_in_graph ( build_loss_fn , num_steps = 200.0 , optimizer = None ) : \n    optimizer = tf . compat . v1 . train . AdamOptimizer ( 0.1 ) if optimizer is None else optimizer \n    def train_loop_body ( step ) : \n        train_op = optimizer . minimize ( build_loss_fn if tf . executing_eagerly ( ) else build_loss_fn ( ) ) \n        return tf . tuple ( tensors = [ tf . add ( step , 1 ) ] , control_inputs = [ train_op ] ) \n    minimize_op = tf . compat . v1 . while_loop ( cond = lambda step : step < num_steps , body = train_loop_body , loop_vars = [ tf . constant ( 0 ) ] , return_same_structure = True ) [ 0 ] \n    return minimize_op "}
{"701": "\ndef moments_of_masked_time_series ( time_series_tensor , broadcast_mask ) : \n    num_unmasked_entries = tf . cast ( tf . reduce_sum ( input_tensor = tf . cast ( ~ broadcast_mask , tf . int32 ) , axis = - 1 ) , time_series_tensor . dtype ) \n    mean = ( tf . reduce_sum ( input_tensor = tf . where ( broadcast_mask , tf . zeros_like ( time_series_tensor ) , time_series_tensor ) , axis = - 1 ) / num_unmasked_entries ) \n    variance = ( tf . reduce_sum ( input_tensor = tf . where ( broadcast_mask , tf . zeros_like ( time_series_tensor ) , ( time_series_tensor - mean [ ... , tf . newaxis ] ) ** 2.0 ) , axis = - 1 ) / num_unmasked_entries ) \n    return mean , variance "}
{"705": "\ndef sum_mvns ( distributions ) : \n    graph_parents = [ tensor for distribution in distributions for tensor in distribution . _graph_parents ] \n    with tf . compat . v1 . name_scope ( 'sum_mvns' , values = graph_parents ) : \n        if all ( [ isinstance ( mvn , tfd . MultivariateNormalDiag ) for mvn in distributions ] ) : \n            return tfd . MultivariateNormalDiag ( loc = sum ( [ mvn . mean ( ) for mvn in distributions ] ) , scale_diag = tf . sqrt ( sum ( [ mvn . scale . diag ** 2.0 for mvn in distributions ] ) ) ) \n        else : \n            raise NotImplementedError ( 'Sums of distributions other than MultivariateNormalDiag are not ' 'currently implemented. (given: {})' . format ( distributions ) ) "}
{"709": "\ndef mix_over_posterior_draws ( means , variances ) : \n    with tf . compat . v1 . name_scope ( 'mix_over_posterior_draws' , values = [ means , variances ] ) : \n        num_posterior_draws = dist_util . prefer_static_value ( tf . shape ( input = means ) ) [ 0 ] \n        component_observations = tfd . Independent ( distribution = tfd . Normal ( loc = dist_util . move_dimension ( means , 0 , - 2.0 ) , scale = tf . sqrt ( dist_util . move_dimension ( variances , 0 , - 2.0 ) ) ) , reinterpreted_batch_ndims = 1 ) \n        return tfd . MixtureSameFamily ( mixture_distribution = tfd . Categorical ( logits = tf . zeros ( [ num_posterior_draws ] , dtype = component_observations . dtype ) ) , components_distribution = component_observations ) "}
{"721": "\ndef visualize_reconstruction ( inputs , reconstruct , num = 3.0 , name = \"reconstruction\" ) : \n    reconstruct = tf . clip_by_value ( reconstruct , 0. , 1. ) \n    inputs_and_reconstruct = tf . concat ( ( inputs [ : num ] , reconstruct [ : num ] ) , axis = 0 ) \n    image_summary ( inputs_and_reconstruct , name ) "}
{"722": "\ndef visualize_qualitative_analysis ( inputs , model , samples = 1 , batch_size = 3.0 , length = 8.0 ) : \n    average = lambda dist : tf . reduce_mean ( input_tensor = dist . mean ( ) , axis = 0 ) \n    with tf . compat . v1 . name_scope ( \"val_reconstruction\" ) : \n        reconstruct = functools . partial ( model . reconstruct , inputs = inputs , samples = samples ) \n        visualize_reconstruction ( inputs , average ( reconstruct ( ) ) ) \n        visualize_reconstruction ( inputs , average ( reconstruct ( sample_static = True ) ) , name = \"static_prior\" ) \n        visualize_reconstruction ( inputs , average ( reconstruct ( sample_dynamic = True ) ) , name = \"dynamic_prior\" ) \n        visualize_reconstruction ( inputs , average ( reconstruct ( swap_static = True ) ) , name = \"swap_static\" ) \n        visualize_reconstruction ( inputs , average ( reconstruct ( swap_dynamic = True ) ) , name = \"swap_dynamic\" ) \n    with tf . compat . v1 . name_scope ( \"generation\" ) : \n        generate = functools . partial ( model . generate , batch_size = batch_size , length = length , samples = samples ) \n        image_summary ( average ( generate ( fix_static = True ) ) , \"fix_static\" ) \n        image_summary ( average ( generate ( fix_dynamic = True ) ) , \"fix_dynamic\" ) "}
{"727": "\ndef call ( self , inputs , state ) : \n    original_shape = inputs . shape \n    if len ( original_shape ) < 2.0 : \n        inputs = tf . reshape ( inputs , [ 1 , - 1 ] ) \n    out , state = self . lstm_cell ( inputs , state ) \n    out = self . output_layer ( out ) \n    correct_shape = tf . concat ( ( original_shape [ : - 1 ] , tf . shape ( input = out ) [ - 1 : ] ) , 0 ) \n    out = tf . reshape ( out , correct_shape ) \n    loc = out [ ... , : self . dimensions ] \n    scale_diag = tf . nn . softplus ( out [ ... , self . dimensions : ] ) + 1e-5 \n    return tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag ) , state "}
{"728": "\ndef call ( self , inputs ) : \n    image_shape = tf . shape ( input = inputs ) [ - 3.0 : ] \n    collapsed_shape = tf . concat ( ( [ - 1 ] , image_shape ) , axis = 0 ) \n    out = tf . reshape ( inputs , collapsed_shape ) \n    out = self . conv1 ( out ) \n    out = self . conv2 ( out ) \n    out = self . conv3 ( out ) \n    out = self . conv4 ( out ) \n    expanded_shape = tf . concat ( ( tf . shape ( input = inputs ) [ : - 3.0 ] , [ - 1 ] ) , axis = 0 ) \n    return tf . reshape ( out , expanded_shape ) "}
{"730": "\ndef reconstruct ( self , inputs , samples = 1 , sample_static = False , sample_dynamic = False , swap_static = False , swap_dynamic = False , fix_static = False , fix_dynamic = False ) : \n    batch_size = tf . shape ( input = inputs ) [ - 5.0 ] \n    length = len ( tf . unstack ( inputs , axis = - 4.0 ) ) \n    features = self . compressor ( inputs ) \n    if sample_static : \n        static_sample , _ = self . sample_static_prior ( samples , batch_size , fix_static ) \n    else : \n        static_sample , _ = self . sample_static_posterior ( features , samples ) \n    if swap_static : \n        static_sample = tf . reverse ( static_sample , axis = [ 1 ] ) \n    if sample_dynamic : \n        dynamic_sample , _ = self . sample_dynamic_prior ( samples , batch_size , length , fix_dynamic ) \n    else : \n        dynamic_sample , _ = self . sample_dynamic_posterior ( features , samples , static_sample ) \n    if swap_dynamic : \n        dynamic_sample = tf . reverse ( dynamic_sample , axis = [ 1 ] ) \n    likelihood = self . decoder ( ( dynamic_sample , static_sample ) ) \n    return likelihood "}
{"732": "\ndef sample_dynamic_prior ( self , samples , batch_size , length , fixed = False ) : \n    if fixed : \n        sample_batch_size = 1 \n    else : \n        sample_batch_size = batch_size \n    sample , state = self . dynamic_prior . zero_state ( [ samples , sample_batch_size ] ) \n    locs = [ ] \n    scale_diags = [ ] \n    sample_list = [ ] \n    for _ in range ( length ) : \n        dist , state = self . dynamic_prior ( sample , state ) \n        sample = dist . sample ( ) \n        locs . append ( dist . parameters [ \"loc\" ] ) \n        scale_diags . append ( dist . parameters [ \"scale_diag\" ] ) \n        sample_list . append ( sample ) \n    sample = tf . stack ( sample_list , axis = 2.0 ) \n    loc = tf . stack ( locs , axis = 2.0 ) \n    scale_diag = tf . stack ( scale_diags , axis = 2.0 ) \n    if fixed : \n        sample = sample + tf . zeros ( [ batch_size , 1 , 1 ] ) \n    return sample , tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag ) "}
{"738": "\ndef vector_size_to_square_matrix_size ( d , validate_args , name = None ) : \n    if isinstance ( d , ( float , int , np . generic , np . ndarray ) ) : \n        n = ( - 1 + np . sqrt ( 1 + 8.0 * d ) ) / 2. \n        if float ( int ( n ) ) != n : \n            raise ValueError ( \"Vector length is not a triangular number.\" ) \n        return int ( n ) \n    else : \n        with tf . name_scope ( name or \"vector_size_to_square_matrix_size\" ) as name : \n            n = ( - 1. + tf . sqrt ( 1 + 8. * tf . cast ( d , dtype = tf . float32 ) ) ) / 2. \n            if validate_args : \n                with tf . control_dependencies ( [ assert_util . assert_equal ( tf . cast ( tf . cast ( n , dtype = tf . int32 ) , dtype = tf . float32 ) , n , message = \"Vector length is not a triangular number\" ) ] ) : \n                    n = tf . identity ( n ) \n            return tf . cast ( n , d . dtype ) "}
{"744": "\ndef log_ndtr ( x , series_order = 3.0 , name = \"log_ndtr\" ) : \n    if not isinstance ( series_order , int ) : \n        raise TypeError ( \"series_order must be a Python integer.\" ) \n    if series_order < 0 : \n        raise ValueError ( \"series_order must be non-negative.\" ) \n    if series_order > 30.0 : \n        raise ValueError ( \"series_order must be <= 30.\" ) \n    with tf . name_scope ( name ) : \n        x = tf . convert_to_tensor ( value = x , name = \"x\" ) \n        if dtype_util . base_equal ( x . dtype , tf . float64 ) : \n            lower_segment = LOGNDTR_FLOAT64_LOWER \n            upper_segment = LOGNDTR_FLOAT64_UPPER \n        elif dtype_util . base_equal ( x . dtype , tf . float32 ) : \n            lower_segment = LOGNDTR_FLOAT32_LOWER \n            upper_segment = LOGNDTR_FLOAT32_UPPER \n        else : \n            raise TypeError ( \"x.dtype=%s is not supported.\" % x . dtype ) \n        return tf . where ( tf . greater ( x , upper_segment ) , - _ndtr ( - x ) , tf . where ( tf . greater ( x , lower_segment ) , tf . math . log ( _ndtr ( tf . maximum ( x , lower_segment ) ) ) , _log_ndtr_lower ( tf . minimum ( x , lower_segment ) , series_order ) ) ) "}
{"745": "\ndef _log_ndtr_asymptotic_series ( x , series_order ) : \n    npdt = dtype_util . as_numpy_dtype ( x . dtype ) \n    if series_order <= 0 : \n        return npdt ( 1 ) \n    x_2 = tf . square ( x ) \n    even_sum = tf . zeros_like ( x ) \n    odd_sum = tf . zeros_like ( x ) \n    x_2n = x_2 \n    for n in range ( 1 , series_order + 1 ) : \n        y = npdt ( _double_factorial ( 2.0 * n - 1 ) ) / x_2n \n        if n % 2.0 : \n            odd_sum += y \n        else : \n            even_sum += y \n        x_2n *= x_2 \n    return 1. + even_sum - odd_sum "}
{"749": "\ndef benchmark_text_messages_hmc ( num_results = int ( 3e3 ) , num_burnin_steps = int ( 3e3 ) , num_leapfrog_steps = 3.0 ) : \n    if not tf . executing_eagerly ( ) : \n        tf . compat . v1 . reset_default_graph ( ) \n    count_data = tf . cast ( tf . concat ( [ tfd . Poisson ( rate = 15. ) . sample ( 43.0 ) , tfd . Poisson ( rate = 25. ) . sample ( 31.0 ) ] , axis = 0 ) , dtype = tf . float32 ) \n    if tf . executing_eagerly ( ) : \n        count_data = count_data . numpy ( ) \n    else : \n        with tf . compat . v1 . Session ( ) : \n            count_data = count_data . eval ( ) \n    def unnormalized_log_posterior ( lambda1 , lambda2 , tau ) : \n        return text_messages_joint_log_prob ( count_data , lambda1 , lambda2 , tau ) \n    if tf . executing_eagerly ( ) : \n        sample_chain = tf . function ( tfp . mcmc . sample_chain ) \n    else : \n        sample_chain = tfp . mcmc . sample_chain \n    step_size = tf . compat . v2 . Variable ( name = 'step_size' , initial_value = tf . constant ( 0.05 , dtype = tf . float32 ) , trainable = False ) \n    def computation ( ) : \n        initial_chain_state = [ tf . constant ( count_data . mean ( ) , name = 'init_lambda1' ) , tf . constant ( count_data . mean ( ) , name = 'init_lambda2' ) , tf . constant ( 0.5 , name = 'init_tau' ) , ] \n        unconstraining_bijectors = [ tfp . bijectors . Exp ( ) , tfp . bijectors . Exp ( ) , tfp . bijectors . Sigmoid ( ) , ] \n        _ , kernel_results = sample_chain ( num_results = num_results , num_burnin_steps = num_burnin_steps , current_state = initial_chain_state , kernel = tfp . mcmc . TransformedTransitionKernel ( inner_kernel = tfp . mcmc . HamiltonianMonteCarlo ( target_log_prob_fn = unnormalized_log_posterior , num_leapfrog_steps = num_leapfrog_steps , step_size = step_size , step_size_update_fn = tfp . mcmc . make_simple_step_size_update_policy ( num_burnin_steps ) , state_gradients_are_stopped = True ) , bijector = unconstraining_bijectors ) ) \n        return kernel_results . inner_results . is_accepted \n    is_accepted_tensor = computation ( ) \n    if not tf . executing_eagerly ( ) : \n        session = tf . compat . v1 . Session ( ) \n        session . run ( tf . compat . v1 . global_variables_initializer ( ) ) \n        session . run ( is_accepted_tensor ) \n    start_time = time . time ( ) \n    if tf . executing_eagerly ( ) : \n        is_accepted = computation ( ) \n    else : \n        is_accepted = session . run ( is_accepted_tensor ) \n    wall_time = time . time ( ) - start_time \n    num_accepted = np . sum ( is_accepted ) \n    acceptance_rate = np . float32 ( num_accepted ) / np . float32 ( num_results ) \n    return dict ( iters = ( num_results + num_burnin_steps ) * num_leapfrog_steps , extras = { 'acceptance_rate' : acceptance_rate } , wall_time = wall_time ) "}
{"753": "\ndef make_iaf_stack ( total_event_size , num_hidden_layers = 2.0 , seed = None , dtype = tf . float32 ) : \n    seed = tfd . SeedStream ( seed , 'make_iaf_stack' ) \n    def make_iaf ( ) : \n        initializer = tf . compat . v2 . keras . initializers . VarianceScaling ( 2.0 * 0.01 , seed = seed ( ) % ( 2.0 ** 31.0 - 1 ) ) \n        made = tfb . AutoregressiveLayer ( params = 2.0 , event_shape = [ total_event_size ] , hidden_units = [ total_event_size ] * num_hidden_layers , activation = tf . nn . elu , kernel_initializer = initializer , dtype = dtype ) \n        def shift_and_scale ( x ) : \n            x . set_shape ( x . shape . merge_with ( [ None ] * ( x . shape . ndims - 1 ) + [ total_event_size ] ) ) \n            return tf . unstack ( made ( x ) , num = 2.0 , axis = - 1 ) \n        return tfb . Invert ( tfb . MaskedAutoregressiveFlow ( shift_and_scale ) ) \n    def make_swap ( ) : \n        permutation = list ( reversed ( range ( total_event_size ) ) ) \n        return tfb . Permute ( permutation ) \n    bijector = make_iaf ( ) \n    bijector = make_swap ( ) ( bijector ) \n    bijector = make_iaf ( ) ( bijector ) \n    bijector = make_swap ( ) ( bijector ) \n    bijector = make_iaf ( ) ( bijector ) \n    bijector = make_swap ( ) ( bijector ) \n    return bijector "}
{"761": "\ndef _decompose_from_posterior_marginals ( model , posterior_means , posterior_covs , parameter_samples ) : \n    try : \n        model . components \n    except AttributeError : \n        raise ValueError ( 'Model decomposed into components must be an instance of' '`tfp.sts.Sum` (passed model {})' . format ( model ) ) \n    with tf . compat . v1 . name_scope ( 'decompose_from_posterior_marginals' ) : \n        latent_sizes = [ component . latent_size for component in model . components ] \n        component_means = tf . split ( posterior_means , latent_sizes , axis = - 1 ) \n        component_covs = _split_covariance_into_marginals ( posterior_covs , latent_sizes ) \n        num_timesteps = dist_util . prefer_static_value ( tf . shape ( input = posterior_means ) ) [ - 2.0 ] \n        component_ssms = model . make_component_state_space_models ( num_timesteps = num_timesteps , param_vals = parameter_samples ) \n        component_predictive_dists = collections . OrderedDict ( ) \n        for ( component , component_ssm , component_mean , component_cov ) in zip ( model . components , component_ssms , component_means , component_covs ) : \n            component_obs_mean , component_obs_cov = ( component_ssm . latents_to_observations ( latent_means = component_mean , latent_covs = component_cov ) ) \n            component_predictive_dists [ component ] = sts_util . mix_over_posterior_draws ( means = component_obs_mean [ ... , 0 ] , variances = component_obs_cov [ ... , 0 , 0 ] ) \n    return component_predictive_dists "}
{"762": "\ndef decompose_by_component ( model , observed_time_series , parameter_samples ) : \n    with tf . compat . v1 . name_scope ( 'decompose_by_component' , values = [ observed_time_series ] ) : \n        [ observed_time_series , is_missing ] = sts_util . canonicalize_observed_time_series_with_mask ( observed_time_series ) \n        num_timesteps = dist_util . prefer_static_value ( tf . shape ( input = observed_time_series ) ) [ - 2.0 ] \n        ssm = model . make_state_space_model ( num_timesteps = num_timesteps , param_vals = parameter_samples ) \n        posterior_means , posterior_covs = ssm . posterior_marginals ( observed_time_series , mask = is_missing ) \n        return _decompose_from_posterior_marginals ( model , posterior_means , posterior_covs , parameter_samples ) "}
{"763": "\ndef decompose_forecast_by_component ( model , forecast_dist , parameter_samples ) : \n    with tf . compat . v1 . name_scope ( 'decompose_forecast_by_component' ) : \n        try : \n            forecast_lgssm = forecast_dist . components_distribution \n            forecast_latent_mean , _ = forecast_lgssm . _joint_mean ( ) \n            forecast_latent_covs , _ = forecast_lgssm . _joint_covariances ( ) \n        except AttributeError as e : \n            raise ValueError ( 'Forecast distribution must be a MixtureSameFamily of' 'LinearGaussianStateSpaceModel distributions, such as returned by' '`tfp.sts.forecast()`. (saw exception: {})' . format ( e ) ) \n        forecast_latent_mean = dist_util . move_dimension ( forecast_latent_mean , source_idx = - 3.0 , dest_idx = 0 ) \n        forecast_latent_covs = dist_util . move_dimension ( forecast_latent_covs , source_idx = - 4.0 , dest_idx = 0 ) \n        return _decompose_from_posterior_marginals ( model , forecast_latent_mean , forecast_latent_covs , parameter_samples ) "}
{"773": "\ndef real_nvp_default_template ( hidden_layers , shift_only = False , activation = tf . nn . relu , name = None , * args , ** kwargs ) : \n    with tf . compat . v2 . name_scope ( name or \"real_nvp_default_template\" ) : \n        def _fn ( x , output_units , ** condition_kwargs ) : \n            if condition_kwargs : \n                raise NotImplementedError ( \"Conditioning not implemented in the default template.\" ) \n            if tensorshape_util . rank ( x . shape ) == 1 : \n                x = x [ tf . newaxis , ... ] \n                reshape_output = lambda x : x [ 0 ] \n            else : \n                reshape_output = lambda x : x \n            for units in hidden_layers : \n                x = tf . compat . v1 . layers . dense ( inputs = x , units = units , activation = activation , * args , ** kwargs ) \n            x = tf . compat . v1 . layers . dense ( inputs = x , units = ( 1 if shift_only else 2.0 ) * output_units , activation = None , * args , ** kwargs ) \n            if shift_only : \n                return reshape_output ( x ) , None \n            shift , log_scale = tf . split ( x , 2.0 , axis = - 1 ) \n            return reshape_output ( shift ) , reshape_output ( log_scale ) \n        return tf . compat . v1 . make_template ( \"real_nvp_default_template\" , _fn ) "}
{"774": "\ndef _uniform_unit_norm ( dimension , shape , dtype , seed ) : \n    raw = normal . Normal ( loc = dtype_util . as_numpy_dtype ( dtype ) ( 0 ) , scale = dtype_util . as_numpy_dtype ( dtype ) ( 1 ) ) . sample ( tf . concat ( [ shape , [ dimension ] ] , axis = 0 ) , seed = seed ( ) ) \n    unit_norm = raw / tf . norm ( tensor = raw , ord = 2.0 , axis = - 1 ) [ ... , tf . newaxis ] \n    return unit_norm "}
{"782": "\ndef _effective_sample_size_single_state ( states , filter_beyond_lag , filter_threshold ) : \n    with tf . compat . v1 . name_scope ( 'effective_sample_size_single_state' , values = [ states , filter_beyond_lag , filter_threshold ] ) : \n        states = tf . convert_to_tensor ( value = states , name = 'states' ) \n        dt = states . dtype \n        auto_corr = stats . auto_correlation ( states , axis = 0 , max_lags = filter_beyond_lag ) \n        if filter_threshold is not None : \n            filter_threshold = tf . convert_to_tensor ( value = filter_threshold , dtype = dt , name = 'filter_threshold' ) \n            mask = auto_corr < filter_threshold \n            mask = tf . cast ( mask , dtype = dt ) \n            mask = tf . cumsum ( mask , axis = 0 ) \n            mask = tf . maximum ( 1. - mask , 0. ) \n            auto_corr *= mask \n        n = _axis_size ( states , axis = 0 ) \n        k = tf . range ( 0. , _axis_size ( auto_corr , axis = 0 ) ) \n        nk_factor = ( n - k ) / n \n        if auto_corr . shape . ndims is not None : \n            new_shape = [ - 1 ] + [ 1 ] * ( auto_corr . shape . ndims - 1 ) \n        else : \n            new_shape = tf . concat ( ( [ - 1 ] , tf . ones ( [ tf . rank ( auto_corr ) - 1 ] , dtype = tf . int32 ) ) , axis = 0 ) \n        nk_factor = tf . reshape ( nk_factor , new_shape ) \n        return n / ( - 1 + 2.0 * tf . reduce_sum ( input_tensor = nk_factor * auto_corr , axis = 0 ) ) "}
{"787": "\ndef quadrature_scheme_lognormal_quantiles ( loc , scale , quadrature_size , validate_args = False , name = None ) : \n    with tf . name_scope ( name or \"quadrature_scheme_lognormal_quantiles\" ) : \n        dist = transformed_distribution . TransformedDistribution ( distribution = normal . Normal ( loc = loc , scale = scale ) , bijector = exp_bijector . Exp ( ) , validate_args = validate_args ) \n        batch_ndims = tensorshape_util . rank ( dist . batch_shape ) \n        if batch_ndims is None : \n            batch_ndims = tf . shape ( input = dist . batch_shape_tensor ( ) ) [ 0 ] \n        def _compute_quantiles ( ) : \n            zero = tf . zeros ( [ ] , dtype = dist . dtype ) \n            edges = tf . linspace ( zero , 1. , quadrature_size + 3.0 ) [ 1 : - 1 ] \n            edges = tf . reshape ( edges , shape = tf . concat ( [ [ - 1 ] , tf . ones ( [ batch_ndims ] , dtype = tf . int32 ) ] , axis = 0 ) ) \n            quantiles = dist . quantile ( edges ) \n            perm = tf . concat ( [ tf . range ( 1 , 1 + batch_ndims ) , [ 0 ] ] , axis = 0 ) \n            quantiles = tf . transpose ( a = quantiles , perm = perm ) \n            return quantiles \n        quantiles = _compute_quantiles ( ) \n        grid = ( quantiles [ ... , : - 1 ] + quantiles [ ... , 1 : ] ) / 2. \n        new_shape = tensorshape_util . concatenate ( dist . batch_shape , [ quadrature_size ] ) \n        tensorshape_util . set_shape ( grid , new_shape ) \n        probs = tf . fill ( dims = [ quadrature_size ] , value = 1. / tf . cast ( quadrature_size , dist . dtype ) ) \n        return grid , probs "}
{"792": "\ndef _left_doubling_increments ( batch_shape , max_doublings , step_size , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'left_doubling_increments' , [ batch_shape , max_doublings , step_size ] ) : \n        step_size = tf . convert_to_tensor ( value = step_size ) \n        dtype = step_size . dtype . base_dtype \n        output_shape = tf . concat ( ( [ max_doublings + 1 ] , batch_shape ) , axis = 0 ) \n        expand_left = distributions . Bernoulli ( 0.5 , dtype = dtype ) . sample ( sample_shape = output_shape , seed = seed ) \n        width_multipliers = tf . cast ( 2.0 ** tf . range ( 0 , max_doublings + 1 ) , dtype = dtype ) \n        widths_shape = tf . concat ( ( [ max_doublings + 1 ] , tf . ones_like ( batch_shape ) ) , axis = 0 ) \n        width_multipliers = tf . reshape ( width_multipliers , shape = widths_shape ) \n        widths = width_multipliers * step_size \n        left_increments = tf . cumsum ( widths * expand_left , exclusive = True , axis = 0 ) \n        return left_increments , widths "}
{"793": "\ndef _find_best_interval_idx ( x , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'find_best_interval_idx' , [ x ] ) : \n        k = tf . shape ( input = x ) [ 0 ] \n        dtype = x . dtype . base_dtype \n        mults = tf . range ( 2.0 * k , k , - 1 , dtype = dtype ) [ : , tf . newaxis ] \n        shifts = tf . range ( k , dtype = dtype ) [ : , tf . newaxis ] \n        indices = tf . argmax ( input = mults * x + shifts , axis = 0 , output_type = dtype ) \n        return indices "}
{"796": "\ndef slice_sampler_one_dim ( target_log_prob , x_initial , step_size = 0.01 , max_doublings = 30.0 , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'slice_sampler_one_dim' , [ x_initial , step_size , max_doublings ] ) : \n        x_initial = tf . convert_to_tensor ( value = x_initial ) \n        dtype = x_initial . dtype . base_dtype \n        log_slice_heights = target_log_prob ( x_initial ) - tf . random . gamma ( tf . shape ( input = x_initial ) , alpha = 1 , dtype = dtype , seed = seed ) \n        upper_bounds , lower_bounds , bounds_satisfied = slice_bounds_by_doubling ( x_initial , target_log_prob , log_slice_heights , max_doublings , step_size , seed = seed ) \n        retval = _sample_with_shrinkage ( x_initial , target_log_prob = target_log_prob , log_slice_heights = log_slice_heights , step_size = step_size , lower_bounds = lower_bounds , upper_bounds = upper_bounds , seed = seed ) \n        return ( retval , target_log_prob ( retval ) , bounds_satisfied , upper_bounds , lower_bounds ) "}
{"800": "\ndef _vggconv_block ( x , filters , kernel , stride , kernel_posterior_fn ) : \n    out = tfp . layers . Convolution2DFlipout ( filters , kernel , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( x ) \n    out = tf . keras . layers . BatchNormalization ( ) ( out ) \n    out = tf . keras . layers . Activation ( 'relu' ) ( out ) \n    out = tfp . layers . Convolution2DFlipout ( filters , kernel , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( out ) \n    out = tf . keras . layers . BatchNormalization ( ) ( out ) \n    out = tf . keras . layers . Activation ( 'relu' ) ( out ) \n    out = tf . keras . layers . MaxPooling2D ( pool_size = ( 2.0 , 2.0 ) , strides = stride ) ( out ) \n    return out "}
{"818": "\ndef pack_images ( images , rows , cols ) : \n    shape = tf . shape ( input = images ) \n    width = shape [ - 3.0 ] \n    height = shape [ - 2.0 ] \n    depth = shape [ - 1 ] \n    images = tf . reshape ( images , ( - 1 , width , height , depth ) ) \n    batch = tf . shape ( input = images ) [ 0 ] \n    rows = tf . minimum ( rows , batch ) \n    cols = tf . minimum ( batch // rows , cols ) \n    images = images [ : rows * cols ] \n    images = tf . reshape ( images , ( rows , cols , width , height , depth ) ) \n    images = tf . transpose ( a = images , perm = [ 0 , 2.0 , 1 , 3.0 , 4.0 ] ) \n    images = tf . reshape ( images , [ 1 , rows * width , cols * height , depth ] ) \n    return images "}
{"823": "\ndef multivariate_normal_tril ( x , dims , layer_fn = tf . compat . v1 . layers . dense , loc_fn = lambda x : x , scale_fn = tril_with_diag_softplus_and_shift , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'multivariate_normal_tril' , [ x , dims ] ) : \n        x = tf . convert_to_tensor ( value = x , name = 'x' ) \n        x = layer_fn ( x , dims + dims * ( dims + 1 ) // 2.0 ) \n        return tfd . MultivariateNormalTriL ( loc = loc_fn ( x [ ... , : dims ] ) , scale_tril = scale_fn ( x [ ... , dims : ] ) ) "}
{"825": "\ndef normal ( x , layer_fn = tf . compat . v1 . layers . dense , loc_fn = lambda x : x , scale_fn = 1. , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'normal' , [ x ] ) : \n        x = tf . convert_to_tensor ( value = x , name = 'x' ) \n        if callable ( scale_fn ) : \n            y = layer_fn ( x , 2.0 ) \n            loc = loc_fn ( y [ ... , 0 ] ) \n            scale = scale_fn ( y [ ... , 1 ] ) \n        else : \n            y = tf . squeeze ( layer_fn ( x , 1 ) , axis = - 1 ) \n            loc = loc_fn ( y ) \n            scale = tf . cast ( scale_fn , loc . dtype . base_dtype ) \n        return tfd . Normal ( loc = loc , scale = scale ) "}
{"829": "\ndef _compute_log_acceptance_correction ( current_state_parts , proposed_state_parts , current_volatility_parts , proposed_volatility_parts , current_drift_parts , proposed_drift_parts , step_size_parts , independent_chain_ndims , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'compute_log_acceptance_correction' , [ current_state_parts , proposed_state_parts , current_volatility_parts , proposed_volatility_parts , current_drift_parts , proposed_drift_parts , step_size_parts , independent_chain_ndims ] ) : \n        proposed_log_density_parts = [ ] \n        dual_log_density_parts = [ ] \n        for [ current_state , proposed_state , current_volatility , proposed_volatility , current_drift , proposed_drift , step_size , ] in zip ( current_state_parts , proposed_state_parts , current_volatility_parts , proposed_volatility_parts , current_drift_parts , proposed_drift_parts , step_size_parts , ) : \n            axis = tf . range ( independent_chain_ndims , tf . rank ( current_state ) ) \n            state_diff = proposed_state - current_state \n            current_volatility *= tf . sqrt ( step_size ) \n            proposed_energy = ( state_diff - current_drift ) / current_volatility \n            proposed_volatility *= tf . sqrt ( step_size ) \n            proposed_energy = ( tf . reduce_sum ( input_tensor = mcmc_util . safe_sum ( [ tf . math . log ( current_volatility ) , 0.5 * ( proposed_energy ** 2.0 ) ] ) , axis = axis ) ) \n            proposed_log_density_parts . append ( - proposed_energy ) \n            dual_energy = ( state_diff + proposed_drift ) / proposed_volatility \n            dual_energy = ( tf . reduce_sum ( input_tensor = mcmc_util . safe_sum ( [ tf . math . log ( proposed_volatility ) , 0.5 * ( dual_energy ** 2.0 ) ] ) , axis = axis ) ) \n            dual_log_density_parts . append ( - dual_energy ) \n        proposed_log_density_reduce = tf . reduce_sum ( input_tensor = tf . stack ( proposed_log_density_parts , axis = - 1 ) , axis = - 1 ) \n        dual_log_density_reduce = tf . reduce_sum ( input_tensor = tf . stack ( dual_log_density_parts , axis = - 1 ) , axis = - 1 ) \n        return mcmc_util . safe_sum ( [ dual_log_density_reduce , - proposed_log_density_reduce ] ) "}
{"830": "\ndef _maybe_call_volatility_fn_and_grads ( volatility_fn , state , volatility_fn_results = None , grads_volatility_fn = None , sample_shape = None , parallel_iterations = 10.0 ) : \n    state_parts = list ( state ) if mcmc_util . is_list_like ( state ) else [ state ] \n    needs_volatility_fn_gradients = grads_volatility_fn is None \n    if volatility_fn_results is None : \n        volatility_fn_results = volatility_fn ( * state_parts ) \n    volatility_fn_results = ( list ( volatility_fn_results ) if mcmc_util . is_list_like ( volatility_fn_results ) else [ volatility_fn_results ] ) \n    if len ( volatility_fn_results ) == 1 : \n        volatility_fn_results *= len ( state_parts ) \n    if len ( state_parts ) != len ( volatility_fn_results ) : \n        raise ValueError ( '`volatility_fn` should return a tensor or a list ' 'of the same length as `current_state`.' ) \n    volatility_fn_results = _maybe_broadcast_volatility ( volatility_fn_results , state_parts ) \n    if grads_volatility_fn is None : \n        [ _ , grads_volatility_fn , ] = diag_jacobian ( xs = state_parts , ys = volatility_fn_results , sample_shape = sample_shape , parallel_iterations = parallel_iterations , fn = volatility_fn ) \n    if needs_volatility_fn_gradients : \n        grads_volatility_fn = [ 2. * g * volatility if g is not None else tf . zeros_like ( fn_arg , dtype = fn_arg . dtype . base_dtype ) for g , volatility , fn_arg in zip ( grads_volatility_fn , volatility_fn_results , state_parts ) ] \n    return volatility_fn_results , grads_volatility_fn "}
{"832": "\ndef make_ar_transition_matrix ( coefficients ) : \n    top_row = tf . expand_dims ( coefficients , - 2.0 ) \n    coef_shape = dist_util . prefer_static_shape ( coefficients ) \n    batch_shape , order = coef_shape [ : - 1 ] , coef_shape [ - 1 ] \n    remaining_rows = tf . concat ( [ tf . eye ( order - 1 , dtype = coefficients . dtype , batch_shape = batch_shape ) , tf . zeros ( tf . concat ( [ batch_shape , ( order - 1 , 1 ) ] , axis = 0 ) , dtype = coefficients . dtype ) ] , axis = - 1 ) \n    ar_matrix = tf . concat ( [ top_row , remaining_rows ] , axis = - 2.0 ) \n    return ar_matrix "}
{"840": "\ndef get_topics_strings ( topics_words , alpha , vocabulary , topics_to_print = 10.0 , words_per_topic = 10.0 ) : \n    alpha = np . squeeze ( alpha , axis = 0 ) \n    highest_weight_topics = np . argsort ( - alpha , kind = \"mergesort\" ) \n    top_words = np . argsort ( - topics_words , axis = 1 ) \n    res = [ ] \n    for topic_idx in highest_weight_topics [ : topics_to_print ] : \n        l = [ \"index={} alpha={:.2f}\" . format ( topic_idx , alpha [ topic_idx ] ) ] \n        l += [ vocabulary [ word ] for word in top_words [ topic_idx , : words_per_topic ] ] \n        res . append ( \" \" . join ( l ) ) \n    return np . array ( res ) "}
{"842": "\ndef build_fake_input_fns ( batch_size ) : \n    num_words = 1000.0 \n    vocabulary = [ str ( i ) for i in range ( num_words ) ] \n    random_sample = np . random . randint ( 10.0 , size = ( batch_size , num_words ) ) . astype ( np . float32 ) \n    def train_input_fn ( ) : \n        dataset = tf . data . Dataset . from_tensor_slices ( random_sample ) \n        dataset = dataset . batch ( batch_size ) . repeat ( ) \n        return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) \n    def eval_input_fn ( ) : \n        dataset = tf . data . Dataset . from_tensor_slices ( random_sample ) \n        dataset = dataset . batch ( batch_size ) \n        return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) \n    return train_input_fn , eval_input_fn , vocabulary "}
{"843": "\ndef build_input_fns ( data_dir , batch_size ) : \n    with open ( download ( data_dir , \"vocab.pkl\" ) , \"r\" ) as f : \n        words_to_idx = pickle . load ( f ) \n    num_words = len ( words_to_idx ) \n    vocabulary = [ None ] * num_words \n    for word , idx in words_to_idx . items ( ) : \n        vocabulary [ idx ] = word \n    def train_input_fn ( ) : \n        dataset = newsgroups_dataset ( data_dir , \"train\" , num_words , shuffle_and_repeat = True ) \n        dataset = dataset . batch ( batch_size ) . prefetch ( 32.0 ) \n        return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) \n    def eval_input_fn ( ) : \n        dataset = newsgroups_dataset ( data_dir , \"test\" , num_words , shuffle_and_repeat = False ) \n        dataset = dataset . batch ( batch_size ) \n        return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) \n    return train_input_fn , eval_input_fn , vocabulary "}
{"845": "\ndef add_ema_control_dependencies ( vector_quantizer , one_hot_assignments , codes , commitment_loss , decay ) : \n    updated_ema_count = moving_averages . assign_moving_average ( vector_quantizer . ema_count , tf . reduce_sum ( input_tensor = one_hot_assignments , axis = [ 0 , 1 ] ) , decay , zero_debias = False ) \n    updated_ema_means = moving_averages . assign_moving_average ( vector_quantizer . ema_means , tf . reduce_sum ( input_tensor = tf . expand_dims ( codes , 2.0 ) * tf . expand_dims ( one_hot_assignments , 3.0 ) , axis = [ 0 , 1 ] ) , decay , zero_debias = False ) \n    perturbed_ema_count = updated_ema_count + 1e-5 \n    with tf . control_dependencies ( [ commitment_loss ] ) : \n        update_means = tf . compat . v1 . assign ( vector_quantizer . codebook , updated_ema_means / perturbed_ema_count [ ... , tf . newaxis ] ) \n        with tf . control_dependencies ( [ update_means ] ) : \n            return tf . identity ( commitment_loss ) "}
{"847": "\ndef visualize_training ( images_val , reconstructed_images_val , random_images_val , log_dir , prefix , viz_n = 10.0 ) : \n    save_imgs ( images_val [ : viz_n ] , os . path . join ( log_dir , \"{}_inputs.png\" . format ( prefix ) ) ) \n    save_imgs ( reconstructed_images_val [ : viz_n ] , os . path . join ( log_dir , \"{}_reconstructions.png\" . format ( prefix ) ) ) \n    if random_images_val is not None : \n        save_imgs ( random_images_val [ : viz_n ] , os . path . join ( log_dir , \"{}_prior_samples.png\" . format ( prefix ) ) ) "}
{"848": "\ndef load_bernoulli_mnist_dataset ( directory , split_name ) : \n    amat_file = download ( directory , FILE_TEMPLATE . format ( split = split_name ) ) \n    dataset = tf . data . TextLineDataset ( amat_file ) \n    str_to_arr = lambda string : np . array ( [ c == b\"1\" for c in string . split ( ) ] ) \n    def _parser ( s ) : \n        booltensor = tf . compat . v1 . py_func ( str_to_arr , [ s ] , tf . bool ) \n        reshaped = tf . reshape ( booltensor , [ 28.0 , 28.0 , 1 ] ) \n        return tf . cast ( reshaped , dtype = tf . float32 ) , tf . constant ( 0 , tf . int32 ) \n    return dataset . map ( _parser ) "}
{"859": "\ndef nelder_mead_one_step ( current_simplex , current_objective_values , objective_function = None , dim = None , func_tolerance = None , position_tolerance = None , batch_evaluate_objective = False , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'nelder_mead_one_step' ) : \n        domain_dtype = current_simplex . dtype . base_dtype \n        order = tf . argsort ( current_objective_values , direction = 'ASCENDING' , stable = True ) \n        ( best_index , worst_index , second_worst_index ) = order [ 0 ] , order [ - 1 ] , order [ - 2.0 ] \n        worst_vertex = current_simplex [ worst_index ] \n        ( best_objective_value , worst_objective_value , second_worst_objective_value ) = ( current_objective_values [ best_index ] , current_objective_values [ worst_index ] , current_objective_values [ second_worst_index ] ) \n        face_centroid = tf . reduce_sum ( input_tensor = current_simplex , axis = 0 ) - worst_vertex \n        face_centroid /= tf . cast ( dim , domain_dtype ) \n        reflected = face_centroid + reflection * ( face_centroid - worst_vertex ) \n        objective_at_reflected = objective_function ( reflected ) \n        num_evaluations = 1 \n        has_converged = _check_convergence ( current_simplex , current_simplex [ best_index ] , best_objective_value , worst_objective_value , func_tolerance , position_tolerance ) \n        def _converged_fn ( ) : \n            return ( True , current_simplex , current_objective_values , 0 ) \n        case0 = has_converged , _converged_fn \n        accept_reflected = ( ( objective_at_reflected < second_worst_objective_value ) & ( objective_at_reflected >= best_objective_value ) ) \n        accept_reflected_fn = _accept_reflected_fn ( current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected ) \n        case1 = accept_reflected , accept_reflected_fn \n        do_expansion = objective_at_reflected < best_objective_value \n        expansion_fn = _expansion_fn ( objective_function , current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected , face_centroid , expansion ) \n        case2 = do_expansion , expansion_fn \n        do_outside_contraction = ( ( objective_at_reflected < worst_objective_value ) & ( objective_at_reflected >= second_worst_objective_value ) ) \n        outside_contraction_fn = _outside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , reflected , objective_at_reflected , contraction , shrinkage , batch_evaluate_objective ) \n        case3 = do_outside_contraction , outside_contraction_fn \n        default_fn = _inside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , worst_objective_value , contraction , shrinkage , batch_evaluate_objective ) \n        ( converged , next_simplex , next_objective_at_simplex , case_evals ) = prefer_static . case ( [ case0 , case1 , case2 , case3 ] , default = default_fn , exclusive = False ) \n        next_simplex . set_shape ( current_simplex . shape ) \n        next_objective_at_simplex . set_shape ( current_objective_values . shape ) \n        return ( converged , next_simplex , next_objective_at_simplex , num_evaluations + case_evals ) "}
{"870": "\ndef plot_weight_posteriors ( names , qm_vals , qs_vals , fname ) : \n    fig = figure . Figure ( figsize = ( 6.0 , 3.0 ) ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    ax = fig . add_subplot ( 1 , 2.0 , 1 ) \n    for n , qm in zip ( names , qm_vals ) : \n        sns . distplot ( qm . flatten ( ) , ax = ax , label = n ) \n    ax . set_title ( \"weight means\" ) \n    ax . set_xlim ( [ - 1.5 , 1.5 ] ) \n    ax . legend ( ) \n    ax = fig . add_subplot ( 1 , 2.0 , 2.0 ) \n    for n , qs in zip ( names , qs_vals ) : \n        sns . distplot ( qs . flatten ( ) , ax = ax ) \n    ax . set_title ( \"weight stddevs\" ) \n    ax . set_xlim ( [ 0 , 1. ] ) \n    fig . tight_layout ( ) \n    canvas . print_figure ( fname , format = \"png\" ) \n    print ( \"saved {}\" . format ( fname ) ) "}
{"871": "\ndef plot_heldout_prediction ( input_vals , probs , fname , n = 10.0 , title = \"\" ) : \n    fig = figure . Figure ( figsize = ( 9.0 , 3.0 * n ) ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    for i in range ( n ) : \n        ax = fig . add_subplot ( n , 3.0 , 3.0 * i + 1 ) \n        ax . imshow ( input_vals [ i , : ] . reshape ( IMAGE_SHAPE [ : - 1 ] ) , interpolation = \"None\" ) \n        ax = fig . add_subplot ( n , 3.0 , 3.0 * i + 2.0 ) \n        for prob_sample in probs : \n            sns . barplot ( np . arange ( 10.0 ) , prob_sample [ i , : ] , alpha = 0.1 , ax = ax ) \n            ax . set_ylim ( [ 0 , 1 ] ) \n        ax . set_title ( \"posterior samples\" ) \n        ax = fig . add_subplot ( n , 3.0 , 3.0 * i + 3.0 ) \n        sns . barplot ( np . arange ( 10.0 ) , np . mean ( probs [ : , i , : ] , axis = 0 ) , ax = ax ) \n        ax . set_ylim ( [ 0 , 1 ] ) \n        ax . set_title ( \"predictive probs\" ) \n    fig . suptitle ( title ) \n    fig . tight_layout ( ) \n    canvas . print_figure ( fname , format = \"png\" ) \n    print ( \"saved {}\" . format ( fname ) ) "}
{"872": "\ndef build_fake_data ( num_examples = 10.0 ) : \n    class Dummy ( object ) : \n        pass \n    num_examples = 10.0 \n    mnist_data = Dummy ( ) \n    mnist_data . train = Dummy ( ) \n    mnist_data . train . images = np . float32 ( np . random . randn ( num_examples , * IMAGE_SHAPE ) ) \n    mnist_data . train . labels = np . int32 ( np . random . permutation ( np . arange ( num_examples ) ) ) \n    mnist_data . train . num_examples = num_examples \n    mnist_data . validation = Dummy ( ) \n    mnist_data . validation . images = np . float32 ( np . random . randn ( num_examples , * IMAGE_SHAPE ) ) \n    mnist_data . validation . labels = np . int32 ( np . random . permutation ( np . arange ( num_examples ) ) ) \n    mnist_data . validation . num_examples = num_examples \n    return mnist_data "}
{"879": "\ndef random_rademacher ( shape , dtype = tf . float32 , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'random_rademacher' , [ shape , seed ] ) : \n        generation_dtype = tf . int64 if tf . as_dtype ( dtype ) != tf . int32 else tf . int32 \n        random_bernoulli = tf . random . uniform ( shape , minval = 0 , maxval = 2.0 , dtype = generation_dtype , seed = seed ) \n        return tf . cast ( 2.0 * random_bernoulli - 1 , dtype ) "}
{"893": "\ndef convergence_criteria_small_relative_norm_weights_change ( tolerance = 1e-5 , norm_order = 2.0 ) : \n    def convergence_criteria_fn ( is_converged_previous , iter_ , model_coefficients_previous , predicted_linear_response_previous , model_coefficients_next , predicted_linear_response_next , response , model , dispersion ) : \n        relative_euclidean_norm = ( tf . norm ( tensor = model_coefficients_previous - model_coefficients_next , ord = norm_order , axis = - 1 ) / ( 1. + tf . norm ( tensor = model_coefficients_previous , ord = norm_order , axis = - 1 ) ) ) \n        return ( iter_ > 0 ) & tf . reduce_all ( input_tensor = relative_euclidean_norm < tolerance ) \n    return convergence_criteria_fn "}
{"894": "\ndef prepare_args ( model_matrix , response , model_coefficients , predicted_linear_response , offset , name = None ) : \n    graph_deps = [ model_matrix , response , model_coefficients , predicted_linear_response , offset ] \n    with tf . compat . v1 . name_scope ( name , 'prepare_args' , graph_deps ) : \n        dtype = dtype_util . common_dtype ( graph_deps , np . float32 ) \n        model_matrix = tf . convert_to_tensor ( value = model_matrix , dtype = dtype , name = 'model_matrix' ) \n        if offset is not None : \n            offset = tf . convert_to_tensor ( value = offset , dtype = dtype , name = 'offset' ) \n        response = tf . convert_to_tensor ( value = response , dtype = dtype , name = 'response' ) \n        use_default_model_coefficients = model_coefficients is None \n        if use_default_model_coefficients : \n            batch_shape = tf . shape ( input = model_matrix ) [ : - 2.0 ] \n            num_columns = tf . shape ( input = model_matrix ) [ - 1 ] \n            model_coefficients = tf . zeros ( shape = tf . concat ( [ batch_shape , [ num_columns ] ] , axis = 0 ) , dtype = dtype , name = 'model_coefficients' ) \n        else : \n            model_coefficients = tf . convert_to_tensor ( value = model_coefficients , dtype = dtype , name = 'model_coefficients' ) \n        if predicted_linear_response is None : \n            if use_default_model_coefficients : \n                if offset is None : \n                    predicted_linear_response = tf . zeros_like ( response , dtype , name = 'predicted_linear_response' ) \n                else : \n                    predicted_linear_response = tf . broadcast_to ( offset , tf . shape ( input = response ) , name = 'predicted_linear_response' ) \n            else : \n                predicted_linear_response = calculate_linear_predictor ( model_matrix , model_coefficients , offset ) \n        else : \n            predicted_linear_response = tf . convert_to_tensor ( value = predicted_linear_response , dtype = dtype , name = 'predicted_linear_response' ) \n    return [ model_matrix , response , model_coefficients , predicted_linear_response , offset , ] "}
{"902": "\ndef mixture_stddev ( mixture_weight_vector , mean_vector , stddev_vector ) : \n    tensorshape_util . assert_has_rank ( mixture_weight_vector . shape , 2.0 ) \n    if not tensorshape_util . is_compatible_with ( mean_vector . shape , mixture_weight_vector . shape ) : \n        raise ValueError ( \"Expecting means to have same shape as mixture weights.\" ) \n    if not tensorshape_util . is_compatible_with ( stddev_vector . shape , mixture_weight_vector . shape ) : \n        raise ValueError ( \"Expecting stddevs to have same shape as mixture weights.\" ) \n    pi_for_dot_prod = tf . expand_dims ( mixture_weight_vector , axis = 1 ) \n    mu_for_dot_prod = tf . expand_dims ( mean_vector , axis = 2.0 ) \n    sigma_for_dot_prod = tf . expand_dims ( stddev_vector , axis = 2.0 ) \n    mean_wa = tf . matmul ( pi_for_dot_prod , mu_for_dot_prod ) \n    mean_wa = tf . reshape ( mean_wa , ( - 1 , ) ) \n    var_wa = tf . matmul ( pi_for_dot_prod , tf . square ( sigma_for_dot_prod ) ) \n    var_wa = tf . reshape ( var_wa , ( - 1 , ) ) \n    sq_mean_wa = tf . matmul ( pi_for_dot_prod , tf . square ( mu_for_dot_prod ) ) \n    sq_mean_wa = tf . reshape ( sq_mean_wa , ( - 1 , ) ) \n    mixture_variance = var_wa + sq_mean_wa - tf . square ( mean_wa ) \n    return tf . sqrt ( mixture_variance ) "}
{"916": "\ndef _largest_integer_by_dtype ( dt ) : \n    if not _is_known_dtype ( dt ) : \n        raise TypeError ( \"Unrecognized dtype: {}\" . format ( dt . name ) ) \n    if dt . is_floating : \n        return int ( 2.0 ** ( np . finfo ( dt . as_numpy_dtype ) . nmant + 1 ) ) \n    if dt . is_integer : \n        return np . iinfo ( dt . as_numpy_dtype ) . max \n    if dt . base_dtype == tf . bool : \n        return int ( 1 ) \n    raise TypeError ( \"Unrecognized dtype: {}\" . format ( dt . name ) ) "}
{"919": "\ndef embed_check_categorical_event_shape ( categorical_param , name = \"embed_check_categorical_event_shape\" ) : \n    with tf . name_scope ( name ) : \n        x = tf . convert_to_tensor ( value = categorical_param , name = \"categorical_param\" ) \n        x_dtype = dtype_util . base_dtype ( x . dtype ) \n        max_event_size = ( _largest_integer_by_dtype ( x_dtype ) if dtype_util . is_floating ( x_dtype ) else 0 ) \n        if max_event_size is 0 : \n            raise TypeError ( \"Unable to validate size of unrecognized dtype \" \"({}).\" . format ( dtype_util . name ( x_dtype ) ) ) \n        try : \n            x_shape_static = tensorshape_util . with_rank_at_least ( x . shape , 1 ) \n        except ValueError : \n            raise ValueError ( \"A categorical-distribution parameter must have \" \"at least 1 dimension.\" ) \n        event_size = tf . compat . dimension_value ( x_shape_static [ - 1 ] ) \n        if event_size is not None : \n            if event_size < 2.0 : \n                raise ValueError ( \"A categorical-distribution parameter must have at \" \"least 2 events.\" ) \n            if event_size > max_event_size : \n                raise ValueError ( \"Number of classes exceeds `dtype` precision, i.e., \" \"{} implies shape ({}) cannot exceed {}.\" . format ( dtype_util . name ( x_dtype ) , event_size , max_event_size ) ) \n            return x \n        else : \n            event_size = tf . shape ( input = x , out_type = tf . int64 , name = \"x_shape\" ) [ - 1 ] \n            return with_dependencies ( [ assert_util . assert_rank_at_least ( x , 1 , message = ( \"A categorical-distribution parameter must have \" \"at least 1 dimension.\" ) ) , assert_util . assert_greater_equal ( tf . shape ( input = x ) [ - 1 ] , 2.0 , message = ( \"A categorical-distribution parameter must have at \" \"least 2 events.\" ) ) , assert_util . assert_less_equal ( event_size , tf . convert_to_tensor ( max_event_size , dtype = tf . int64 ) , message = \"Number of classes exceeds `dtype` precision, \" \"i.e., {} dtype cannot exceed {} shape.\" . format ( dtype_util . name ( x_dtype ) , max_event_size ) ) , ] , x ) "}
{"921": "\ndef rotate_transpose ( x , shift , name = \"rotate_transpose\" ) : \n    with tf . name_scope ( name ) : \n        x = tf . convert_to_tensor ( value = x , name = \"x\" ) \n        shift = tf . convert_to_tensor ( value = shift , name = \"shift\" ) \n        assert_util . assert_integer ( shift ) \n        shift_value_static = tf . get_static_value ( shift ) \n        ndims = tensorshape_util . rank ( x . shape ) \n        if ndims is not None and shift_value_static is not None : \n            if ndims < 2.0 : \n                return x \n            shift_value_static = np . sign ( shift_value_static ) * ( abs ( shift_value_static ) % ndims ) \n            if shift_value_static == 0 : \n                return x \n            perm = np . roll ( np . arange ( ndims ) , shift_value_static ) \n            return tf . transpose ( a = x , perm = perm ) \n        else : \n            ndims = tf . rank ( x ) \n            shift = tf . where ( tf . less ( shift , 0 ) , - shift % ndims , ndims - shift % ndims ) \n            first = tf . range ( 0 , shift ) \n            last = tf . range ( shift , ndims ) \n            perm = tf . concat ( [ last , first ] , 0 ) \n            return tf . transpose ( a = x , perm = perm ) "}
{"924": "\ndef gen_new_seed ( seed , salt ) : \n    if seed is None : \n        return None \n    string = ( str ( seed ) + salt ) . encode ( \"utf-8\" ) \n    return int ( hashlib . md5 ( string ) . hexdigest ( ) [ : 8.0 ] , 16.0 ) & 0x7FFFFFFF "}
{"927": "\ndef process_quadrature_grid_and_probs ( quadrature_grid_and_probs , dtype , validate_args , name = None ) : \n    with tf . name_scope ( name or \"process_quadrature_grid_and_probs\" ) : \n        if quadrature_grid_and_probs is None : \n            grid , probs = np . polynomial . hermite . hermgauss ( deg = 8.0 ) \n            grid = grid . astype ( dtype_util . as_numpy_dtype ( dtype ) ) \n            probs = probs . astype ( dtype_util . as_numpy_dtype ( dtype ) ) \n            probs /= np . linalg . norm ( probs , ord = 1 , keepdims = True ) \n            grid = tf . convert_to_tensor ( value = grid , name = \"grid\" , dtype = dtype ) \n            probs = tf . convert_to_tensor ( value = probs , name = \"probs\" , dtype = dtype ) \n            return grid , probs \n        grid , probs = tuple ( quadrature_grid_and_probs ) \n        grid = tf . convert_to_tensor ( value = grid , name = \"grid\" , dtype = dtype ) \n        probs = tf . convert_to_tensor ( value = probs , name = \"unnormalized_probs\" , dtype = dtype ) \n        probs /= tf . norm ( tensor = probs , ord = 1 , axis = - 1 , keepdims = True , name = \"probs\" ) \n        def _static_event_size ( x ) : \n            return tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( x . shape , 1 ) [ - 1 ] ) \n        m , n = _static_event_size ( probs ) , _static_event_size ( grid ) \n        if m is not None and n is not None : \n            if m != n : \n                raise ValueError ( \"`quadrature_grid_and_probs` must be a `tuple` of \" \"same-length zero-th-dimension `Tensor`s \" \"(saw lengths {}, {})\" . format ( m , n ) ) \n        elif validate_args : \n            assertions = [ assert_util . assert_equal ( dimension_size ( probs , axis = - 1 ) , dimension_size ( grid , axis = - 1 ) , message = ( \"`quadrature_grid_and_probs` must be a `tuple` of \" \"same-length zero-th-dimension `Tensor`s\" ) ) , ] \n            with tf . control_dependencies ( assertions ) : \n                grid = tf . identity ( grid ) \n                probs = tf . identity ( probs ) \n        return grid , probs "}
{"943": "\ndef linear_gaussian_update ( prior_mean , prior_cov , observation_matrix , observation_noise , x_observed ) : \n    observation_size_is_static_and_scalar = ( tf . compat . dimension_value ( observation_matrix . shape [ - 2.0 ] ) == 1 ) \n    x_expected = _propagate_mean ( prior_mean , observation_matrix , observation_noise ) \n    tmp_obs_cov = observation_matrix . matmul ( prior_cov ) \n    predicted_obs_cov = ( observation_matrix . matmul ( tmp_obs_cov , adjoint_arg = True ) + observation_noise . covariance ( ) ) \n    if observation_size_is_static_and_scalar : \n        gain_transpose = tmp_obs_cov / predicted_obs_cov \n    else : \n        predicted_obs_cov_chol = tf . linalg . cholesky ( predicted_obs_cov ) \n        gain_transpose = tf . linalg . cholesky_solve ( predicted_obs_cov_chol , tmp_obs_cov ) \n    posterior_mean = ( prior_mean + tf . linalg . matmul ( gain_transpose , x_observed - x_expected , adjoint_a = True ) ) \n    tmp_term = - observation_matrix . matmul ( gain_transpose , adjoint = True ) \n    tmp_term = tf . linalg . set_diag ( tmp_term , tf . linalg . diag_part ( tmp_term ) + 1 ) \n    posterior_cov = ( tf . linalg . matmul ( tmp_term , tf . linalg . matmul ( prior_cov , tmp_term ) , adjoint_a = True ) + tf . linalg . matmul ( gain_transpose , tf . linalg . matmul ( observation_noise . covariance ( ) , gain_transpose ) , adjoint_a = True ) ) \n    if observation_size_is_static_and_scalar : \n        predictive_dist = independent . Independent ( normal . Normal ( loc = x_expected [ ... , 0 ] , scale = tf . sqrt ( predicted_obs_cov [ ... , 0 ] ) ) , reinterpreted_batch_ndims = 1 ) \n        predictive_dist . covariance = lambda : predicted_obs_cov \n    else : \n        predictive_dist = mvn_tril . MultivariateNormalTriL ( loc = x_expected [ ... , 0 ] , scale_tril = predicted_obs_cov_chol ) \n    return posterior_mean , posterior_cov , predictive_dist "}
{"950": "\ndef backward_smoothing_pass ( self , filtered_means , filtered_covs , predicted_means , predicted_covs ) : \n    with tf . name_scope ( \"backward_pass\" ) : \n        filtered_means = tf . convert_to_tensor ( value = filtered_means , name = \"filtered_means\" ) \n        filtered_covs = tf . convert_to_tensor ( value = filtered_covs , name = \"filtered_covs\" ) \n        predicted_means = tf . convert_to_tensor ( value = predicted_means , name = \"predicted_means\" ) \n        predicted_covs = tf . convert_to_tensor ( value = predicted_covs , name = \"predicted_covs\" ) \n        filtered_means = distribution_util . move_dimension ( filtered_means , - 2.0 , 0 ) \n        filtered_covs = distribution_util . move_dimension ( filtered_covs , - 3.0 , 0 ) \n        predicted_means = distribution_util . move_dimension ( predicted_means , - 2.0 , 0 ) \n        predicted_covs = distribution_util . move_dimension ( predicted_covs , - 3.0 , 0 ) \n        filtered_means = filtered_means [ ... , tf . newaxis ] \n        predicted_means = predicted_means [ ... , tf . newaxis ] \n        initial_backward_mean = predicted_means [ - 1 , ... ] \n        initial_backward_cov = predicted_covs [ - 1 , ... ] \n        num_timesteps = tf . shape ( input = filtered_means ) [ 0 ] \n        initial_state = BackwardPassState ( backward_mean = initial_backward_mean , backward_cov = initial_backward_cov , timestep = self . initial_step + num_timesteps - 1 ) \n        update_step_fn = build_backward_pass_step ( self . get_transition_matrix_for_timestep ) \n        posterior_states = tf . scan ( update_step_fn , elems = ( filtered_means , filtered_covs , predicted_means , predicted_covs ) , initializer = initial_state , reverse = True ) \n        posterior_means = distribution_util . move_dimension ( posterior_states . backward_mean [ ... , 0 ] , 0 , - 2.0 ) \n        posterior_covs = distribution_util . move_dimension ( posterior_states . backward_cov , 0 , - 3.0 ) \n        return ( posterior_means , posterior_covs ) "}
{"951": "\ndef _joint_sample_n ( self , n , seed = None ) : \n    with tf . name_scope ( \"sample_n_joint\" ) : \n        stream = seed_stream . SeedStream ( seed , salt = \"LinearGaussianStateSpaceModel_sample_n_joint\" ) \n        sample_and_batch_shape = distribution_util . prefer_static_value ( tf . concat ( [ [ n ] , self . batch_shape_tensor ( ) ] , axis = 0 ) ) \n        with tf . control_dependencies ( self . runtime_assertions ) : \n            initial_latent = self . initial_state_prior . sample ( sample_shape = _augment_sample_shape ( self . initial_state_prior , sample_and_batch_shape , self . validate_args ) , seed = stream ( ) ) \n            initial_latent = initial_latent [ ... , tf . newaxis ] \n        initial_observation_matrix = ( self . get_observation_matrix_for_timestep ( self . initial_step ) ) \n        initial_observation_noise = ( self . get_observation_noise_for_timestep ( self . initial_step ) ) \n        initial_observation_pred = initial_observation_matrix . matmul ( initial_latent ) \n        initial_observation = ( initial_observation_pred + initial_observation_noise . sample ( sample_shape = _augment_sample_shape ( initial_observation_noise , sample_and_batch_shape , self . validate_args ) , seed = stream ( ) ) [ ... , tf . newaxis ] ) \n        sample_step = build_kalman_sample_step ( self . get_transition_matrix_for_timestep , self . get_transition_noise_for_timestep , self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep , full_sample_and_batch_shape = sample_and_batch_shape , stream = stream , validate_args = self . validate_args ) \n        ( latents , observations ) = tf . scan ( sample_step , elems = tf . range ( self . initial_step + 1 , self . final_step ) , initializer = ( initial_latent , initial_observation ) ) \n        latents = tf . concat ( [ initial_latent [ tf . newaxis , ... ] , latents ] , axis = 0 ) \n        observations = tf . concat ( [ initial_observation [ tf . newaxis , ... ] , observations ] , axis = 0 ) \n        latents = tf . squeeze ( latents , - 1 ) \n        latents = distribution_util . move_dimension ( latents , 0 , - 2.0 ) \n        observations = tf . squeeze ( observations , - 1 ) \n        observations = distribution_util . move_dimension ( observations , 0 , - 2.0 ) \n    return latents , observations "}
{"953": "\ndef _joint_mean ( self ) : \n    with tf . name_scope ( \"mean_joint\" ) : \n        with tf . control_dependencies ( self . runtime_assertions ) : \n            initial_latent_mean = _broadcast_to_shape ( self . initial_state_prior . mean ( ) [ ... , tf . newaxis ] , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . latent_size , 1 ] ] , axis = 0 ) ) \n        initial_observation_mean = _propagate_mean ( initial_latent_mean , self . get_observation_matrix_for_timestep ( self . initial_step ) , self . get_observation_noise_for_timestep ( self . initial_step ) ) \n        mean_step = build_kalman_mean_step ( self . get_transition_matrix_for_timestep , self . get_transition_noise_for_timestep , self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) \n        ( latent_means , observation_means ) = tf . scan ( mean_step , elems = tf . range ( self . initial_step + 1 , self . final_step ) , initializer = ( initial_latent_mean , initial_observation_mean ) ) \n        latent_means = tf . concat ( [ initial_latent_mean [ tf . newaxis , ... ] , latent_means ] , axis = 0 ) \n        observation_means = tf . concat ( [ initial_observation_mean [ tf . newaxis , ... ] , observation_means ] , axis = 0 ) \n        latent_means = tf . squeeze ( latent_means , - 1 ) \n        latent_means = distribution_util . move_dimension ( latent_means , 0 , - 2.0 ) \n        observation_means = tf . squeeze ( observation_means , - 1 ) \n        observation_means = distribution_util . move_dimension ( observation_means , 0 , - 2.0 ) \n        return latent_means , observation_means "}
{"954": "\ndef _joint_covariances ( self ) : \n    with tf . name_scope ( \"covariance_joint\" ) : \n        with tf . control_dependencies ( self . runtime_assertions ) : \n            initial_latent_cov = _broadcast_to_shape ( self . initial_state_prior . covariance ( ) , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . latent_size , self . latent_size ] ] , axis = 0 ) ) \n        initial_observation_cov = _propagate_cov ( initial_latent_cov , self . get_observation_matrix_for_timestep ( self . initial_step ) , self . get_observation_noise_for_timestep ( self . initial_step ) ) \n        cov_step = build_kalman_cov_step ( self . get_transition_matrix_for_timestep , self . get_transition_noise_for_timestep , self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) \n        ( latent_covs , observation_covs ) = tf . scan ( cov_step , elems = tf . range ( self . initial_step + 1 , self . final_step ) , initializer = ( initial_latent_cov , initial_observation_cov ) ) \n        latent_covs = tf . concat ( [ initial_latent_cov [ tf . newaxis , ... ] , latent_covs ] , axis = 0 ) \n        observation_covs = tf . concat ( [ initial_observation_cov [ tf . newaxis , ... ] , observation_covs ] , axis = 0 ) \n        latent_covs = distribution_util . move_dimension ( latent_covs , 0 , - 3.0 ) \n        observation_covs = distribution_util . move_dimension ( observation_covs , 0 , - 3.0 ) \n        return latent_covs , observation_covs "}
{"955": "\ndef latents_to_observations ( self , latent_means , latent_covs ) : \n    with tf . name_scope ( \"latents_to_observations\" ) : \n        pushforward_latents_step = build_pushforward_latents_step ( self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) \n        latent_means = distribution_util . move_dimension ( latent_means , source_idx = - 2.0 , dest_idx = 0 ) \n        latent_means = latent_means [ ... , tf . newaxis ] \n        latent_covs = distribution_util . move_dimension ( latent_covs , source_idx = - 3.0 , dest_idx = 0 ) \n        ( initial_observation_mean , initial_observation_cov ) = pushforward_latents_step ( _ = None , latent_t_mean_cov = ( self . initial_step , latent_means [ self . initial_step ] , latent_covs [ self . initial_step ] ) ) \n        timesteps = tf . range ( self . initial_step , self . initial_step + self . num_timesteps ) \n        observation_means , observation_covs = tf . scan ( pushforward_latents_step , elems = ( timesteps , latent_means , latent_covs ) , initializer = ( initial_observation_mean , initial_observation_cov ) , parallel_iterations = 10000.0 ) \n        observation_means = distribution_util . move_dimension ( observation_means [ ... , 0 ] , source_idx = 0 , dest_idx = - 2.0 ) \n        observation_covs = distribution_util . move_dimension ( observation_covs , source_idx = 0 , dest_idx = - 3.0 ) \n        return observation_means , observation_covs "}
{"956": "\ndef _log_normalization ( self ) : \n    event_dim = tf . compat . dimension_value ( self . event_shape [ 0 ] ) \n    if event_dim is None : \n        raise ValueError ( 'vMF _log_normalizer currently only supports ' 'statically known event shape' ) \n    safe_conc = tf . where ( self . concentration > 0 , self . concentration , tf . ones_like ( self . concentration ) ) \n    safe_lognorm = ( ( event_dim / 2.0 - 1 ) * tf . math . log ( safe_conc ) - ( event_dim / 2.0 ) * np . log ( 2.0 * np . pi ) - tf . math . log ( _bessel_ive ( event_dim / 2.0 - 1 , safe_conc ) ) - tf . abs ( safe_conc ) ) \n    log_nsphere_surface_area = ( np . log ( 2. ) + ( event_dim / 2.0 ) * np . log ( np . pi ) - tf . math . lgamma ( tf . cast ( event_dim / 2.0 , self . dtype ) ) ) \n    return tf . where ( self . concentration > 0 , - safe_lognorm , log_nsphere_surface_area * tf . ones_like ( safe_lognorm ) ) "}
{"958": "\ndef _rotate ( self , samples ) : \n    event_dim = ( tf . compat . dimension_value ( self . event_shape [ 0 ] ) or self . _event_shape_tensor ( ) [ 0 ] ) \n    basis = tf . concat ( [ [ 1. ] , tf . zeros ( [ event_dim - 1 ] , dtype = self . dtype ) ] , axis = 0 ) , \n    u = tf . nn . l2_normalize ( basis - self . mean_direction , axis = - 1 ) \n    return samples - 2.0 * tf . reduce_sum ( input_tensor = samples * u , axis = - 1 , keepdims = True ) * u "}
{"959": "\ndef _sample_3d ( self , n , seed = None ) : \n    seed = seed_stream . SeedStream ( seed , salt = 'von_mises_fisher_3d' ) \n    u_shape = tf . concat ( [ [ n ] , self . _batch_shape_tensor ( ) ] , axis = 0 ) \n    z = tf . random . uniform ( u_shape , seed = seed ( ) , dtype = self . dtype ) \n    safe_conc = tf . where ( self . concentration > 0 , self . concentration , tf . ones_like ( self . concentration ) ) \n    safe_z = tf . where ( z > 0 , z , tf . ones_like ( z ) ) \n    safe_u = 1 + tf . reduce_logsumexp ( input_tensor = [ tf . math . log ( safe_z ) , tf . math . log1p ( - safe_z ) - 2.0 * safe_conc ] , axis = 0 ) / safe_conc \n    u = tf . where ( self . concentration > tf . zeros_like ( safe_u ) , safe_u , 2.0 * z - 1 ) \n    u = tf . where ( tf . equal ( z , 0 ) , - tf . ones_like ( u ) , u ) \n    if not self . _allow_nan_stats : \n        u = tf . debugging . check_numerics ( u , 'u in _sample_3d' ) \n    return u [ ... , tf . newaxis ] "}
{"970": "\ndef smart_for_loop ( loop_num_iter , body_fn , initial_loop_vars , parallel_iterations = 10.0 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'smart_for_loop' , [ loop_num_iter , initial_loop_vars ] ) : \n        loop_num_iter_ = tf . get_static_value ( loop_num_iter ) \n        if ( loop_num_iter_ is None or tf . executing_eagerly ( ) or control_flow_util . GraphOrParentsInXlaContext ( tf . compat . v1 . get_default_graph ( ) ) ) : \n            loop_num_iter = tf . cast ( loop_num_iter , dtype = tf . int32 ) \n            return tf . while_loop ( cond = lambda i , * args : i < loop_num_iter , body = lambda i , * args : [ i + 1 ] + list ( body_fn ( * args ) ) , loop_vars = [ np . int32 ( 0 ) ] + initial_loop_vars , parallel_iterations = parallel_iterations ) [ 1 : ] \n        result = initial_loop_vars \n        for _ in range ( loop_num_iter_ ) : \n            result = body_fn ( * result ) \n        return result "}
{"971": "\ndef trace_scan ( loop_fn , initial_state , elems , trace_fn , parallel_iterations = 10.0 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'trace_scan' , [ initial_state , elems ] ) , tf . compat . v1 . variable_scope ( tf . compat . v1 . get_variable_scope ( ) ) as vs : \n        if vs . caching_device is None and not tf . executing_eagerly ( ) : \n            vs . set_caching_device ( lambda op : op . device ) \n        initial_state = tf . nest . map_structure ( lambda x : tf . convert_to_tensor ( value = x , name = 'initial_state' ) , initial_state ) \n        elems = tf . convert_to_tensor ( value = elems , name = 'elems' ) \n        static_length = elems . shape [ 0 ] \n        if tf . compat . dimension_value ( static_length ) is None : \n            length = tf . shape ( input = elems ) [ 0 ] \n        else : \n            length = tf . convert_to_tensor ( value = static_length , dtype = tf . int32 , name = 'length' ) \n        elems_array = tf . TensorArray ( elems . dtype , size = length , element_shape = elems . shape [ 1 : ] ) \n        elems_array = elems_array . unstack ( elems ) \n        trace_arrays = tf . nest . map_structure ( lambda x : tf . TensorArray ( x . dtype , size = length , element_shape = x . shape ) , trace_fn ( initial_state ) ) \n        def _body ( i , state , trace_arrays ) : \n            state = loop_fn ( state , elems_array . read ( i ) ) \n            trace_arrays = tf . nest . pack_sequence_as ( trace_arrays , [ a . write ( i , v ) for a , v in zip ( tf . nest . flatten ( trace_arrays ) , tf . nest . flatten ( trace_fn ( state ) ) ) ] ) \n            return i + 1 , state , trace_arrays \n        _ , final_state , trace_arrays = tf . while_loop ( cond = lambda i , * args : i < length , body = _body , loop_vars = ( 0 , initial_state , trace_arrays ) , parallel_iterations = parallel_iterations ) \n        stacked_trace = tf . nest . map_structure ( lambda x : x . stack ( ) , trace_arrays ) \n        def _merge_static_length ( x ) : \n            x . set_shape ( tf . TensorShape ( static_length ) . concatenate ( x . shape [ 1 : ] ) ) \n            return x \n        stacked_trace = tf . nest . map_structure ( _merge_static_length , stacked_trace ) \n        return final_state , stacked_trace "}
{"977": "\ndef _maybe_check_valid_shape ( shape , validate_args ) : \n    if not dtype_util . is_integer ( shape . dtype ) : \n        raise TypeError ( '{} dtype ({}) should be `int`-like.' . format ( shape , dtype_util . name ( shape . dtype ) ) ) \n    assertions = [ ] \n    message = '`{}` rank should be <= 1.' \n    if tensorshape_util . rank ( shape . shape ) is not None : \n        if tensorshape_util . rank ( shape . shape ) > 1 : \n            raise ValueError ( message . format ( shape ) ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_less ( tf . rank ( shape ) , 2.0 , message = message . format ( shape ) ) ) \n    shape_ = tf . get_static_value ( shape ) \n    message = '`{}` elements must have at most one `-1`.' \n    if shape_ is not None : \n        if sum ( shape_ == - 1 ) > 1 : \n            raise ValueError ( message . format ( shape ) ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_less ( tf . reduce_sum ( input_tensor = tf . cast ( tf . equal ( shape , - 1 ) , tf . int32 ) ) , 2.0 , message = message . format ( shape ) ) ) \n    message = '`{}` elements must be either positive integers or `-1`.' \n    if shape_ is not None : \n        if np . any ( shape_ < - 1 ) : \n            raise ValueError ( message . format ( shape ) ) \n    elif validate_args : \n        assertions . append ( assert_util . assert_greater ( shape , - 2.0 , message = message . format ( shape ) ) ) \n    return assertions "}
{"986": "\ndef default_exchange_proposed_fn ( prob_exchange ) : \n    def default_exchange_proposed_fn_ ( num_replica , seed = None ) : \n        seed_stream = distributions . SeedStream ( seed , 'default_exchange_proposed_fn' ) \n        zero_start = tf . random . uniform ( [ ] , seed = seed_stream ( ) ) > 0.5 \n        if num_replica % 2.0 == 0 : \n            def _exchange ( ) : \n                flat_exchange = tf . range ( num_replica ) \n                if num_replica > 2.0 : \n                    start = tf . cast ( ~ zero_start , dtype = tf . int32 ) \n                    end = num_replica - start \n                    flat_exchange = flat_exchange [ start : end ] \n                return tf . reshape ( flat_exchange , [ tf . size ( input = flat_exchange ) // 2.0 , 2.0 ] ) \n        else : \n            def _exchange ( ) : \n                start = tf . cast ( zero_start , dtype = tf . int32 ) \n                end = num_replica - tf . cast ( ~ zero_start , dtype = tf . int32 ) \n                flat_exchange = tf . range ( num_replica ) [ start : end ] \n                return tf . reshape ( flat_exchange , [ tf . size ( input = flat_exchange ) // 2.0 , 2.0 ] ) \n        def _null_exchange ( ) : \n            return tf . reshape ( tf . cast ( [ ] , dtype = tf . int32 ) , shape = [ 0 , 2.0 ] ) \n        return tf . cond ( pred = tf . random . uniform ( [ ] , seed = seed_stream ( ) ) < prob_exchange , true_fn = _exchange , false_fn = _null_exchange ) \n    return default_exchange_proposed_fn_ "}
{"988": "\ndef _get_exchanged_states ( self , old_states , exchange_proposed , exchange_proposed_n , sampled_replica_states , sampled_replica_results ) : \n    with tf . compat . v1 . name_scope ( 'get_exchanged_states' ) : \n        target_log_probs = [ ] \n        for replica in range ( self . num_replica ) : \n            replica_log_prob = _get_field ( sampled_replica_results [ replica ] , 'target_log_prob' ) \n            inverse_temp = self . inverse_temperatures [ replica ] \n            target_log_probs . append ( replica_log_prob / inverse_temp ) \n        target_log_probs = tf . stack ( target_log_probs , axis = 0 ) \n        dtype = target_log_probs . dtype \n        num_state_parts = len ( sampled_replica_states [ 0 ] ) \n        exchanged_states = [ tf . TensorArray ( dtype , size = self . num_replica , dynamic_size = False , tensor_array_name = 'exchanged_states' , element_shape = sampled_replica_states [ 0 ] [ k ] . shape ) for k in range ( num_state_parts ) ] \n        sample_shape = tf . concat ( ( [ self . num_replica // 2.0 ] , tf . shape ( input = target_log_probs ) [ 1 : ] ) , axis = 0 ) \n        log_uniforms = tf . math . log ( tf . random . uniform ( shape = sample_shape , dtype = dtype , seed = self . _seed_stream ( ) ) ) \n        def _swap ( is_exchange_accepted , x , y ) : \n            with tf . compat . v1 . name_scope ( 'swap_where_exchange_accepted' ) : \n                new_x = mcmc_util . choose ( is_exchange_accepted , y , x ) \n                new_y = mcmc_util . choose ( is_exchange_accepted , x , y ) \n            return new_x , new_y \n        def cond ( i , unused_exchanged_states ) : \n            return i < exchange_proposed_n \n        def body ( i , exchanged_states ) : \n            m , n = tf . unstack ( exchange_proposed [ i ] ) \n            temp_diff = self . inverse_temperatures [ m ] - self . inverse_temperatures [ n ] \n            log_accept_ratio = mcmc_util . safe_sum ( [ - temp_diff * target_log_probs [ m ] , temp_diff * target_log_probs [ n ] ] ) \n            is_exchange_accepted = log_uniforms [ i ] < log_accept_ratio \n            for k in range ( num_state_parts ) : \n                new_m , new_n = _swap ( is_exchange_accepted , old_states [ k ] . read ( m ) , old_states [ k ] . read ( n ) ) \n                exchanged_states [ k ] = exchanged_states [ k ] . write ( m , new_m ) \n                exchanged_states [ k ] = exchanged_states [ k ] . write ( n , new_n ) \n            return i + 1 , exchanged_states \n        return tf . while_loop ( cond = cond , body = body , loop_vars = [ tf . constant ( 0 ) , exchanged_states ] ) [ 1 ] "}
{"1001": "\ndef _bisect ( value_and_gradients_function , initial_args , f_lim ) : \n    def _loop_cond ( curr ) : \n        return ~ tf . reduce_all ( input_tensor = curr . stopped ) \n    def _loop_body ( curr ) : \n        mid = value_and_gradients_function ( ( curr . left . x + curr . right . x ) / 2.0 ) \n        failed = ( curr . failed | ~ is_finite ( mid ) | tf . equal ( mid . x , curr . left . x ) | tf . equal ( mid . x , curr . right . x ) ) \n        to_update = ~ ( curr . stopped | failed ) \n        update_left = ( mid . df < 0 ) & ( mid . f <= f_lim ) \n        left = val_where ( to_update & update_left , mid , curr . left ) \n        right = val_where ( to_update & ~ update_left , mid , curr . right ) \n        stopped = curr . stopped | failed | ( right . df >= 0 ) \n        return [ _IntermediateResult ( iteration = curr . iteration , stopped = stopped , failed = failed , num_evals = curr . num_evals + 1 , left = left , right = right ) ] \n    return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ initial_args ] ) [ 0 ] "}
{"1003": "\ndef _satisfies_wolfe ( val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) : \n    exact_wolfe_suff_dec = ( sufficient_decrease_param * val_0 . df >= ( val_c . f - val_0 . f ) / val_c . x ) \n    wolfe_curvature = val_c . df >= curvature_param * val_0 . df \n    exact_wolfe = exact_wolfe_suff_dec & wolfe_curvature \n    approx_wolfe_applies = val_c . f <= f_lim \n    approx_wolfe_suff_dec = ( ( 2.0 * sufficient_decrease_param - 1 ) * val_0 . df >= val_c . df ) \n    approx_wolfe = approx_wolfe_applies & approx_wolfe_suff_dec & wolfe_curvature \n    is_satisfied = exact_wolfe | approx_wolfe \n    return is_satisfied "}
{"1010": "\ndef bayesian_resnet ( input_shape , num_classes = 10.0 , kernel_posterior_scale_mean = - 9.0 , kernel_posterior_scale_stddev = 0.1 , kernel_posterior_scale_constraint = 0.2 ) : \n    filters = [ 64.0 , 128.0 , 256.0 , 512.0 ] \n    kernels = [ 3.0 , 3.0 , 3.0 , 3.0 ] \n    strides = [ 1 , 2.0 , 2.0 , 2.0 ] \n    def _untransformed_scale_constraint ( t ) : \n        return tf . clip_by_value ( t , - 1000.0 , tf . math . log ( kernel_posterior_scale_constraint ) ) \n    kernel_posterior_fn = tfp . layers . default_mean_field_normal_fn ( untransformed_scale_initializer = tf . compat . v1 . initializers . random_normal ( mean = kernel_posterior_scale_mean , stddev = kernel_posterior_scale_stddev ) , untransformed_scale_constraint = _untransformed_scale_constraint ) \n    image = tf . keras . layers . Input ( shape = input_shape , dtype = 'float32' ) \n    x = tfp . layers . Convolution2DFlipout ( 64.0 , 3.0 , strides = 1 , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( image ) \n    for i in range ( len ( kernels ) ) : \n        x = _resnet_block ( x , filters [ i ] , kernels [ i ] , strides [ i ] , kernel_posterior_fn ) \n    x = tf . keras . layers . BatchNormalization ( ) ( x ) \n    x = tf . keras . layers . Activation ( 'relu' ) ( x ) \n    x = tf . keras . layers . AveragePooling2D ( 4.0 , 1 ) ( x ) \n    x = tf . keras . layers . Flatten ( ) ( x ) \n    x = tfp . layers . DenseFlipout ( num_classes , kernel_posterior_fn = kernel_posterior_fn ) ( x ) \n    model = tf . keras . Model ( inputs = image , outputs = x , name = 'resnet18' ) \n    return model "}
{"1015": "\ndef sample_chain ( num_results , current_state , previous_kernel_results = None , kernel = None , num_burnin_steps = 0 , num_steps_between_results = 0 , trace_fn = lambda current_state , kernel_results : kernel_results , return_final_kernel_results = False , parallel_iterations = 10.0 , name = None , ) : \n    if not kernel . is_calibrated : \n        warnings . warn ( \"supplied `TransitionKernel` is not calibrated. Markov \" \"chain may not converge to intended target distribution.\" ) \n    with tf . compat . v1 . name_scope ( name , \"mcmc_sample_chain\" , [ num_results , num_burnin_steps , num_steps_between_results ] ) : \n        num_results = tf . convert_to_tensor ( value = num_results , dtype = tf . int32 , name = \"num_results\" ) \n        num_burnin_steps = tf . convert_to_tensor ( value = num_burnin_steps , dtype = tf . int32 , name = \"num_burnin_steps\" ) \n        num_steps_between_results = tf . convert_to_tensor ( value = num_steps_between_results , dtype = tf . int32 , name = \"num_steps_between_results\" ) \n        current_state = tf . nest . map_structure ( lambda x : tf . convert_to_tensor ( value = x , name = \"current_state\" ) , current_state ) \n        if previous_kernel_results is None : \n            previous_kernel_results = kernel . bootstrap_results ( current_state ) \n        if trace_fn is None : \n            trace_fn = lambda * args : ( ) \n            no_trace = True \n        else : \n            no_trace = False \n        if trace_fn is sample_chain . __defaults__ [ 4.0 ] : \n            warnings . warn ( \"Tracing all kernel results by default is deprecated. Set \" \"the `trace_fn` argument to None (the future default \" \"value) or an explicit callback that traces the values \" \"you are interested in.\" ) \n        def _trace_scan_fn ( state_and_results , num_steps ) : \n            next_state , current_kernel_results = mcmc_util . smart_for_loop ( loop_num_iter = num_steps , body_fn = kernel . one_step , initial_loop_vars = list ( state_and_results ) , parallel_iterations = parallel_iterations ) \n            return next_state , current_kernel_results \n        ( _ , final_kernel_results ) , ( all_states , trace ) = mcmc_util . trace_scan ( loop_fn = _trace_scan_fn , initial_state = ( current_state , previous_kernel_results ) , elems = tf . one_hot ( indices = 0 , depth = num_results , on_value = 1 + num_burnin_steps , off_value = 1 + num_steps_between_results , dtype = tf . int32 ) , trace_fn = lambda state_and_results : ( state_and_results [ 0 ] , trace_fn ( * state_and_results ) ) , parallel_iterations = parallel_iterations ) \n        if return_final_kernel_results : \n            return CheckpointableStatesAndTrace ( all_states = all_states , trace = trace , final_kernel_results = final_kernel_results ) \n        else : \n            if no_trace : \n                return all_states \n            else : \n                return StatesAndTrace ( all_states = all_states , trace = trace ) "}
{"1016": "\ndef deep_exponential_family ( data_size , feature_size , units , shape ) : \n    w2 = ed . Gamma ( 0.1 , 0.3 , sample_shape = [ units [ 2.0 ] , units [ 1 ] ] , name = \"w2\" ) \n    w1 = ed . Gamma ( 0.1 , 0.3 , sample_shape = [ units [ 1 ] , units [ 0 ] ] , name = \"w1\" ) \n    w0 = ed . Gamma ( 0.1 , 0.3 , sample_shape = [ units [ 0 ] , feature_size ] , name = \"w0\" ) \n    z2 = ed . Gamma ( 0.1 , 0.1 , sample_shape = [ data_size , units [ 2.0 ] ] , name = \"z2\" ) \n    z1 = ed . Gamma ( shape , shape / tf . matmul ( z2 , w2 ) , name = \"z1\" ) \n    z0 = ed . Gamma ( shape , shape / tf . matmul ( z1 , w1 ) , name = \"z0\" ) \n    x = ed . Poisson ( tf . matmul ( z0 , w0 ) , name = \"x\" ) \n    return x "}
{"1019": "\ndef load_nips2011_papers ( path ) : \n    path = os . path . expanduser ( path ) \n    filename = \"NIPS_1987-2015.csv\" \n    filepath = os . path . join ( path , filename ) \n    if not os . path . exists ( filepath ) : \n        url = ( \"https://archive.ics.uci.edu/ml/machine-learning-databases/\" \"00371/NIPS_1987-2015.csv\" ) \n        if not tf . io . gfile . exists ( path ) : \n            tf . io . gfile . makedirs ( path ) \n        print ( \"Downloading %s to %s\" % ( url , filepath ) ) \n        urllib . request . urlretrieve ( url , filepath ) \n    with open ( filepath ) as f : \n        iterator = csv . reader ( f ) \n        documents = next ( iterator ) [ 1 : ] \n        words = [ ] \n        x_train = [ ] \n        for row in iterator : \n            words . append ( row [ 0 ] ) \n            x_train . append ( row [ 1 : ] ) \n    x_train = np . array ( x_train , dtype = np . int ) \n    doc_idx = [ i for i , document in enumerate ( documents ) if document . startswith ( \"2011\" ) ] \n    documents = [ documents [ doc ] for doc in doc_idx ] \n    x_train = x_train [ : , doc_idx ] \n    word_idx = np . logical_and ( np . sum ( x_train != 0 , 1 ) >= 2.0 , np . sum ( x_train , 1 ) >= 10.0 ) \n    words = [ word for word , idx in zip ( words , word_idx ) if idx ] \n    bag_of_words = x_train [ word_idx , : ] . T \n    return bag_of_words , words "}
{"1025": "\ndef create_seq ( character , action_metadata , direction , length = 8.0 , start = 0 ) : \n    sprite_start = ( action_metadata [ 0 ] + direction ) * FRAME_SIZE \n    sprite_end = ( action_metadata [ 0 ] + direction + 1 ) * FRAME_SIZE \n    sprite_line = character [ sprite_start : sprite_end , ... ] \n    frames = tf . stack ( tf . split ( sprite_line , 13.0 , axis = 1 ) ) \n    frames = frames [ 0 : action_metadata [ 1 ] ] \n    frames = tf . roll ( frames , shift = - start , axis = 0 ) \n    frames = tf . tile ( frames , [ 2.0 , 1 , 1 , 1 ] ) \n    frames = frames [ : length ] \n    frames = tf . cast ( frames , dtype = tf . float32 ) \n    frames . set_shape ( [ length , FRAME_SIZE , FRAME_SIZE , CHANNELS ] ) \n    return frames "}
{"1026": "\ndef create_random_seq ( character , action_metadata , direction , length = 8.0 ) : \n    start = tf . random . uniform ( [ ] , maxval = action_metadata [ 1 ] , dtype = tf . int32 ) \n    return create_seq ( character , action_metadata , direction , length , start ) "}
{"1027": "\ndef create_sprites_dataset ( characters , actions , directions , channels = 3.0 , length = 8.0 , shuffle = False , fake_data = False ) : \n    if fake_data : \n        dummy_image = tf . random . normal ( [ HEIGHT , WIDTH , CHANNELS ] ) \n    else : \n        basedir = download_sprites ( ) \n    action_names = [ action . name for action in actions ] \n    action_metadata = [ ( action . start_row , action . frames ) for action in actions ] \n    direction_rows = [ direction . row_offset for direction in directions ] \n    chars = tf . data . Dataset . from_tensor_slices ( characters ) \n    act_names = tf . data . Dataset . from_tensor_slices ( action_names ) . repeat ( ) \n    acts_metadata = tf . data . Dataset . from_tensor_slices ( action_metadata ) . repeat ( ) \n    dir_rows = tf . data . Dataset . from_tensor_slices ( direction_rows ) . repeat ( ) \n    if shuffle : \n        chars = chars . shuffle ( len ( characters ) ) \n    dataset = tf . data . Dataset . zip ( ( chars , act_names , acts_metadata , dir_rows ) ) \n    skin_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( SKIN_COLORS ) ) \n    hair_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( HAIRSTYLES ) ) \n    top_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( TOPS ) ) \n    pants_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( PANTS ) ) \n    action_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( action_names ) ) \n    def process_example ( attrs , act_name , act_metadata , dir_row_offset ) : \n        skin_name = attrs [ 0 ] \n        hair_name = attrs [ 1 ] \n        top_name = attrs [ 2.0 ] \n        pants_name = attrs [ 3.0 ] \n        if fake_data : \n            char = dummy_image \n        else : \n            skin = read_image ( basedir + os . sep + skin_name ) \n            hair = read_image ( basedir + os . sep + hair_name ) \n            top = read_image ( basedir + os . sep + top_name ) \n            pants = read_image ( basedir + os . sep + pants_name ) \n            char = create_character ( skin , hair , top , pants ) \n        if shuffle : \n            seq = create_random_seq ( char , act_metadata , dir_row_offset , length ) \n        else : \n            seq = create_seq ( char , act_metadata , dir_row_offset , length ) \n        seq = seq [ ... , : channels ] \n        skin_idx = skin_table . lookup ( skin_name ) \n        hair_idx = hair_table . lookup ( hair_name ) \n        top_idx = top_table . lookup ( top_name ) \n        pants_idx = pants_table . lookup ( pants_name ) \n        act_idx = action_table . lookup ( act_name ) \n        return ( seq , skin_idx , hair_idx , top_idx , pants_idx , act_idx , skin_name , hair_name , top_name , pants_name , act_name ) \n    dataset = dataset . map ( process_example ) \n    return dataset "}
{"1031": "\ndef build_fake_data ( ) : \n    num_examples = 10.0 \n    x_train = np . random . rand ( num_examples , * IMAGE_SHAPE ) . astype ( np . float32 ) \n    y_train = np . random . permutation ( np . arange ( num_examples ) ) . astype ( np . int32 ) \n    x_test = np . random . rand ( num_examples , * IMAGE_SHAPE ) . astype ( np . float32 ) \n    y_test = np . random . permutation ( np . arange ( num_examples ) ) . astype ( np . int32 ) \n    return ( x_train , y_train ) , ( x_test , y_test ) "}
{"1032": "\ndef count_integers ( arr , weights = None , minlength = None , maxlength = None , axis = None , dtype = tf . int32 , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'count_integers' , values = [ arr , weights , minlength , maxlength , axis ] ) : \n        if axis is None : \n            return tf . math . bincount ( arr , weights = weights , minlength = minlength , maxlength = maxlength , dtype = dtype ) \n        arr = tf . convert_to_tensor ( value = arr , dtype = tf . int32 , name = 'arr' ) \n        arr_ndims = _get_static_ndims ( arr , expect_static = True ) \n        axis = _make_static_axis_non_negative_list ( axis , arr_ndims ) \n        not_axis = sorted ( set ( range ( arr_ndims ) ) . difference ( axis ) ) \n        if not not_axis : \n            return tf . math . bincount ( arr , weights = weights , minlength = minlength , maxlength = maxlength , dtype = dtype ) \n        flat_arr = _move_dims_to_flat_end ( arr , not_axis , arr_ndims , right_end = False ) \n        if weights is None : \n            def one_bincount ( arr_slice ) : \n                return tf . math . bincount ( arr_slice , weights = None , minlength = minlength , maxlength = maxlength , dtype = dtype ) \n            flat_counts = tf . map_fn ( one_bincount , elems = flat_arr , dtype = dtype ) \n        else : \n            weights = tf . convert_to_tensor ( value = weights , name = 'weights' ) \n            _get_static_ndims ( weights , expect_static = True , expect_ndims = arr_ndims ) \n            flat_weights = _move_dims_to_flat_end ( weights , not_axis , arr_ndims , right_end = False ) \n            def one_bincount ( arr_and_weights_slices ) : \n                arr_slice , weights_slice = arr_and_weights_slices \n                return tf . math . bincount ( arr_slice , weights = weights_slice , minlength = minlength , maxlength = maxlength , dtype = dtype ) \n            flat_counts = tf . map_fn ( one_bincount , elems = [ flat_arr , flat_weights ] , dtype = weights . dtype ) \n        flat_counts_t = tf . transpose ( a = flat_counts , perm = [ 1 , 0 ] ) \n        _get_static_ndims ( flat_counts_t , expect_ndims = 2.0 , expect_static = True ) \n        not_axis_shape = tf . gather ( tf . shape ( input = arr ) , indices = not_axis ) \n        out_shape = tf . concat ( [ [ - 1 ] , not_axis_shape ] , axis = 0 ) \n        return tf . reshape ( flat_counts_t , out_shape ) "}
{"1033": "\ndef find_bins ( x , edges , extend_lower_interval = False , extend_upper_interval = False , dtype = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , default_name = 'find_bins' , values = [ x , edges ] ) : \n        in_type = dtype_util . common_dtype ( [ x , edges ] , preferred_dtype = tf . float32 ) \n        edges = tf . convert_to_tensor ( value = edges , name = 'edges' , dtype = in_type ) \n        x = tf . convert_to_tensor ( value = x , name = 'x' , dtype = in_type ) \n        if ( tf . compat . dimension_value ( edges . shape [ 0 ] ) is not None and tf . compat . dimension_value ( edges . shape [ 0 ] ) < 2.0 ) : \n            raise ValueError ( 'First dimension of `edges` must have length > 1 to index 1 or ' 'more bin. Found: {}' . format ( edges . shape ) ) \n        flattening_x = edges . shape . ndims == 1 and x . shape . ndims > 1 \n        if flattening_x : \n            x_orig_shape = tf . shape ( input = x ) \n            x = tf . reshape ( x , [ - 1 ] ) \n        if dtype is None : \n            dtype = in_type \n        dtype = tf . as_dtype ( dtype ) \n        x_permed = distribution_util . rotate_transpose ( x , shift = - 1 ) \n        edges_permed = distribution_util . rotate_transpose ( edges , shift = - 1 ) \n        searchsorted_type = dtype if dtype in [ tf . int32 , tf . int64 ] else None \n        almost_output_permed = tf . searchsorted ( sorted_sequence = edges_permed , values = x_permed , side = 'right' , out_type = searchsorted_type ) \n        almost_output = tf . cast ( distribution_util . rotate_transpose ( almost_output_permed , shift = 1 ) , dtype ) \n        bins = tf . clip_by_value ( almost_output - 1 , tf . cast ( 0 , dtype ) , tf . cast ( tf . shape ( input = edges ) [ 0 ] - 2.0 , dtype ) ) \n        if not extend_lower_interval : \n            low_fill = np . nan if dtype . is_floating else - 1 \n            bins = tf . where ( x < tf . expand_dims ( edges [ 0 ] , 0 ) , tf . fill ( tf . shape ( input = x ) , tf . cast ( low_fill , dtype ) ) , bins ) \n        if not extend_upper_interval : \n            up_fill = np . nan if dtype . is_floating else tf . shape ( input = edges ) [ 0 ] - 1 \n            bins = tf . where ( x > tf . expand_dims ( edges [ - 1 ] , 0 ) , tf . fill ( tf . shape ( input = x ) , tf . cast ( up_fill , dtype ) ) , bins ) \n        if flattening_x : \n            bins = tf . reshape ( bins , x_orig_shape ) \n        return bins "}
{"1035": "\ndef quantiles ( x , num_quantiles , axis = None , interpolation = None , keep_dims = False , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'quantiles' , values = [ x , num_quantiles , axis ] ) : \n        x = tf . convert_to_tensor ( value = x , name = 'x' ) \n        return percentile ( x , q = tf . linspace ( tf . convert_to_tensor ( value = 0 , dtype = tf . float64 ) , tf . convert_to_tensor ( value = 100.0 , dtype = tf . float64 ) , num = num_quantiles + 1 ) , axis = axis , interpolation = interpolation , keep_dims = keep_dims , validate_args = validate_args , preserve_gradients = False ) "}
{"1044": "\ndef jensen_shannon ( logu , self_normalized = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , \"jensen_shannon\" , [ logu ] ) : \n        logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) \n        npdt = logu . dtype . as_numpy_dtype \n        y = tf . nn . softplus ( logu ) \n        if self_normalized : \n            y -= np . log ( 2.0 ) . astype ( npdt ) \n        return tf . exp ( logu ) * logu - ( 1. + tf . exp ( logu ) ) * y "}
{"1061": "\ndef minimize ( value_and_gradients_function , initial_position , tolerance = 1e-8 , x_tolerance = 0 , f_relative_tolerance = 0 , initial_inverse_hessian_estimate = None , max_iterations = 50.0 , parallel_iterations = 1 , stopping_condition = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_position , tolerance , initial_inverse_hessian_estimate ] ) : \n        initial_position = tf . convert_to_tensor ( value = initial_position , name = 'initial_position' ) \n        dtype = initial_position . dtype . base_dtype \n        tolerance = tf . convert_to_tensor ( value = tolerance , dtype = dtype , name = 'grad_tolerance' ) \n        f_relative_tolerance = tf . convert_to_tensor ( value = f_relative_tolerance , dtype = dtype , name = 'f_relative_tolerance' ) \n        x_tolerance = tf . convert_to_tensor ( value = x_tolerance , dtype = dtype , name = 'x_tolerance' ) \n        max_iterations = tf . convert_to_tensor ( value = max_iterations , name = 'max_iterations' ) \n        input_shape = distribution_util . prefer_static_shape ( initial_position ) \n        batch_shape , domain_size = input_shape [ : - 1 ] , input_shape [ - 1 ] \n        if stopping_condition is None : \n            stopping_condition = bfgs_utils . converged_all \n        control_inputs = None \n        if initial_inverse_hessian_estimate is None : \n            initial_inv_hessian = tf . eye ( domain_size , batch_shape = batch_shape , dtype = dtype , name = 'initial_inv_hessian' ) \n        else : \n            initial_inv_hessian = tf . convert_to_tensor ( value = initial_inverse_hessian_estimate , dtype = dtype , name = 'initial_inv_hessian' ) \n            control_inputs = _inv_hessian_control_inputs ( initial_inv_hessian ) \n            hessian_shape = tf . concat ( [ batch_shape , [ domain_size , domain_size ] ] , 0 ) \n            initial_inv_hessian = tf . broadcast_to ( initial_inv_hessian , hessian_shape ) \n        def _cond ( state ) : \n            return ( ( state . num_iterations < max_iterations ) & tf . logical_not ( stopping_condition ( state . converged , state . failed ) ) ) \n        def _body ( state ) : \n            search_direction = _get_search_direction ( state . inverse_hessian_estimate , state . objective_gradient ) \n            derivative_at_start_pt = tf . reduce_sum ( input_tensor = state . objective_gradient * search_direction , axis = - 1 ) \n            needs_reset = ( ~ state . failed & ~ state . converged & ( derivative_at_start_pt >= 0 ) ) \n            search_direction_reset = _get_search_direction ( initial_inv_hessian , state . objective_gradient ) \n            actual_serch_direction = tf . where ( needs_reset , search_direction_reset , search_direction ) \n            actual_inv_hessian = tf . where ( needs_reset , initial_inv_hessian , state . inverse_hessian_estimate ) \n            current_state = bfgs_utils . update_fields ( state , inverse_hessian_estimate = actual_inv_hessian ) \n            next_state = bfgs_utils . line_search_step ( current_state , value_and_gradients_function , actual_serch_direction , tolerance , f_relative_tolerance , x_tolerance , stopping_condition ) \n            return [ _update_inv_hessian ( current_state , next_state ) ] \n        kwargs = bfgs_utils . get_initial_state_args ( value_and_gradients_function , initial_position , tolerance , control_inputs ) \n        kwargs [ 'inverse_hessian_estimate' ] = initial_inv_hessian \n        initial_state = BfgsOptimizerResults ( ** kwargs ) \n        return tf . while_loop ( cond = _cond , body = _body , loop_vars = [ initial_state ] , parallel_iterations = parallel_iterations ) [ 0 ] "}
{"1062": "\ndef _inv_hessian_control_inputs ( inv_hessian ) : \n    is_positive_definite = tf . reduce_all ( input_tensor = tf . math . is_finite ( tf . linalg . cholesky ( inv_hessian ) ) , axis = [ - 1 , - 2.0 ] ) \n    is_symmetric = tf . equal ( bfgs_utils . norm ( inv_hessian - _batch_transpose ( inv_hessian ) , dims = 2.0 ) , 0 ) \n    return [ tf . Assert ( is_positive_definite , [ 'Initial inverse Hessian is not positive definite.' , inv_hessian ] ) , tf . Assert ( is_symmetric , [ 'Initial inverse Hessian is not symmetric' , inv_hessian ] ) ] "}
{"1066": "\ndef _tensor_product ( t1 , t2 ) : \n    return tf . matmul ( tf . expand_dims ( t1 , axis = - 1 ) , tf . expand_dims ( t2 , axis = - 2.0 ) ) "}
{"1067": "\ndef _batch_transpose ( mat ) : \n    n = distribution_util . prefer_static_rank ( mat ) \n    perm = tf . range ( n ) \n    perm = tf . concat ( [ perm [ : - 2.0 ] , [ perm [ - 1 ] , perm [ - 2.0 ] ] ] , axis = 0 ) \n    return tf . transpose ( a = mat , perm = perm ) "}
{"1072": "\ndef minimize ( value_and_gradients_function , initial_position , num_correction_pairs = 10.0 , tolerance = 1e-8 , x_tolerance = 0 , f_relative_tolerance = 0 , initial_inverse_hessian_estimate = None , max_iterations = 50.0 , parallel_iterations = 1 , stopping_condition = None , name = None ) : \n    if initial_inverse_hessian_estimate is not None : \n        raise NotImplementedError ( 'Support of initial_inverse_hessian_estimate arg not yet implemented' ) \n    if stopping_condition is None : \n        stopping_condition = bfgs_utils . converged_all \n    with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_position , tolerance ] ) : \n        initial_position = tf . convert_to_tensor ( value = initial_position , name = 'initial_position' ) \n        dtype = initial_position . dtype . base_dtype \n        tolerance = tf . convert_to_tensor ( value = tolerance , dtype = dtype , name = 'grad_tolerance' ) \n        f_relative_tolerance = tf . convert_to_tensor ( value = f_relative_tolerance , dtype = dtype , name = 'f_relative_tolerance' ) \n        x_tolerance = tf . convert_to_tensor ( value = x_tolerance , dtype = dtype , name = 'x_tolerance' ) \n        max_iterations = tf . convert_to_tensor ( value = max_iterations , name = 'max_iterations' ) \n        def _cond ( state ) : \n            return ( ( state . num_iterations < max_iterations ) & tf . logical_not ( stopping_condition ( state . converged , state . failed ) ) ) \n        def _body ( current_state ) : \n            search_direction = _get_search_direction ( current_state ) \n            next_state = bfgs_utils . line_search_step ( current_state , value_and_gradients_function , search_direction , tolerance , f_relative_tolerance , x_tolerance , stopping_condition ) \n            should_update = ~ ( next_state . converged | next_state . failed ) \n            state_after_inv_hessian_update = bfgs_utils . update_fields ( next_state , position_deltas = _queue_push ( current_state . position_deltas , should_update , next_state . position - current_state . position ) , gradient_deltas = _queue_push ( current_state . gradient_deltas , should_update , next_state . objective_gradient - current_state . objective_gradient ) ) \n            return [ state_after_inv_hessian_update ] \n        initial_state = _get_initial_state ( value_and_gradients_function , initial_position , num_correction_pairs , tolerance ) \n        return tf . while_loop ( cond = _cond , body = _body , loop_vars = [ initial_state ] , parallel_iterations = parallel_iterations ) [ 0 ] "}
{"1079": "\ndef _uniform_correlation_like_matrix ( num_rows , batch_shape , dtype , seed ) : \n    num_entries = num_rows * ( num_rows + 1 ) / 2.0 \n    ones = tf . ones ( shape = [ num_entries ] , dtype = dtype ) \n    unifs = uniform . Uniform ( - ones , ones ) . sample ( batch_shape , seed = seed ) \n    tril = util . fill_triangular ( unifs ) \n    symmetric = tril + tf . linalg . matrix_transpose ( tril ) \n    diagonal_ones = tf . ones ( shape = util . pad ( batch_shape , axis = 0 , back = True , value = num_rows ) , dtype = dtype ) \n    return tf . linalg . set_diag ( symmetric , diagonal_ones ) "}
{"1081": "\ndef _clopper_pearson_confidence_interval ( samples , error_rate ) : \n    if optimize is None or stats is None : \n        raise ValueError ( \"Scipy is required for computing Clopper-Pearson confidence intervals\" ) \n    if len ( samples . shape ) != 1 : \n        raise ValueError ( \"Batch semantics not implemented\" ) \n    n = len ( samples ) \n    low = np . amin ( samples ) \n    high = np . amax ( samples ) \n    successes = np . count_nonzero ( samples - low ) \n    failures = np . count_nonzero ( samples - high ) \n    if successes + failures != n : \n        uniques = np . unique ( samples ) \n        msg = ( \"Purportedly Bernoulli distribution had distinct samples\" \" {}, {}, and {}\" . format ( uniques [ 0 ] , uniques [ 1 ] , uniques [ 2.0 ] ) ) \n        raise ValueError ( msg ) \n    def p_small_enough ( p ) : \n        prob = stats . binom . logcdf ( successes , n , p ) \n        return prob - np . log ( error_rate / 2. ) \n    def p_big_enough ( p ) : \n        prob = stats . binom . logsf ( successes , n , p ) \n        return prob - np . log ( error_rate / 2. ) \n    high_p = optimize . brentq ( p_small_enough , float ( successes ) / n , 1. , rtol = 1e-9 ) \n    low_p = optimize . brentq ( p_big_enough , 0. , float ( successes ) / n , rtol = 1e-9 ) \n    low_interval = low + ( high - low ) * low_p \n    high_interval = low + ( high - low ) * high_p \n    return ( low_interval , high_interval ) "}
{"1082": "\ndef compute_true_volumes ( det_bounds , dim , num_samples , error_rate = 1e-6 , seed = 42.0 ) : \n    bounds = { } \n    with tf . compat . v1 . Session ( ) as sess : \n        rej_weights , _ = correlation_matrix_volume_rejection_samples ( det_bounds , dim , [ num_samples , len ( det_bounds ) ] , np . float32 , seed = seed ) \n        rej_weights = sess . run ( rej_weights ) \n        for rw , det in zip ( np . rollaxis ( rej_weights , 1 ) , det_bounds ) : \n            template = ( \"Estimating volume of {}x{} correlation \" \"matrices with determinant >= {}.\" ) \n            print ( template . format ( dim , dim , det ) ) \n            sys . stdout . flush ( ) \n            bounds [ det ] = _clopper_pearson_confidence_interval ( rw , error_rate = error_rate ) \n        return bounds "}
{"1083": "\ndef _von_mises_cdf_series ( x , concentration , num_terms , dtype ) : \n    num_terms = tf . cast ( num_terms , dtype = dtype ) \n    def loop_body ( n , rn , drn_dconcentration , vn , dvn_dconcentration ) : \n        denominator = 2. * n / concentration + rn \n        ddenominator_dk = - 2. * n / concentration ** 2.0 + drn_dconcentration \n        rn = 1. / denominator \n        drn_dconcentration = - ddenominator_dk / denominator ** 2.0 \n        multiplier = tf . sin ( n * x ) / n + vn \n        vn = rn * multiplier \n        dvn_dconcentration = ( drn_dconcentration * multiplier + rn * dvn_dconcentration ) \n        n -= 1. \n        return n , rn , drn_dconcentration , vn , dvn_dconcentration \n    ( _ , _ , _ , vn , dvn_dconcentration ) = tf . while_loop ( cond = lambda n , * _ : n > 0. , body = loop_body , loop_vars = ( num_terms , tf . zeros_like ( x , name = \"rn\" ) , tf . zeros_like ( x , name = \"drn_dconcentration\" ) , tf . zeros_like ( x , name = \"vn\" ) , tf . zeros_like ( x , name = \"dvn_dconcentration\" ) , ) , ) \n    cdf = .5 + x / ( 2. * np . pi ) + vn / np . pi \n    dcdf_dconcentration = dvn_dconcentration / np . pi \n    cdf_clipped = tf . clip_by_value ( cdf , 0. , 1. ) \n    dcdf_dconcentration *= tf . cast ( ( cdf >= 0. ) & ( cdf <= 1. ) , dtype ) \n    return cdf_clipped , dcdf_dconcentration "}
{"1084": "\ndef _von_mises_cdf_normal ( x , concentration , dtype ) : \n    def cdf_func ( concentration ) : \n        z = ( ( np . sqrt ( 2. / np . pi ) / tf . math . bessel_i0e ( concentration ) ) * tf . sin ( .5 * x ) ) \n        z2 = z ** 2.0 \n        z3 = z2 * z \n        z4 = z2 ** 2.0 \n        c = 24. * concentration \n        c1 = 56. \n        xi = z - z3 / ( ( c - 2. * z2 - 16. ) / 3. - ( z4 + ( 7. / 4. ) * z2 + 167. / 2. ) / ( c - c1 - z2 + 3. ) ) ** 2.0 \n        distrib = normal . Normal ( tf . cast ( 0. , dtype ) , tf . cast ( 1. , dtype ) ) \n        return distrib . cdf ( xi ) \n    return value_and_gradient ( cdf_func , concentration ) "}
{"1086": "\ndef minimize ( objective_function , initial_population = None , initial_position = None , population_size = 50.0 , population_stddev = 1. , max_iterations = 100.0 , func_tolerance = 0 , position_tolerance = 1e-8 , differential_weight = 0.5 , crossover_prob = 0.9 , seed = None , name = None ) : \n    if initial_population is None and initial_position is None : \n        raise ValueError ( 'Either the initial population or the initial position ' 'must be specified.' ) \n    if initial_population is not None and initial_position is not None : \n        raise ValueError ( 'Only one of initial population or initial position ' 'should be specified' ) \n    with tf . compat . v1 . name_scope ( name , default_name = 'minimize' , values = [ initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ] ) : \n        ( was_iterable , population , population_values , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ) = _get_initial_args ( objective_function , initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob , seed ) \n        def evolve_body ( loop_vars ) : \n            next_population , next_population_values = one_step ( objective_function , loop_vars . population , population_values = loop_vars . population_values , differential_weight = differential_weight , crossover_prob = crossover_prob , seed = seed ) \n            converged = _check_convergence ( next_population , next_population_values , func_tolerance , position_tolerance ) \n            failed = _check_failure ( next_population_values ) \n            return [ _MinimizeLoopVars ( converged = converged , failed = failed , num_iterations = loop_vars . num_iterations + 1 , population = next_population , population_values = next_population_values ) ] \n        def evolve_cond ( loop_vars ) : \n            should_stop = ( loop_vars . failed | loop_vars . converged | ( max_iterations is not None and loop_vars . num_iterations >= max_iterations ) ) \n            return ~ should_stop \n        initial_vars = _MinimizeLoopVars ( converged = tf . convert_to_tensor ( value = False ) , failed = tf . convert_to_tensor ( value = False ) , num_iterations = tf . convert_to_tensor ( value = 0 ) , population = population , population_values = population_values ) \n        final_state = tf . while_loop ( cond = evolve_cond , body = evolve_body , loop_vars = ( initial_vars , ) ) [ 0 ] \n        best_position , best_values = _find_best_in_population ( final_state . population , final_state . population_values ) \n        final_population = final_state . population \n        if not was_iterable : \n            final_population = final_population [ 0 ] \n            best_position = best_position [ 0 ] \n        return DifferentialEvolutionOptimizerResults ( converged = final_state . converged , failed = final_state . failed , position = best_position , objective_value = best_values , final_population = final_population , final_objective_values = final_state . population_values , initial_population = population , initial_objective_values = population_values , num_iterations = final_state . num_iterations ) "}
{"1089": "\ndef _check_convergence ( population , population_values , func_tolerance , position_tolerance ) : \n    value_range = tf . math . abs ( tf . math . reduce_max ( input_tensor = population_values ) - tf . math . reduce_min ( input_tensor = population_values ) ) \n    value_converged = value_range <= func_tolerance \n    half_tol = position_tolerance / 2.0 \n    def part_converged ( part ) : \n        return tf . math . reduce_max ( input_tensor = tf . math . abs ( part - part [ 0 ] ) ) <= half_tol \n    x_converged = tf . math . reduce_all ( input_tensor = [ part_converged ( part ) for part in population ] ) \n    return value_converged | x_converged "}
{"1092": "\ndef _get_mutants ( population , population_size , mixing_indices , differential_weight ) : \n    mixing_indices = tf . reshape ( mixing_indices , [ - 1 ] ) \n    weights = tf . stack ( [ 1.0 , differential_weight , - differential_weight ] ) \n    def _mutant_part ( population_part ) : \n        donors = tf . gather ( population_part , mixing_indices ) \n        donors = tf . transpose ( a = tf . reshape ( donors , [ population_size , 3.0 , - 1 ] ) , perm = [ 0 , 2.0 , 1 ] ) \n        return tf . math . reduce_sum ( input_tensor = donors * weights , axis = - 1 ) \n    return [ _mutant_part ( population_part ) for population_part in population ] "}
{"1093": "\ndef _get_mixing_indices ( size , seed = None , name = None ) : \n    with tf . compat . v1 . name_scope ( name , default_name = 'get_mixing_indices' , values = [ size ] ) : \n        size = tf . convert_to_tensor ( value = size ) \n        dtype = size . dtype \n        seed_stream = distributions . SeedStream ( seed , salt = 'get_mixing_indices' ) \n        first = tf . random . uniform ( [ size ] , maxval = size - 1 , dtype = dtype , seed = seed_stream ( ) ) \n        second = tf . random . uniform ( [ size ] , maxval = size - 2.0 , dtype = dtype , seed = seed_stream ( ) ) \n        third = tf . random . uniform ( [ size ] , maxval = size - 3.0 , dtype = dtype , seed = seed_stream ( ) ) \n        second = tf . where ( first < second , x = second , y = second + 1 ) \n        smaller = tf . math . minimum ( first , second ) \n        larger = tf . math . maximum ( first , second ) \n        third = tf . where ( third < smaller , x = third , y = third + 1 ) \n        third = tf . where ( third < larger , x = third , y = third + 1 ) \n        sample = tf . stack ( [ first , second , third ] , axis = 1 ) \n        to_avoid = tf . expand_dims ( tf . range ( size ) , axis = - 1 ) \n        sample = tf . where ( sample < to_avoid , x = sample , y = sample + 1 ) \n        return sample "}
{"1098": "\ndef build_input_pipeline ( train_images , batch_size ) : \n    training_dataset = tf . data . Dataset . from_tensor_slices ( train_images ) \n    training_batches = training_dataset . shuffle ( 50000.0 , reshuffle_each_iteration = True ) . repeat ( ) . batch ( batch_size ) \n    training_iterator = tf . compat . v1 . data . make_one_shot_iterator ( training_batches ) \n    images = training_iterator . get_next ( ) \n    return images "}
{"1099": "\ndef plot_generated_images ( images , fname ) : \n    fig = plt . figure ( figsize = ( 4.0 , 4.0 ) ) \n    canvas = backend_agg . FigureCanvasAgg ( fig ) \n    for i , image in enumerate ( images ) : \n        ax = fig . add_subplot ( 4.0 , 4.0 , i + 1 ) \n        plt . axis ( 'off' ) \n        ax . set_xticklabels ( [ ] ) \n        ax . set_yticklabels ( [ ] ) \n        ax . imshow ( image . reshape ( IMAGE_SHAPE [ : - 1 ] ) , cmap = 'Greys_r' ) \n    fig . tight_layout ( ) \n    plt . subplots_adjust ( wspace = 0.05 , hspace = 0.05 ) \n    canvas . print_figure ( fname , format = 'png' ) "}
{"1105": "\ndef matrix_rank ( a , tol = None , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'matrix_rank' , [ a , tol ] ) : \n        a = tf . convert_to_tensor ( value = a , dtype_hint = tf . float32 , name = 'a' ) \n        assertions = _maybe_validate_matrix ( a , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                a = tf . identity ( a ) \n        s = tf . linalg . svd ( a , compute_uv = False ) \n        if tol is None : \n            if a . shape [ - 2.0 : ] . is_fully_defined ( ) : \n                m = np . max ( a . shape [ - 2.0 : ] . as_list ( ) ) \n            else : \n                m = tf . reduce_max ( input_tensor = tf . shape ( input = a ) [ - 2.0 : ] ) \n            eps = np . finfo ( a . dtype . as_numpy_dtype ) . eps \n            tol = ( eps * tf . cast ( m , a . dtype ) * tf . reduce_max ( input_tensor = s , axis = - 1 , keepdims = True ) ) \n        return tf . reduce_sum ( input_tensor = tf . cast ( s > tol , tf . int32 ) , axis = - 1 ) "}
{"1106": "\ndef pinv ( a , rcond = None , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'pinv' , [ a , rcond ] ) : \n        a = tf . convert_to_tensor ( value = a , name = 'a' ) \n        assertions = _maybe_validate_matrix ( a , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                a = tf . identity ( a ) \n        dtype = a . dtype . as_numpy_dtype \n        if rcond is None : \n            def get_dim_size ( dim ) : \n                if tf . compat . dimension_value ( a . shape [ dim ] ) is not None : \n                    return tf . compat . dimension_value ( a . shape [ dim ] ) \n                return tf . shape ( input = a ) [ dim ] \n            num_rows = get_dim_size ( - 2.0 ) \n            num_cols = get_dim_size ( - 1 ) \n            if isinstance ( num_rows , int ) and isinstance ( num_cols , int ) : \n                max_rows_cols = float ( max ( num_rows , num_cols ) ) \n            else : \n                max_rows_cols = tf . cast ( tf . maximum ( num_rows , num_cols ) , dtype ) \n            rcond = 10. * max_rows_cols * np . finfo ( dtype ) . eps \n        rcond = tf . convert_to_tensor ( value = rcond , dtype = dtype , name = 'rcond' ) \n        [ singular_values , left_singular_vectors , right_singular_vectors , ] = tf . linalg . svd ( a , full_matrices = False , compute_uv = True ) \n        cutoff = rcond * tf . reduce_max ( input_tensor = singular_values , axis = - 1 ) \n        singular_values = tf . where ( singular_values > cutoff [ ... , tf . newaxis ] , singular_values , tf . fill ( tf . shape ( input = singular_values ) , np . array ( np . inf , dtype ) ) ) \n        a_pinv = tf . matmul ( right_singular_vectors / singular_values [ ... , tf . newaxis , : ] , left_singular_vectors , adjoint_b = True ) \n        if a . shape . ndims is not None : \n            a_pinv . set_shape ( a . shape [ : - 2.0 ] . concatenate ( [ a . shape [ - 1 ] , a . shape [ - 2.0 ] ] ) ) \n        return a_pinv "}
{"1107": "\ndef lu_solve ( lower_upper , perm , rhs , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'lu_solve' , [ lower_upper , perm , rhs ] ) : \n        lower_upper = tf . convert_to_tensor ( value = lower_upper , dtype_hint = tf . float32 , name = 'lower_upper' ) \n        perm = tf . convert_to_tensor ( value = perm , dtype_hint = tf . int32 , name = 'perm' ) \n        rhs = tf . convert_to_tensor ( value = rhs , dtype_hint = lower_upper . dtype , name = 'rhs' ) \n        assertions = _lu_solve_assertions ( lower_upper , perm , rhs , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                lower_upper = tf . identity ( lower_upper ) \n                perm = tf . identity ( perm ) \n                rhs = tf . identity ( rhs ) \n        if rhs . shape . ndims == 2.0 and perm . shape . ndims == 1 : \n            permuted_rhs = tf . gather ( rhs , perm , axis = - 2.0 ) \n        else : \n            rhs_shape = tf . shape ( input = rhs ) \n            broadcast_batch_shape = tf . broadcast_dynamic_shape ( rhs_shape [ : - 2.0 ] , tf . shape ( input = perm ) [ : - 1 ] ) \n            d , m = rhs_shape [ - 2.0 ] , rhs_shape [ - 1 ] \n            rhs_broadcast_shape = tf . concat ( [ broadcast_batch_shape , [ d , m ] ] , axis = 0 ) \n            broadcast_rhs = tf . broadcast_to ( rhs , rhs_broadcast_shape ) \n            broadcast_rhs = tf . reshape ( broadcast_rhs , [ - 1 , d , m ] ) \n            broadcast_perm = tf . broadcast_to ( perm , rhs_broadcast_shape [ : - 1 ] ) \n            broadcast_perm = tf . reshape ( broadcast_perm , [ - 1 , d ] ) \n            broadcast_batch_size = tf . reduce_prod ( input_tensor = broadcast_batch_shape ) \n            broadcast_batch_indices = tf . broadcast_to ( tf . range ( broadcast_batch_size ) [ : , tf . newaxis ] , [ broadcast_batch_size , d ] ) \n            broadcast_perm = tf . stack ( [ broadcast_batch_indices , broadcast_perm ] , axis = - 1 ) \n            permuted_rhs = tf . gather_nd ( broadcast_rhs , broadcast_perm ) \n            permuted_rhs = tf . reshape ( permuted_rhs , rhs_broadcast_shape ) \n        lower = tf . linalg . set_diag ( tf . linalg . band_part ( lower_upper , num_lower = - 1 , num_upper = 0 ) , tf . ones ( tf . shape ( input = lower_upper ) [ : - 1 ] , dtype = lower_upper . dtype ) ) \n        return linear_operator_util . matrix_triangular_solve_with_broadcast ( lower_upper , linear_operator_util . matrix_triangular_solve_with_broadcast ( lower , permuted_rhs ) , lower = False ) "}
{"1108": "\ndef lu_matrix_inverse ( lower_upper , perm , validate_args = False , name = None ) : \n    with tf . compat . v1 . name_scope ( name , 'lu_matrix_inverse' , [ lower_upper , perm ] ) : \n        lower_upper = tf . convert_to_tensor ( value = lower_upper , dtype_hint = tf . float32 , name = 'lower_upper' ) \n        perm = tf . convert_to_tensor ( value = perm , dtype_hint = tf . int32 , name = 'perm' ) \n        assertions = _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) \n        if assertions : \n            with tf . control_dependencies ( assertions ) : \n                lower_upper = tf . identity ( lower_upper ) \n                perm = tf . identity ( perm ) \n        shape = tf . shape ( input = lower_upper ) \n        return lu_solve ( lower_upper , perm , rhs = tf . eye ( shape [ - 1 ] , batch_shape = shape [ : - 2.0 ] , dtype = lower_upper . dtype ) , validate_args = False ) "}
{"1109": "\ndef _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) : \n    assertions = [ ] \n    message = 'Input `lower_upper` must have at least 2 dimensions.' \n    if lower_upper . shape . ndims is not None : \n        if lower_upper . shape . ndims < 2.0 : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank_at_least ( lower_upper , rank = 2.0 , message = message ) ) \n    message = '`rank(lower_upper)` must equal `rank(perm) + 1`' \n    if lower_upper . shape . ndims is not None and perm . shape . ndims is not None : \n        if lower_upper . shape . ndims != perm . shape . ndims + 1 : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank ( lower_upper , rank = tf . rank ( perm ) + 1 , message = message ) ) \n    message = '`lower_upper` must be square.' \n    if lower_upper . shape [ : - 2.0 ] . is_fully_defined ( ) : \n        if lower_upper . shape [ - 2.0 ] != lower_upper . shape [ - 1 ] : \n            raise ValueError ( message ) \n    elif validate_args : \n        m , n = tf . split ( tf . shape ( input = lower_upper ) [ - 2.0 : ] , num_or_size_splits = 2.0 ) \n        assertions . append ( tf . compat . v1 . assert_equal ( m , n , message = message ) ) \n    return assertions "}
{"1110": "\ndef _lu_solve_assertions ( lower_upper , perm , rhs , validate_args ) : \n    assertions = _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) \n    message = 'Input `rhs` must have at least 2 dimensions.' \n    if rhs . shape . ndims is not None : \n        if rhs . shape . ndims < 2.0 : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank_at_least ( rhs , rank = 2.0 , message = message ) ) \n    message = '`lower_upper.shape[-1]` must equal `rhs.shape[-1]`.' \n    if ( tf . compat . dimension_value ( lower_upper . shape [ - 1 ] ) is not None and tf . compat . dimension_value ( rhs . shape [ - 2.0 ] ) is not None ) : \n        if lower_upper . shape [ - 1 ] != rhs . shape [ - 2.0 ] : \n            raise ValueError ( message ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_equal ( tf . shape ( input = lower_upper ) [ - 1 ] , tf . shape ( input = rhs ) [ - 2.0 ] , message = message ) ) \n    return assertions "}
{"1111": "\ndef _sparse_block_diag ( sp_a ) : \n    sp_a_shape = tf . convert_to_tensor ( value = _get_shape ( sp_a , tf . int64 ) ) \n    ind_mat = tf . concat ( [ [ sp_a_shape [ - 2.0 : ] ] , tf . eye ( 2.0 , dtype = tf . int64 ) ] , axis = 0 ) \n    indices = tf . matmul ( sp_a . indices , ind_mat ) \n    dense_shape = sp_a_shape [ 0 ] * sp_a_shape [ 1 : ] \n    return tf . SparseTensor ( indices = indices , values = sp_a . values , dense_shape = dense_shape ) "}
{"1112": "\ndef _maybe_validate_matrix ( a , validate_args ) : \n    assertions = [ ] \n    if not a . dtype . is_floating : \n        raise TypeError ( 'Input `a` must have `float`-like `dtype` ' '(saw {}).' . format ( a . dtype . name ) ) \n    if a . shape . ndims is not None : \n        if a . shape . ndims < 2.0 : \n            raise ValueError ( 'Input `a` must have at least 2 dimensions ' '(saw: {}).' . format ( a . shape . ndims ) ) \n    elif validate_args : \n        assertions . append ( tf . compat . v1 . assert_rank_at_least ( a , rank = 2.0 , message = 'Input `a` must have at least 2 dimensions.' ) ) \n    return assertions "}
{"1113": "\ndef _grad_neg_log_likelihood_and_fim ( model_matrix , linear_response , response , model ) : \n    mean , variance , grad_mean = model ( linear_response ) \n    is_valid = ( tf . math . is_finite ( grad_mean ) & tf . not_equal ( grad_mean , 0. ) & tf . math . is_finite ( variance ) & ( variance > 0. ) ) \n    def _mask_if_invalid ( x , mask ) : \n        mask = tf . fill ( tf . shape ( input = x ) , value = np . array ( mask , x . dtype . as_numpy_dtype ) ) \n        return tf . where ( is_valid , x , mask ) \n    v = ( response - mean ) * _mask_if_invalid ( grad_mean , 1 ) / _mask_if_invalid ( variance , np . inf ) \n    grad_log_likelihood = sparse_or_dense_matvecmul ( model_matrix , v , adjoint_a = True ) \n    fim_middle = _mask_if_invalid ( grad_mean , 0. ) ** 2.0 / _mask_if_invalid ( variance , np . inf ) \n    return - grad_log_likelihood , fim_middle "}
{"1124": "\ndef draw_sample ( num_samples , num_classes , logits , num_trials , dtype , seed ) : \n    with tf . name_scope ( \"multinomial.draw_sample\" ) : \n        num_trials = tf . ones_like ( logits [ ... , 0 ] , dtype = num_trials . dtype ) * num_trials \n        logits = tf . ones_like ( num_trials [ ... , tf . newaxis ] , dtype = logits . dtype ) * logits \n        flat_logits = tf . reshape ( logits , [ - 1 , num_classes ] ) \n        flat_num_trials = num_samples * tf . reshape ( num_trials , [ - 1 ] ) \n        def _sample_one_batch_member ( args ) : \n            logits , num_cat_samples = args [ 0 ] , args [ 1 ] \n            x = tf . random . categorical ( logits [ tf . newaxis , ... ] , num_cat_samples , seed = seed ) \n            x = tf . reshape ( x , shape = [ num_samples , - 1 ] ) \n            x = tf . one_hot ( x , depth = num_classes ) \n            x = tf . reduce_sum ( input_tensor = x , axis = - 2.0 ) \n            return tf . cast ( x , dtype = dtype ) \n        x = tf . map_fn ( _sample_one_batch_member , [ flat_logits , flat_num_trials ] , dtype = dtype ) \n        x = tf . transpose ( a = x , perm = [ 1 , 0 , 2.0 ] ) \n        final_shape = tf . concat ( [ [ num_samples ] , tf . shape ( input = num_trials ) , [ num_classes ] ] , axis = 0 ) \n        x = tf . reshape ( x , final_shape ) \n        return x "}
{"1132": "\ndef variational_loss ( self , observations , observation_index_points = None , kl_weight = 1. , name = 'variational_loss' ) : \n    with tf . name_scope ( name or 'variational_gp_loss' ) : \n        if observation_index_points is None : \n            observation_index_points = self . _index_points \n        observation_index_points = tf . convert_to_tensor ( value = observation_index_points , dtype = self . _dtype , name = 'observation_index_points' ) \n        observations = tf . convert_to_tensor ( value = observations , dtype = self . _dtype , name = 'observations' ) \n        kl_weight = tf . convert_to_tensor ( value = kl_weight , dtype = self . _dtype , name = 'kl_weight' ) \n        kzx = self . kernel . matrix ( self . _inducing_index_points , observation_index_points ) \n        kzx_linop = tf . linalg . LinearOperatorFullMatrix ( kzx ) \n        loc = ( self . _mean_fn ( observation_index_points ) + kzx_linop . matvec ( self . _kzz_inv_varloc , adjoint = True ) ) \n        likelihood = independent . Independent ( normal . Normal ( loc = loc , scale = tf . sqrt ( self . _observation_noise_variance + self . _jitter ) , name = 'NormalLikelihood' ) , reinterpreted_batch_ndims = 1 ) \n        obs_ll = likelihood . log_prob ( observations ) \n        chol_kzz_linop = tf . linalg . LinearOperatorLowerTriangular ( self . _chol_kzz ) \n        chol_kzz_inv_kzx = chol_kzz_linop . solve ( kzx ) \n        kzz_inv_kzx = chol_kzz_linop . solve ( chol_kzz_inv_kzx , adjoint = True ) \n        kxx_diag = tf . linalg . diag_part ( self . kernel . matrix ( observation_index_points , observation_index_points ) ) \n        ktilde_trace_term = ( tf . reduce_sum ( input_tensor = kxx_diag , axis = - 1 ) - tf . reduce_sum ( input_tensor = chol_kzz_inv_kzx ** 2.0 , axis = [ - 2.0 , - 1 ] ) ) \n        other_trace_term = tf . reduce_sum ( input_tensor = ( self . _variational_inducing_observations_posterior . scale . matmul ( kzz_inv_kzx ) ** 2.0 ) , axis = [ - 2.0 , - 1 ] ) \n        trace_term = ( .5 * ( ktilde_trace_term + other_trace_term ) / self . _observation_noise_variance ) \n        inducing_prior = gaussian_process . GaussianProcess ( kernel = self . _kernel , mean_fn = self . _mean_fn , index_points = self . _inducing_index_points , observation_noise_variance = self . _observation_noise_variance ) \n        kl_term = kl_weight * kullback_leibler . kl_divergence ( self . _variational_inducing_observations_posterior , inducing_prior ) \n        lower_bound = ( obs_ll - trace_term - kl_term ) \n        return - tf . reduce_mean ( input_tensor = lower_bound ) "}
{"1138": "\ndef build_constrained_seasonal_transition_noise ( drift_scale , num_seasons , is_last_day_of_season ) : \n    drift_scale_tril_nonzeros = tf . concat ( [ tf . ones ( [ num_seasons - 1 , 1 ] , dtype = drift_scale . dtype ) , tf . zeros ( [ num_seasons - 1 , num_seasons - 2.0 ] , dtype = drift_scale . dtype ) ] , axis = - 1 ) \n    drift_scale_tril = ( drift_scale_tril_nonzeros * drift_scale [ ... , tf . newaxis , tf . newaxis ] / num_seasons ) \n    def seasonal_transition_noise ( t ) : \n        noise_scale_tril = dist_util . pick_scalar_condition ( is_last_day_of_season ( t ) , drift_scale_tril , tf . zeros_like ( drift_scale_tril ) ) \n        return tfd . MultivariateNormalTriL ( loc = tf . zeros ( num_seasons - 1 , dtype = drift_scale . dtype ) , scale_tril = noise_scale_tril ) \n    return seasonal_transition_noise "}
{"1147": "\ndef create ( model , training_set , criterion , end_trigger = None , batch_size = 32.0 , optim_method = None , cores = None , bigdl_type = \"float\" ) : \n    if not end_trigger : \n        end_trigger = MaxEpoch ( 1 ) \n    if not optim_method : \n        optim_method = SGD ( ) \n    if isinstance ( training_set , RDD ) or isinstance ( training_set , DataSet ) : \n        return DistriOptimizer ( model = model , training_rdd = training_set , criterion = criterion , end_trigger = end_trigger , batch_size = batch_size , optim_method = optim_method , bigdl_type = bigdl_type ) \n    elif isinstance ( training_set , tuple ) and len ( training_set ) == 2.0 : \n        x , y = training_set \n        return LocalOptimizer ( X = x , Y = y , model = model , criterion = criterion , end_trigger = end_trigger , batch_size = batch_size , optim_method = optim_method , cores = cores , bigdl_type = \"float\" ) \n    else : \n        raise Exception ( \"Not supported training set: %s\" % type ( training_set ) ) "}
{"1151": "\ndef get_news20 ( source_dir = \"./data/news20/\" ) : \n    news_dir = download_news20 ( source_dir ) \n    texts = [ ] \n    label_id = 0 \n    for name in sorted ( os . listdir ( news_dir ) ) : \n        path = os . path . join ( news_dir , name ) \n        label_id += 1 \n        if os . path . isdir ( path ) : \n            for fname in sorted ( os . listdir ( path ) ) : \n                if fname . isdigit ( ) : \n                    fpath = os . path . join ( path , fname ) \n                    if sys . version_info < ( 3.0 , ) : \n                        f = open ( fpath ) \n                    else : \n                        f = open ( fpath , encoding = 'latin-1' ) \n                    content = f . read ( ) \n                    texts . append ( ( content , label_id ) ) \n                    f . close ( ) \n    print ( 'Found %s texts.' % len ( texts ) ) \n    return texts "}
{"1152": "\ndef get_glove_w2v ( source_dir = \"./data/news20/\" , dim = 100.0 ) : \n    w2v_dir = download_glove_w2v ( source_dir ) \n    w2v_path = os . path . join ( w2v_dir , \"glove.6B.%sd.txt\" % dim ) \n    if sys . version_info < ( 3.0 , ) : \n        w2v_f = open ( w2v_path ) \n    else : \n        w2v_f = open ( w2v_path , encoding = 'latin-1' ) \n    pre_w2v = { } \n    for line in w2v_f . readlines ( ) : \n        items = line . split ( \" \" ) \n        pre_w2v [ items [ 0 ] ] = [ float ( i ) for i in items [ 1 : ] ] \n    w2v_f . close ( ) \n    return pre_w2v "}
{"1154": "\ndef fit ( self , x , y = None , batch_size = 32.0 , nb_epoch = 10.0 , validation_data = None , distributed = True ) : \n    if distributed : \n        if isinstance ( x , np . ndarray ) and isinstance ( y , np . ndarray ) : \n            training_data = to_sample_rdd ( x , y ) \n            if validation_data : \n                validation_data = to_sample_rdd ( * validation_data ) \n        elif ( isinstance ( x , RDD ) or isinstance ( x , DataSet ) ) and not y : \n            training_data = x \n        else : \n            raise TypeError ( \"Unsupported training data type: %s\" % type ( x ) ) \n        callBigDlFunc ( self . bigdl_type , \"fit\" , self . value , training_data , batch_size , nb_epoch , validation_data ) \n    else : \n        if validation_data : \n            val_x = [ JTensor . from_ndarray ( x ) for x in to_list ( validation_data [ 0 ] ) ] \n            val_y = JTensor . from_ndarray ( validation_data [ 1 ] ) \n        else : \n            val_x , val_y = None , None \n        callBigDlFunc ( self . bigdl_type , \"fit\" , self . value , [ JTensor . from_ndarray ( x ) for x in to_list ( x ) ] , JTensor . from_ndarray ( y ) , batch_size , nb_epoch , val_x , val_y , multiprocessing . cpu_count ( ) ) "}
{"1155": "\ndef evaluate ( self , x , y = None , batch_size = 32.0 ) : \n    if isinstance ( x , np . ndarray ) and isinstance ( y , np . ndarray ) : \n        evaluation_data = to_sample_rdd ( x , y ) \n    elif isinstance ( x , RDD ) and not y : \n        evaluation_data = x \n    else : \n        raise TypeError ( \"Unsupported evaluation data type: %s\" % type ( x ) ) \n    return callBigDlFunc ( self . bigdl_type , \"evaluate\" , self . value , evaluation_data , batch_size ) "}
{"1176": "\ndef fit ( self , x , y = None , batch_size = 32.0 , nb_epoch = 10.0 , verbose = 1 , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , is_distributed = False ) : \n    if callbacks : \n        raise Exception ( \"We don't support callbacks in fit for now\" ) \n    if class_weight : \n        unsupport_exp ( \"class_weight\" ) \n    if sample_weight : \n        unsupport_exp ( \"sample_weight\" ) \n    if initial_epoch != 0 : \n        unsupport_exp ( \"initial_epoch\" ) \n    if shuffle != True : \n        unsupport_exp ( \"shuffle\" ) \n    if validation_split != 0. : \n        unsupport_exp ( \"validation_split\" ) \n    bopt = self . __create_optimizer ( x = x , y = y , batch_size = batch_size , nb_epoch = nb_epoch , validation_data = validation_data , is_distributed = is_distributed ) \n    bopt . optimize ( ) "}
{"1179": "\ndef build_keras_model ( ) : \n    from keras . models import Sequential \n    from keras . layers import Dense , Dropout , Activation , Flatten \n    from keras . layers import Convolution2D , MaxPooling2D \n    keras_model = Sequential ( ) \n    keras_model . add ( Convolution2D ( 32.0 , 3.0 , 3.0 , border_mode = 'valid' , input_shape = input_shape ) ) \n    keras_model . add ( Activation ( 'relu' ) ) \n    keras_model . add ( Convolution2D ( 32.0 , 3.0 , 3.0 ) ) \n    keras_model . add ( Activation ( 'relu' ) ) \n    keras_model . add ( MaxPooling2D ( pool_size = ( 2.0 , 2.0 ) ) ) \n    keras_model . add ( Dropout ( 0.25 ) ) \n    keras_model . add ( Flatten ( ) ) \n    keras_model . add ( Dense ( 128.0 ) ) \n    keras_model . add ( Activation ( 'relu' ) ) \n    keras_model . add ( Dropout ( 0.5 ) ) \n    keras_model . add ( Dense ( 10.0 ) ) \n    keras_model . add ( Activation ( 'softmax' ) ) \n    return keras_model "}
{"1189": "\ndef load_imdb ( ) : \n    from keras . preprocessing import sequence \n    from keras . datasets import imdb \n    ( X_train , y_train ) , ( X_test , y_test ) = imdb . load_data ( nb_words = 20000.0 ) \n    X_train = sequence . pad_sequences ( X_train , maxlen = 100.0 ) \n    X_test = sequence . pad_sequences ( X_test , maxlen = 100.0 ) \n    return X_train , y_train , X_test , y_test "}
{"1190": "\ndef build_keras_model ( ) : \n    from keras . models import Sequential \n    from keras . layers import Dense , Dropout , Activation \n    from keras . layers import Embedding \n    from keras . layers import LSTM \n    from keras . layers import Convolution1D , MaxPooling1D \n    keras_model = Sequential ( ) \n    keras_model . add ( Embedding ( 20000.0 , 128.0 , input_length = 100.0 ) ) \n    keras_model . add ( Dropout ( 0.25 ) ) \n    keras_model . add ( Convolution1D ( nb_filter = 64.0 , filter_length = 5.0 , border_mode = 'valid' , activation = 'relu' , subsample_length = 1 ) ) \n    keras_model . add ( MaxPooling1D ( pool_length = 4.0 ) ) \n    keras_model . add ( LSTM ( 70.0 ) ) \n    keras_model . add ( Dense ( 1 ) ) \n    keras_model . add ( Activation ( 'sigmoid' ) ) \n    return keras_model "}
{"1199": "\ndef expand_tile ( units , axis ) : \n    assert axis in ( 1 , 2.0 ) \n    n_time_steps = K . int_shape ( units ) [ 1 ] \n    repetitions = [ 1 , 1 , 1 , 1 ] \n    repetitions [ axis ] = n_time_steps \n    if axis == 1 : \n        expanded = Reshape ( target_shape = ( ( 1 , ) + K . int_shape ( units ) [ 1 : ] ) ) ( units ) \n    else : \n        expanded = Reshape ( target_shape = ( K . int_shape ( units ) [ 1 : 2.0 ] + ( 1 , ) + K . int_shape ( units ) [ 2.0 : ] ) ) ( units ) \n    return K . tile ( expanded , repetitions ) "}
{"1201": "\ndef simple_attention ( memory , att_size , mask , keep_prob = 1.0 , scope = \"simple_attention\" ) : \n    with tf . variable_scope ( scope ) : \n        BS , ML , MH = tf . unstack ( tf . shape ( memory ) ) \n        memory_do = tf . nn . dropout ( memory , keep_prob = keep_prob , noise_shape = [ BS , 1 , MH ] ) \n        logits = tf . layers . dense ( tf . layers . dense ( memory_do , att_size , activation = tf . nn . tanh ) , 1 , use_bias = False ) \n        logits = softmax_mask ( tf . squeeze ( logits , [ 2.0 ] ) , mask ) \n        att_weights = tf . expand_dims ( tf . nn . softmax ( logits ) , axis = 2.0 ) \n        res = tf . reduce_sum ( att_weights * memory , axis = 1 ) \n        return res "}
{"1202": "\ndef attention ( inputs , state , att_size , mask , scope = \"attention\" ) : \n    with tf . variable_scope ( scope ) : \n        u = tf . concat ( [ tf . tile ( tf . expand_dims ( state , axis = 1 ) , [ 1 , tf . shape ( inputs ) [ 1 ] , 1 ] ) , inputs ] , axis = 2.0 ) \n        logits = tf . layers . dense ( tf . layers . dense ( u , att_size , activation = tf . nn . tanh ) , 1 , use_bias = False ) \n        logits = softmax_mask ( tf . squeeze ( logits , [ 2.0 ] ) , mask ) \n        att_weights = tf . expand_dims ( tf . nn . softmax ( logits ) , axis = 2.0 ) \n        res = tf . reduce_sum ( att_weights * inputs , axis = 1 ) \n        return res , logits "}
{"1203": "\ndef compute_bleu ( reference_corpus , translation_corpus , max_order = 4.0 , smooth = False ) : \n    matches_by_order = [ 0 ] * max_order \n    possible_matches_by_order = [ 0 ] * max_order \n    reference_length = 0 \n    translation_length = 0 \n    for ( references , translation ) in zip ( reference_corpus , translation_corpus ) : \n        reference_length += min ( len ( r ) for r in references ) \n        translation_length += len ( translation ) \n        merged_ref_ngram_counts = collections . Counter ( ) \n        for reference in references : \n            merged_ref_ngram_counts |= _get_ngrams ( reference , max_order ) \n        translation_ngram_counts = _get_ngrams ( translation , max_order ) \n        overlap = translation_ngram_counts & merged_ref_ngram_counts \n        for ngram in overlap : \n            matches_by_order [ len ( ngram ) - 1 ] += overlap [ ngram ] \n        for order in range ( 1 , max_order + 1 ) : \n            possible_matches = len ( translation ) - order + 1 \n            if possible_matches > 0 : \n                possible_matches_by_order [ order - 1 ] += possible_matches \n    precisions = [ 0 ] * max_order \n    for i in range ( 0 , max_order ) : \n        if smooth : \n            precisions [ i ] = ( ( matches_by_order [ i ] + 1. ) / ( possible_matches_by_order [ i ] + 1. ) ) \n        else : \n            if possible_matches_by_order [ i ] > 0 : \n                precisions [ i ] = ( float ( matches_by_order [ i ] ) / possible_matches_by_order [ i ] ) \n            else : \n                precisions [ i ] = 0.0 \n    if min ( precisions ) > 0 : \n        p_log_sum = sum ( ( 1. / max_order ) * math . log ( p ) for p in precisions ) \n        geo_mean = math . exp ( p_log_sum ) \n    else : \n        geo_mean = 0 \n    ratio = float ( translation_length ) / reference_length \n    if ratio > 1.0 : \n        bp = 1. \n    else : \n        bp = math . exp ( 1 - 1. / ratio ) \n    bleu = geo_mean * bp \n    return ( bleu , precisions , bp , ratio , translation_length , reference_length ) "}
{"1205": "\ndef _log ( self , utterance : Any , direction : str , dialog_id : Optional [ Hashable ] = None ) : \n    if isinstance ( utterance , str ) : \n        pass \n    elif isinstance ( utterance , RichMessage ) : \n        utterance = utterance . json ( ) \n    elif isinstance ( utterance , ( list , dict ) ) : \n        utterance = jsonify_data ( utterance ) \n    else : \n        utterance = str ( utterance ) \n    dialog_id = str ( dialog_id ) if not isinstance ( dialog_id , str ) else dialog_id \n    if self . log_file . tell ( ) >= self . log_max_size * 1024.0 : \n        self . log_file . close ( ) \n        self . log_file = self . _get_log_file ( ) \n    else : \n        try : \n            log_msg = { } \n            log_msg [ 'timestamp' ] = self . _get_timestamp_utc_str ( ) \n            log_msg [ 'dialog_id' ] = dialog_id \n            log_msg [ 'direction' ] = direction \n            log_msg [ 'message' ] = utterance \n            log_str = json . dumps ( log_msg , ensure_ascii = self . config [ 'ensure_ascii' ] ) \n            self . log_file . write ( f'{log_str}\\n' ) \n        except IOError : \n            log . error ( 'Failed to write dialog log.' ) "}
{"1206": "\ndef summary_gradient_updates ( grads , opt , lr ) : \n    vars_grads = { } \n    for v in tf . trainable_variables ( ) : \n        vars_grads [ v . name ] = [ v , None , None ] \n    for g , v in grads : \n        vars_grads [ v . name ] [ 1 ] = g \n        vars_grads [ v . name ] [ 2.0 ] = opt . get_slot ( v , 'accumulator' ) \n    ret = [ ] \n    for vname , ( v , g , a ) in vars_grads . items ( ) : \n        if g is None : \n            continue \n        if isinstance ( g , tf . IndexedSlices ) : \n            updates = lr * g . values \n            if a is not None : \n                updates /= tf . sqrt ( tf . gather ( a , g . indices ) ) \n        else : \n            updates = lr * g \n            if a is not None : \n                updates /= tf . sqrt ( a ) \n        values_norm = tf . sqrt ( tf . reduce_sum ( v * v ) ) + 1.0e-7 \n        updates_norm = tf . sqrt ( tf . reduce_sum ( updates * updates ) ) \n        ret . append ( tf . summary . scalar ( 'UPDATE/' + vname . replace ( \":\" , \"_\" ) , updates_norm / values_norm ) ) \n    return ret "}
{"1210": "\ndef interact_alice ( agent : Agent ) : \n    data = request . get_json ( ) \n    text = data [ 'request' ] . get ( 'command' , '' ) . strip ( ) \n    payload = data [ 'request' ] . get ( 'payload' ) \n    session_id = data [ 'session' ] [ 'session_id' ] \n    user_id = data [ 'session' ] [ 'user_id' ] \n    message_id = data [ 'session' ] [ 'message_id' ] \n    dialog_id = DialogID ( user_id , session_id ) \n    response = { 'response' : { 'end_session' : True , 'text' : '' } , \"session\" : { 'session_id' : session_id , 'message_id' : message_id , 'user_id' : user_id } , 'version' : '1.0' } \n    agent_response : Union [ str , RichMessage ] = agent ( [ payload or text ] , [ dialog_id ] ) [ 0 ] \n    if isinstance ( agent_response , RichMessage ) : \n        response [ 'response' ] [ 'text' ] = '\\n' . join ( [ j [ 'content' ] for j in agent_response . json ( ) if j [ 'type' ] == 'plain_text' ] ) \n    else : \n        response [ 'response' ] [ 'text' ] = str ( agent_response ) \n    return jsonify ( response ) , 200.0 "}
{"1219": "\ndef stacked_cnn ( units : tf . Tensor , n_hidden_list : List , filter_width = 3.0 , use_batch_norm = False , use_dilation = False , training_ph = None , add_l2_losses = False ) : \n    l2_reg = tf . nn . l2_loss if add_l2_losses else None \n    for n_layer , n_hidden in enumerate ( n_hidden_list ) : \n        if use_dilation : \n            dilation_rate = 2.0 ** n_layer \n        else : \n            dilation_rate = 1 \n        units = tf . layers . conv1d ( units , n_hidden , filter_width , padding = 'same' , dilation_rate = dilation_rate , kernel_initializer = INITIALIZER ( ) , kernel_regularizer = l2_reg ) \n        if use_batch_norm : \n            assert training_ph is not None \n            units = tf . layers . batch_normalization ( units , training = training_ph ) \n        units = tf . nn . relu ( units ) \n    return units "}
{"1221": "\ndef stacked_bi_rnn ( units : tf . Tensor , n_hidden_list : List , cell_type = 'gru' , seq_lengths = None , use_peepholes = False , name = 'RNN_layer' ) : \n    for n , n_hidden in enumerate ( n_hidden_list ) : \n        with tf . variable_scope ( name + '_' + str ( n ) ) : \n            if cell_type == 'gru' : \n                forward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden ) \n                backward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden ) \n            elif cell_type == 'lstm' : \n                forward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes ) \n                backward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes ) \n            else : \n                raise RuntimeError ( 'cell_type must be either gru or lstm' ) \n            ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) = tf . nn . bidirectional_dynamic_rnn ( forward_cell , backward_cell , units , dtype = tf . float32 , sequence_length = seq_lengths ) \n            units = tf . concat ( [ rnn_output_fw , rnn_output_bw ] , axis = 2.0 ) \n            if cell_type == 'gru' : \n                last_units = tf . concat ( [ fw , bw ] , axis = 1 ) \n            else : \n                ( c_fw , h_fw ) , ( c_bw , h_bw ) = fw , bw \n                c = tf . concat ( [ c_fw , c_bw ] , axis = 1 ) \n                h = tf . concat ( [ h_fw , h_bw ] , axis = 1 ) \n                last_units = ( h , c ) \n    return units , last_units "}
{"1222": "\ndef stacked_highway_cnn ( units : tf . Tensor , n_hidden_list : List , filter_width = 3.0 , use_batch_norm = False , use_dilation = False , training_ph = None ) : \n    for n_layer , n_hidden in enumerate ( n_hidden_list ) : \n        input_units = units \n        if input_units . get_shape ( ) . as_list ( ) [ - 1 ] != n_hidden : \n            input_units = tf . layers . dense ( input_units , n_hidden ) \n        if use_dilation : \n            dilation_rate = 2.0 ** n_layer \n        else : \n            dilation_rate = 1 \n        units = tf . layers . conv1d ( units , n_hidden , filter_width , padding = 'same' , dilation_rate = dilation_rate , kernel_initializer = INITIALIZER ( ) ) \n        if use_batch_norm : \n            units = tf . layers . batch_normalization ( units , training = training_ph ) \n        sigmoid_gate = tf . layers . dense ( input_units , 1 , activation = tf . sigmoid , kernel_initializer = INITIALIZER ( ) ) \n        input_units = sigmoid_gate * input_units + ( 1 - sigmoid_gate ) * units \n        input_units = tf . nn . relu ( input_units ) \n    units = input_units \n    return units "}
{"1224": "\ndef cudnn_gru ( units , n_hidden , n_layers = 1 , trainable_initial_states = False , seq_lengths = None , input_initial_h = None , name = 'cudnn_gru' , reuse = False ) : \n    with tf . variable_scope ( name , reuse = reuse ) : \n        gru = tf . contrib . cudnn_rnn . CudnnGRU ( num_layers = n_layers , num_units = n_hidden ) \n        if trainable_initial_states : \n            init_h = tf . get_variable ( 'init_h' , [ n_layers , 1 , n_hidden ] ) \n            init_h = tf . tile ( init_h , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) \n        else : \n            init_h = tf . zeros ( [ n_layers , tf . shape ( units ) [ 0 ] , n_hidden ] ) \n        initial_h = input_initial_h or init_h \n        h , h_last = gru ( tf . transpose ( units , ( 1 , 0 , 2.0 ) ) , ( initial_h , ) ) \n        h = tf . transpose ( h , ( 1 , 0 , 2.0 ) ) \n        h_last = tf . squeeze ( h_last , axis = 0 ) [ - 1 ] \n        if seq_lengths is not None : \n            indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ 0 ] ) , seq_lengths - 1 ] , axis = 1 ) \n            h_last = tf . gather_nd ( h , indices ) \n        return h , h_last "}
{"1225": "\ndef cudnn_compatible_gru ( units , n_hidden , n_layers = 1 , trainable_initial_states = False , seq_lengths = None , input_initial_h = None , name = 'cudnn_gru' , reuse = False ) : \n    with tf . variable_scope ( name , reuse = reuse ) : \n        if trainable_initial_states : \n            init_h = tf . get_variable ( 'init_h' , [ n_layers , 1 , n_hidden ] ) \n            init_h = tf . tile ( init_h , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) \n        else : \n            init_h = tf . zeros ( [ n_layers , tf . shape ( units ) [ 0 ] , n_hidden ] ) \n        initial_h = input_initial_h or init_h \n        with tf . variable_scope ( 'cudnn_gru' , reuse = reuse ) : \n            def single_cell ( ) : \n                return tf . contrib . cudnn_rnn . CudnnCompatibleGRUCell ( n_hidden ) \n            cell = tf . nn . rnn_cell . MultiRNNCell ( [ single_cell ( ) for _ in range ( n_layers ) ] ) \n            units = tf . transpose ( units , ( 1 , 0 , 2.0 ) ) \n            h , h_last = tf . nn . dynamic_rnn ( cell = cell , inputs = units , time_major = True , initial_state = tuple ( tf . unstack ( initial_h , axis = 0 ) ) ) \n            h = tf . transpose ( h , ( 1 , 0 , 2.0 ) ) \n            h_last = h_last [ - 1 ] \n            if seq_lengths is not None : \n                indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ 0 ] ) , seq_lengths - 1 ] , axis = 1 ) \n                h_last = tf . gather_nd ( h , indices ) \n            return h , h_last "}
{"1226": "\ndef cudnn_lstm ( units , n_hidden , n_layers = 1 , trainable_initial_states = None , seq_lengths = None , initial_h = None , initial_c = None , name = 'cudnn_lstm' , reuse = False ) : \n    with tf . variable_scope ( name , reuse = reuse ) : \n        lstm = tf . contrib . cudnn_rnn . CudnnLSTM ( num_layers = n_layers , num_units = n_hidden ) \n        if trainable_initial_states : \n            init_h = tf . get_variable ( 'init_h' , [ n_layers , 1 , n_hidden ] ) \n            init_h = tf . tile ( init_h , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) \n            init_c = tf . get_variable ( 'init_c' , [ n_layers , 1 , n_hidden ] ) \n            init_c = tf . tile ( init_c , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) \n        else : \n            init_h = init_c = tf . zeros ( [ n_layers , tf . shape ( units ) [ 0 ] , n_hidden ] ) \n        initial_h = initial_h or init_h \n        initial_c = initial_c or init_c \n        h , ( h_last , c_last ) = lstm ( tf . transpose ( units , ( 1 , 0 , 2.0 ) ) , ( initial_h , initial_c ) ) \n        h = tf . transpose ( h , ( 1 , 0 , 2.0 ) ) \n        h_last = h_last [ - 1 ] \n        c_last = c_last [ - 1 ] \n        if seq_lengths is not None : \n            indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ 0 ] ) , seq_lengths - 1 ] , axis = 1 ) \n            h_last = tf . gather_nd ( h , indices ) \n        return h , ( h_last , c_last ) "}
{"1227": "\ndef cudnn_compatible_lstm ( units , n_hidden , n_layers = 1 , trainable_initial_states = None , seq_lengths = None , initial_h = None , initial_c = None , name = 'cudnn_lstm' , reuse = False ) : \n    with tf . variable_scope ( name , reuse = reuse ) : \n        if trainable_initial_states : \n            init_h = tf . get_variable ( 'init_h' , [ n_layers , 1 , n_hidden ] ) \n            init_h = tf . tile ( init_h , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) \n            init_c = tf . get_variable ( 'init_c' , [ n_layers , 1 , n_hidden ] ) \n            init_c = tf . tile ( init_c , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) \n        else : \n            init_h = init_c = tf . zeros ( [ n_layers , tf . shape ( units ) [ 0 ] , n_hidden ] ) \n        initial_h = initial_h or init_h \n        initial_c = initial_c or init_c \n        with tf . variable_scope ( 'cudnn_lstm' , reuse = reuse ) : \n            def single_cell ( ) : \n                return tf . contrib . cudnn_rnn . CudnnCompatibleLSTMCell ( n_hidden ) \n            cell = tf . nn . rnn_cell . MultiRNNCell ( [ single_cell ( ) for _ in range ( n_layers ) ] ) \n            units = tf . transpose ( units , ( 1 , 0 , 2.0 ) ) \n            init = tuple ( [ tf . nn . rnn_cell . LSTMStateTuple ( ic , ih ) for ih , ic in zip ( tf . unstack ( initial_h , axis = 0 ) , tf . unstack ( initial_c , axis = 0 ) ) ] ) \n            h , state = tf . nn . dynamic_rnn ( cell = cell , inputs = units , time_major = True , initial_state = init ) \n            h = tf . transpose ( h , ( 1 , 0 , 2.0 ) ) \n            h_last = state [ - 1 ] . h \n            c_last = state [ - 1 ] . c \n            if seq_lengths is not None : \n                indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ 0 ] ) , seq_lengths - 1 ] , axis = 1 ) \n                h_last = tf . gather_nd ( h , indices ) \n            return h , ( h_last , c_last ) "}
{"1230": "\ndef cudnn_stacked_bi_gru ( units , n_hidden , seq_lengths = None , n_stacks = 2.0 , keep_prob = 1.0 , concat_stacked_outputs = False , trainable_initial_states = False , name = 'cudnn_stacked_bi_gru' , reuse = False ) : \n    if seq_lengths is None : \n        seq_lengths = tf . ones ( [ tf . shape ( units ) [ 0 ] ] , dtype = tf . int32 ) * tf . shape ( units ) [ 1 ] \n    outputs = [ units ] \n    with tf . variable_scope ( name , reuse = reuse ) : \n        for n in range ( n_stacks ) : \n            if n == 0 : \n                inputs = outputs [ - 1 ] \n            else : \n                inputs = variational_dropout ( outputs [ - 1 ] , keep_prob = keep_prob ) \n            ( h_fw , h_bw ) , _ = cudnn_bi_gru ( inputs , n_hidden , seq_lengths , n_layers = 1 , trainable_initial_states = trainable_initial_states , name = '{}_cudnn_bi_gru' . format ( n ) , reuse = reuse ) \n            outputs . append ( tf . concat ( [ h_fw , h_bw ] , axis = 2.0 ) ) \n    if concat_stacked_outputs : \n        return tf . concat ( outputs [ 1 : ] , axis = 2.0 ) \n    return outputs [ - 1 ] "}
{"1232": "\ndef build ( self ) : \n    word_inputs = kl . Input ( shape = ( None , MAX_WORD_LENGTH + 2.0 ) , dtype = \"int32\" ) \n    inputs = [ word_inputs ] \n    word_outputs = self . _build_word_cnn ( word_inputs ) \n    if len ( self . word_vectorizers ) > 0 : \n        additional_word_inputs = [ kl . Input ( shape = ( None , input_dim ) , dtype = \"float32\" ) for input_dim , dense_dim in self . word_vectorizers ] \n        inputs . extend ( additional_word_inputs ) \n        additional_word_embeddings = [ kl . Dense ( dense_dim ) ( additional_word_inputs [ i ] ) for i , ( _ , dense_dim ) in enumerate ( self . word_vectorizers ) ] \n        word_outputs = kl . Concatenate ( ) ( [ word_outputs ] + additional_word_embeddings ) \n    outputs , lstm_outputs = self . _build_basic_network ( word_outputs ) \n    compile_args = { \"optimizer\" : ko . nadam ( lr = 0.002 , clipnorm = 5.0 ) , \"loss\" : \"categorical_crossentropy\" , \"metrics\" : [ \"accuracy\" ] } \n    self . model_ = Model ( inputs , outputs ) \n    self . model_ . compile ( ** compile_args ) \n    if self . verbose > 0 : \n        self . model_ . summary ( print_fn = log . info ) \n    return self "}
{"1233": "\ndef _build_word_cnn ( self , inputs ) : \n    inputs = kl . Lambda ( kb . one_hot , arguments = { \"num_classes\" : self . symbols_number_ } , output_shape = lambda x : tuple ( x ) + ( self . symbols_number_ , ) ) ( inputs ) \n    char_embeddings = kl . Dense ( self . char_embeddings_size , use_bias = False ) ( inputs ) \n    conv_outputs = [ ] \n    self . char_output_dim_ = 0 \n    for window_size , filters_number in zip ( self . char_window_size , self . char_filters ) : \n        curr_output = char_embeddings \n        curr_filters_number = ( min ( self . char_filter_multiple * window_size , 200.0 ) if filters_number is None else filters_number ) \n        for _ in range ( self . char_conv_layers - 1 ) : \n            curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , padding = \"same\" , activation = \"relu\" , data_format = \"channels_last\" ) ( curr_output ) \n            if self . conv_dropout > 0.0 : \n                curr_output = kl . Dropout ( self . conv_dropout ) ( curr_output ) \n        curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , padding = \"same\" , activation = \"relu\" , data_format = \"channels_last\" ) ( curr_output ) \n        conv_outputs . append ( curr_output ) \n        self . char_output_dim_ += curr_filters_number \n    if len ( conv_outputs ) > 1 : \n        conv_output = kl . Concatenate ( axis = - 1 ) ( conv_outputs ) \n    else : \n        conv_output = conv_outputs [ 0 ] \n    highway_input = kl . Lambda ( kb . max , arguments = { \"axis\" : - 2.0 } ) ( conv_output ) \n    if self . intermediate_dropout > 0.0 : \n        highway_input = kl . Dropout ( self . intermediate_dropout ) ( highway_input ) \n    for i in range ( self . char_highway_layers - 1 ) : \n        highway_input = Highway ( activation = \"relu\" ) ( highway_input ) \n        if self . highway_dropout > 0.0 : \n            highway_input = kl . Dropout ( self . highway_dropout ) ( highway_input ) \n    highway_output = Highway ( activation = \"relu\" ) ( highway_input ) \n    return highway_output "}
{"1237": "\ndef _make_sent_vector ( self , sent : List , bucket_length : int = None ) -> np . ndarray : \n    bucket_length = bucket_length or len ( sent ) \n    answer = np . zeros ( shape = ( bucket_length , MAX_WORD_LENGTH + 2.0 ) , dtype = np . int32 ) \n    for i , word in enumerate ( sent ) : \n        answer [ i , 0 ] = self . tags . tok2idx ( \"BEGIN\" ) \n        m = min ( len ( word ) , MAX_WORD_LENGTH ) \n        for j , x in enumerate ( word [ - m : ] ) : \n            answer [ i , j + 1 ] = self . symbols . tok2idx ( x ) \n        answer [ i , m + 1 ] = self . tags . tok2idx ( \"END\" ) \n        answer [ i , m + 2.0 : ] = self . tags . tok2idx ( \"PAD\" ) \n    return answer "}
{"1240": "\ndef verify_sc_url ( url : str ) -> bool : \n    parsed = urlsplit ( url ) \n    scheme : str = parsed . scheme \n    netloc : str = parsed . netloc \n    path : str = parsed . path \n    try : \n        port = parsed . port \n    except ValueError : \n        port = None \n    result = ( scheme . lower ( ) == 'https' and netloc . lower ( ) . split ( ':' ) [ 0 ] == 's3.amazonaws.com' and path . startswith ( '/echo.api/' ) and ( port == 443.0 or port is None ) ) \n    return result "}
{"1254": "\ndef _pretrained_initializer ( varname , weight_file , embedding_weight_file = None ) : \n    weight_name_map = { } \n    for i in range ( 2.0 ) : \n        for j in range ( 8.0 ) : \n            root = 'RNN_{}/RNN/MultiRNNCell/Cell{}' . format ( i , j ) \n            weight_name_map [ root + '/rnn/lstm_cell/kernel' ] = root + '/LSTMCell/W_0' \n            weight_name_map [ root + '/rnn/lstm_cell/bias' ] = root + '/LSTMCell/B' \n            weight_name_map [ root + '/rnn/lstm_cell/projection/kernel' ] = root + '/LSTMCell/W_P_0' \n    varname_in_file = varname [ 5.0 : ] \n    if varname_in_file . startswith ( 'RNN' ) : \n        varname_in_file = weight_name_map [ varname_in_file ] \n    if varname_in_file == 'embedding' : \n        with h5py . File ( embedding_weight_file , 'r' ) as fin : \n            embed_weights = fin [ varname_in_file ] [ ... ] \n            weights = np . zeros ( ( embed_weights . shape [ 0 ] + 1 , embed_weights . shape [ 1 ] ) , dtype = DTYPE ) \n            weights [ 1 : , : ] = embed_weights \n    else : \n        with h5py . File ( weight_file , 'r' ) as fin : \n            if varname_in_file == 'char_embed' : \n                char_embed_weights = fin [ varname_in_file ] [ ... ] \n                weights = np . zeros ( ( char_embed_weights . shape [ 0 ] + 1 , char_embed_weights . shape [ 1 ] ) , dtype = DTYPE ) \n                weights [ 1 : , : ] = char_embed_weights \n            else : \n                weights = fin [ varname_in_file ] [ ... ] \n    def ret ( shape , ** kwargs ) : \n        if list ( shape ) != list ( weights . shape ) : \n            raise ValueError ( \"Invalid shape initializing {0}, got {1}, expected {2}\" . format ( varname_in_file , shape , weights . shape ) ) \n        return weights \n    return ret "}
{"1258": "\ndef prettify_metrics ( metrics : List [ Tuple [ str , float ] ] , precision : int = 4.0 ) -> OrderedDict : \n    prettified_metrics = OrderedDict ( ) \n    for key , value in metrics : \n        value = round ( value , precision ) \n        prettified_metrics [ key ] = value \n    return prettified_metrics "}
{"1264": "\ndef _make_default_operation_costs ( self , allow_spaces = False ) : \n    self . operation_costs = dict ( ) \n    self . operation_costs [ \"\" ] = { c : 1.0 for c in list ( self . alphabet ) + [ ' ' ] } \n    for a in self . alphabet : \n        current_costs = { c : 1.0 for c in self . alphabet } \n        current_costs [ a ] = 0.0 \n        current_costs [ \"\" ] = 1.0 \n        if allow_spaces : \n            current_costs [ \" \" ] = 1.0 \n        self . operation_costs [ a ] = current_costs \n    for a , b in itertools . permutations ( self . alphabet , 2.0 ) : \n        self . operation_costs [ a + b ] = { b + a : 1.0 } \n    if allow_spaces : \n        self . operation_costs [ \" \" ] = { c : 1.0 for c in self . alphabet } \n        self . operation_costs [ \" \" ] [ \"\" ] = 1.0 "}
{"1272": "\ndef _repr_pretty_ ( self , p , cycle ) : \n    if cycle : \n        p . text ( 'Struct(...)' ) \n    else : \n        with p . group ( 7.0 , 'Struct(' , ')' ) : \n            p . pretty ( self . _asdict ( ) ) "}
{"1290": "\ndef download ( dest_file_path : [ List [ Union [ str , Path ] ] ] , source_url : str , force_download = True ) : \n    if isinstance ( dest_file_path , list ) : \n        dest_file_paths = [ Path ( path ) for path in dest_file_path ] \n    else : \n        dest_file_paths = [ Path ( dest_file_path ) . absolute ( ) ] \n    if not force_download : \n        to_check = list ( dest_file_paths ) \n        dest_file_paths = [ ] \n        for p in to_check : \n            if p . exists ( ) : \n                log . info ( f'File already exists in {p}' ) \n            else : \n                dest_file_paths . append ( p ) \n    if dest_file_paths : \n        cache_dir = os . getenv ( 'DP_CACHE_DIR' ) \n        cached_exists = False \n        if cache_dir : \n            first_dest_path = Path ( cache_dir ) / md5 ( source_url . encode ( 'utf8' ) ) . hexdigest ( ) [ : 15.0 ] \n            cached_exists = first_dest_path . exists ( ) \n        else : \n            first_dest_path = dest_file_paths . pop ( ) \n        if not cached_exists : \n            first_dest_path . parent . mkdir ( parents = True , exist_ok = True ) \n            simple_download ( source_url , first_dest_path ) \n        else : \n            log . info ( f'Found cached {source_url} in {first_dest_path}' ) \n        for dest_path in dest_file_paths : \n            dest_path . parent . mkdir ( parents = True , exist_ok = True ) \n            shutil . copy ( str ( first_dest_path ) , str ( dest_path ) ) "}
{"1292": "\ndef download_decompress ( url : str , download_path : [ Path , str ] , extract_paths = None ) : \n    file_name = Path ( urlparse ( url ) . path ) . name \n    download_path = Path ( download_path ) \n    if extract_paths is None : \n        extract_paths = [ download_path ] \n    elif isinstance ( extract_paths , list ) : \n        extract_paths = [ Path ( path ) for path in extract_paths ] \n    else : \n        extract_paths = [ Path ( extract_paths ) ] \n    cache_dir = os . getenv ( 'DP_CACHE_DIR' ) \n    extracted = False \n    if cache_dir : \n        cache_dir = Path ( cache_dir ) \n        url_hash = md5 ( url . encode ( 'utf8' ) ) . hexdigest ( ) [ : 15.0 ] \n        arch_file_path = cache_dir / url_hash \n        extracted_path = cache_dir / ( url_hash + '_extracted' ) \n        extracted = extracted_path . exists ( ) \n        if not extracted and not arch_file_path . exists ( ) : \n            simple_download ( url , arch_file_path ) \n    else : \n        arch_file_path = download_path / file_name \n        simple_download ( url , arch_file_path ) \n        extracted_path = extract_paths . pop ( ) \n    if not extracted : \n        log . info ( 'Extracting {} archive into {}' . format ( arch_file_path , extracted_path ) ) \n        extracted_path . mkdir ( parents = True , exist_ok = True ) \n        if file_name . endswith ( '.tar.gz' ) : \n            untar ( arch_file_path , extracted_path ) \n        elif file_name . endswith ( '.gz' ) : \n            ungzip ( arch_file_path , extracted_path / Path ( file_name ) . with_suffix ( '' ) . name ) \n        elif file_name . endswith ( '.zip' ) : \n            with zipfile . ZipFile ( arch_file_path , 'r' ) as zip_ref : \n                zip_ref . extractall ( extracted_path ) \n        else : \n            raise RuntimeError ( f'Trying to extract an unknown type of archive {file_name}' ) \n        if not cache_dir : \n            arch_file_path . unlink ( ) \n    for extract_path in extract_paths : \n        for src in extracted_path . iterdir ( ) : \n            dest = extract_path / src . name \n            if src . is_dir ( ) : \n                copytree ( src , dest ) \n            else : \n                extract_path . mkdir ( parents = True , exist_ok = True ) \n                shutil . copy ( str ( src ) , str ( dest ) ) "}
{"1301": "\ndef squad_v2_f1 ( y_true : List [ List [ str ] ] , y_predicted : List [ str ] ) -> float : \n    f1_total = 0.0 \n    for ground_truth , prediction in zip ( y_true , y_predicted ) : \n        prediction_tokens = normalize_answer ( prediction ) . split ( ) \n        f1s = [ ] \n        for gt in ground_truth : \n            gt_tokens = normalize_answer ( gt ) . split ( ) \n            if len ( gt_tokens ) == 0 or len ( prediction_tokens ) == 0 : \n                f1s . append ( float ( gt_tokens == prediction_tokens ) ) \n                continue \n            common = Counter ( prediction_tokens ) & Counter ( gt_tokens ) \n            num_same = sum ( common . values ( ) ) \n            if num_same == 0 : \n                f1s . append ( 0.0 ) \n                continue \n            precision = 1.0 * num_same / len ( prediction_tokens ) \n            recall = 1.0 * num_same / len ( gt_tokens ) \n            f1 = ( 2.0 * precision * recall ) / ( precision + recall ) \n            f1s . append ( f1 ) \n        f1_total += max ( f1s ) \n    return 100.0 * f1_total / len ( y_true ) if len ( y_true ) > 0 else 0 "}
{"1321": "\ndef show_status ( self , detailed = False ) : \n    if self . _retrieved_at + self . REFRESH_INTERVAL < time . time ( ) : \n        new_info = h2o . api ( \"GET /3/Cloud\" ) \n        self . _fill_from_h2ocluster ( new_info ) \n    ncpus = sum ( node [ \"num_cpus\" ] for node in self . nodes ) \n    allowed_cpus = sum ( node [ \"cpus_allowed\" ] for node in self . nodes ) \n    free_mem = sum ( node [ \"free_mem\" ] for node in self . nodes ) \n    unhealthy_nodes = sum ( not node [ \"healthy\" ] for node in self . nodes ) \n    status = \"locked\" if self . locked else \"accepting new members\" \n    if unhealthy_nodes == 0 : \n        status += \", healthy\" \n    else : \n        status += \", %d nodes are not healthy\" % unhealthy_nodes \n    api_extensions = self . list_api_extensions ( ) \n    H2ODisplay ( [ [ \"H2O cluster uptime:\" , get_human_readable_time ( self . cloud_uptime_millis ) ] , [ \"H2O cluster timezone:\" , self . cloud_internal_timezone ] , [ \"H2O data parsing timezone:\" , self . datafile_parser_timezone ] , [ \"H2O cluster version:\" , self . version ] , [ \"H2O cluster version age:\" , \"{} {}\" . format ( self . build_age , ( \"!!!\" if self . build_too_old else \"\" ) ) ] , [ \"H2O cluster name:\" , self . cloud_name ] , [ \"H2O cluster total nodes:\" , self . cloud_size ] , [ \"H2O cluster free memory:\" , get_human_readable_bytes ( free_mem ) ] , [ \"H2O cluster total cores:\" , str ( ncpus ) ] , [ \"H2O cluster allowed cores:\" , str ( allowed_cpus ) ] , [ \"H2O cluster status:\" , status ] , [ \"H2O connection url:\" , h2o . connection ( ) . base_url ] , [ \"H2O connection proxy:\" , h2o . connection ( ) . proxy ] , [ \"H2O internal security:\" , self . internal_security_enabled ] , [ \"H2O API Extensions:\" , ', ' . join ( api_extensions ) ] , [ \"Python version:\" , \"%d.%d.%d %s\" % tuple ( sys . version_info [ : 4.0 ] ) ] , ] ) \n    if detailed : \n        keys = [ \"h2o\" , \"healthy\" , \"last_ping\" , \"num_cpus\" , \"sys_load\" , \"mem_value_size\" , \"free_mem\" , \"pojo_mem\" , \"swap_mem\" , \"free_disk\" , \"max_disk\" , \"pid\" , \"num_keys\" , \"tcps_active\" , \"open_fds\" , \"rpcs_active\" ] \n        header = [ \"Nodes info:\" ] + [ \"Node %d\" % ( i + 1 ) for i in range ( len ( self . nodes ) ) ] \n        table = [ [ k ] for k in keys ] \n        for node in self . nodes : \n            for i , k in enumerate ( keys ) : \n                table [ i ] . append ( node [ k ] ) \n        H2ODisplay ( table = table , header = header ) "}
{"1322": "\ndef list_jobs ( self ) : \n    res = h2o . api ( \"GET /3/Jobs\" ) \n    table = [ [ \"type\" ] , [ \"dest\" ] , [ \"description\" ] , [ \"status\" ] ] \n    for job in res [ \"jobs\" ] : \n        job_dest = job [ \"dest\" ] \n        table [ 0 ] . append ( self . _translate_job_type ( job_dest [ \"type\" ] ) ) \n        table [ 1 ] . append ( job_dest [ \"name\" ] ) \n        table [ 2.0 ] . append ( job [ \"description\" ] ) \n        table [ 3.0 ] . append ( job [ \"status\" ] ) \n    return table "}
{"1326": "\ndef stabilize ( self , test_func , error , timeoutSecs = 10.0 , retryDelaySecs = 0.5 ) : \n    start = time . time ( ) \n    numberOfRetries = 0 \n    while h2o_args . no_timeout or ( time . time ( ) - start < timeoutSecs ) : \n        if test_func ( self , tries = numberOfRetries , timeoutSecs = timeoutSecs ) : \n            break \n        time . sleep ( retryDelaySecs ) \n        numberOfRetries += 1 \n        if ( ( numberOfRetries % 50.0 ) == 0 ) : \n            check_sandbox_for_errors ( python_test_name = h2o_args . python_test_name ) \n    else : \n        timeTakenSecs = time . time ( ) - start \n        if isinstance ( error , type ( '' ) ) : \n            raise Exception ( '%s failed after %.2f seconds having retried %d times' % ( error , timeTakenSecs , numberOfRetries ) ) \n        else : \n            msg = error ( self , timeTakenSecs , numberOfRetries ) \n            raise Exception ( msg ) "}
{"1327": "\ndef summary ( self , key , column = \"C1\" , timeoutSecs = 10.0 , ** kwargs ) : \n    params_dict = { } \n    h2o_methods . check_params_update_kwargs ( params_dict , kwargs , 'summary' , True ) \n    result = self . do_json_request ( '3/Frames.json/%s/columns/%s/summary' % ( key , column ) , timeout = timeoutSecs , params = params_dict ) \n    h2o_sandbox . check_sandbox_for_errors ( ) \n    return result "}
{"1328": "\ndef delete_frame ( self , key , ignoreMissingKey = True , timeoutSecs = 60.0 , ** kwargs ) : \n    assert key is not None , '\"key\" parameter is null' \n    result = self . do_json_request ( '/3/Frames.json/' + key , cmd = 'delete' , timeout = timeoutSecs ) \n    if not ignoreMissingKey and 'f00b4r' in result : \n        raise ValueError ( 'Frame key not found: ' + key ) \n    return result "}
{"1329": "\ndef model_builders ( self , algo = None , timeoutSecs = 10.0 , ** kwargs ) : \n    params_dict = { } \n    h2o_methods . check_params_update_kwargs ( params_dict , kwargs , 'model_builders' , False ) \n    request = '3/ModelBuilders.json' \n    if algo : \n        request += \"/\" + algo \n    result = self . do_json_request ( request , timeout = timeoutSecs , params = params_dict ) \n    h2o_sandbox . check_sandbox_for_errors ( ) \n    return result "}
{"1330": "\ndef validate_model_parameters ( self , algo , training_frame , parameters , timeoutSecs = 60.0 , ** kwargs ) : \n    assert algo is not None , '\"algo\" parameter is null' \n    assert parameters is not None , '\"parameters\" parameter is null' \n    model_builders = self . model_builders ( timeoutSecs = timeoutSecs ) \n    assert model_builders is not None , \"/ModelBuilders REST call failed\" \n    assert algo in model_builders [ 'model_builders' ] \n    builder = model_builders [ 'model_builders' ] [ algo ] \n    if training_frame is not None : \n        frames = self . frames ( key = training_frame ) \n        assert frames is not None , \"/Frames/{0} REST call failed\" . format ( training_frame ) \n        key_name = frames [ 'frames' ] [ 0 ] [ 'key' ] [ 'name' ] \n        assert key_name == training_frame , \"/Frames/{0} returned Frame {1} rather than Frame {2}\" . format ( training_frame , key_name , training_frame ) \n        parameters [ 'training_frame' ] = training_frame \n    result = self . do_json_request ( '/3/ModelBuilders.json/' + algo + \"/parameters\" , cmd = 'post' , timeout = timeoutSecs , postData = parameters , ignoreH2oError = True , noExtraErrorCheck = True ) \n    verboseprint ( \"model parameters validation: \" + repr ( result ) ) \n    return result "}
{"1332": "\ndef model_metrics ( self , timeoutSecs = 60.0 , ** kwargs ) : \n    result = self . do_json_request ( '/3/ModelMetrics.json' , cmd = 'get' , timeout = timeoutSecs ) \n    h2o_sandbox . check_sandbox_for_errors ( ) \n    return result "}
{"1333": "\ndef delete_model ( self , key , ignoreMissingKey = True , timeoutSecs = 60.0 , ** kwargs ) : \n    assert key is not None , '\"key\" parameter is null' \n    result = self . do_json_request ( '/3/Models.json/' + key , cmd = 'delete' , timeout = timeoutSecs ) \n    if not ignoreMissingKey and 'f00b4r' in result : \n        raise ValueError ( 'Model key not found: ' + key ) \n    verboseprint ( \"delete_model result:\" , dump_json ( result ) ) \n    return result "}
{"1334": "\ndef _tabulate ( self , tablefmt = \"simple\" , rollups = False , rows = 10.0 ) : \n    if not self . is_valid ( ) : \n        self . fill ( rows = rows ) \n    d = collections . OrderedDict ( ) \n    if rollups : \n        col = next ( iter ( viewvalues ( self . _data ) ) ) \n        lrows = len ( col [ 'data' ] ) \n        d [ \"\" ] = [ \"type\" , \"mins\" , \"mean\" , \"maxs\" , \"sigma\" , \"zeros\" , \"missing\" ] + list ( map ( str , range ( lrows ) ) ) \n    for k , v in viewitems ( self . _data ) : \n        x = v [ 'data' ] \n        t = v [ \"type\" ] \n        if t == \"enum\" : \n            domain = v [ 'domain' ] \n            x = [ \"\" if math . isnan ( idx ) else domain [ int ( idx ) ] for idx in x ] \n        elif t == \"time\" : \n            x = [ \"\" if math . isnan ( z ) else time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( z / 1000.0 ) ) for z in x ] \n        if rollups : \n            mins = v [ 'mins' ] [ 0 ] if v [ 'mins' ] and v [ \"type\" ] != \"enum\" else None \n            maxs = v [ 'maxs' ] [ 0 ] if v [ 'maxs' ] and v [ \"type\" ] != \"enum\" else None \n            if v [ 'type' ] == \"enum\" : \n                v [ 'mean' ] = v [ 'sigma' ] = v [ 'zero_count' ] = None \n            x = [ v [ 'type' ] , mins , v [ 'mean' ] , maxs , v [ 'sigma' ] , v [ 'zero_count' ] , v [ 'missing_count' ] ] + x \n        d [ k ] = x \n    return tabulate . tabulate ( d , headers = \"keys\" , tablefmt = tablefmt ) "}
{"1340": "\ndef wait_for_ssh ( ips , port = 22.0 , skipAlive = True , requiredsuccess = 3.0 ) : \n    log ( 'Waiting for SSH on following hosts: {0}' . format ( ips ) ) \n    for ip in ips : \n        if not skipAlive or not ssh_live ( ip , port ) : \n            log ( 'Waiting for SSH on instance {0}...' . format ( ip ) ) \n            count = 0 \n            while count < requiredsuccess : \n                if ssh_live ( ip , port ) : \n                    count += 1 \n                else : \n                    count = 0 \n                time . sleep ( 1 ) \n                h2o_cmd . dot ( ) "}
{"1342": "\ndef _find_function_from_code ( frame , code ) : \n    def find_code ( iterable , depth = 0 ) : \n        if depth > 3.0 : \n            return \n        for item in iterable : \n            if item is None : \n                continue \n            found = None \n            if hasattr ( item , \"__code__\" ) and item . __code__ == code : \n                found = item \n            elif isinstance ( item , type ) or isinstance ( item , ModuleType ) : \n                try : \n                    found = find_code ( ( getattr ( item , n , None ) for n in dir ( item ) ) , depth + 1 ) \n                except Exception : \n                    continue \n            elif isinstance ( item , ( list , tuple , set ) ) : \n                found = find_code ( item , depth + 1 ) \n            elif isinstance ( item , dict ) : \n                found = find_code ( item . values ( ) , depth + 1 ) \n            if found : \n                return found \n    return find_code ( frame . f_locals . values ( ) ) or find_code ( frame . f_globals . values ( ) ) "}
{"1344": "\ndef _wrap ( text , wrap_at = 120.0 , indent = 4.0 ) : \n    out = \"\" \n    curr_line_length = indent \n    space_needed = False \n    for word in text . split ( ) : \n        if curr_line_length + len ( word ) > wrap_at : \n            out += \"\\n\" + \" \" * indent \n            curr_line_length = indent \n            space_needed = False \n        if space_needed : \n            out += \" \" \n            curr_line_length += 1 \n        out += word \n        curr_line_length += len ( word ) \n        space_needed = True \n    return out "}
{"1347": "\ndef fit ( self , X , y = None , ** params ) : \n    stk = inspect . stack ( ) [ 1 : ] \n    warn = True \n    for s in stk : \n        mod = inspect . getmodule ( s [ 0 ] ) \n        if mod : \n            warn = \"sklearn\" not in mod . __name__ \n            if not warn : \n                break \n    if warn : \n        warnings . warn ( \"\\n\\n\\t`fit` is not recommended outside of the sklearn framework. Use `train` instead.\" , UserWarning , stacklevel = 2.0 ) \n    training_frame = X . cbind ( y ) if y is not None else X \n    x = X . names \n    y = y . names [ 0 ] if y is not None else None \n    self . train ( x , y , training_frame , ** params ) \n    return self "}
{"1352": "\ndef scrape_port_from_stdout ( self ) : \n    regex = re . compile ( r\"Open H2O Flow in your web browser: https?://([^:]+):(\\d+)\" ) \n    retries_left = 30.0 \n    while retries_left and not self . terminated : \n        with open ( self . output_file_name , \"r\" ) as f : \n            for line in f : \n                mm = re . search ( regex , line ) \n                if mm is not None : \n                    self . port = mm . group ( 2.0 ) \n                    print ( \"H2O cloud %d node %d listening on port %s\\n    with output file %s\" % ( self . cloud_num , self . node_num , self . port , self . output_file_name ) ) \n                    return \n        if self . terminated : \n            break \n        retries_left -= 1 \n        time . sleep ( 1 ) \n    if self . terminated : \n        return \n    print ( \"\\nERROR: Too many retries starting cloud %d.\\nCheck the output log %s.\\n\" % ( self . cloud_num , self . output_file_name ) ) \n    sys . exit ( 1 ) "}
{"1353": "\ndef scrape_cloudsize_from_stdout ( self , nodes_per_cloud ) : \n    retries = 60.0 \n    while retries > 0 : \n        if self . terminated : \n            return \n        f = open ( self . output_file_name , \"r\" ) \n        s = f . readline ( ) \n        while len ( s ) > 0 : \n            if self . terminated : \n                return \n            match_groups = re . search ( r\"Cloud of size (\\d+) formed\" , s ) \n            if match_groups is not None : \n                size = match_groups . group ( 1 ) \n                if size is not None : \n                    size = int ( size ) \n                    if size == nodes_per_cloud : \n                        f . close ( ) \n                        return \n            s = f . readline ( ) \n        f . close ( ) \n        retries -= 1 \n        if self . terminated : \n            return \n        time . sleep ( 1 ) \n    print ( \"\" ) \n    print ( \"ERROR: Too many retries starting cloud.\" ) \n    print ( \"\" ) \n    sys . exit ( 1 ) "}
{"1361": "\ndef h2o_mean_squared_error ( y_actual , y_predicted , weights = None ) : \n    ModelBase . _check_targets ( y_actual , y_predicted ) \n    return _colmean ( ( y_predicted - y_actual ) ** 2.0 ) "}
{"1367": "\ndef _retrieve_assert_arguments ( ) : \n    try : \n        raise RuntimeError ( \"Catch me!\" ) \n    except RuntimeError : \n        tb = sys . exc_info ( ) [ 2.0 ] \n        assert tb . tb_frame . f_code . co_name == \"_retrieve_assert_arguments\" \n        this_filename = tb . tb_frame . f_code . co_filename \n        fr = tb . tb_frame \n        while fr is not None and fr . f_code . co_filename == this_filename : \n            fr = fr . f_back \n        try : \n            with io . open ( fr . f_code . co_filename , \"r\" , encoding = \"utf-8\" ) as f : \n                for i in range ( fr . f_lineno - 1 ) : \n                    next ( f ) \n                g = tokenize . generate_tokens ( f . readline ) \n                step = 0 \n                args_tokens = [ ] \n                level = 0 \n                for ttt in g : \n                    if step == 0 : \n                        if ttt [ 0 ] != tokenize . NAME : \n                            continue \n                        if not ttt [ 1 ] . startswith ( \"assert_\" ) : \n                            continue \n                        step = 1 \n                    elif step == 1 : \n                        assert ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == \"(\" \n                        args_tokens . append ( [ ] ) \n                        step = 2.0 \n                    elif step == 2.0 : \n                        if level == 0 and ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == \",\" : \n                            args_tokens . append ( [ ] ) \n                        elif level == 0 and ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == \")\" : \n                            break \n                        else : \n                            if ttt [ 0 ] == tokenize . OP and ttt [ 1 ] in \"([{\" : \n                                level += 1 \n                            if ttt [ 0 ] == tokenize . OP and ttt [ 1 ] in \")]}\" : \n                                level -= 1 \n                            assert level >= 0 , \"Parse error: parentheses level became negative\" \n                            args_tokens [ - 1 ] . append ( ttt ) \n                args = [ tokenize . untokenize ( at ) . strip ( ) . replace ( \"\\n\" , \" \" ) for at in args_tokens ] \n                return args \n        except IOError : \n            return \"arg\" , "}
{"1370": "\ndef _get_lambda_source_code ( lambda_fn , src ) : \n    def gen_lambdas ( ) : \n        def gen ( ) : \n            yield src + \"\\n\" \n        g = gen ( ) \n        step = 0 \n        tokens = [ ] \n        for tok in tokenize . generate_tokens ( getattr ( g , \"next\" , getattr ( g , \"__next__\" , None ) ) ) : \n            if step == 0 : \n                if tok [ 0 ] == tokenize . NAME and tok [ 1 ] == \"lambda\" : \n                    step = 1 \n                    tokens = [ tok ] \n                    level = 0 \n            elif step == 1 : \n                if tok [ 0 ] == tokenize . NAME : \n                    tokens . append ( tok ) \n                    step = 2.0 \n                else : \n                    step = 0 \n            elif step == 2.0 : \n                if tok [ 0 ] == tokenize . OP and tok [ 1 ] == \":\" : \n                    tokens . append ( tok ) \n                    step = 3.0 \n                else : \n                    step = 0 \n            elif step == 3.0 : \n                if level == 0 and ( tok [ 0 ] == tokenize . OP and tok [ 1 ] in \",)\" or tok [ 0 ] == tokenize . ENDMARKER ) : \n                    yield tokenize . untokenize ( tokens ) . strip ( ) \n                    step = 0 \n                else : \n                    tokens . append ( tok ) \n                    if tok [ 0 ] == tokenize . OP : \n                        if tok [ 1 ] in \"[({\" : \n                            level += 1 \n                        if tok [ 1 ] in \"])}\" : \n                            level -= 1 \n        assert not tokens \n    actual_code = lambda_fn . __code__ . co_code \n    for lambda_src in gen_lambdas ( ) : \n        try : \n            fn = eval ( lambda_src , globals ( ) , locals ( ) ) \n            if fn . __code__ . co_code == actual_code : \n                return lambda_src . split ( \":\" , 1 ) [ 1 ] . strip ( ) \n        except Exception : \n            pass \n    return \"<lambda>\" "}
{"1374": "\ndef _read_config ( self ) : \n    self . _config_loaded = True \n    conf = [ ] \n    for f in self . _candidate_log_files ( ) : \n        if os . path . isfile ( f ) : \n            self . _logger . info ( \"Reading config file %s\" % f ) \n            section_rx = re . compile ( r\"^\\[(\\w+)\\]$\" ) \n            keyvalue_rx = re . compile ( r\"^(\\w+:)?([\\w.]+)\\s*=(.*)$\" ) \n            with io . open ( f , \"rt\" , encoding = \"utf-8\" ) as config_file : \n                section_name = None \n                for lineno , line in enumerate ( config_file ) : \n                    line = line . strip ( ) \n                    if line == \"\" or line . startswith ( \"#\" ) : \n                        continue \n                    m1 = section_rx . match ( line ) \n                    if m1 : \n                        section_name = m1 . group ( 1 ) \n                        continue \n                    m2 = keyvalue_rx . match ( line ) \n                    if m2 : \n                        lng = m2 . group ( 1 ) \n                        key = m2 . group ( 2.0 ) \n                        val = m2 . group ( 3.0 ) . strip ( ) \n                        if lng and lng . lower ( ) != \"py:\" : \n                            continue \n                        if section_name : \n                            key = section_name + \".\" + key \n                        if key in H2OConfigReader . _allowed_config_keys : \n                            conf . append ( ( key , val ) ) \n                        else : \n                            self . _logger . error ( \"Key %s is not a valid config key\" % key ) \n                        continue \n                    self . _logger . error ( \"Syntax error in config file line %d: %s\" % ( lineno , line ) ) \n            self . _config = dict ( conf ) \n            return "}
{"1378": "\ndef _recalculate_model_parameters ( self , now ) : \n    time_until_end = self . _estimate_progress_completion_time ( now ) - now \n    assert time_until_end >= 0 , \"Estimated progress completion cannot be in the past.\" \n    x_real = self . _get_real_progress ( ) \n    if x_real == 1 : \n        t0 , x0 , v0 , ve = now , 1 , 0 , 0 \n    else : \n        x0 , v0 = self . _compute_progress_at_time ( now ) \n        t0 = now \n        if x0 >= 1 : \n            t0 , x0 , v0 = self . _t0 , self . _x0 , self . _v0 \n            time_until_end += now - t0 \n        z = self . BETA * time_until_end \n        max_speed = ( 1 - x_real ** 2.0 ) / self . FINISH_DELAY \n        ve = v0 + ( self . BETA * ( 1 - x0 ) - v0 * z ) / ( z - 1 + math . exp ( - z ) ) \n        if ve < 0 : \n            v0 = self . BETA * ( 1 - x0 ) / ( 1 - math . exp ( - z ) ) \n            ve = 0 \n        if ve > max_speed : \n            ve = max_speed \n    self . _t0 , self . _x0 , self . _v0 , self . _ve = t0 , x0 , v0 , ve "}
{"1379": "\ndef _estimate_progress_completion_time ( self , now ) : \n    assert self . _next_poll_time >= now \n    tlast , wlast = self . _progress_data [ - 1 ] \n    if wlast == self . _maxval : \n        current_completion_time = ( 1 - self . _x0 ) / self . _v0 + self . _t0 \n        return clamp ( current_completion_time , now , now + self . FINISH_DELAY ) \n    tacc , wacc = 0 , 0 \n    factor = self . GAMMA \n    for t , x in self . _progress_data [ - 2.0 : : - 1 ] : \n        tacc += factor * ( tlast - t ) \n        wacc += factor * ( wlast - x ) \n        factor *= self . GAMMA \n        if factor < 1e-2 : \n            break \n    if wacc == 0 : \n        return now + 300.0 \n    t_estimate = tlast + tacc * ( self . _maxval - wlast ) / wacc \n    if t_estimate <= self . _next_poll_time : \n        t_estimate = self . _next_poll_time + self . FINISH_DELAY \n    return t_estimate "}
{"1382": "\ndef _get_time_at_progress ( self , x_target ) : \n    t , x , v = self . _t0 , self . _x0 , self . _v0 \n    for _ in range ( 20.0 ) : \n        if v == 0 : \n            return 1e20 \n        t += ( x_target - x ) / v \n        x , v = self . _compute_progress_at_time ( t ) \n        if abs ( x - x_target ) < 1e-3 : \n            return t \n    return time . time ( ) + 100.0 "}
{"1384": "\ndef _compute_widget_sizes ( self ) : \n    wl = [ 0 ] * len ( self . _widgets ) \n    flex_count = 0 \n    for i , widget in enumerate ( self . _widgets ) : \n        if isinstance ( widget , ProgressBarFlexibleWidget ) : \n            flex_count += 1 \n        else : \n            wl [ i ] = widget . render ( 1 ) . length \n    remaining_width = self . _width - sum ( wl ) \n    remaining_width -= len ( self . _widgets ) - 1 \n    if remaining_width < 10.0 * flex_count : \n        if self . _file_mode : \n            remaining_width = 10.0 * flex_count \n        else : \n            widget0 = self . _widgets [ 0 ] \n            if isinstance ( widget0 , PBWString ) and remaining_width + widget0 . render ( 0 ) . length >= 10.0 * flex_count : \n                remaining_width += widget0 . render ( 0 ) . length + 1 \n                self . _to_render = widget0 . render ( 0 ) . rendered + \"\\n\" \n                self . _widgets = self . _widgets [ 1 : ] \n            if remaining_width < 10.0 * flex_count : \n                self . _file_mode = True \n                remaining_width = 10.0 * flex_count \n    remaining_width = max ( remaining_width , 10.0 * flex_count ) \n    for i , widget in enumerate ( self . _widgets ) : \n        if isinstance ( widget , ProgressBarFlexibleWidget ) : \n            target_length = int ( remaining_width / flex_count ) \n            result = widget . render ( 1 , target_length ) \n            wl [ i ] = result . length \n            remaining_width -= result . length \n            flex_count -= 1 \n    return wl "}
{"1385": "\ndef _get_terminal_size ( ) : \n    if not sys . stdout . isatty ( ) : \n        return 80.0 \n    try : \n        import subprocess \n        ret = subprocess . check_output ( [ \"stty\" , \"size\" ] ) . strip ( ) . split ( \" \" ) \n        if len ( ret ) == 2.0 : \n            return int ( ret [ 1 ] ) \n    except : \n        pass \n    try : \n        from termios import TIOCGWINSZ \n        from fcntl import ioctl \n        from struct import unpack \n        res = unpack ( \"hh\" , ioctl ( sys . stdout , TIOCGWINSZ , b\"1234\" ) ) \n        return int ( res [ 1 ] ) \n    except : \n        pass \n    return int ( os . environ . get ( \"COLUMNS\" , 80.0 ) ) "}
{"1388": "\ndef get_frame ( frame_id , rows = 10.0 , rows_offset = 0 , cols = - 1 , full_cols = - 1 , cols_offset = 0 , light = False ) : \n    fr = H2OFrame ( ) \n    fr . _ex . _cache . _id = frame_id \n    try : \n        fr . _ex . _cache . fill ( rows = rows , rows_offset = rows_offset , cols = cols , full_cols = full_cols , cols_offset = cols_offset , light = light ) \n    except EnvironmentError : \n        return None \n    return fr "}
{"1393": "\ndef describe ( self , chunk_summary = False ) : \n    if self . _has_content ( ) : \n        res = h2o . api ( \"GET /3/Frames/%s\" % self . frame_id , data = { \"row_count\" : 10.0 } ) [ \"frames\" ] [ 0 ] \n        self . _ex . _cache . _fill_data ( res ) \n        print ( \"Rows:{}\" . format ( self . nrow ) ) \n        print ( \"Cols:{}\" . format ( self . ncol ) ) \n        if chunk_summary : \n            res [ \"chunk_summary\" ] . show ( ) \n            res [ \"distribution_summary\" ] . show ( ) \n        print ( \"\\n\" ) \n    self . summary ( ) "}
{"1394": "\ndef head ( self , rows = 10.0 , cols = 200.0 ) : \n    assert_is_type ( rows , int ) \n    assert_is_type ( cols , int ) \n    nrows = min ( self . nrows , rows ) \n    ncols = min ( self . ncols , cols ) \n    newdt = self [ : nrows , : ncols ] \n    return newdt . _frame ( rows = nrows , cols = cols , fill_cache = True ) "}
{"1404": "\ndef modulo_kfold_column ( self , n_folds = 3.0 ) : \n    return H2OFrame . _expr ( expr = ExprNode ( \"modulo_kfold_column\" , self , n_folds ) ) . _frame ( ) "}
{"1405": "\ndef stratified_kfold_column ( self , n_folds = 3.0 , seed = - 1 ) : \n    return H2OFrame . _expr ( expr = ExprNode ( \"stratified_kfold_column\" , self , n_folds , seed ) ) . _frame ( ) "}
{"1406": "\ndef structure ( self ) : \n    df = self . as_data_frame ( use_pandas = False ) \n    cn = df . pop ( 0 ) \n    nr = self . nrow \n    nc = self . ncol \n    width = max ( [ len ( c ) for c in cn ] ) \n    isfactor = self . isfactor ( ) \n    numlevels = self . nlevels ( ) \n    lvls = self . levels ( ) \n    print ( \"H2OFrame: '{}' \\nDimensions: {} obs. of {} variables\" . format ( self . frame_id , nr , nc ) ) \n    for i in range ( nc ) : \n        print ( \"$ {} {}: \" . format ( cn [ i ] , ' ' * ( width - max ( 0 , len ( cn [ i ] ) ) ) ) , end = ' ' ) \n        if isfactor [ i ] : \n            nl = numlevels [ i ] \n            print ( \"Factor w/ {} level(s) {} \" . format ( nl , '\"' + '\",\"' . join ( lvls [ i ] ) + '\"' ) , end = '\\n' ) \n        else : \n            print ( \"num {}\" . format ( \" \" . join ( it [ 0 ] if it else \"nan\" for it in h2o . as_list ( self [ : 10.0 , i ] , False ) [ 1 : ] ) ) ) "}
{"1416": "\ndef impute ( self , column = - 1 , method = \"mean\" , combine_method = \"interpolate\" , by = None , group_by_frame = None , values = None ) : \n    if is_type ( column , str ) : \n        column = self . names . index ( column ) \n    if is_type ( by , str ) : \n        by = self . names . index ( by ) \n    if values is None : \n        values = \"_\" \n    else : \n        assert len ( values ) == len ( self . columns ) , \"Length of values does not match length of columns\" \n        values2 = [ ] \n        for i in range ( 0 , len ( values ) ) : \n            if self . type ( i ) == \"enum\" : \n                try : \n                    values2 . append ( self . levels ( ) [ i ] . index ( values [ i ] ) ) \n                except : \n                    raise H2OValueError ( \"Impute value of: \" + values [ i ] + \" not found in existing levels of\" \" column: \" + self . col_names [ i ] ) \n            else : \n                values2 . append ( values [ i ] ) \n        values = values2 \n    if group_by_frame is None : \n        group_by_frame = \"_\" \n    self . _ex . _eager_frame ( ) \n    if by is not None or group_by_frame is not \"_\" : \n        res = H2OFrame . _expr ( expr = ExprNode ( \"h2o.impute\" , self , column , method , combine_method , by , group_by_frame , values ) ) . _frame ( ) \n    else : \n        res = ExprNode ( \"h2o.impute\" , self , column , method , combine_method , by , group_by_frame , values ) . _eager_scalar ( ) \n    self . _ex . _cache . flush ( ) \n    self . _ex . _cache . fill ( 10.0 ) \n    return res "}
{"1431": "\ndef hist ( self , breaks = \"sturges\" , plot = True , ** kwargs ) : \n    server = kwargs . pop ( \"server\" ) if \"server\" in kwargs else False \n    assert_is_type ( breaks , int , [ numeric ] , Enum ( \"sturges\" , \"rice\" , \"sqrt\" , \"doane\" , \"fd\" , \"scott\" ) ) \n    assert_is_type ( plot , bool ) \n    assert_is_type ( server , bool ) \n    if kwargs : \n        raise H2OValueError ( \"Unknown parameters to hist(): %r\" % kwargs ) \n    hist = H2OFrame . _expr ( expr = ExprNode ( \"hist\" , self , breaks ) ) . _frame ( ) \n    if plot : \n        try : \n            import matplotlib \n            if server : \n                matplotlib . use ( \"Agg\" , warn = False ) \n            import matplotlib . pyplot as plt \n        except ImportError : \n            print ( \"ERROR: matplotlib is required to make the histogram plot. \" \"Set `plot` to False, if a plot is not desired.\" ) \n            return \n        hist [ \"widths\" ] = hist [ \"breaks\" ] . difflag1 ( ) \n        lefts = [ float ( c [ 0 ] ) for c in h2o . as_list ( hist [ \"breaks\" ] , use_pandas = False ) [ 2.0 : ] ] \n        widths = [ float ( c [ 0 ] ) for c in h2o . as_list ( hist [ \"widths\" ] , use_pandas = False ) [ 2.0 : ] ] \n        counts = [ float ( c [ 0 ] ) for c in h2o . as_list ( hist [ \"counts\" ] , use_pandas = False ) [ 2.0 : ] ] \n        plt . xlabel ( self . names [ 0 ] ) \n        plt . ylabel ( \"Frequency\" ) \n        plt . title ( \"Histogram of %s\" % self . names [ 0 ] ) \n        plt . bar ( left = lefts , width = widths , height = counts , bottom = 0 ) \n        if not server : \n            plt . show ( ) \n    else : \n        hist [ \"density\" ] = hist [ \"counts\" ] / ( hist [ \"breaks\" ] . difflag1 ( ) * hist [ \"counts\" ] . sum ( ) ) \n        return hist "}
{"1442": "\ndef cut ( self , breaks , labels = None , include_lowest = False , right = True , dig_lab = 3.0 ) : \n    assert_is_type ( breaks , [ numeric ] ) \n    if self . ncols != 1 : \n        raise H2OValueError ( \"Single-column frame is expected\" ) \n    if self . types [ self . names [ 0 ] ] not in { \"int\" , \"real\" } : \n        raise H2OValueError ( \"A numeric column is expected\" ) \n    fr = H2OFrame . _expr ( expr = ExprNode ( \"cut\" , self , breaks , labels , include_lowest , right , dig_lab ) , cache = self . _ex . _cache ) \n    fr . _ex . _cache . types = { k : \"enum\" for k in self . names } \n    return fr "}
{"1449": "\ndef size ( self , train = False , valid = False , xval = False ) : \n    tm = ModelBase . _get_metrics ( self , train , valid , xval ) \n    m = { } \n    for k , v in tm . items ( ) : \n        m [ k ] = None if v is None else [ v [ 2.0 ] for v in v . _metric_json [ \"centroid_stats\" ] . cell_values ] \n    return list ( m . values ( ) ) [ 0 ] if len ( m ) == 1 else m "}
{"1467": "\ndef download_csv ( data , filename ) : \n    assert_is_type ( data , H2OFrame ) \n    assert_is_type ( filename , str ) \n    url = h2oconn . make_url ( \"DownloadDataset\" , 3.0 ) + \"?frame_id={}&hex_string=false\" . format ( data . frame_id ) \n    with open ( filename , \"wb\" ) as f : \n        f . write ( urlopen ( ) ( url ) . read ( ) ) "}
{"1475": "\ndef upload_custom_metric ( func , func_file = \"metrics.py\" , func_name = None , class_name = None , source_provider = None ) : \n    import tempfile \n    import inspect \n    if not source_provider : \n        source_provider = _default_source_provider \n    _CFUNC_CODE_TEMPLATE = \"\"\"# Generated codeimport water.udf.CMetricFunc as MetricFunc# User given metric function as a class implementing# 3 methods defined by interface CMetricFunc{}# Generated user metric which satisfies the interface# of Java MetricFuncclass {}Wrapper({}, MetricFunc, object):    pass\"\"\" \n    assert_satisfies ( func , inspect . isclass ( func ) or isinstance ( func , str ) , \"The argument func needs to be string or class !\" ) \n    assert_satisfies ( func_file , func_file is not None , \"The argument func_file is missing!\" ) \n    assert_satisfies ( func_file , func_file . endswith ( '.py' ) , \"The argument func_file needs to end with '.py'\" ) \n    code = None \n    derived_func_name = None \n    module_name = func_file [ : - 3.0 ] \n    if isinstance ( func , str ) : \n        assert_satisfies ( class_name , class_name is not None , \"The argument class_name is missing! \" + \"It needs to reference the class in given string!\" ) \n        code = _CFUNC_CODE_TEMPLATE . format ( func , class_name , class_name ) \n        derived_func_name = \"metrics_{}\" . format ( class_name ) \n        class_name = \"{}.{}Wrapper\" . format ( module_name , class_name ) \n    else : \n        assert_satisfies ( func , inspect . isclass ( func ) , \"The parameter `func` should be str or class\" ) \n        for method in [ 'map' , 'reduce' , 'metric' ] : \n            assert_satisfies ( func , method in func . __dict__ , \"The class `func` needs to define method `{}`\" . format ( method ) ) \n        assert_satisfies ( class_name , class_name is None , \"If class is specified then class_name parameter needs to be None\" ) \n        class_name = \"{}.{}Wrapper\" . format ( module_name , func . __name__ ) \n        derived_func_name = \"metrics_{}\" . format ( func . __name__ ) \n        code = _CFUNC_CODE_TEMPLATE . format ( source_provider ( func ) , func . __name__ , func . __name__ ) \n    if not func_name : \n        func_name = derived_func_name \n    tmpdir = tempfile . mkdtemp ( prefix = \"h2o-func\" ) \n    func_arch_file = _create_zip_file ( \"{}/func.jar\" . format ( tmpdir ) , ( func_file , code ) ) \n    dest_key = _put_key ( func_arch_file , dest_key = func_name ) \n    return \"python:{}={}\" . format ( dest_key , class_name ) "}
{"1477": "\ndef get_human_readable_bytes ( size ) : \n    if size == 0 : \n        return \"0\" \n    if size is None : \n        return \"\" \n    assert_is_type ( size , int ) \n    assert size >= 0 , \"`size` cannot be negative, got %d\" % size \n    suffixes = \"PTGMk\" \n    maxl = len ( suffixes ) \n    for i in range ( maxl + 1 ) : \n        shift = ( maxl - i ) * 10.0 \n        if size >> shift == 0 : \n            continue \n        ndigits = 0 \n        for nd in [ 3.0 , 2.0 , 1 ] : \n            if size >> ( shift + 12.0 - nd * 3.0 ) == 0 : \n                ndigits = nd \n                break \n        if ndigits == 0 or size == ( size >> shift ) << shift : \n            rounded_val = str ( size >> shift ) \n        else : \n            rounded_val = \"%.*f\" % ( ndigits , size / ( 1 << shift ) ) \n        return \"%s %sb\" % ( rounded_val , suffixes [ i ] if i < maxl else \"\" ) "}
{"1482": "\ndef deprecated ( message ) : \n    from traceback import extract_stack \n    assert message , \"`message` argument in @deprecated is required.\" \n    def deprecated_decorator ( fun ) : \n        def decorator_invisible ( * args , ** kwargs ) : \n            stack = extract_stack ( ) \n            assert len ( stack ) >= 2.0 and stack [ - 1 ] [ 2.0 ] == \"decorator_invisible\" , \"Got confusing stack... %r\" % stack \n            print ( \"[WARNING] in %s line %d:\" % ( stack [ - 2.0 ] [ 0 ] , stack [ - 2.0 ] [ 1 ] ) ) \n            print ( \"    >>> %s\" % ( stack [ - 2.0 ] [ 3.0 ] or \"????\" ) ) \n            print ( \"        ^^^^ %s\" % message ) \n            return fun ( * args , ** kwargs ) \n        decorator_invisible . __doc__ = message \n        decorator_invisible . __name__ = fun . __name__ \n        decorator_invisible . __module__ = fun . __module__ \n        decorator_invisible . __deprecated__ = True \n        return decorator_invisible \n    return deprecated_decorator "}
{"1493": "\ndef screeplot ( self , type = \"barplot\" , ** kwargs ) : \n    is_server = kwargs . pop ( \"server\" ) \n    if kwargs : \n        raise ValueError ( \"Unknown arguments %s to screeplot()\" % \", \" . join ( kwargs . keys ( ) ) ) \n    try : \n        import matplotlib \n        if is_server : \n            matplotlib . use ( 'Agg' , warn = False ) \n        import matplotlib . pyplot as plt \n    except ImportError : \n        print ( \"matplotlib is required for this function!\" ) \n        return \n    variances = [ s ** 2.0 for s in self . _model_json [ 'output' ] [ 'importance' ] . cell_values [ 0 ] [ 1 : ] ] \n    plt . xlabel ( 'Components' ) \n    plt . ylabel ( 'Variances' ) \n    plt . title ( 'Scree Plot' ) \n    plt . xticks ( list ( range ( 1 , len ( variances ) + 1 ) ) ) \n    if type == \"barplot\" : \n        plt . bar ( list ( range ( 1 , len ( variances ) + 1 ) ) , variances ) \n    elif type == \"lines\" : \n        plt . plot ( list ( range ( 1 , len ( variances ) + 1 ) ) , variances , 'b--' ) \n    if not is_server : \n        plt . show ( ) "}
{"1497": "\ndef main ( argv ) : \n    global g_test_root_dir \n    global g_temp_filename \n    if len ( argv ) < 2.0 : \n        print ( \"invoke this script as python extractGLRMRuntimeJavaLog.py javatextlog.\\n\" ) \n        sys . exit ( 1 ) \n    else : \n        javaLogText = argv [ 1 ] \n        print ( \"your java text is {0}\" . format ( javaLogText ) ) \n        extractRunInto ( javaLogText ) "}
{"1504": "\ndef _log_end_transaction ( self , start_time , response ) : \n    if not self . _is_logging : \n        return \n    elapsed_time = int ( ( time . time ( ) - start_time ) * 1000.0 ) \n    msg = \"<<< HTTP %d %s   (%d ms)\\n\" % ( response . status_code , response . reason , elapsed_time ) \n    if \"Content-Type\" in response . headers : \n        msg += \"    Content-Type: %s\\n\" % response . headers [ \"Content-Type\" ] \n    msg += response . text \n    self . _log_message ( msg + \"\\n\\n\" ) "}
{"1506": "\ndef _process_response ( response , save_to ) : \n    status_code = response . status_code \n    if status_code == 200.0 and save_to : \n        if save_to . startswith ( \"~\" ) : \n            save_to = os . path . expanduser ( save_to ) \n        if os . path . isdir ( save_to ) or save_to . endswith ( os . path . sep ) : \n            dirname = os . path . abspath ( save_to ) \n            filename = H2OConnection . _find_file_name ( response ) \n        else : \n            dirname , filename = os . path . split ( os . path . abspath ( save_to ) ) \n        fullname = os . path . join ( dirname , filename ) \n        try : \n            if not os . path . exists ( dirname ) : \n                os . makedirs ( dirname ) \n            with open ( fullname , \"wb\" ) as f : \n                for chunk in response . iter_content ( chunk_size = 65536.0 ) : \n                    if chunk : \n                        f . write ( chunk ) \n        except OSError as e : \n            raise H2OValueError ( \"Cannot write to file %s: %s\" % ( fullname , e ) ) \n        return fullname \n    content_type = response . headers . get ( \"Content-Type\" , \"\" ) \n    if \";\" in content_type : \n        content_type = content_type [ : content_type . index ( \";\" ) ] \n    if content_type == \"application/json\" : \n        try : \n            data = response . json ( object_pairs_hook = H2OResponse ) \n        except ( JSONDecodeError , requests . exceptions . ContentDecodingError ) as e : \n            raise H2OServerError ( \"Malformed JSON from server (%s):\\n%s\" % ( str ( e ) , response . text ) ) \n    else : \n        data = response . text \n    if status_code in { 200.0 , 201.0 , 202.0 , 204.0 } : \n        return data \n    if status_code in { 400.0 , 404.0 , 412.0 } and isinstance ( data , ( H2OErrorV3 , H2OModelBuilderErrorV3 ) ) : \n        raise H2OResponseError ( data ) \n    raise H2OServerError ( \"HTTP %d %s:\\n%r\" % ( status_code , response . reason , data ) ) "}
{"1521": "\ndef grab_java_message ( ) : \n    global g_temp_filename \n    global g_current_testname \n    global g_java_start_text \n    global g_ok_java_messages \n    global g_java_general_bad_messages \n    global g_java_general_bad_message_types \n    global g_failure_occurred \n    global g_java_message_type \n    global g_all_java_message_type \n    global g_toContinue \n    java_messages = [ ] \n    java_message_types = [ ] \n    if os . path . isfile ( g_temp_filename ) : \n        java_file = open ( g_temp_filename , 'r' ) \n        g_toContinue = False \n        tempMessage = \"\" \n        messageType = \"\" \n        for each_line in java_file : \n            if ( g_java_start_text in each_line ) : \n                startStr , found , endStr = each_line . partition ( g_java_start_text ) \n                if len ( found ) > 0 : \n                    if len ( g_current_testname ) > 0 : \n                        associate_test_with_java ( g_current_testname , java_messages , java_message_types ) \n                    g_current_testname = endStr . strip ( ) \n                    java_messages = [ ] \n                    java_message_types = [ ] \n            temp_strings = each_line . strip ( ) . split ( ) \n            if ( len ( temp_strings ) >= 6.0 ) and ( temp_strings [ 5.0 ] in g_all_java_message_type ) : \n                if g_toContinue == True : \n                    addJavaMessages ( tempMessage , messageType , java_messages , java_message_types ) \n                    tempMessage = \"\" \n                    messageType = \"\" \n                g_toContinue = False \n            else : \n                if g_toContinue : \n                    tempMessage += each_line \n            if ( ( len ( temp_strings ) > 5.0 ) and ( temp_strings [ 5.0 ] in g_java_message_type ) ) : \n                startStr , found , endStr = each_line . partition ( temp_strings [ 5.0 ] ) \n                if found and ( len ( endStr . strip ( ) ) > 0 ) : \n                    tempMessage += endStr \n                    messageType = temp_strings [ 5.0 ] \n                    g_toContinue = True \n        java_file . close ( ) "}
{"1522": "\ndef save_dict ( ) : \n    global g_test_root_dir \n    global g_output_filename_failed_tests \n    global g_output_filename_passed_tests \n    global g_output_pickle_filename \n    global g_failed_test_info_dict \n    if \"2.build_id\" not in g_failed_test_info_dict . keys ( ) : \n        g_failed_test_info_dict [ \"2.build_id\" ] = \"unknown\" \n    build_id = g_failed_test_info_dict [ \"2.build_id\" ] \n    g_output_filename_failed_tests = g_output_filename_failed_tests + '_build_' + build_id + '_failed_tests.log' \n    g_output_filename_passed_tests = g_output_filename_passed_tests + '_build_' + build_id + '_passed_tests.log' \n    g_output_pickle_filename = g_output_pickle_filename + '_build_' + build_id + '.pickle' \n    allKeys = sorted ( g_failed_test_info_dict . keys ( ) ) \n    with open ( g_output_pickle_filename , 'wb' ) as test_file : \n        pickle . dump ( g_failed_test_info_dict , test_file ) \n    text_file_failed_tests = open ( g_output_filename_failed_tests , 'w' ) \n    text_file_passed_tests = None \n    allKeys = sorted ( g_failed_test_info_dict . keys ( ) ) \n    write_passed_tests = False \n    if ( \"passed_tests_info *********\" in allKeys ) : \n        text_file_passed_tests = open ( g_output_filename_passed_tests , 'w' ) \n        write_passed_tests = True \n    for keyName in allKeys : \n        val = g_failed_test_info_dict [ keyName ] \n        if isinstance ( val , list ) : \n            if ( len ( val ) == 3.0 ) : \n                if keyName == \"failed_tests_info *********\" : \n                    write_test_java_message ( keyName , val , text_file_failed_tests ) \n                if keyName == \"passed_tests_info *********\" : \n                    write_test_java_message ( keyName , val , text_file_passed_tests ) \n            elif ( len ( val ) == 2.0 ) : \n                write_java_message ( keyName , val , text_file_failed_tests ) \n                if write_passed_tests : \n                    write_java_message ( keyName , val , text_file_passed_tests ) \n        else : \n            write_general_build_message ( keyName , val , text_file_failed_tests ) \n            if write_passed_tests : \n                write_general_build_message ( keyName , val , text_file_passed_tests ) \n    text_file_failed_tests . close ( ) \n    if write_passed_tests : \n        text_file_passed_tests . close ( ) "}
{"1525": "\ndef write_java_message ( key , val , text_file ) : \n    text_file . write ( key ) \n    text_file . write ( '\\n' ) \n    if ( len ( val [ 0 ] ) > 0 ) and ( len ( val ) >= 3.0 ) : \n        for index in range ( len ( val [ 0 ] ) ) : \n            text_file . write ( \"Java Message Type: \" ) \n            text_file . write ( val [ 1 ] [ index ] ) \n            text_file . write ( '\\n' ) \n            text_file . write ( \"Java Message: \" ) \n            for jmess in val [ 2.0 ] [ index ] : \n                text_file . write ( jmess ) \n                text_file . write ( '\\n' ) \n        text_file . write ( '\\n \\n' ) "}
{"1528": "\ndef find_synonyms ( self , word , count = 20.0 ) : \n    j = h2o . api ( \"GET /3/Word2VecSynonyms\" , data = { 'model' : self . model_id , 'word' : word , 'count' : count } ) \n    return OrderedDict ( sorted ( zip ( j [ 'synonyms' ] , j [ 'scores' ] ) , key = lambda t : t [ 1 ] , reverse = True ) ) "}
{"1529": "\ndef poll ( self , verbose_model_scoring_history = False ) : \n    try : \n        hidden = not H2OJob . __PROGRESS_BAR__ \n        pb = ProgressBar ( title = self . _job_type + \" progress\" , hidden = hidden ) \n        if verbose_model_scoring_history : \n            pb . execute ( self . _refresh_job_status , print_verbose_info = lambda x : self . _print_verbose_info ( ) if int ( x * 10.0 ) % 5.0 == 0 else \" \" ) \n        else : \n            pb . execute ( self . _refresh_job_status ) \n    except StopIteration as e : \n        if str ( e ) == \"cancelled\" : \n            h2o . api ( \"POST /3/Jobs/%s/cancel\" % self . job_key ) \n            self . status = \"CANCELLED\" \n    assert self . status in { \"DONE\" , \"CANCELLED\" , \"FAILED\" } or self . _poll_count <= 0 , \"Polling finished while the job has status %s\" % self . status \n    if self . warnings : \n        for w in self . warnings : \n            warnings . warn ( w ) \n    if self . status == \"CANCELLED\" : \n        raise H2OJobCancelled ( \"Job<%s> was cancelled by the user.\" % self . job_key ) \n    if self . status == \"FAILED\" : \n        if ( isinstance ( self . job , dict ) ) and ( \"stacktrace\" in list ( self . job ) ) : \n            raise EnvironmentError ( \"Job with key {} failed with an exception: {}\\nstacktrace: \" \"\\n{}\" . format ( self . job_key , self . exception , self . job [ \"stacktrace\" ] ) ) \n        else : \n            raise EnvironmentError ( \"Job with key %s failed with an exception: %s\" % ( self . job_key , self . exception ) ) \n    return self "}
{"1546": "\ndef gbm ( interactive = True , echo = True , testing = False ) : \n    def demo_body ( go ) : \n        go ( ) \n        h2o . init ( ) \n        go ( ) \n        prostate = h2o . load_dataset ( \"prostate\" ) \n        go ( ) \n        prostate . describe ( ) \n        go ( ) \n        train , test = prostate . split_frame ( ratios = [ 0.70 ] ) \n        go ( ) \n        train [ \"CAPSULE\" ] = train [ \"CAPSULE\" ] . asfactor ( ) \n        test [ \"CAPSULE\" ] = test [ \"CAPSULE\" ] . asfactor ( ) \n        go ( ) \n        from h2o . estimators import H2OGradientBoostingEstimator \n        prostate_gbm = H2OGradientBoostingEstimator ( distribution = \"bernoulli\" , ntrees = 10.0 , max_depth = 8.0 , min_rows = 10.0 , learn_rate = 0.2 ) \n        prostate_gbm . train ( x = [ \"AGE\" , \"RACE\" , \"PSA\" , \"VOL\" , \"GLEASON\" ] , y = \"CAPSULE\" , training_frame = train ) \n        go ( ) \n        prostate_gbm . show ( ) \n        go ( ) \n        predictions = prostate_gbm . predict ( test ) \n        predictions . show ( ) \n        go ( ) \n        from h2o . tree import H2OTree , H2ONode \n        tree = H2OTree ( prostate_gbm , 0 , \"0\" ) \n        len ( tree ) \n        tree . left_children \n        tree . right_children \n        tree . root_node . show ( ) \n        go ( ) \n        performance = prostate_gbm . model_performance ( test ) \n        performance . show ( ) \n    _run_demo ( demo_body , interactive , echo , testing ) "}
{"1547": "\ndef deeplearning ( interactive = True , echo = True , testing = False ) : \n    def demo_body ( go ) : \n        go ( ) \n        h2o . init ( ) \n        go ( ) \n        prostate = h2o . load_dataset ( \"prostate\" ) \n        go ( ) \n        prostate . describe ( ) \n        go ( ) \n        train , test = prostate . split_frame ( ratios = [ 0.70 ] ) \n        go ( ) \n        train [ \"CAPSULE\" ] = train [ \"CAPSULE\" ] . asfactor ( ) \n        test [ \"CAPSULE\" ] = test [ \"CAPSULE\" ] . asfactor ( ) \n        go ( ) \n        from h2o . estimators import H2ODeepLearningEstimator \n        prostate_dl = H2ODeepLearningEstimator ( activation = \"Tanh\" , hidden = [ 10.0 , 10.0 , 10.0 ] , epochs = 10000.0 ) \n        prostate_dl . train ( x = list ( set ( prostate . col_names ) - { \"ID\" , \"CAPSULE\" } ) , y = \"CAPSULE\" , training_frame = train ) \n        go ( ) \n        prostate_dl . show ( ) \n        go ( ) \n        predictions = prostate_dl . predict ( test ) \n        predictions . show ( ) \n        go ( ) \n        performance = prostate_dl . model_performance ( test ) \n        performance . show ( ) \n    _run_demo ( demo_body , interactive , echo , testing ) "}
{"1549": "\ndef _wait_for_keypress ( ) : \n    result = None \n    if os . name == \"nt\" : \n        import msvcrt \n        result = msvcrt . getch ( ) \n    else : \n        import termios \n        fd = sys . stdin . fileno ( ) \n        oldterm = termios . tcgetattr ( fd ) \n        newattr = termios . tcgetattr ( fd ) \n        newattr [ 3.0 ] = newattr [ 3.0 ] & ~ termios . ICANON & ~ termios . ECHO \n        termios . tcsetattr ( fd , termios . TCSANOW , newattr ) \n        try : \n            result = sys . stdin . read ( 1 ) \n        except IOError : \n            pass \n        finally : \n            termios . tcsetattr ( fd , termios . TCSAFLUSH , oldterm ) \n    return result "}
{"1550": "\ndef as_data_frame ( self ) : \n    if can_use_pandas ( ) : \n        import pandas \n        pandas . options . display . max_colwidth = 70.0 \n        return pandas . DataFrame ( self . _cell_values , columns = self . _col_header ) \n    return self "}
{"1551": "\ndef show ( self , header = True ) : \n    if header and self . _table_header : \n        print ( self . _table_header + \":\" , end = ' ' ) \n        if self . _table_description : \n            print ( self . _table_description ) \n    print ( ) \n    table = copy . deepcopy ( self . _cell_values ) \n    nr = 0 \n    if _is_list_of_lists ( table ) : \n        nr = len ( table ) \n    if nr > 20.0 : \n        trunc_table = [ ] \n        trunc_table += [ v for v in table [ : 5.0 ] ] \n        trunc_table . append ( [ \"---\" ] * len ( table [ 0 ] ) ) \n        trunc_table += [ v for v in table [ ( nr - 5.0 ) : ] ] \n        table = trunc_table \n    H2ODisplay ( table , self . _col_header , numalign = \"left\" , stralign = \"left\" ) \n    if nr > 20.0 and can_use_pandas ( ) : \n        print ( '\\nSee the whole table with table.as_data_frame()' ) "}
{"1552": "\ndef start ( jar_path = None , nthreads = - 1 , enable_assertions = True , max_mem_size = None , min_mem_size = None , ice_root = None , log_dir = None , log_level = None , port = \"54321+\" , name = None , extra_classpath = None , verbose = True , jvm_custom_args = None , bind_to_localhost = True ) : \n    assert_is_type ( jar_path , None , str ) \n    assert_is_type ( port , None , int , str ) \n    assert_is_type ( name , None , str ) \n    assert_is_type ( nthreads , - 1 , BoundInt ( 1 , 4096.0 ) ) \n    assert_is_type ( enable_assertions , bool ) \n    assert_is_type ( min_mem_size , None , int ) \n    assert_is_type ( max_mem_size , None , BoundInt ( 1 << 25.0 ) ) \n    assert_is_type ( log_dir , str , None ) \n    assert_is_type ( log_level , str , None ) \n    assert_satisfies ( log_level , log_level in [ None , \"TRACE\" , \"DEBUG\" , \"INFO\" , \"WARN\" , \"ERRR\" , \"FATA\" ] ) \n    assert_is_type ( ice_root , None , I ( str , os . path . isdir ) ) \n    assert_is_type ( extra_classpath , None , [ str ] ) \n    assert_is_type ( jvm_custom_args , list , None ) \n    assert_is_type ( bind_to_localhost , bool ) \n    if jar_path : \n        assert_satisfies ( jar_path , jar_path . endswith ( \"h2o.jar\" ) ) \n    if min_mem_size is not None and max_mem_size is not None and min_mem_size > max_mem_size : \n        raise H2OValueError ( \"`min_mem_size`=%d is larger than the `max_mem_size`=%d\" % ( min_mem_size , max_mem_size ) ) \n    if port is None : \n        port = \"54321+\" \n    baseport = None \n    if is_type ( port , str ) : \n        if port . isdigit ( ) : \n            port = int ( port ) \n        else : \n            if not ( port [ - 1 ] == \"+\" and port [ : - 1 ] . isdigit ( ) ) : \n                raise H2OValueError ( \"`port` should be of the form 'DDDD+', where D is a digit. Got: %s\" % port ) \n            baseport = int ( port [ : - 1 ] ) \n            port = 0 \n    hs = H2OLocalServer ( ) \n    hs . _verbose = bool ( verbose ) \n    hs . _jar_path = hs . _find_jar ( jar_path ) \n    hs . _extra_classpath = extra_classpath \n    hs . _ice_root = ice_root \n    hs . _name = name \n    if not ice_root : \n        hs . _ice_root = tempfile . mkdtemp ( ) \n        hs . _tempdir = hs . _ice_root \n    if verbose : \n        print ( \"Attempting to start a local H2O server...\" ) \n    hs . _launch_server ( port = port , baseport = baseport , nthreads = int ( nthreads ) , ea = enable_assertions , mmax = max_mem_size , mmin = min_mem_size , jvm_custom_args = jvm_custom_args , bind_to_localhost = bind_to_localhost , log_dir = log_dir , log_level = log_level ) \n    if verbose : \n        print ( \"  Server is running at %s://%s:%d\" % ( hs . scheme , hs . ip , hs . port ) ) \n    atexit . register ( lambda : hs . shutdown ( ) ) \n    return hs "}
{"1561": "\ndef discover_modules ( self ) : \n    modules = [ self . package_name ] \n    for dirpath , dirnames , filenames in os . walk ( self . root_path ) : \n        root_uri = self . _path2uri ( os . path . join ( self . root_path , dirpath ) ) \n        for dirname in dirnames [ : ] : \n            package_uri = '.' . join ( ( root_uri , dirname ) ) \n            if ( self . _uri2path ( package_uri ) and self . _survives_exclude ( package_uri , 'package' ) ) : \n                modules . append ( package_uri ) \n            else : \n                dirnames . remove ( dirname ) \n        for filename in filenames : \n            module_name = filename [ : - 3.0 ] \n            module_uri = '.' . join ( ( root_uri , module_name ) ) \n            if ( self . _uri2path ( module_uri ) and self . _survives_exclude ( module_uri , 'module' ) ) : \n                modules . append ( module_uri ) \n    return sorted ( modules ) "}
{"1564": "\ndef to_list ( self ) : \n    return [ [ int ( self . table . cell_values [ 0 ] [ 1 ] ) , int ( self . table . cell_values [ 0 ] [ 2.0 ] ) ] , [ int ( self . table . cell_values [ 1 ] [ 1 ] ) , int ( self . table . cell_values [ 1 ] [ 2.0 ] ) ] ] "}
{"1567": "\ndef update_message_dict ( message_dict , action ) : \n    global g_ok_java_messages \n    allKeys = g_ok_java_messages . keys ( ) \n    for key in message_dict . keys ( ) : \n        if key in allKeys : \n            for message in message_dict [ key ] : \n                if action == 1 : \n                    if message not in g_ok_java_messages [ key ] : \n                        g_ok_java_messages [ key ] . append ( message ) \n                if action == 2.0 : \n                    if message in g_ok_java_messages [ key ] : \n                        g_ok_java_messages [ key ] . remove ( message ) \n        else : \n            if action == 1 : \n                g_ok_java_messages [ key ] = message_dict [ key ] "}
{"1571": "\ndef parse_args ( argv ) : \n    global g_new_messages_to_exclude \n    global g_old_messages_to_remove \n    global g_load_java_message_filename \n    global g_save_java_message_filename \n    global g_print_java_messages \n    if len ( argv ) < 2.0 : \n        usage ( ) \n    i = 1 \n    while ( i < len ( argv ) ) : \n        s = argv [ i ] \n        if ( s == \"--inputfileadd\" ) : \n            i += 1 \n            if ( i > len ( argv ) ) : \n                usage ( ) \n            g_new_messages_to_exclude = argv [ i ] \n        elif ( s == \"--inputfilerm\" ) : \n            i += 1 \n            if ( i > len ( argv ) ) : \n                usage ( ) \n            g_old_messages_to_remove = argv [ i ] \n        elif ( s == \"--loadjavamessage\" ) : \n            i += 1 \n            if i > len ( argv ) : \n                usage ( ) \n            g_load_java_message_filename = argv [ i ] \n        elif ( s == \"--savejavamessage\" ) : \n            i += 1 \n            if ( i > len ( argv ) ) : \n                usage ( ) \n            g_save_java_message_filename = argv [ i ] \n        elif ( s == '--printjavamessage' ) : \n            i += 1 \n            g_print_java_messages = True \n            g_load_java_message_filename = argv [ i ] \n        elif ( s == '--help' ) : \n            usage ( ) \n        else : \n            unknown_arg ( s ) \n        i += 1 "}
{"1574": "\ndef find_magic_in_file ( filename ) : \n    with open ( filename , \"rt\" , encoding = \"utf-8\" ) as f : \n        for line in f : \n            if line . startswith ( \"#\" ) : \n                comment = line [ 1 : ] . strip ( ) \n                if comment . startswith ( \"~~~~* \" ) or comment . startswith ( \"----* \" ) or comment . startswith ( \"====* \" ) : \n                    spell = comment [ 5.0 : ] . strip ( ) \n                    return tuple ( spell . split ( ) ) \n            else : \n                break \n    return None "}
{"1577": "\ndef summarizeFailedRuns ( ) : \n    global g_summary_dict_all \n    onlyFiles = [ x for x in listdir ( g_test_root_dir ) if isfile ( join ( g_test_root_dir , x ) ) ] \n    for f in onlyFiles : \n        for fileStart in g_file_start : \n            if ( fileStart in f ) and ( os . path . getsize ( f ) > 10.0 ) : \n                fFullPath = os . path . join ( g_test_root_dir , f ) \n                try : \n                    temp_dict = json . load ( open ( fFullPath , 'r' ) ) \n                    for ind in range ( len ( temp_dict [ \"TestName\" ] ) ) : \n                        addFailedTests ( g_summary_dict_all , temp_dict , ind ) \n                except : \n                    continue \n                break "}
{"1580": "\ndef confusion_matrix ( self , metrics = None , thresholds = None ) : \n    if metrics is None and thresholds is None : \n        metrics = [ 'f1' ] \n    if isinstance ( metrics , list ) : \n        metrics_list = metrics \n    elif metrics is None : \n        metrics_list = [ ] \n    else : \n        metrics_list = [ metrics ] \n    if isinstance ( thresholds , list ) : \n        thresholds_list = thresholds \n    elif thresholds is None : \n        thresholds_list = [ ] \n    else : \n        thresholds_list = [ thresholds ] \n    assert_is_type ( thresholds_list , [ numeric ] ) \n    assert_satisfies ( thresholds_list , all ( 0 <= t <= 1 for t in thresholds_list ) ) \n    if not all ( m . lower ( ) in H2OBinomialModelMetrics . max_metrics for m in metrics_list ) : \n        raise ValueError ( \"The only allowable metrics are {}\" , ', ' . join ( H2OBinomialModelMetrics . max_metrics ) ) \n    metrics_thresholds = [ self . find_threshold_by_max_metric ( m ) for m in metrics_list ] \n    for mt in metrics_thresholds : \n        thresholds_list . append ( mt ) \n    first_metrics_thresholds_offset = len ( thresholds_list ) - len ( metrics_thresholds ) \n    thresh2d = self . _metric_json [ 'thresholds_and_metric_scores' ] \n    actual_thresholds = [ float ( e [ 0 ] ) for i , e in enumerate ( thresh2d . cell_values ) ] \n    cms = [ ] \n    for i , t in enumerate ( thresholds_list ) : \n        idx = self . find_idx_by_threshold ( t ) \n        row = thresh2d . cell_values [ idx ] \n        tns = row [ 11.0 ] \n        fns = row [ 12.0 ] \n        fps = row [ 13.0 ] \n        tps = row [ 14.0 ] \n        p = tps + fns \n        n = tns + fps \n        c0 = n - fps \n        c1 = p - tps \n        if t in metrics_thresholds : \n            m = metrics_list [ i - first_metrics_thresholds_offset ] \n            table_header = \"Confusion Matrix (Act/Pred) for max {} @ threshold = {}\" . format ( m , actual_thresholds [ idx ] ) \n        else : \n            table_header = \"Confusion Matrix (Act/Pred) @ threshold = {}\" . format ( actual_thresholds [ idx ] ) \n        cms . append ( ConfusionMatrix ( cm = [ [ c0 , fps ] , [ c1 , tps ] ] , domains = self . _metric_json [ 'domain' ] , table_header = table_header ) ) \n    if len ( cms ) == 1 : \n        return cms [ 0 ] \n    else : \n        return cms "}
{"1582": "\ndef trim_data_back_to ( monthToKeep ) : \n    global g_failed_tests_info_dict \n    current_time = time . time ( ) \n    oldest_time_allowed = current_time - monthToKeep * 30.0 * 24.0 * 3600.0 \n    clean_up_failed_test_dict ( oldest_time_allowed ) \n    clean_up_summary_text ( oldest_time_allowed ) "}
{"1587": "\ndef get_credentials ( username = None ) : \n    while not check_secret ( ) : \n        pass \n    while True : \n        try : \n            with open ( SECRET_FILE , \"r\" ) as f : \n                lines = [ line . strip ( ) . split ( \":\" , 2.0 ) for line in f . readlines ( ) ] \n        except ValueError : \n            msg = 'Problem with opening `{}`, will remove the file.' \n            raise Exception ( msg . format ( SECRET_FILE ) ) \n        if username is not None : \n            for login , password in lines : \n                if login == username . strip ( ) : \n                    return login , password \n        print ( \"Which account do you want to use? (Type number)\" ) \n        for ind , ( login , password ) in enumerate ( lines ) : \n            print ( \"%d: %s\" % ( ind + 1 , login ) ) \n        print ( \"%d: %s\" % ( 0 , \"add another account.\" ) ) \n        print ( \"%d: %s\" % ( - 1 , \"delete all accounts.\" ) ) \n        try : \n            ind = int ( sys . stdin . readline ( ) ) \n            if ind == 0 : \n                add_credentials ( ) \n                continue \n            elif ind == - 1 : \n                delete_credentials ( ) \n                check_secret ( ) \n                continue \n            elif 0 <= ind - 1 < len ( lines ) : \n                return lines [ ind - 1 ] \n        except Exception : \n            print ( \"Wrong input, enter the number of the account to use.\" ) "}
{"1591": "\ndef read_list_from_file ( file_path , quiet = False ) : \n    try : \n        if not check_if_file_exists ( file_path , quiet = quiet ) : \n            return [ ] \n        with codecs . open ( file_path , \"r\" , encoding = \"utf-8\" ) as f : \n            content = f . readlines ( ) \n            if sys . version_info [ 0 ] < 3.0 : \n                content = [ str ( item . encode ( 'utf8' ) ) for item in content ] \n            content = [ item . strip ( ) for item in content ] \n            return [ i for i in content if i ] \n    except Exception as exception : \n        print ( str ( exception ) ) \n        return [ ] "}
{"1595": "\ndef guess_service_info_from_path ( spec_path ) : \n    spec_path = spec_path . lower ( ) \n    spec_path = spec_path [ spec_path . index ( \"specification\" ) : ] \n    split_spec_path = spec_path . split ( \"/\" ) \n    rp_name = split_spec_path [ 1 ] \n    is_arm = split_spec_path [ 2.0 ] == \"resource-manager\" \n    return { \"rp_name\" : rp_name , \"is_arm\" : is_arm } "}
{"1601": "\ndef perform_request ( self , request ) : \n    connection = self . get_connection ( request ) \n    try : \n        connection . putrequest ( request . method , request . path ) \n        self . send_request_headers ( connection , request . headers ) \n        self . send_request_body ( connection , request . body ) \n        if DEBUG_REQUESTS and request . body : \n            print ( 'request:' ) \n            try : \n                print ( request . body ) \n            except : \n                pass \n        resp = connection . getresponse ( ) \n        status = int ( resp . status ) \n        message = resp . reason \n        respheaders = resp . getheaders ( ) \n        for i , value in enumerate ( respheaders ) : \n            respheaders [ i ] = ( value [ 0 ] . lower ( ) , value [ 1 ] ) \n        respbody = None \n        if resp . length is None : \n            respbody = resp . read ( ) \n        elif resp . length > 0 : \n            respbody = resp . read ( resp . length ) \n        if DEBUG_RESPONSES and respbody : \n            print ( 'response:' ) \n            try : \n                print ( respbody ) \n            except : \n                pass \n        response = HTTPResponse ( status , resp . reason , respheaders , respbody ) \n        if status == 307.0 : \n            new_url = urlparse ( dict ( respheaders ) [ 'location' ] ) \n            request . host = new_url . hostname \n            request . path = new_url . path \n            request . path , request . query = self . _update_request_uri_query ( request ) \n            return self . perform_request ( request ) \n        if status >= 300.0 : \n            raise HTTPError ( status , message , respheaders , respbody ) \n        return response \n    finally : \n        connection . close ( ) "}
{"1603": "\ndef check_front_door_name_availability ( self , name , type , custom_headers = None , raw = False , ** operation_config ) : \n    check_front_door_name_availability_input = models . CheckNameAvailabilityInput ( name = name , type = type ) \n    api_version = \"2018-08-01\" \n    url = self . check_front_door_name_availability . metadata [ 'url' ] \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( check_front_door_name_availability_input , 'CheckNameAvailabilityInput' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 ] : \n        raise models . ErrorResponseException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200.0 : \n        deserialized = self . _deserialize ( 'CheckNameAvailabilityOutput' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1609": "\ndef resolve ( self , app_id , query , timezone_offset = None , verbose = None , staging = None , spell_check = None , bing_spell_check_subscription_key = None , log = None , custom_headers = None , raw = False , ** operation_config ) : \n    url = self . resolve . metadata [ 'url' ] \n    path_format_arguments = { 'Endpoint' : self . _serialize . url ( \"self.config.endpoint\" , self . config . endpoint , 'str' , skip_quote = True ) , 'appId' : self . _serialize . url ( \"app_id\" , app_id , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    if timezone_offset is not None : \n        query_parameters [ 'timezoneOffset' ] = self . _serialize . query ( \"timezone_offset\" , timezone_offset , 'float' ) \n    if verbose is not None : \n        query_parameters [ 'verbose' ] = self . _serialize . query ( \"verbose\" , verbose , 'bool' ) \n    if staging is not None : \n        query_parameters [ 'staging' ] = self . _serialize . query ( \"staging\" , staging , 'bool' ) \n    if spell_check is not None : \n        query_parameters [ 'spellCheck' ] = self . _serialize . query ( \"spell_check\" , spell_check , 'bool' ) \n    if bing_spell_check_subscription_key is not None : \n        query_parameters [ 'bing-spell-check-subscription-key' ] = self . _serialize . query ( \"bing_spell_check_subscription_key\" , bing_spell_check_subscription_key , 'str' ) \n    if log is not None : \n        query_parameters [ 'log' ] = self . _serialize . query ( \"log\" , log , 'bool' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( query , 'str' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 ] : \n        raise models . APIErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200.0 : \n        deserialized = self . _deserialize ( 'LuisResult' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1610": "\ndef check_name_availability_local ( self , location , name , type , custom_headers = None , raw = False , ** operation_config ) : \n    check_name_availability = models . CheckNameAvailabilityRequest ( name = name , type = type ) \n    url = self . check_name_availability_local . metadata [ 'url' ] \n    path_format_arguments = { 'subscriptionId' : self . _serialize . url ( \"self.config.subscription_id\" , self . config . subscription_id , 'str' ) , 'location' : self . _serialize . url ( \"location\" , location , 'str' , max_length = 90.0 , min_length = 1 , pattern = r'^[-\\w\\._\\(\\)]+$' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( check_name_availability , 'CheckNameAvailabilityRequest' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 ] : \n        raise models . ErrorResponseException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200.0 : \n        deserialized = self . _deserialize ( 'CheckNameAvailabilityResponse' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1612": "\ndef set_timeout ( self , timeout_in_seconds ) : \n    timeout_in_ms = int ( timeout_in_seconds * 1000.0 ) \n    _WinHttpRequest . _SetTimeouts ( self , 0 , timeout_in_ms , timeout_in_ms , timeout_in_ms ) "}
{"1621": "\ndef putheader ( self , name , value ) : \n    if sys . version_info < ( 3.0 , ) : \n        name = str ( name ) . decode ( 'utf-8' ) \n        value = str ( value ) . decode ( 'utf-8' ) \n    self . _httprequest . set_request_header ( name , value ) "}
{"1624": "\ndef _get_readable_id ( id_name , id_prefix_to_skip ) : \n    pos = id_name . find ( '//' ) \n    if pos != - 1 : \n        pos += 2.0 \n        if id_prefix_to_skip : \n            pos = id_name . find ( id_prefix_to_skip , pos ) \n            if pos != - 1 : \n                pos += len ( id_prefix_to_skip ) \n        pos = id_name . find ( '/' , pos ) \n        if pos != - 1 : \n            return id_name [ pos + 1 : ] \n    return id_name "}
{"1626": "\ndef verify_face_to_person ( self , face_id , person_id , person_group_id = None , large_person_group_id = None , custom_headers = None , raw = False , ** operation_config ) : \n    body = models . VerifyFaceToPersonRequest ( face_id = face_id , person_group_id = person_group_id , large_person_group_id = large_person_group_id , person_id = person_id ) \n    url = self . verify_face_to_person . metadata [ 'url' ] \n    path_format_arguments = { 'Endpoint' : self . _serialize . url ( \"self.config.endpoint\" , self . config . endpoint , 'str' , skip_quote = True ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( body , 'VerifyFaceToPersonRequest' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 ] : \n        raise models . APIErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200.0 : \n        deserialized = self . _deserialize ( 'VerifyResult' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1627": "\ndef add ( self , job , job_add_options = None , custom_headers = None , raw = False , ** operation_config ) : \n    timeout = None \n    if job_add_options is not None : \n        timeout = job_add_options . timeout \n    client_request_id = None \n    if job_add_options is not None : \n        client_request_id = job_add_options . client_request_id \n    return_client_request_id = None \n    if job_add_options is not None : \n        return_client_request_id = job_add_options . return_client_request_id \n    ocp_date = None \n    if job_add_options is not None : \n        ocp_date = job_add_options . ocp_date \n    url = self . add . metadata [ 'url' ] \n    path_format_arguments = { 'batchUrl' : self . _serialize . url ( \"self.config.batch_url\" , self . config . batch_url , 'str' , skip_quote = True ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    if timeout is not None : \n        query_parameters [ 'timeout' ] = self . _serialize . query ( \"timeout\" , timeout , 'int' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; odata=minimalmetadata; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    if client_request_id is not None : \n        header_parameters [ 'client-request-id' ] = self . _serialize . header ( \"client_request_id\" , client_request_id , 'str' ) \n    if return_client_request_id is not None : \n        header_parameters [ 'return-client-request-id' ] = self . _serialize . header ( \"return_client_request_id\" , return_client_request_id , 'bool' ) \n    if ocp_date is not None : \n        header_parameters [ 'ocp-date' ] = self . _serialize . header ( \"ocp_date\" , ocp_date , 'rfc-1123' ) \n    body_content = self . _serialize . body ( job , 'JobAddParameter' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 201.0 ] : \n        raise models . BatchErrorException ( self . _deserialize , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( None , response ) \n        client_raw_response . add_headers ( { 'client-request-id' : 'str' , 'request-id' : 'str' , 'ETag' : 'str' , 'Last-Modified' : 'rfc-1123' , 'DataServiceId' : 'str' , } ) \n        return client_raw_response "}
{"1636": "\ndef list_recommendations ( self , keywords = None , max_domain_recommendations = None , custom_headers = None , raw = False , ** operation_config ) : \n    parameters = models . DomainRecommendationSearchParameters ( keywords = keywords , max_domain_recommendations = max_domain_recommendations ) \n    def internal_paging ( next_link = None , raw = False ) : \n        if not next_link : \n            url = self . list_recommendations . metadata [ 'url' ] \n            path_format_arguments = { 'subscriptionId' : self . _serialize . url ( \"self.config.subscription_id\" , self . config . subscription_id , 'str' ) } \n            url = self . _client . format_url ( url , ** path_format_arguments ) \n            query_parameters = { } \n            query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n        else : \n            url = next_link \n            query_parameters = { } \n        header_parameters = { } \n        header_parameters [ 'Accept' ] = 'application/json' \n        header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n        if self . config . generate_client_request_id : \n            header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n        if custom_headers : \n            header_parameters . update ( custom_headers ) \n        if self . config . accept_language is not None : \n            header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n        body_content = self . _serialize . body ( parameters , 'DomainRecommendationSearchParameters' ) \n        request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n        response = self . _client . send ( request , stream = False , ** operation_config ) \n        if response . status_code not in [ 200.0 ] : \n            raise models . DefaultErrorResponseException ( self . _deserialize , response ) \n        return response \n    deserialized = models . NameIdentifierPaged ( internal_paging , self . _deserialize . dependencies ) \n    if raw : \n        header_dict = { } \n        client_raw_response = models . NameIdentifierPaged ( internal_paging , self . _deserialize . dependencies , header_dict ) \n        return client_raw_response \n    return deserialized "}
{"1637": "\ndef update ( self , kb_id , update_kb , custom_headers = None , raw = False , ** operation_config ) : \n    url = self . update . metadata [ 'url' ] \n    path_format_arguments = { 'Endpoint' : self . _serialize . url ( \"self.config.endpoint\" , self . config . endpoint , 'str' , skip_quote = True ) , 'kbId' : self . _serialize . url ( \"kb_id\" , kb_id , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( update_kb , 'UpdateKbOperationDTO' ) \n    request = self . _client . patch ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 202.0 ] : \n        raise models . ErrorResponseException ( self . _deserialize , response ) \n    deserialized = None \n    header_dict = { } \n    if response . status_code == 202.0 : \n        deserialized = self . _deserialize ( 'Operation' , response ) \n        header_dict = { 'Location' : 'str' , } \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        client_raw_response . add_headers ( header_dict ) \n        return client_raw_response \n    return deserialized "}
{"1638": "\ndef get_member_groups ( self , object_id , security_enabled_only , additional_properties = None , custom_headers = None , raw = False , ** operation_config ) : \n    parameters = models . UserGetMemberGroupsParameters ( additional_properties = additional_properties , security_enabled_only = security_enabled_only ) \n    def internal_paging ( next_link = None , raw = False ) : \n        if not next_link : \n            url = self . get_member_groups . metadata [ 'url' ] \n            path_format_arguments = { 'objectId' : self . _serialize . url ( \"object_id\" , object_id , 'str' ) , 'tenantID' : self . _serialize . url ( \"self.config.tenant_id\" , self . config . tenant_id , 'str' ) } \n            url = self . _client . format_url ( url , ** path_format_arguments ) \n            query_parameters = { } \n            query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n        else : \n            url = next_link \n            query_parameters = { } \n        header_parameters = { } \n        header_parameters [ 'Accept' ] = 'application/json' \n        header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n        if self . config . generate_client_request_id : \n            header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n        if custom_headers : \n            header_parameters . update ( custom_headers ) \n        if self . config . accept_language is not None : \n            header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n        body_content = self . _serialize . body ( parameters , 'UserGetMemberGroupsParameters' ) \n        request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n        response = self . _client . send ( request , stream = False , ** operation_config ) \n        if response . status_code not in [ 200.0 ] : \n            raise models . GraphErrorException ( self . _deserialize , response ) \n        return response \n    deserialized = models . StrPaged ( internal_paging , self . _deserialize . dependencies ) \n    if raw : \n        header_dict = { } \n        client_raw_response = models . StrPaged ( internal_paging , self . _deserialize . dependencies , header_dict ) \n        return client_raw_response \n    return deserialized "}
{"1643": "\ndef replace ( self , word_alterations , custom_headers = None , raw = False , ** operation_config ) : \n    word_alterations1 = models . WordAlterationsDTO ( word_alterations = word_alterations ) \n    url = self . replace . metadata [ 'url' ] \n    path_format_arguments = { 'Endpoint' : self . _serialize . url ( \"self.config.endpoint\" , self . config . endpoint , 'str' , skip_quote = True ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( word_alterations1 , 'WordAlterationsDTO' ) \n    request = self . _client . put ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 204.0 ] : \n        raise models . ErrorResponseException ( self . _deserialize , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( None , response ) \n        return client_raw_response "}
{"1644": "\ndef add_value ( self , secret_resource_name , secret_value_resource_name , name , value = None , custom_headers = None , raw = False , ** operation_config ) : \n    secret_value_resource_description = models . SecretValueResourceDescription ( name = name , value = value ) \n    url = self . add_value . metadata [ 'url' ] \n    path_format_arguments = { 'secretResourceName' : self . _serialize . url ( \"secret_resource_name\" , secret_resource_name , 'str' , skip_quote = True ) , 'secretValueResourceName' : self . _serialize . url ( \"secret_value_resource_name\" , secret_value_resource_name , 'str' , skip_quote = True ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( secret_value_resource_description , 'SecretValueResourceDescription' ) \n    request = self . _client . put ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 , 201.0 , 202.0 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200.0 : \n        deserialized = self . _deserialize ( 'SecretValueResourceDescription' , response ) \n    if response . status_code == 201.0 : \n        deserialized = self . _deserialize ( 'SecretValueResourceDescription' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1714": "\ndef summarize_for_management_group ( self , management_group_name , query_options = None , custom_headers = None , raw = False , ** operation_config ) : \n    top = None \n    if query_options is not None : \n        top = query_options . top \n    from_parameter = None \n    if query_options is not None : \n        from_parameter = query_options . from_property \n    to = None \n    if query_options is not None : \n        to = query_options . to \n    filter = None \n    if query_options is not None : \n        filter = query_options . filter \n    url = self . summarize_for_management_group . metadata [ 'url' ] \n    path_format_arguments = { 'policyStatesSummaryResource' : self . _serialize . url ( \"self.policy_states_summary_resource\" , self . policy_states_summary_resource , 'str' ) , 'managementGroupsNamespace' : self . _serialize . url ( \"self.management_groups_namespace\" , self . management_groups_namespace , 'str' ) , 'managementGroupName' : self . _serialize . url ( \"management_group_name\" , management_group_name , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    if top is not None : \n        query_parameters [ '$top' ] = self . _serialize . query ( \"top\" , top , 'int' , minimum = 0 ) \n    if from_parameter is not None : \n        query_parameters [ '$from' ] = self . _serialize . query ( \"from_parameter\" , from_parameter , 'iso-8601' ) \n    if to is not None : \n        query_parameters [ '$to' ] = self . _serialize . query ( \"to\" , to , 'iso-8601' ) \n    if filter is not None : \n        query_parameters [ '$filter' ] = self . _serialize . query ( \"filter\" , filter , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    request = self . _client . post ( url , query_parameters , header_parameters ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 ] : \n        raise models . QueryFailureException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200.0 : \n        deserialized = self . _deserialize ( 'SummarizeResults' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1716": "\ndef fetch_next ( self , max_batch_size = None , timeout = None ) : \n    self . _can_run ( ) \n    wrapped_batch = [ ] \n    max_batch_size = max_batch_size or self . _handler . _prefetch \n    try : \n        timeout_ms = 1000.0 * timeout if timeout else 0 \n        batch = self . _handler . receive_message_batch ( max_batch_size = max_batch_size , timeout = timeout_ms ) \n        for received in batch : \n            message = self . _build_message ( received ) \n            wrapped_batch . append ( message ) \n    except Exception as e : \n        self . _handle_exception ( e ) \n    return wrapped_batch "}
{"1719": "\ndef convert_to_single_placement_group ( self , resource_group_name , vm_scale_set_name , active_placement_group_id = None , custom_headers = None , raw = False , ** operation_config ) : \n    parameters = models . VMScaleSetConvertToSinglePlacementGroupInput ( active_placement_group_id = active_placement_group_id ) \n    url = self . convert_to_single_placement_group . metadata [ 'url' ] \n    path_format_arguments = { 'resourceGroupName' : self . _serialize . url ( \"resource_group_name\" , resource_group_name , 'str' ) , 'vmScaleSetName' : self . _serialize . url ( \"vm_scale_set_name\" , vm_scale_set_name , 'str' ) , 'subscriptionId' : self . _serialize . url ( \"self.config.subscription_id\" , self . config . subscription_id , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( parameters , 'VMScaleSetConvertToSinglePlacementGroupInput' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 ] : \n        exp = CloudError ( response ) \n        exp . request_id = response . headers . get ( 'x-ms-request-id' ) \n        raise exp \n    if raw : \n        client_raw_response = ClientRawResponse ( None , response ) \n        return client_raw_response "}
{"1720": "\ndef screen_text ( self , text_content_type , text_content , language = None , autocorrect = False , pii = False , list_id = None , classify = False , custom_headers = None , raw = False , callback = None , ** operation_config ) : \n    url = self . screen_text . metadata [ 'url' ] \n    path_format_arguments = { 'Endpoint' : self . _serialize . url ( \"self.config.endpoint\" , self . config . endpoint , 'str' , skip_quote = True ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    if language is not None : \n        query_parameters [ 'language' ] = self . _serialize . query ( \"language\" , language , 'str' ) \n    if autocorrect is not None : \n        query_parameters [ 'autocorrect' ] = self . _serialize . query ( \"autocorrect\" , autocorrect , 'bool' ) \n    if pii is not None : \n        query_parameters [ 'PII' ] = self . _serialize . query ( \"pii\" , pii , 'bool' ) \n    if list_id is not None : \n        query_parameters [ 'listId' ] = self . _serialize . query ( \"list_id\" , list_id , 'str' ) \n    if classify is not None : \n        query_parameters [ 'classify' ] = self . _serialize . query ( \"classify\" , classify , 'bool' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'text/plain' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    header_parameters [ 'Content-Type' ] = self . _serialize . header ( \"text_content_type\" , text_content_type , 'str' ) \n    body_content = self . _client . stream_upload ( text_content , callback ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 ] : \n        raise models . APIErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200.0 : \n        deserialized = self . _deserialize ( 'Screen' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1721": "\ndef create_key ( self , vault_base_url , key_name , kty , key_size = None , key_ops = None , key_attributes = None , tags = None , curve = None , custom_headers = None , raw = False , ** operation_config ) : \n    parameters = models . KeyCreateParameters ( kty = kty , key_size = key_size , key_ops = key_ops , key_attributes = key_attributes , tags = tags , curve = curve ) \n    url = self . create_key . metadata [ 'url' ] \n    path_format_arguments = { 'vaultBaseUrl' : self . _serialize . url ( \"vault_base_url\" , vault_base_url , 'str' , skip_quote = True ) , 'key-name' : self . _serialize . url ( \"key_name\" , key_name , 'str' , pattern = r'^[0-9a-zA-Z-]+$' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( parameters , 'KeyCreateParameters' ) \n    request = self . _client . post ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , body_content , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 ] : \n        raise models . KeyVaultErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200.0 : \n        deserialized = self . _deserialize ( 'KeyBundle' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1722": "\ndef import_key ( self , vault_base_url , key_name , key , hsm = None , key_attributes = None , tags = None , custom_headers = None , raw = False , ** operation_config ) : \n    parameters = models . KeyImportParameters ( hsm = hsm , key = key , key_attributes = key_attributes , tags = tags ) \n    url = self . import_key . metadata [ 'url' ] \n    path_format_arguments = { 'vaultBaseUrl' : self . _serialize . url ( \"vault_base_url\" , vault_base_url , 'str' , skip_quote = True ) , 'key-name' : self . _serialize . url ( \"key_name\" , key_name , 'str' , pattern = r'^[0-9a-zA-Z-]+$' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( parameters , 'KeyImportParameters' ) \n    request = self . _client . put ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , body_content , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 ] : \n        raise models . KeyVaultErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200.0 : \n        deserialized = self . _deserialize ( 'KeyBundle' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1723": "\ndef update_key ( self , vault_base_url , key_name , key_version , key_ops = None , key_attributes = None , tags = None , custom_headers = None , raw = False , ** operation_config ) : \n    parameters = models . KeyUpdateParameters ( key_ops = key_ops , key_attributes = key_attributes , tags = tags ) \n    url = self . update_key . metadata [ 'url' ] \n    path_format_arguments = { 'vaultBaseUrl' : self . _serialize . url ( \"vault_base_url\" , vault_base_url , 'str' , skip_quote = True ) , 'key-name' : self . _serialize . url ( \"key_name\" , key_name , 'str' ) , 'key-version' : self . _serialize . url ( \"key_version\" , key_version , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( parameters , 'KeyUpdateParameters' ) \n    request = self . _client . patch ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , body_content , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 ] : \n        raise models . KeyVaultErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200.0 : \n        deserialized = self . _deserialize ( 'KeyBundle' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1724": "\ndef set_secret ( self , vault_base_url , secret_name , value , tags = None , content_type = None , secret_attributes = None , custom_headers = None , raw = False , ** operation_config ) : \n    parameters = models . SecretSetParameters ( value = value , tags = tags , content_type = content_type , secret_attributes = secret_attributes ) \n    url = self . set_secret . metadata [ 'url' ] \n    path_format_arguments = { 'vaultBaseUrl' : self . _serialize . url ( \"vault_base_url\" , vault_base_url , 'str' , skip_quote = True ) , 'secret-name' : self . _serialize . url ( \"secret_name\" , secret_name , 'str' , pattern = r'^[0-9a-zA-Z-]+$' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( parameters , 'SecretSetParameters' ) \n    request = self . _client . put ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , body_content , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 ] : \n        raise models . KeyVaultErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200.0 : \n        deserialized = self . _deserialize ( 'SecretBundle' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1725": "\ndef set_certificate_issuer ( self , vault_base_url , issuer_name , provider , credentials = None , organization_details = None , attributes = None , custom_headers = None , raw = False , ** operation_config ) : \n    parameter = models . CertificateIssuerSetParameters ( provider = provider , credentials = credentials , organization_details = organization_details , attributes = attributes ) \n    url = self . set_certificate_issuer . metadata [ 'url' ] \n    path_format_arguments = { 'vaultBaseUrl' : self . _serialize . url ( \"vault_base_url\" , vault_base_url , 'str' , skip_quote = True ) , 'issuer-name' : self . _serialize . url ( \"issuer_name\" , issuer_name , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    body_content = self . _serialize . body ( parameter , 'CertificateIssuerSetParameters' ) \n    request = self . _client . put ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , body_content , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 ] : \n        raise models . KeyVaultErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200.0 : \n        deserialized = self . _deserialize ( 'IssuerBundle' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1731": "\ndef get_receiver ( self , session = None , prefetch = 0 , mode = ReceiveSettleMode . PeekLock , idle_timeout = 0 , ** kwargs ) : \n    if self . entity and not self . requires_session and session : \n        raise ValueError ( \"A session cannot be used with a non-sessionful entitiy.\" ) \n    if self . entity and self . requires_session and not session : \n        raise ValueError ( \"This entity requires a session.\" ) \n    if int ( prefetch ) < 0 or int ( prefetch ) > 50000.0 : \n        raise ValueError ( \"Prefetch must be an integer between 0 and 50000 inclusive.\" ) \n    prefetch += 1 \n    handler_id = str ( uuid . uuid4 ( ) ) \n    if session : \n        return SessionReceiver ( handler_id , self . entity_uri , self . auth_config , session = session , loop = self . loop , debug = self . debug , timeout = int ( idle_timeout * 1000.0 ) , prefetch = prefetch , mode = mode , ** kwargs ) \n    return Receiver ( handler_id , self . entity_uri , self . auth_config , loop = self . loop , debug = self . debug , timeout = int ( idle_timeout * 1000.0 ) , prefetch = prefetch , mode = mode , ** kwargs ) "}
{"1732": "\ndef get_deadletter_receiver ( self , transfer_deadletter = False , prefetch = 0 , mode = ReceiveSettleMode . PeekLock , idle_timeout = 0 , ** kwargs ) : \n    if int ( prefetch ) < 0 or int ( prefetch ) > 50000.0 : \n        raise ValueError ( \"Prefetch must be an integer between 0 and 50000 inclusive.\" ) \n    prefetch += 1 \n    handler_id = str ( uuid . uuid4 ( ) ) \n    if transfer_deadletter : \n        entity_uri = self . mgmt_client . format_transfer_dead_letter_queue_name ( self . entity_uri ) \n    else : \n        entity_uri = self . mgmt_client . format_dead_letter_queue_name ( self . entity_uri ) \n    return Receiver ( handler_id , entity_uri , self . auth_config , loop = self . loop , debug = self . debug , timeout = int ( idle_timeout * 1000.0 ) , prefetch = prefetch , mode = mode , ** kwargs ) "}
{"1736": "\ndef wait_for_operation_status ( self , request_id , wait_for_status = 'Succeeded' , timeout = 30.0 , sleep_interval = 5.0 , progress_callback = wait_for_operation_status_progress_default_callback , success_callback = wait_for_operation_status_success_default_callback , failure_callback = wait_for_operation_status_failure_default_callback ) : \n    loops = timeout // sleep_interval + 1 \n    start_time = time . time ( ) \n    for _ in range ( int ( loops ) ) : \n        result = self . get_operation_status ( request_id ) \n        elapsed = time . time ( ) - start_time \n        if result . status == wait_for_status : \n            if success_callback is not None : \n                success_callback ( elapsed ) \n            return result \n        elif result . error : \n            if failure_callback is not None : \n                ex = AzureAsyncOperationHttpError ( _ERROR_ASYNC_OP_FAILURE , result . status , result ) \n                failure_callback ( elapsed , ex ) \n            return result \n        else : \n            if progress_callback is not None : \n                progress_callback ( elapsed ) \n            time . sleep ( sleep_interval ) \n    if failure_callback is not None : \n        ex = AzureAsyncOperationHttpError ( _ERROR_ASYNC_OP_TIMEOUT , result . status , result ) \n        failure_callback ( elapsed , ex ) \n    return result "}
{"1740": "\ndef get_certificates ( self , vault_base_url , maxresults = None , include_pending = None , custom_headers = None , raw = False , ** operation_config ) : \n    def internal_paging ( next_link = None , raw = False ) : \n        if not next_link : \n            url = self . get_certificates . metadata [ 'url' ] \n            path_format_arguments = { 'vaultBaseUrl' : self . _serialize . url ( \"vault_base_url\" , vault_base_url , 'str' , skip_quote = True ) } \n            url = self . _client . format_url ( url , ** path_format_arguments ) \n            query_parameters = { } \n            if maxresults is not None : \n                query_parameters [ 'maxresults' ] = self . _serialize . query ( \"maxresults\" , maxresults , 'int' , maximum = 25.0 , minimum = 1 ) \n            if include_pending is not None : \n                query_parameters [ 'includePending' ] = self . _serialize . query ( \"include_pending\" , include_pending , 'bool' ) \n            query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n        else : \n            url = next_link \n            query_parameters = { } \n        header_parameters = { } \n        header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n        if self . config . generate_client_request_id : \n            header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n        if custom_headers : \n            header_parameters . update ( custom_headers ) \n        if self . config . accept_language is not None : \n            header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n        request = self . _client . get ( url , query_parameters ) \n        response = self . _client . send ( request , header_parameters , stream = False , ** operation_config ) \n        if response . status_code not in [ 200.0 ] : \n            raise models . KeyVaultErrorException ( self . _deserialize , response ) \n        return response \n    deserialized = models . CertificateItemPaged ( internal_paging , self . _deserialize . dependencies ) \n    if raw : \n        header_dict = { } \n        client_raw_response = models . CertificateItemPaged ( internal_paging , self . _deserialize . dependencies , header_dict ) \n        return client_raw_response \n    return deserialized "}
{"1769": "\ndef list_agreements ( self , name , include_privacy = None , for_transfer = None , custom_headers = None , raw = False , ** operation_config ) : \n    agreement_option = models . TopLevelDomainAgreementOption ( include_privacy = include_privacy , for_transfer = for_transfer ) \n    def internal_paging ( next_link = None , raw = False ) : \n        if not next_link : \n            url = self . list_agreements . metadata [ 'url' ] \n            path_format_arguments = { 'name' : self . _serialize . url ( \"name\" , name , 'str' ) , 'subscriptionId' : self . _serialize . url ( \"self.config.subscription_id\" , self . config . subscription_id , 'str' ) } \n            url = self . _client . format_url ( url , ** path_format_arguments ) \n            query_parameters = { } \n            query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n        else : \n            url = next_link \n            query_parameters = { } \n        header_parameters = { } \n        header_parameters [ 'Accept' ] = 'application/json' \n        header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n        if self . config . generate_client_request_id : \n            header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n        if custom_headers : \n            header_parameters . update ( custom_headers ) \n        if self . config . accept_language is not None : \n            header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n        body_content = self . _serialize . body ( agreement_option , 'TopLevelDomainAgreementOption' ) \n        request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n        response = self . _client . send ( request , stream = False , ** operation_config ) \n        if response . status_code not in [ 200.0 ] : \n            raise models . DefaultErrorResponseException ( self . _deserialize , response ) \n        return response \n    deserialized = models . TldLegalAgreementPaged ( internal_paging , self . _deserialize . dependencies ) \n    if raw : \n        header_dict = { } \n        client_raw_response = models . TldLegalAgreementPaged ( internal_paging , self . _deserialize . dependencies , header_dict ) \n        return client_raw_response \n    return deserialized "}
{"1778": "\ndef _error_handler ( error ) : \n    if error . condition == b'com.microsoft:server-busy' : \n        return errors . ErrorAction ( retry = True , backoff = 4.0 ) \n    if error . condition == b'com.microsoft:timeout' : \n        return errors . ErrorAction ( retry = True , backoff = 2.0 ) \n    if error . condition == b'com.microsoft:operation-cancelled' : \n        return errors . ErrorAction ( retry = True ) \n    if error . condition == b\"com.microsoft:container-close\" : \n        return errors . ErrorAction ( retry = True , backoff = 4.0 ) \n    if error . condition in _NO_RETRY_ERRORS : \n        return errors . ErrorAction ( retry = False ) \n    return errors . ErrorAction ( retry = True ) "}
{"1794": "\ndef receive_queue_message ( self , queue_name , peek_lock = True , timeout = 60.0 ) : \n    if peek_lock : \n        return self . peek_lock_queue_message ( queue_name , timeout ) \n    return self . read_delete_queue_message ( queue_name , timeout ) "}
{"1795": "\ndef receive_subscription_message ( self , topic_name , subscription_name , peek_lock = True , timeout = 60.0 ) : \n    if peek_lock : \n        return self . peek_lock_subscription_message ( topic_name , subscription_name , timeout ) \n    return self . read_delete_subscription_message ( topic_name , subscription_name , timeout ) "}
{"1802": "\ndef _token_is_expired ( self , token ) : \n    time_pos_begin = token . find ( 'ExpiresOn=' ) + len ( 'ExpiresOn=' ) \n    time_pos_end = token . find ( '&' , time_pos_begin ) \n    token_expire_time = int ( token [ time_pos_begin : time_pos_end ] ) \n    time_now = time . mktime ( time . localtime ( ) ) \n    return ( token_expire_time - time_now ) < 30.0 "}
{"1810": "\ndef as_batch_body ( self ) : \n    if sys . version_info >= ( 3.0 , ) and isinstance ( self . body , bytes ) : \n        body = self . body . decode ( 'utf-8' ) \n    else : \n        body = self . body \n    result = { 'Body' : body } \n    if self . custom_properties : \n        result [ 'UserProperties' ] = { name : self . _serialize_basic_properties_value ( value ) for name , value in self . custom_properties . items ( ) } \n    if self . broker_properties : \n        result [ 'BrokerProperties' ] = { name : self . _serialize_basic_properties_value ( value ) for name , value in self . broker_properties . items ( ) } \n    return result "}
{"1811": "\ndef get_cluster_health ( self , nodes_health_state_filter = 0 , applications_health_state_filter = 0 , events_health_state_filter = 0 , exclude_health_statistics = False , include_system_application_health_statistics = False , timeout = 60.0 , custom_headers = None , raw = False , ** operation_config ) : \n    api_version = \"6.0\" \n    url = self . get_cluster_health . metadata [ 'url' ] \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    if nodes_health_state_filter is not None : \n        query_parameters [ 'NodesHealthStateFilter' ] = self . _serialize . query ( \"nodes_health_state_filter\" , nodes_health_state_filter , 'int' ) \n    if applications_health_state_filter is not None : \n        query_parameters [ 'ApplicationsHealthStateFilter' ] = self . _serialize . query ( \"applications_health_state_filter\" , applications_health_state_filter , 'int' ) \n    if events_health_state_filter is not None : \n        query_parameters [ 'EventsHealthStateFilter' ] = self . _serialize . query ( \"events_health_state_filter\" , events_health_state_filter , 'int' ) \n    if exclude_health_statistics is not None : \n        query_parameters [ 'ExcludeHealthStatistics' ] = self . _serialize . query ( \"exclude_health_statistics\" , exclude_health_statistics , 'bool' ) \n    if include_system_application_health_statistics is not None : \n        query_parameters [ 'IncludeSystemApplicationHealthStatistics' ] = self . _serialize . query ( \"include_system_application_health_statistics\" , include_system_application_health_statistics , 'bool' ) \n    if timeout is not None : \n        query_parameters [ 'timeout' ] = self . _serialize . query ( \"timeout\" , timeout , 'long' , maximum = 4294967295.0 , minimum = 1 ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    request = self . _client . get ( url , query_parameters , header_parameters ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200.0 : \n        deserialized = self . _deserialize ( 'ClusterHealth' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1812": "\ndef get_cluster_health_using_policy ( self , nodes_health_state_filter = 0 , applications_health_state_filter = 0 , events_health_state_filter = 0 , exclude_health_statistics = False , include_system_application_health_statistics = False , timeout = 60.0 , application_health_policy_map = None , cluster_health_policy = None , custom_headers = None , raw = False , ** operation_config ) : \n    cluster_health_policies = None \n    if application_health_policy_map is not None or cluster_health_policy is not None : \n        cluster_health_policies = models . ClusterHealthPolicies ( application_health_policy_map = application_health_policy_map , cluster_health_policy = cluster_health_policy ) \n    api_version = \"6.0\" \n    url = self . get_cluster_health_using_policy . metadata [ 'url' ] \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    if nodes_health_state_filter is not None : \n        query_parameters [ 'NodesHealthStateFilter' ] = self . _serialize . query ( \"nodes_health_state_filter\" , nodes_health_state_filter , 'int' ) \n    if applications_health_state_filter is not None : \n        query_parameters [ 'ApplicationsHealthStateFilter' ] = self . _serialize . query ( \"applications_health_state_filter\" , applications_health_state_filter , 'int' ) \n    if events_health_state_filter is not None : \n        query_parameters [ 'EventsHealthStateFilter' ] = self . _serialize . query ( \"events_health_state_filter\" , events_health_state_filter , 'int' ) \n    if exclude_health_statistics is not None : \n        query_parameters [ 'ExcludeHealthStatistics' ] = self . _serialize . query ( \"exclude_health_statistics\" , exclude_health_statistics , 'bool' ) \n    if include_system_application_health_statistics is not None : \n        query_parameters [ 'IncludeSystemApplicationHealthStatistics' ] = self . _serialize . query ( \"include_system_application_health_statistics\" , include_system_application_health_statistics , 'bool' ) \n    if timeout is not None : \n        query_parameters [ 'timeout' ] = self . _serialize . query ( \"timeout\" , timeout , 'long' , maximum = 4294967295.0 , minimum = 1 ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    if cluster_health_policies is not None : \n        body_content = self . _serialize . body ( cluster_health_policies , 'ClusterHealthPolicies' ) \n    else : \n        body_content = None \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200.0 : \n        deserialized = self . _deserialize ( 'ClusterHealth' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1813": "\ndef unprovision_application_type ( self , application_type_name , application_type_version , timeout = 60.0 , async_parameter = None , custom_headers = None , raw = False , ** operation_config ) : \n    unprovision_application_type_description_info = models . UnprovisionApplicationTypeDescriptionInfo ( application_type_version = application_type_version , async_property = async_parameter ) \n    api_version = \"6.0\" \n    url = self . unprovision_application_type . metadata [ 'url' ] \n    path_format_arguments = { 'applicationTypeName' : self . _serialize . url ( \"application_type_name\" , application_type_name , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    if timeout is not None : \n        query_parameters [ 'timeout' ] = self . _serialize . query ( \"timeout\" , timeout , 'long' , maximum = 4294967295.0 , minimum = 1 ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( unprovision_application_type_description_info , 'UnprovisionApplicationTypeDescriptionInfo' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 , 202.0 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( None , response ) \n        return client_raw_response "}
{"1814": "\ndef get_repair_task_list ( self , task_id_filter = None , state_filter = None , executor_filter = None , custom_headers = None , raw = False , ** operation_config ) : \n    api_version = \"6.0\" \n    url = self . get_repair_task_list . metadata [ 'url' ] \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    if task_id_filter is not None : \n        query_parameters [ 'TaskIdFilter' ] = self . _serialize . query ( \"task_id_filter\" , task_id_filter , 'str' ) \n    if state_filter is not None : \n        query_parameters [ 'StateFilter' ] = self . _serialize . query ( \"state_filter\" , state_filter , 'int' ) \n    if executor_filter is not None : \n        query_parameters [ 'ExecutorFilter' ] = self . _serialize . query ( \"executor_filter\" , executor_filter , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    request = self . _client . get ( url , query_parameters , header_parameters ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200.0 : \n        deserialized = self . _deserialize ( '[RepairTask]' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1815": "\ndef submit_property_batch ( self , name_id , timeout = 60.0 , operations = None , custom_headers = None , raw = False , ** operation_config ) : \n    property_batch_description_list = models . PropertyBatchDescriptionList ( operations = operations ) \n    api_version = \"6.0\" \n    url = self . submit_property_batch . metadata [ 'url' ] \n    path_format_arguments = { 'nameId' : self . _serialize . url ( \"name_id\" , name_id , 'str' , skip_quote = True ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"api_version\" , api_version , 'str' ) \n    if timeout is not None : \n        query_parameters [ 'timeout' ] = self . _serialize . query ( \"timeout\" , timeout , 'long' , maximum = 4294967295.0 , minimum = 1 ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( property_batch_description_list , 'PropertyBatchDescriptionList' ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 , 409.0 ] : \n        raise models . FabricErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200.0 : \n        deserialized = self . _deserialize ( 'SuccessfulPropertyBatchInfo' , response ) \n    if response . status_code == 409.0 : \n        deserialized = self . _deserialize ( 'FailedPropertyBatchInfo' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1818": "\ndef list_slot_differences_slot ( self , resource_group_name , name , slot , target_slot , preserve_vnet , custom_headers = None , raw = False , ** operation_config ) : \n    slot_swap_entity = models . CsmSlotEntity ( target_slot = target_slot , preserve_vnet = preserve_vnet ) \n    def internal_paging ( next_link = None , raw = False ) : \n        if not next_link : \n            url = self . list_slot_differences_slot . metadata [ 'url' ] \n            path_format_arguments = { 'resourceGroupName' : self . _serialize . url ( \"resource_group_name\" , resource_group_name , 'str' , max_length = 90.0 , min_length = 1 , pattern = r'^[-\\w\\._\\(\\)]+[^\\.]$' ) , 'name' : self . _serialize . url ( \"name\" , name , 'str' ) , 'slot' : self . _serialize . url ( \"slot\" , slot , 'str' ) , 'subscriptionId' : self . _serialize . url ( \"self.config.subscription_id\" , self . config . subscription_id , 'str' ) } \n            url = self . _client . format_url ( url , ** path_format_arguments ) \n            query_parameters = { } \n            query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n        else : \n            url = next_link \n            query_parameters = { } \n        header_parameters = { } \n        header_parameters [ 'Accept' ] = 'application/json' \n        header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n        if self . config . generate_client_request_id : \n            header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n        if custom_headers : \n            header_parameters . update ( custom_headers ) \n        if self . config . accept_language is not None : \n            header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n        body_content = self . _serialize . body ( slot_swap_entity , 'CsmSlotEntity' ) \n        request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n        response = self . _client . send ( request , stream = False , ** operation_config ) \n        if response . status_code not in [ 200.0 ] : \n            raise models . DefaultErrorResponseException ( self . _deserialize , response ) \n        return response \n    deserialized = models . SlotDifferencePaged ( internal_paging , self . _deserialize . dependencies ) \n    if raw : \n        header_dict = { } \n        client_raw_response = models . SlotDifferencePaged ( internal_paging , self . _deserialize . dependencies , header_dict ) \n        return client_raw_response \n    return deserialized "}
{"1820": "\ndef get_by_type ( self , app_id , event_type , timespan = None , filter = None , search = None , orderby = None , select = None , skip = None , top = None , format = None , count = None , apply = None , custom_headers = None , raw = False , ** operation_config ) : \n    url = self . get_by_type . metadata [ 'url' ] \n    path_format_arguments = { 'appId' : self . _serialize . url ( \"app_id\" , app_id , 'str' ) , 'eventType' : self . _serialize . url ( \"event_type\" , event_type , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    if timespan is not None : \n        query_parameters [ 'timespan' ] = self . _serialize . query ( \"timespan\" , timespan , 'str' ) \n    if filter is not None : \n        query_parameters [ '$filter' ] = self . _serialize . query ( \"filter\" , filter , 'str' ) \n    if search is not None : \n        query_parameters [ '$search' ] = self . _serialize . query ( \"search\" , search , 'str' ) \n    if orderby is not None : \n        query_parameters [ '$orderby' ] = self . _serialize . query ( \"orderby\" , orderby , 'str' ) \n    if select is not None : \n        query_parameters [ '$select' ] = self . _serialize . query ( \"select\" , select , 'str' ) \n    if skip is not None : \n        query_parameters [ '$skip' ] = self . _serialize . query ( \"skip\" , skip , 'int' ) \n    if top is not None : \n        query_parameters [ '$top' ] = self . _serialize . query ( \"top\" , top , 'int' ) \n    if format is not None : \n        query_parameters [ '$format' ] = self . _serialize . query ( \"format\" , format , 'str' ) \n    if count is not None : \n        query_parameters [ '$count' ] = self . _serialize . query ( \"count\" , count , 'bool' ) \n    if apply is not None : \n        query_parameters [ '$apply' ] = self . _serialize . query ( \"apply\" , apply , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    request = self . _client . get ( url , query_parameters , header_parameters ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 ] : \n        raise models . ErrorResponseException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200.0 : \n        deserialized = self . _deserialize ( 'EventsResults' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1821": "\ndef add_face_from_stream ( self , large_face_list_id , image , user_data = None , target_face = None , custom_headers = None , raw = False , callback = None , ** operation_config ) : \n    url = self . add_face_from_stream . metadata [ 'url' ] \n    path_format_arguments = { 'Endpoint' : self . _serialize . url ( \"self.config.endpoint\" , self . config . endpoint , 'str' , skip_quote = True ) , 'largeFaceListId' : self . _serialize . url ( \"large_face_list_id\" , large_face_list_id , 'str' , max_length = 64.0 , pattern = r'^[a-z0-9-_]+$' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    if user_data is not None : \n        query_parameters [ 'userData' ] = self . _serialize . query ( \"user_data\" , user_data , 'str' , max_length = 1024.0 ) \n    if target_face is not None : \n        query_parameters [ 'targetFace' ] = self . _serialize . query ( \"target_face\" , target_face , '[int]' , div = ',' ) \n    header_parameters = { } \n    header_parameters [ 'Accept' ] = 'application/json' \n    header_parameters [ 'Content-Type' ] = 'application/octet-stream' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _client . stream_upload ( image , callback ) \n    request = self . _client . post ( url , query_parameters , header_parameters , body_content ) \n    response = self . _client . send ( request , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 ] : \n        raise models . APIErrorException ( self . _deserialize , response ) \n    deserialized = None \n    if response . status_code == 200.0 : \n        deserialized = self . _deserialize ( 'PersistedFace' , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( deserialized , response ) \n        return client_raw_response \n    return deserialized "}
{"1824": "\ndef publish_events ( self , topic_hostname , events , custom_headers = None , raw = False , ** operation_config ) : \n    url = self . publish_events . metadata [ 'url' ] \n    path_format_arguments = { 'topicHostname' : self . _serialize . url ( \"topic_hostname\" , topic_hostname , 'str' , skip_quote = True ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    body_content = self . _serialize . body ( events , '[EventGridEvent]' ) \n    request = self . _client . post ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , body_content , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 ] : \n        raise HttpOperationError ( self . _deserialize , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( None , response ) \n        return client_raw_response "}
{"1827": "\ndef list_query_results_for_management_group ( self , management_group_name , query_options = None , custom_headers = None , raw = False , ** operation_config ) : \n    top = None \n    if query_options is not None : \n        top = query_options . top \n    filter = None \n    if query_options is not None : \n        filter = query_options . filter \n    def internal_paging ( next_link = None , raw = False ) : \n        if not next_link : \n            url = self . list_query_results_for_management_group . metadata [ 'url' ] \n            path_format_arguments = { 'managementGroupsNamespace' : self . _serialize . url ( \"self.management_groups_namespace\" , self . management_groups_namespace , 'str' ) , 'managementGroupName' : self . _serialize . url ( \"management_group_name\" , management_group_name , 'str' ) , 'policyTrackedResourcesResource' : self . _serialize . url ( \"self.policy_tracked_resources_resource\" , self . policy_tracked_resources_resource , 'str' ) } \n            url = self . _client . format_url ( url , ** path_format_arguments ) \n            query_parameters = { } \n            query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n            if top is not None : \n                query_parameters [ '$top' ] = self . _serialize . query ( \"top\" , top , 'int' , minimum = 0 ) \n            if filter is not None : \n                query_parameters [ '$filter' ] = self . _serialize . query ( \"filter\" , filter , 'str' ) \n        else : \n            url = next_link \n            query_parameters = { } \n        header_parameters = { } \n        header_parameters [ 'Accept' ] = 'application/json' \n        if self . config . generate_client_request_id : \n            header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n        if custom_headers : \n            header_parameters . update ( custom_headers ) \n        if self . config . accept_language is not None : \n            header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n        request = self . _client . post ( url , query_parameters , header_parameters ) \n        response = self . _client . send ( request , stream = False , ** operation_config ) \n        if response . status_code not in [ 200.0 ] : \n            raise models . QueryFailureException ( self . _deserialize , response ) \n        return response \n    deserialized = models . PolicyTrackedResourcePaged ( internal_paging , self . _deserialize . dependencies ) \n    if raw : \n        header_dict = { } \n        client_raw_response = models . PolicyTrackedResourcePaged ( internal_paging , self . _deserialize . dependencies , header_dict ) \n        return client_raw_response \n    return deserialized "}
{"1828": "\ndef create_queue ( self , queue_name , lock_duration = 30.0 , max_size_in_megabytes = None , requires_duplicate_detection = False , requires_session = False , default_message_time_to_live = None , dead_lettering_on_message_expiration = False , duplicate_detection_history_time_window = None , max_delivery_count = None , enable_batched_operations = None ) : \n    queue_properties = Queue ( lock_duration = \"PT{}S\" . format ( int ( lock_duration ) ) , max_size_in_megabytes = max_size_in_megabytes , requires_duplicate_detection = requires_duplicate_detection , requires_session = requires_session , default_message_time_to_live = default_message_time_to_live , dead_lettering_on_message_expiration = dead_lettering_on_message_expiration , duplicate_detection_history_time_window = duplicate_detection_history_time_window , max_delivery_count = max_delivery_count , enable_batched_operations = enable_batched_operations ) \n    try : \n        return self . mgmt_client . create_queue ( queue_name , queue = queue_properties , fail_on_exist = True ) \n    except requests . exceptions . ConnectionError as e : \n        raise ServiceBusConnectionError ( \"Namespace: {} not found\" . format ( self . service_namespace ) , e ) "}
{"1832": "\ndef create_subscription ( self , topic_name , subscription_name , lock_duration = 30.0 , requires_session = None , default_message_time_to_live = None , dead_lettering_on_message_expiration = None , dead_lettering_on_filter_evaluation_exceptions = None , enable_batched_operations = None , max_delivery_count = None ) : \n    sub_properties = Subscription ( lock_duration = \"PT{}S\" . format ( int ( lock_duration ) ) , requires_session = requires_session , default_message_time_to_live = default_message_time_to_live , dead_lettering_on_message_expiration = dead_lettering_on_message_expiration , dead_lettering_on_filter_evaluation_exceptions = dead_lettering_on_filter_evaluation_exceptions , max_delivery_count = max_delivery_count , enable_batched_operations = enable_batched_operations ) \n    try : \n        return self . mgmt_client . create_subscription ( topic_name , subscription_name , subscription = sub_properties , fail_on_exist = True ) \n    except requests . exceptions . ConnectionError as e : \n        raise ServiceBusConnectionError ( \"Namespace: {} not found\" . format ( self . service_namespace ) , e ) "}
{"1840": "\ndef _bulk_add_tasks ( self , results_queue , chunk_tasks_to_add ) : \n    try : \n        add_collection_response = self . _original_add_collection ( self . _client , self . _job_id , chunk_tasks_to_add , self . _task_add_collection_options , self . _custom_headers , self . _raw ) \n    except BatchErrorException as e : \n        if e . error . code == \"RequestBodyTooLarge\" : \n            if len ( chunk_tasks_to_add ) == 1 : \n                failed_task = chunk_tasks_to_add . pop ( ) \n                self . errors . appendleft ( e ) \n                _LOGGER . error ( \"Failed to add task with ID %s due to the body\" \" exceeding the maximum request size\" , failed_task . id ) \n            else : \n                midpoint = int ( len ( chunk_tasks_to_add ) / 2.0 ) \n                with self . _max_tasks_lock : \n                    if midpoint < self . _max_tasks_per_request : \n                        self . _max_tasks_per_request = midpoint \n                        _LOGGER . info ( \"Amount of tasks per request reduced from %s to %s due to the\" \" request body being too large\" , str ( self . _max_tasks_per_request ) , str ( midpoint ) ) \n                self . tasks_to_add . extendleft ( chunk_tasks_to_add [ midpoint : ] ) \n                self . _bulk_add_tasks ( results_queue , chunk_tasks_to_add [ : midpoint ] ) \n        elif 500.0 <= e . response . status_code <= 599.0 : \n            self . tasks_to_add . extendleft ( chunk_tasks_to_add ) \n        else : \n            self . tasks_to_add . extendleft ( chunk_tasks_to_add ) \n            self . errors . appendleft ( e ) \n    except Exception as e : \n        self . tasks_to_add . extendleft ( chunk_tasks_to_add ) \n        self . errors . appendleft ( e ) \n    else : \n        try : \n            add_collection_response = add_collection_response . output \n        except AttributeError : \n            pass \n        for task_result in add_collection_response . value : \n            if task_result . status == TaskAddStatus . server_error : \n                with self . _pending_queue_lock : \n                    for task in chunk_tasks_to_add : \n                        if task . id == task_result . task_id : \n                            self . tasks_to_add . appendleft ( task ) \n            elif ( task_result . status == TaskAddStatus . client_error and not task_result . error . code == \"TaskExists\" ) : \n                self . failure_tasks . appendleft ( task_result ) \n            else : \n                results_queue . appendleft ( task_result ) "}
{"1847": "\ndef _convert_etree_element_to_queue ( entry_element ) : \n    queue = Queue ( ) \n    invalid_queue = True \n    queue_element = entry_element . find ( './atom:content/sb:QueueDescription' , _etree_sb_feed_namespaces ) \n    if queue_element is not None : \n        mappings = [ ( 'LockDuration' , 'lock_duration' , None ) , ( 'MaxSizeInMegabytes' , 'max_size_in_megabytes' , int ) , ( 'RequiresDuplicateDetection' , 'requires_duplicate_detection' , _parse_bool ) , ( 'RequiresSession' , 'requires_session' , _parse_bool ) , ( 'DefaultMessageTimeToLive' , 'default_message_time_to_live' , None ) , ( 'DeadLetteringOnMessageExpiration' , 'dead_lettering_on_message_expiration' , _parse_bool ) , ( 'DuplicateDetectionHistoryTimeWindow' , 'duplicate_detection_history_time_window' , None ) , ( 'EnableBatchedOperations' , 'enable_batched_operations' , _parse_bool ) , ( 'MaxDeliveryCount' , 'max_delivery_count' , int ) , ( 'MessageCount' , 'message_count' , int ) , ( 'SizeInBytes' , 'size_in_bytes' , int ) , ] \n        for mapping in mappings : \n            if _read_etree_element ( queue_element , mapping [ 0 ] , queue , mapping [ 1 ] , mapping [ 2.0 ] ) : \n                invalid_queue = False \n    if invalid_queue : \n        raise AzureServiceBusResourceNotFound ( _ERROR_QUEUE_NOT_FOUND ) \n    for name , value in _ETreeXmlToObject . get_entry_properties_from_element ( entry_element , True ) . items ( ) : \n        setattr ( queue , name , value ) \n    return queue "}
{"1848": "\ndef _convert_etree_element_to_topic ( entry_element ) : \n    topic = Topic ( ) \n    invalid_topic = True \n    topic_element = entry_element . find ( './atom:content/sb:TopicDescription' , _etree_sb_feed_namespaces ) \n    if topic_element is not None : \n        mappings = [ ( 'DefaultMessageTimeToLive' , 'default_message_time_to_live' , None ) , ( 'MaxSizeInMegabytes' , 'max_size_in_megabytes' , int ) , ( 'RequiresDuplicateDetection' , 'requires_duplicate_detection' , _parse_bool ) , ( 'DuplicateDetectionHistoryTimeWindow' , 'duplicate_detection_history_time_window' , None ) , ( 'EnableBatchedOperations' , 'enable_batched_operations' , _parse_bool ) , ( 'SizeInBytes' , 'size_in_bytes' , int ) , ] \n        for mapping in mappings : \n            if _read_etree_element ( topic_element , mapping [ 0 ] , topic , mapping [ 1 ] , mapping [ 2.0 ] ) : \n                invalid_topic = False \n    if invalid_topic : \n        raise AzureServiceBusResourceNotFound ( _ERROR_TOPIC_NOT_FOUND ) \n    for name , value in _ETreeXmlToObject . get_entry_properties_from_element ( entry_element , True ) . items ( ) : \n        setattr ( topic , name , value ) \n    return topic "}
{"1849": "\ndef _convert_etree_element_to_subscription ( entry_element ) : \n    subscription = Subscription ( ) \n    subscription_element = entry_element . find ( './atom:content/sb:SubscriptionDescription' , _etree_sb_feed_namespaces ) \n    if subscription_element is not None : \n        mappings = [ ( 'LockDuration' , 'lock_duration' , None ) , ( 'RequiresSession' , 'requires_session' , _parse_bool ) , ( 'DefaultMessageTimeToLive' , 'default_message_time_to_live' , None ) , ( 'DeadLetteringOnFilterEvaluationExceptions' , 'dead_lettering_on_filter_evaluation_exceptions' , _parse_bool ) , ( 'DeadLetteringOnMessageExpiration' , 'dead_lettering_on_message_expiration' , _parse_bool ) , ( 'EnableBatchedOperations' , 'enable_batched_operations' , _parse_bool ) , ( 'MaxDeliveryCount' , 'max_delivery_count' , int ) , ( 'MessageCount' , 'message_count' , int ) , ] \n        for mapping in mappings : \n            _read_etree_element ( subscription_element , mapping [ 0 ] , subscription , mapping [ 1 ] , mapping [ 2.0 ] ) \n    for name , value in _ETreeXmlToObject . get_entry_properties_from_element ( entry_element , True , '/subscriptions' ) . items ( ) : \n        setattr ( subscription , name , value ) \n    return subscription "}
{"1850": "\ndef create ( self , resource_group_name , account_name , certificate_name , parameters , if_match = None , if_none_match = None , custom_headers = None , raw = False , ** operation_config ) : \n    raw_result = self . _create_initial ( resource_group_name = resource_group_name , account_name = account_name , certificate_name = certificate_name , parameters = parameters , if_match = if_match , if_none_match = if_none_match , custom_headers = custom_headers , raw = True , ** operation_config ) \n    if raw : \n        return raw_result \n    def long_running_send ( ) : \n        return raw_result . response \n    def get_long_running_status ( status_link , headers = None ) : \n        request = self . _client . get ( status_link ) \n        if headers : \n            request . headers . update ( headers ) \n        header_parameters = { } \n        header_parameters [ 'x-ms-client-request-id' ] = raw_result . response . request . headers [ 'x-ms-client-request-id' ] \n        return self . _client . send ( request , header_parameters , stream = False , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if response . status_code not in [ 200.0 ] : \n            exp = CloudError ( response ) \n            exp . request_id = response . headers . get ( 'x-ms-request-id' ) \n            raise exp \n        header_dict = { 'ETag' : 'str' , } \n        deserialized = self . _deserialize ( 'Certificate' , response ) \n        if raw : \n            client_raw_response = ClientRawResponse ( deserialized , response ) \n            client_raw_response . add_headers ( header_dict ) \n            return client_raw_response \n        return deserialized \n    long_running_operation_timeout = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    return AzureOperationPoller ( long_running_send , get_long_running_output , get_long_running_status , long_running_operation_timeout ) "}
{"1851": "\ndef delete ( self , resource_group_name , account_name , certificate_name , custom_headers = None , raw = False , ** operation_config ) : \n    raw_result = self . _delete_initial ( resource_group_name = resource_group_name , account_name = account_name , certificate_name = certificate_name , custom_headers = custom_headers , raw = True , ** operation_config ) \n    if raw : \n        return raw_result \n    def long_running_send ( ) : \n        return raw_result . response \n    def get_long_running_status ( status_link , headers = None ) : \n        request = self . _client . get ( status_link ) \n        if headers : \n            request . headers . update ( headers ) \n        header_parameters = { } \n        header_parameters [ 'x-ms-client-request-id' ] = raw_result . response . request . headers [ 'x-ms-client-request-id' ] \n        return self . _client . send ( request , header_parameters , stream = False , ** operation_config ) \n    def get_long_running_output ( response ) : \n        if response . status_code not in [ 200.0 , 202.0 , 204.0 ] : \n            exp = CloudError ( response ) \n            exp . request_id = response . headers . get ( 'x-ms-request-id' ) \n            raise exp \n        if raw : \n            client_raw_response = ClientRawResponse ( None , response ) \n            client_raw_response . add_headers ( { 'Location' : 'str' , 'Retry-After' : 'int' , } ) \n            return client_raw_response \n    long_running_operation_timeout = operation_config . get ( 'long_running_operation_timeout' , self . config . long_running_operation_timeout ) \n    return AzureOperationPoller ( long_running_send , get_long_running_output , get_long_running_status , long_running_operation_timeout ) "}
{"1857": "\ndef delete ( self , resource_group_name , if_match , provisioning_service_name , certificate_name , certificatename = None , certificateraw_bytes = None , certificateis_verified = None , certificatepurpose = None , certificatecreated = None , certificatelast_updated = None , certificatehas_private_key = None , certificatenonce = None , custom_headers = None , raw = False , ** operation_config ) : \n    url = self . delete . metadata [ 'url' ] \n    path_format_arguments = { 'subscriptionId' : self . _serialize . url ( \"self.config.subscription_id\" , self . config . subscription_id , 'str' ) , 'resourceGroupName' : self . _serialize . url ( \"resource_group_name\" , resource_group_name , 'str' ) , 'provisioningServiceName' : self . _serialize . url ( \"provisioning_service_name\" , provisioning_service_name , 'str' ) , 'certificateName' : self . _serialize . url ( \"certificate_name\" , certificate_name , 'str' ) } \n    url = self . _client . format_url ( url , ** path_format_arguments ) \n    query_parameters = { } \n    if certificatename is not None : \n        query_parameters [ 'certificate.name' ] = self . _serialize . query ( \"certificatename\" , certificatename , 'str' ) \n    if certificateraw_bytes is not None : \n        query_parameters [ 'certificate.rawBytes' ] = self . _serialize . query ( \"certificateraw_bytes\" , certificateraw_bytes , 'bytearray' ) \n    if certificateis_verified is not None : \n        query_parameters [ 'certificate.isVerified' ] = self . _serialize . query ( \"certificateis_verified\" , certificateis_verified , 'bool' ) \n    if certificatepurpose is not None : \n        query_parameters [ 'certificate.purpose' ] = self . _serialize . query ( \"certificatepurpose\" , certificatepurpose , 'str' ) \n    if certificatecreated is not None : \n        query_parameters [ 'certificate.created' ] = self . _serialize . query ( \"certificatecreated\" , certificatecreated , 'iso-8601' ) \n    if certificatelast_updated is not None : \n        query_parameters [ 'certificate.lastUpdated' ] = self . _serialize . query ( \"certificatelast_updated\" , certificatelast_updated , 'iso-8601' ) \n    if certificatehas_private_key is not None : \n        query_parameters [ 'certificate.hasPrivateKey' ] = self . _serialize . query ( \"certificatehas_private_key\" , certificatehas_private_key , 'bool' ) \n    if certificatenonce is not None : \n        query_parameters [ 'certificate.nonce' ] = self . _serialize . query ( \"certificatenonce\" , certificatenonce , 'str' ) \n    query_parameters [ 'api-version' ] = self . _serialize . query ( \"self.api_version\" , self . api_version , 'str' ) \n    header_parameters = { } \n    header_parameters [ 'Content-Type' ] = 'application/json; charset=utf-8' \n    if self . config . generate_client_request_id : \n        header_parameters [ 'x-ms-client-request-id' ] = str ( uuid . uuid1 ( ) ) \n    if custom_headers : \n        header_parameters . update ( custom_headers ) \n    header_parameters [ 'If-Match' ] = self . _serialize . header ( \"if_match\" , if_match , 'str' ) \n    if self . config . accept_language is not None : \n        header_parameters [ 'accept-language' ] = self . _serialize . header ( \"self.config.accept_language\" , self . config . accept_language , 'str' ) \n    request = self . _client . delete ( url , query_parameters ) \n    response = self . _client . send ( request , header_parameters , stream = False , ** operation_config ) \n    if response . status_code not in [ 200.0 , 204.0 ] : \n        raise models . ErrorDetailsException ( self . _deserialize , response ) \n    if raw : \n        client_raw_response = ClientRawResponse ( None , response ) \n        return client_raw_response "}
{"1887": "\ndef format_time ( elapsed ) : \n    hours = int ( elapsed / ( 60.0 * 60.0 ) ) \n    minutes = int ( ( elapsed % ( 60.0 * 60.0 ) ) / 60.0 ) \n    seconds = int ( elapsed % 60.0 ) \n    rval = \"\" \n    if hours : \n        rval += \"{0}h\" . format ( hours ) \n    if elapsed > 60.0 : \n        rval += \"{0}m\" . format ( minutes ) \n    rval += \"{0}s\" . format ( seconds ) \n    return rval "}
{"1889": "\ndef progress ( iterator , prefix ) : \n    if terminal_width ( prefix ) > 25.0 : \n        prefix = ( \"..\" + get_cut_prefix ( prefix , 23.0 ) ) \n    speed_updated = start = time ( ) \n    speed_written = written = 0 \n    speed_history = deque ( maxlen = 5.0 ) \n    for data in iterator : \n        yield data \n        now = time ( ) \n        elapsed = now - start \n        written += len ( data ) \n        speed_elapsed = now - speed_updated \n        if speed_elapsed >= 0.5 : \n            speed_history . appendleft ( ( written - speed_written , speed_updated , ) ) \n            speed_updated = now \n            speed_written = written \n            speed_history_written = sum ( h [ 0 ] for h in speed_history ) \n            speed_history_elapsed = now - speed_history [ - 1 ] [ 1 ] \n            speed = speed_history_written / speed_history_elapsed \n            status = create_status_line ( prefix = prefix , written = format_filesize ( written ) , elapsed = format_time ( elapsed ) , speed = format_filesize ( speed ) ) \n            print_inplace ( status ) \n    sys . stderr . write ( \"\\n\" ) \n    sys . stderr . flush ( ) "}
{"1890": "\ndef segment_numbers ( self ) : \n    log . debug ( \"Generating segment numbers for {0} playlist (id={1})\" . format ( self . root . type , self . parent . id ) ) \n    if self . root . type == u\"static\" : \n        available_iter = repeat ( epoch_start ) \n        duration = self . period . duration . seconds or self . root . mediaPresentationDuration . seconds \n        if duration : \n            number_iter = range ( self . startNumber , int ( duration / self . duration_seconds ) + 1 ) \n        else : \n            number_iter = count ( self . startNumber ) \n    else : \n        now = datetime . datetime . now ( utc ) \n        if self . presentationTimeOffset : \n            since_start = ( now - self . presentationTimeOffset ) - self . root . availabilityStartTime \n            available_start_date = self . root . availabilityStartTime + self . presentationTimeOffset + since_start \n            available_start = available_start_date \n        else : \n            since_start = now - self . root . availabilityStartTime \n            available_start = now \n        suggested_delay = datetime . timedelta ( seconds = ( self . root . suggestedPresentationDelay . total_seconds ( ) if self . root . suggestedPresentationDelay else 3.0 ) ) \n        number_iter = count ( self . startNumber + int ( ( since_start - suggested_delay - self . root . minBufferTime ) . total_seconds ( ) / self . duration_seconds ) ) \n        available_iter = count_dt ( available_start , step = datetime . timedelta ( seconds = self . duration_seconds ) ) \n    for number , available_at in izip ( number_iter , available_iter ) : \n        yield number , available_at "}
{"1891": "\ndef segments ( self , ** kwargs ) : \n    segmentBase = self . segmentBase or self . walk_back_get_attr ( \"segmentBase\" ) \n    segmentLists = self . segmentList or self . walk_back_get_attr ( \"segmentList\" ) \n    segmentTemplate = self . segmentTemplate or self . walk_back_get_attr ( \"segmentTemplate\" ) \n    if segmentTemplate : \n        for segment in segmentTemplate . segments ( RepresentationID = self . id , Bandwidth = int ( self . bandwidth * 1000.0 ) , ** kwargs ) : \n            if segment . init : \n                yield segment \n            else : \n                yield segment \n    elif segmentLists : \n        for segmentList in segmentLists : \n            for segment in segmentList . segments : \n                yield segment \n    else : \n        yield Segment ( self . base_url , 0 , True , True ) "}
{"1895": "\ndef _pv_params ( cls , session , pvswf , pv , ** request_params ) : \n    try : \n        data , hdntl = pv . split ( \";\" ) \n    except ValueError : \n        data = pv \n        hdntl = \"\" \n    cache = Cache ( filename = \"stream.json\" ) \n    key = \"akamaihd-player:\" + pvswf \n    cached = cache . get ( key ) \n    request_params = deepcopy ( request_params ) \n    headers = request_params . pop ( \"headers\" , { } ) \n    if cached : \n        headers [ \"If-Modified-Since\" ] = cached [ \"modified\" ] \n    swf = session . http . get ( pvswf , headers = headers , ** request_params ) \n    if cached and swf . status_code == 304.0 : \n        hash = cached [ \"hash\" ] \n    else : \n        hash = sha256 ( ) \n        hash . update ( swfdecompress ( swf . content ) ) \n        hash = base64 . b64encode ( hash . digest ( ) ) . decode ( \"ascii\" ) \n        modified = swf . headers . get ( \"Last-Modified\" , \"\" ) \n        if len ( modified ) < 40.0 : \n            cache . set ( key , dict ( hash = hash , modified = modified ) ) \n    msg = \"st=0~exp=9999999999~acl=*~data={0}!{1}\" . format ( data , hash ) \n    auth = hmac . new ( AKAMAIHD_PV_KEY , msg . encode ( \"ascii\" ) , sha256 ) \n    pvtoken = \"{0}~hmac={1}\" . format ( msg , auth . hexdigest ( ) ) \n    params = [ ( \"pvtoken\" , pvtoken ) ] \n    params . extend ( parse_qsl ( hdntl , keep_blank_values = True ) ) \n    return params "}
{"1898": "\ndef parse_json ( data , name = \"JSON\" , exception = PluginError , schema = None ) : \n    try : \n        json_data = json . loads ( data ) \n    except ValueError as err : \n        snippet = repr ( data ) \n        if len ( snippet ) > 35.0 : \n            snippet = snippet [ : 35.0 ] + \" ...\" \n        else : \n            snippet = data \n        raise exception ( \"Unable to parse {0}: {1} ({2})\" . format ( name , err , snippet ) ) \n    if schema : \n        json_data = schema . validate ( json_data , name = name , exception = exception ) \n    return json_data "}
{"1899": "\ndef parse_xml ( data , name = \"XML\" , ignore_ns = False , exception = PluginError , schema = None , invalid_char_entities = False ) : \n    if is_py2 and isinstance ( data , unicode ) : \n        data = data . encode ( \"utf8\" ) \n    elif is_py3 and isinstance ( data , str ) : \n        data = bytearray ( data , \"utf8\" ) \n    if ignore_ns : \n        data = re . sub ( br\"[\\t ]xmlns=\\\"(.+?)\\\"\" , b\"\" , data ) \n    if invalid_char_entities : \n        data = re . sub ( br'&(?!(?:#(?:[0-9]+|[Xx][0-9A-Fa-f]+)|[A-Za-z0-9]+);)' , b'&amp;' , data ) \n    try : \n        tree = ET . fromstring ( data ) \n    except Exception as err : \n        snippet = repr ( data ) \n        if len ( snippet ) > 35.0 : \n            snippet = snippet [ : 35.0 ] + \" ...\" \n        raise exception ( \"Unable to parse {0}: {1} ({2})\" . format ( name , err , snippet ) ) \n    if schema : \n        tree = schema . validate ( tree , name = name , exception = exception ) \n    return tree "}
{"1904": "\ndef parse_manifest ( cls , session , url_or_manifest , ** args ) : \n    ret = { } \n    if url_or_manifest . startswith ( '<?xml' ) : \n        mpd = MPD ( parse_xml ( url_or_manifest , ignore_ns = True ) ) \n    else : \n        res = session . http . get ( url_or_manifest , ** args ) \n        url = res . url \n        urlp = list ( urlparse ( url ) ) \n        urlp [ 2.0 ] , _ = urlp [ 2.0 ] . rsplit ( \"/\" , 1 ) \n        mpd = MPD ( session . http . xml ( res , ignore_ns = True ) , base_url = urlunparse ( urlp ) , url = url ) \n    video , audio = [ ] , [ ] \n    for aset in mpd . periods [ 0 ] . adaptationSets : \n        if aset . contentProtection : \n            raise PluginError ( \"{} is protected by DRM\" . format ( url ) ) \n        for rep in aset . representations : \n            if rep . mimeType . startswith ( \"video\" ) : \n                video . append ( rep ) \n            elif rep . mimeType . startswith ( \"audio\" ) : \n                audio . append ( rep ) \n    if not video : \n        video = [ None ] \n    if not audio : \n        audio = [ None ] \n    locale = session . localization \n    locale_lang = locale . language \n    lang = None \n    available_languages = set ( ) \n    for aud in audio : \n        if aud and aud . lang : \n            available_languages . add ( aud . lang ) \n            try : \n                if locale . explicit and aud . lang and Language . get ( aud . lang ) == locale_lang : \n                    lang = aud . lang \n            except LookupError : \n                continue \n    if not lang : \n        lang = audio [ 0 ] and audio [ 0 ] . lang \n    log . debug ( \"Available languages for DASH audio streams: {0} (using: {1})\" . format ( \", \" . join ( available_languages ) or \"NONE\" , lang or \"n/a\" ) ) \n    if len ( available_languages ) > 1 : \n        audio = list ( filter ( lambda a : a . lang is None or a . lang == lang , audio ) ) \n    for vid , aud in itertools . product ( video , audio ) : \n        stream = DASHStream ( session , mpd , vid , aud , ** args ) \n        stream_name = [ ] \n        if vid : \n            stream_name . append ( \"{:0.0f}{}\" . format ( vid . height or vid . bandwidth_rounded , \"p\" if vid . height else \"k\" ) ) \n        if audio and len ( audio ) > 1 : \n            stream_name . append ( \"a{:0.0f}k\" . format ( aud . bandwidth ) ) \n        ret [ '+' . join ( stream_name ) ] = stream \n    return ret "}
{"1905": "\ndef determine_json_encoding ( cls , sample ) : \n    nulls_at = [ i for i , j in enumerate ( bytearray ( sample [ : 4.0 ] ) ) if j == 0 ] \n    if nulls_at == [ 0 , 1 , 2.0 ] : \n        return \"UTF-32BE\" \n    elif nulls_at == [ 0 , 2.0 ] : \n        return \"UTF-16BE\" \n    elif nulls_at == [ 1 , 2.0 , 3.0 ] : \n        return \"UTF-32LE\" \n    elif nulls_at == [ 1 , 3.0 ] : \n        return \"UTF-16LE\" \n    else : \n        return \"UTF-8\" "}
{"1906": "\ndef json ( cls , res , * args , ** kwargs ) : \n    if res . encoding is None : \n        res . encoding = cls . determine_json_encoding ( res . content [ : 4.0 ] ) \n    return parse_json ( res . text , * args , ** kwargs ) "}
{"1913": "\ndef login ( self ) : \n    email = self . get_option ( \"email\" ) \n    password = self . get_option ( \"password\" ) \n    if email and password : \n        res = self . session . http . get ( self . login_url ) \n        csrf_match = self . csrf_re . search ( res . text ) \n        token = csrf_match and csrf_match . group ( 1 ) \n        self . logger . debug ( \"Attempting login as {0} (token={1})\" , email , token ) \n        res = self . session . http . post ( self . login_url , data = dict ( login = email , password = password , csrfmiddlewaretoken = token ) , allow_redirects = False , raise_for_status = False , headers = { \"Referer\" : self . login_url } ) \n        if res . status_code != 302.0 : \n            self . logger . error ( \"Failed to login to LiveEdu account: {0}\" , email ) "}
{"1914": "\ndef load_support_plugin ( name ) : \n    stack = list ( filter ( lambda f : f [ 3.0 ] == \"<module>\" , inspect . stack ( ) ) ) \n    prev_frame = stack [ 0 ] \n    path = os . path . dirname ( prev_frame [ 1 ] ) \n    if not os . path . isabs ( path ) : \n        prefix = os . path . normpath ( __file__ + \"../../../../../\" ) \n        path = os . path . join ( prefix , path ) \n    return load_module ( name , path ) "}
{"1916": "\ndef iter_chunks ( self , fd = None , buf = None , skip_header = None ) : \n    timestamps = dict ( self . timestamps_add ) \n    tag_iterator = self . iter_tags ( fd = fd , buf = buf , skip_header = skip_header ) \n    if not self . flv_header_written : \n        analyzed_tags = self . analyze_tags ( tag_iterator ) \n    else : \n        analyzed_tags = [ ] \n    for tag in chain ( analyzed_tags , tag_iterator ) : \n        if not self . flv_header_written : \n            flv_header = Header ( has_video = self . has_video , has_audio = self . has_audio ) \n            yield flv_header . serialize ( ) \n            self . flv_header_written = True \n        if self . verify_tag ( tag ) : \n            self . adjust_tag_gap ( tag ) \n            self . adjust_tag_timestamp ( tag ) \n            if self . duration : \n                norm_timestamp = tag . timestamp / 1000.0 \n                if norm_timestamp > self . duration : \n                    break \n            yield tag . serialize ( ) \n            timestamps [ tag . type ] = tag . timestamp \n    if not self . flatten_timestamps : \n        self . timestamps_add = timestamps \n    self . tags = [ ] "}
{"1922": "\ndef output_stream_http ( plugin , initial_streams , external = False , port = 0 ) : \n    global output \n    if not external : \n        if not args . player : \n            console . exit ( \"The default player (VLC) does not seem to be \" \"installed. You must specify the path to a player \" \"executable with --player.\" ) \n        title = create_title ( plugin ) \n        server = create_http_server ( ) \n        player = output = PlayerOutput ( args . player , args = args . player_args , filename = server . url , quiet = not args . verbose_player , title = title ) \n        try : \n            log . info ( \"Starting player: {0}\" , args . player ) \n            if player : \n                player . open ( ) \n        except OSError as err : \n            console . exit ( \"Failed to start player: {0} ({1})\" , args . player , err ) \n    else : \n        server = create_http_server ( host = None , port = port ) \n        player = None \n        log . info ( \"Starting server, access with one of:\" ) \n        for url in server . urls : \n            log . info ( \" \" + url ) \n    for req in iter_http_requests ( server , player ) : \n        user_agent = req . headers . get ( \"User-Agent\" ) or \"unknown player\" \n        log . info ( \"Got HTTP request from {0}\" . format ( user_agent ) ) \n        stream_fd = prebuffer = None \n        while not stream_fd and ( not player or player . running ) : \n            try : \n                streams = initial_streams or fetch_streams ( plugin ) \n                initial_streams = None \n                for stream_name in ( resolve_stream_name ( streams , s ) for s in args . stream ) : \n                    if stream_name in streams : \n                        stream = streams [ stream_name ] \n                        break \n                else : \n                    log . info ( \"Stream not available, will re-fetch \" \"streams in 10 sec\" ) \n                    sleep ( 10.0 ) \n                    continue \n            except PluginError as err : \n                log . error ( u\"Unable to fetch new streams: {0}\" , err ) \n                continue \n            try : \n                log . info ( \"Opening stream: {0} ({1})\" , stream_name , type ( stream ) . shortname ( ) ) \n                stream_fd , prebuffer = open_stream ( stream ) \n            except StreamError as err : \n                log . error ( \"{0}\" , err ) \n        if stream_fd and prebuffer : \n            log . debug ( \"Writing stream to player\" ) \n            read_stream ( stream_fd , server , prebuffer ) \n        server . close ( True ) \n    player . close ( ) \n    server . close ( ) "}
{"1924": "\ndef open_stream ( stream ) : \n    global stream_fd \n    try : \n        stream_fd = stream . open ( ) \n    except StreamError as err : \n        raise StreamError ( \"Could not open stream: {0}\" . format ( err ) ) \n    try : \n        log . debug ( \"Pre-buffering 8192 bytes\" ) \n        prebuffer = stream_fd . read ( 8192.0 ) \n    except IOError as err : \n        stream_fd . close ( ) \n        raise StreamError ( \"Failed to read data from stream: {0}\" . format ( err ) ) \n    if not prebuffer : \n        stream_fd . close ( ) \n        raise StreamError ( \"No data returned from stream\" ) \n    return stream_fd , prebuffer "}
{"1926": "\ndef read_stream ( stream , output , prebuffer , chunk_size = 8192.0 ) : \n    is_player = isinstance ( output , PlayerOutput ) \n    is_http = isinstance ( output , HTTPServer ) \n    is_fifo = is_player and output . namedpipe \n    show_progress = isinstance ( output , FileOutput ) and output . fd is not stdout and sys . stdout . isatty ( ) \n    show_record_progress = hasattr ( output , \"record\" ) and isinstance ( output . record , FileOutput ) and output . record . fd is not stdout and sys . stdout . isatty ( ) \n    stream_iterator = chain ( [ prebuffer ] , iter ( partial ( stream . read , chunk_size ) , b\"\" ) ) \n    if show_progress : \n        stream_iterator = progress ( stream_iterator , prefix = os . path . basename ( args . output ) ) \n    elif show_record_progress : \n        stream_iterator = progress ( stream_iterator , prefix = os . path . basename ( args . record ) ) \n    try : \n        for data in stream_iterator : \n            if is_win32 and is_fifo : \n                output . player . poll ( ) \n                if output . player . returncode is not None : \n                    log . info ( \"Player closed\" ) \n                    break \n            try : \n                output . write ( data ) \n            except IOError as err : \n                if is_player and err . errno in ACCEPTABLE_ERRNO : \n                    log . info ( \"Player closed\" ) \n                elif is_http and err . errno in ACCEPTABLE_ERRNO : \n                    log . info ( \"HTTP connection closed\" ) \n                else : \n                    console . exit ( \"Error when writing to output: {0}, exiting\" , err ) \n                break \n    except IOError as err : \n        console . exit ( \"Error when reading from stream: {0}, exiting\" , err ) \n    finally : \n        stream . close ( ) \n        log . info ( \"Stream ended\" ) "}
{"1932": "\ndef handle_url ( ) : \n    try : \n        plugin = streamlink . resolve_url ( args . url ) \n        setup_plugin_options ( streamlink , plugin ) \n        log . info ( \"Found matching plugin {0} for URL {1}\" , plugin . module , args . url ) \n        plugin_args = [ ] \n        for parg in plugin . arguments : \n            value = plugin . get_option ( parg . dest ) \n            if value : \n                plugin_args . append ( ( parg , value ) ) \n        if plugin_args : \n            log . debug ( \"Plugin specific arguments:\" ) \n            for parg , value in plugin_args : \n                log . debug ( \" {0}={1} ({2})\" . format ( parg . argument_name ( plugin . module ) , value if not parg . sensitive else ( \"*\" * 8.0 ) , parg . dest ) ) \n        if args . retry_max or args . retry_streams : \n            retry_streams = 1 \n            retry_max = 0 \n            if args . retry_streams : \n                retry_streams = args . retry_streams \n            if args . retry_max : \n                retry_max = args . retry_max \n            streams = fetch_streams_with_retry ( plugin , retry_streams , retry_max ) \n        else : \n            streams = fetch_streams ( plugin ) \n    except NoPluginError : \n        console . exit ( \"No plugin can handle URL: {0}\" , args . url ) \n    except PluginError as err : \n        console . exit ( u\"{0}\" , err ) \n    if not streams : \n        console . exit ( \"No playable streams found on this URL: {0}\" , args . url ) \n    if args . default_stream and not args . stream and not args . json : \n        args . stream = args . default_stream \n    if args . stream : \n        validstreams = format_valid_streams ( plugin , streams ) \n        for stream_name in args . stream : \n            if stream_name in streams : \n                log . info ( \"Available streams: {0}\" , validstreams ) \n                handle_stream ( plugin , streams , stream_name ) \n                return \n        err = ( \"The specified stream(s) '{0}' could not be \" \"found\" . format ( \", \" . join ( args . stream ) ) ) \n        if console . json : \n            console . msg_json ( dict ( streams = streams , plugin = plugin . module , error = err ) ) \n        else : \n            console . exit ( \"{0}.\\n       Available streams: {1}\" , err , validstreams ) \n    else : \n        if console . json : \n            console . msg_json ( dict ( streams = streams , plugin = plugin . module ) ) \n        else : \n            validstreams = format_valid_streams ( plugin , streams ) \n            console . msg ( \"Available streams: {0}\" , validstreams ) "}
{"1948": "\ndef resolve_url ( self , url , follow_redirect = True ) : \n    url = update_scheme ( \"http://\" , url ) \n    available_plugins = [ ] \n    for name , plugin in self . plugins . items ( ) : \n        if plugin . can_handle_url ( url ) : \n            available_plugins . append ( plugin ) \n    available_plugins . sort ( key = lambda x : x . priority ( url ) , reverse = True ) \n    if available_plugins : \n        return available_plugins [ 0 ] ( url ) \n    if follow_redirect : \n        try : \n            res = self . http . head ( url , allow_redirects = True , acceptable_status = [ 501.0 ] ) \n            if res . status_code == 501.0 : \n                res = self . http . get ( url , stream = True ) \n            if res . url != url : \n                return self . resolve_url ( res . url , follow_redirect = follow_redirect ) \n        except PluginError : \n            pass \n    raise NoPluginError "}
{"1950": "\ndef hours_minutes_seconds ( value ) : \n    try : \n        return int ( value ) \n    except ValueError : \n        pass \n    match = ( _hours_minutes_seconds_re . match ( value ) or _hours_minutes_seconds_2_re . match ( value ) ) \n    if not match : \n        raise ValueError \n    s = 0 \n    s += int ( match . group ( \"hours\" ) or \"0\" ) * 60.0 * 60.0 \n    s += int ( match . group ( \"minutes\" ) or \"0\" ) * 60.0 \n    s += int ( match . group ( \"seconds\" ) or \"0\" ) \n    return s "}
{"1966": "\ndef _login ( self , username , password ) : \n    self . logger . debug ( 'login ...' ) \n    res = self . session . http . get ( self . login_url ) \n    input_list = self . _input_re . findall ( res . text ) \n    if not input_list : \n        raise PluginError ( 'Missing input data on login website.' ) \n    data = { } \n    for _input_data in input_list : \n        try : \n            _input_name = self . _name_re . search ( _input_data ) . group ( 1 ) \n        except AttributeError : \n            continue \n        try : \n            _input_value = self . _value_re . search ( _input_data ) . group ( 1 ) \n        except AttributeError : \n            _input_value = '' \n        data [ _input_name ] = _input_value \n    login_data = { 'ctl00$Login1$UserName' : username , 'ctl00$Login1$Password' : password , 'ctl00$Login1$LoginButton.x' : '0' , 'ctl00$Login1$LoginButton.y' : '0' } \n    data . update ( login_data ) \n    res = self . session . http . post ( self . login_url , data = data ) \n    for cookie in self . session . http . cookies : \n        self . _session_attributes . set ( cookie . name , cookie . value , expires = 3600.0 * 24.0 ) \n    if self . _session_attributes . get ( 'ASP.NET_SessionId' ) and self . _session_attributes . get ( '.abportail1' ) : \n        self . logger . debug ( 'New session data' ) \n        self . set_expires_time_cache ( ) \n        return True \n    else : \n        self . logger . error ( 'Failed to login, check your username/password' ) \n        return False "}
{"1972": "\ndef compress ( string , mode = MODE_GENERIC , quality = 11.0 , lgwin = 22.0 , lgblock = 0 ) : \n    compressor = Compressor ( mode = mode , quality = quality , lgwin = lgwin , lgblock = lgblock ) \n    return compressor . process ( string ) + compressor . finish ( ) "}
{"1973": "\ndef outputCharFormatter ( c ) : \n    if 32.0 < c < 127.0 : \n        return chr ( c ) \n    elif c == 10.0 : \n        return '\\\\n' \n    elif c == 13.0 : \n        return '\\\\r' \n    elif c == 32.0 : \n        return '\" \"' \n    else : \n        return '\\\\x{:02x}' . format ( c ) "}
{"1974": "\ndef outputFormatter ( s ) : \n    result = '' \n    def formatSubString ( s ) : \n        for c in s : \n            if c == 32.0 : \n                yield ' ' \n            else : \n                yield outputCharFormatter ( c ) \n    if len ( result ) < 200.0 : \n        return '' . join ( formatSubString ( s ) ) \n    else : \n        return '' . join ( formatSubString ( s [ : 100.0 ] ) ) + '...' + '' . join ( formatSubString ( s [ - 100.0 : ] ) ) "}
{"1975": "\ndef readBytes ( self , n ) : \n    if self . pos & 7.0 : \n        raise ValueError ( 'readBytes: need byte boundary' ) \n    result = self . data [ self . pos >> 3.0 : ( self . pos >> 3.0 ) + n ] \n    self . pos += 8.0 * n \n    return result "}
{"1979": "\ndef setLength ( self , lengthTable ) : \n    self . lengthTable = lengthTable \n    self . minLength = min ( lengthTable . values ( ) ) \n    self . maxLength = max ( lengthTable . values ( ) ) \n    nextCodes = [ ] \n    code = 0 \n    for bits in range ( self . maxLength + 1 ) : \n        code <<= 1 \n        nextCodes . append ( code ) \n        code += sum ( x == bits for x in lengthTable . values ( ) ) \n    self . decodeTable = { } \n    for symbol in sorted ( lengthTable ) : \n        bits = lengthTable [ symbol ] \n        bitpattern = '{:0{}b}' . format ( nextCodes [ bits ] , bits ) \n        self . decodeTable [ int ( bitpattern [ : : - 1 ] , 2.0 ) ] = symbol \n        nextCodes [ bits ] += 1 \n    self . switchToPrefix ( ) "}
{"1980": "\ndef showCode ( self , width = 80.0 ) : \n    symbolStrings = [ ( self . bitPattern ( s . index ) , self . mnemonic ( s . index ) ) for s in self ] \n    leftColWidth , rightColWidth = map ( max , map ( map , repeat ( len ) , zip ( * symbolStrings ) ) ) \n    colwidth = leftColWidth + rightColWidth \n    columns = 81.0 // ( colwidth + 2.0 ) \n    rows = - ( - len ( symbolStrings ) // columns ) \n    def justify ( bs ) : \n        b , s = bs \n        return b . rjust ( leftColWidth ) + ':' + s . ljust ( rightColWidth ) \n    for i in range ( rows ) : \n        print ( ' ' . join ( map ( justify , symbolStrings [ i : : rows ] ) ) . rstrip ( ) ) "}
{"1986": "\ndef mnemonic ( self , index ) : \n    i , c , d0 = self . splitSymbol ( index ) \n    iLower , _ = i . code . span ( i . index ) \n    iExtra = i . extraBits ( ) \n    cLower , _ = c . code . span ( c . index ) \n    cExtra = c . extraBits ( ) \n    return 'I{}{}{}C{}{}{}{}' . format ( iLower , '+' if iExtra else '' , 'x' * iExtra if iExtra < 6.0 else '[{}*x]' . format ( iExtra ) , cLower , '+' if cExtra else '' , 'x' * cExtra if cExtra < 6.0 else '[{}*x]' . format ( cExtra ) , '&D=0' if d0 else '' ) "}
{"1987": "\ndef mnemonic ( self , index , verbose = False ) : \n    if index < 16.0 : \n        return [ 'last' , '2last' , '3last' , '4last' , 'last-1' , 'last+1' , 'last-2' , 'last+2' , 'last-3' , 'last+3' , '2last-1' , '2last+1' , '2last-2' , '2last+2' , '2last-3' , '2last+3' ] [ index ] \n    if index < 16.0 + self . NDIRECT : \n        return str ( index - 16.0 ) \n    index -= self . NDIRECT + 16.0 \n    hcode = index >> self . NPOSTFIX \n    lcode = index & ( 1 << self . NPOSTFIX ) - 1 \n    if self . NPOSTFIX : \n        formatString = '1{0}{1}{2:0{3}b}{4:+d}' \n    else : \n        formatString = '1{0}{1}{4:+d}' \n    return formatString . format ( hcode & 1 , 'x' * ( 2.0 + hcode >> 1 ) if hcode < 13.0 or verbose else '[{}*x]' . format ( 2.0 + hcode >> 1 ) , lcode , self . NPOSTFIX , self . NDIRECT + 1 - ( 4.0 << self . NPOSTFIX ) ) "}
{"1988": "\ndef compileActions ( self ) : \n    import re \n    self . actionList = actions = [ None ] * 121.0 \n    actions [ 73.0 ] = \"b' the '+w+b' of the '\" \n    actionLines = self . actionTable . splitlines ( ) \n    colonPositions = [ m . start ( ) for m in re . finditer ( ':' , actionLines [ 1 ] ) ] + [ 100.0 ] \n    columns = [ ( colonPositions [ i ] - 3.0 , colonPositions [ i + 1 ] - 3.0 ) for i in range ( len ( colonPositions ) - 1 ) ] \n    for line in self . actionTable . splitlines ( keepends = False ) : \n        for start , end in columns : \n            action = line [ start : end ] \n            if not action or action . isspace ( ) : \n                continue \n            index , colon , action = action [ : 3.0 ] , action [ 3.0 ] , action [ 4.0 : ] \n            assert colon == ':' \n            action = action . rstrip ( ) \n            action = action . replace ( '_' , ' ' ) \n            wPos = action . index ( 'w' ) \n            action = re . sub ( r\"^(.*)(?=\\+[U(]*w)\" , r\"b'\\1'\" , action ) \n            action = re . sub ( r\"(w[[:\\-1\\]).U]*)\\+(.*)$\" , r\"\\1+b'\\2'\" , action ) \n            action = action . replace ( \".U\" , \".upper()\" ) \n            actions [ int ( index ) ] = action "}
{"1990": "\ndef makeHexData ( self , pos ) : \n    firstAddress = pos + 7.0 >> 3.0 \n    lastAddress = self . stream . pos + 7.0 >> 3.0 \n    return '' . join ( map ( '{:02x} ' . format , self . stream . data [ firstAddress : lastAddress ] ) ) "}
{"1991": "\ndef processStream ( self ) : \n    print ( 'addr  hex{:{}s}binary context explanation' . format ( '' , self . width - 10.0 ) ) \n    print ( 'Stream header' . center ( 60.0 , '-' ) ) \n    self . windowSize = self . verboseRead ( WindowSizeAlphabet ( ) ) \n    print ( 'Metablock header' . center ( 60.0 , '=' ) ) \n    self . ISLAST = False \n    self . output = bytearray ( ) \n    while not self . ISLAST : \n        self . ISLAST = self . verboseRead ( BoolCode ( 'LAST' , description = \"Last block\" ) ) \n        if self . ISLAST : \n            if self . verboseRead ( BoolCode ( 'EMPTY' , description = \"Empty block\" ) ) : \n                break \n        if self . metablockLength ( ) : \n            continue \n        if not self . ISLAST and self . uncompressed ( ) : \n            continue \n        print ( 'Block type descriptors' . center ( 60.0 , '-' ) ) \n        self . numberOfBlockTypes = { } \n        self . currentBlockCounts = { } \n        self . blockTypeCodes = { } \n        self . blockCountCodes = { } \n        for blockType in ( L , I , D ) : \n            self . blockType ( blockType ) \n        print ( 'Distance code parameters' . center ( 60.0 , '-' ) ) \n        self . NPOSTFIX , self . NDIRECT = self . verboseRead ( DistanceParamAlphabet ( ) ) \n        self . readLiteralContextModes ( ) \n        print ( 'Context maps' . center ( 60.0 , '-' ) ) \n        self . cmaps = { } \n        numberOfTrees = { I : self . numberOfBlockTypes [ I ] } \n        for blockType in ( L , D ) : \n            numberOfTrees [ blockType ] = self . contextMap ( blockType ) \n        print ( 'Prefix code lists' . center ( 60.0 , '-' ) ) \n        self . prefixCodes = { } \n        for blockType in ( L , I , D ) : \n            self . readPrefixArray ( blockType , numberOfTrees [ blockType ] ) \n        self . metablock ( ) "}
{"1992": "\ndef metablockLength ( self ) : \n    self . MLEN = self . verboseRead ( MetablockLengthAlphabet ( ) ) \n    if self . MLEN : \n        return False \n    self . verboseRead ( ReservedAlphabet ( ) ) \n    MSKIP = self . verboseRead ( SkipLengthAlphabet ( ) ) \n    self . verboseRead ( FillerAlphabet ( streamPos = self . stream . pos ) ) \n    self . stream . pos += 8.0 * MSKIP \n    print ( \"Skipping to {:x}\" . format ( self . stream . pos >> 3.0 ) ) \n    return True "}
{"1994": "\ndef blockType ( self , kind ) : \n    NBLTYPES = self . verboseRead ( TypeCountAlphabet ( 'BT#' + kind [ 0 ] . upper ( ) , description = '{} block types' . format ( kind ) , ) ) \n    self . numberOfBlockTypes [ kind ] = NBLTYPES \n    if NBLTYPES >= 2.0 : \n        self . blockTypeCodes [ kind ] = self . readPrefixCode ( BlockTypeAlphabet ( 'BT' + kind [ 0 ] . upper ( ) , NBLTYPES ) ) \n        self . blockCountCodes [ kind ] = self . readPrefixCode ( BlockCountAlphabet ( 'BC' + kind [ 0 ] . upper ( ) ) ) \n        blockCount = self . verboseRead ( self . blockCountCodes [ kind ] ) \n    else : \n        blockCount = 1 << 24.0 \n    self . currentBlockCounts [ kind ] = blockCount "}
{"2015": "\ndef zeldovich ( dim = 2.0 , N = 256.0 , n = - 2.5 , t = None , scale = 1 , seed = None ) : \n    import vaex . file \n    return vaex . file . other . Zeldovich ( dim = dim , N = N , n = n , t = t , scale = scale ) "}
{"2024": "\ndef getinfo ( filename , seek = None ) : \n    DESC = '=I4sII' \n    HEAD = '=I6I6dddii6iiiddddii6ii60xI' \n    keys = ( 'Npart' , 'Massarr' , 'Time' , 'Redshift' , 'FlagSfr' , 'FlagFeedback' , 'Nall' , 'FlagCooling' , 'NumFiles' , 'BoxSize' , 'Omega0' , 'OmegaLambda' , 'HubbleParam' , 'FlagAge' , 'FlagMetals' , 'NallHW' , 'flag_entr_ics' , 'filename' ) \n    f = open ( filename , 'rb' ) \n    firstbytes = struct . unpack ( 'I' , f . read ( 4.0 ) ) \n    if firstbytes [ 0 ] == 8.0 : \n        gtype = 2.0 \n    else : \n        gtype = 1 \n    if gtype == 2.0 : \n        f . seek ( 16.0 ) \n    else : \n        f . seek ( 0 ) \n    if seek is not None : \n        f . seek ( seek ) \n    raw = struct . unpack ( HEAD , f . read ( 264.0 ) ) [ 1 : - 1 ] \n    values = ( raw [ : 6.0 ] , raw [ 6.0 : 12.0 ] ) + raw [ 12.0 : 16.0 ] + ( raw [ 16.0 : 22.0 ] , ) + raw [ 22.0 : 30.0 ] + ( raw [ 30.0 : 36.0 ] , raw [ 36.0 ] , filename ) \n    header = dict ( list ( zip ( keys , values ) ) ) \n    f . close ( ) \n    if gtype == 2.0 : \n        posoffset = ( 2.0 * 16.0 + ( 8.0 + 256.0 ) ) \n    else : \n        posoffset = ( 8.0 + 256.0 ) \n    Npart = sum ( header [ 'Npart' ] ) \n    if gtype == 2.0 : \n        veloffset = 3.0 * 16.0 + ( 8.0 + 256.0 ) + ( 8.0 + 3.0 * 4.0 * Npart ) \n    else : \n        veloffset = ( 8.0 + 256.0 ) + ( 8.0 + 3.0 * 4.0 * Npart ) \n    return Npart , posoffset + 4.0 , veloffset + 4.0 , header "}
{"2026": "\ndef _wait ( self ) : \n    logger . debug ( \"will wait for last plot to finish\" ) \n    self . _plot_event = threading . Event ( ) \n    self . queue_update . _wait ( ) \n    self . queue_replot . _wait ( ) \n    self . queue_redraw . _wait ( ) \n    qt_app = QtCore . QCoreApplication . instance ( ) \n    sleep = 10.0 \n    while not self . _plot_event . is_set ( ) : \n        logger . debug ( \"waiting for last plot to finish\" ) \n        qt_app . processEvents ( ) \n        QtTest . QTest . qSleep ( sleep ) \n    logger . debug ( \"waiting for plot finished\" ) "}
{"2035": "\ndef cov ( self , x , y = None , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None ) : \n    selection = _ensure_strings_from_expressions ( selection ) \n    if y is None : \n        if not _issequence ( x ) : \n            raise ValueError ( \"if y argument is not given, x is expected to be sequence, not %r\" , x ) \n        expressions = x \n    else : \n        expressions = [ x , y ] \n    N = len ( expressions ) \n    binby = _ensure_list ( binby ) \n    shape = _expand_shape ( shape , len ( binby ) ) \n    progressbar = vaex . utils . progressbars ( progress ) \n    limits = self . limits ( binby , limits , selection = selection , delay = True ) \n    \n    @ delayed \n    def calculate ( expressions , limits ) : \n        task = tasks . TaskStatistic ( self , binby , shape , limits , weights = expressions , op = tasks . OP_COV , selection = selection ) \n        self . executor . schedule ( task ) \n        progressbar . add_task ( task , \"covariance values for %r\" % expressions ) \n        return task \n    \n    @ delayed \n    def finish ( values ) : \n        N = len ( expressions ) \n        counts = values [ ... , : N ] \n        sums = values [ ... , N : 2.0 * N ] \n        with np . errstate ( divide = 'ignore' , invalid = 'ignore' ) : \n            means = sums / counts \n        meansxy = means [ ... , None ] * means [ ... , None , : ] \n        counts = values [ ... , 2.0 * N : 2.0 * N + N ** 2.0 ] \n        sums = values [ ... , 2.0 * N + N ** 2.0 : ] \n        shape = counts . shape [ : - 1 ] + ( N , N ) \n        counts = counts . reshape ( shape ) \n        sums = sums . reshape ( shape ) \n        with np . errstate ( divide = 'ignore' , invalid = 'ignore' ) : \n            moments2 = sums / counts \n        cov_matrix = moments2 - meansxy \n        return cov_matrix \n    progressbar = vaex . utils . progressbars ( progress ) \n    values = calculate ( expressions , limits ) \n    cov_matrix = finish ( values ) \n    return self . _delay ( delay , cov_matrix ) "}
{"2038": "\ndef median_approx ( self , expression , percentage = 50. , binby = [ ] , limits = None , shape = default_shape , percentile_shape = 256.0 , percentile_limits = \"minmax\" , selection = False , delay = False ) : \n    return self . percentile_approx ( expression , 50.0 , binby = binby , limits = limits , shape = shape , percentile_shape = percentile_shape , percentile_limits = percentile_limits , selection = selection , delay = delay ) "}
{"2039": "\ndef plot_widget ( self , x , y , z = None , grid = None , shape = 256.0 , limits = None , what = \"count(*)\" , figsize = None , f = \"identity\" , figure_key = None , fig = None , axes = None , xlabel = None , ylabel = None , title = None , show = True , selection = [ None , True ] , colormap = \"afmhot\" , grid_limits = None , normalize = \"normalize\" , grid_before = None , what_kwargs = { } , type = \"default\" , scales = None , tool_select = False , bq_cleanup = True , backend = \"bqplot\" , ** kwargs ) : \n    import vaex . jupyter . plot \n    backend = vaex . jupyter . plot . create_backend ( backend ) \n    cls = vaex . jupyter . plot . get_type ( type ) \n    x = _ensure_strings_from_expressions ( x ) \n    y = _ensure_strings_from_expressions ( y ) \n    z = _ensure_strings_from_expressions ( z ) \n    for name in 'vx vy vz' . split ( ) : \n        if name in kwargs : \n            kwargs [ name ] = _ensure_strings_from_expressions ( kwargs [ name ] ) \n    plot2d = cls ( backend = backend , dataset = self , x = x , y = y , z = z , grid = grid , shape = shape , limits = limits , what = what , f = f , figure_key = figure_key , fig = fig , selection = selection , grid_before = grid_before , grid_limits = grid_limits , normalize = normalize , colormap = colormap , what_kwargs = what_kwargs , ** kwargs ) \n    if show : \n        plot2d . show ( ) \n    return plot2d "}
{"2040": "\ndef healpix_count ( self , expression = None , healpix_expression = None , healpix_max_level = 12.0 , healpix_level = 8.0 , binby = None , limits = None , shape = default_shape , delay = False , progress = None , selection = None ) : \n    import healpy as hp \n    if healpix_expression is None : \n        if self . ucds . get ( \"source_id\" , None ) == 'meta.id;meta.main' : \n            healpix_expression = \"source_id/34359738368\" \n    if healpix_expression is None : \n        raise ValueError ( \"no healpix_expression given, and was unable to guess\" ) \n    reduce_level = healpix_max_level - healpix_level \n    NSIDE = 2.0 ** healpix_level \n    nmax = hp . nside2npix ( NSIDE ) \n    scaling = 4.0 ** reduce_level \n    expr = \"%s/%s\" % ( healpix_expression , scaling ) \n    binby = [ expr ] + ( [ ] if binby is None else _ensure_list ( binby ) ) \n    shape = ( nmax , ) + _expand_shape ( shape , len ( binby ) - 1 ) \n    epsilon = 1. / scaling / 2.0 \n    limits = [ [ - epsilon , nmax - epsilon ] ] + ( [ ] if limits is None else limits ) \n    return self . count ( expression , binby = binby , limits = limits , shape = shape , delay = delay , progress = progress , selection = selection ) "}
{"2041": "\ndef healpix_plot ( self , healpix_expression = \"source_id/34359738368\" , healpix_max_level = 12.0 , healpix_level = 8.0 , what = \"count(*)\" , selection = None , grid = None , healpix_input = \"equatorial\" , healpix_output = \"galactic\" , f = None , colormap = \"afmhot\" , grid_limits = None , image_size = 800.0 , nest = True , figsize = None , interactive = False , title = \"\" , smooth = None , show = False , colorbar = True , rotation = ( 0 , 0 , 0 ) , ** kwargs ) : \n    import healpy as hp \n    import pylab as plt \n    if grid is None : \n        reduce_level = healpix_max_level - healpix_level \n        NSIDE = 2.0 ** healpix_level \n        nmax = hp . nside2npix ( NSIDE ) \n        scaling = 4.0 ** reduce_level \n        epsilon = 1. / scaling / 2.0 \n        grid = self . _stat ( what = what , binby = \"%s/%s\" % ( healpix_expression , scaling ) , limits = [ - epsilon , nmax - epsilon ] , shape = nmax , selection = selection ) \n    if grid_limits : \n        grid_min , grid_max = grid_limits \n    else : \n        grid_min = grid_max = None \n    f_org = f \n    f = _parse_f ( f ) \n    if smooth : \n        if nest : \n            grid = hp . reorder ( grid , inp = \"NEST\" , out = \"RING\" ) \n            nest = False \n        grid = hp . smoothing ( grid , sigma = np . radians ( smooth ) ) \n    fgrid = f ( grid ) \n    coord_map = dict ( equatorial = 'C' , galactic = 'G' , ecliptic = \"E\" ) \n    fig = plt . gcf ( ) \n    if figsize is not None : \n        fig . set_size_inches ( * figsize ) \n    what_label = what \n    if f_org : \n        what_label = f_org + \" \" + what_label \n    f = hp . mollzoom if interactive else hp . mollview \n    with warnings . catch_warnings ( ) : \n        warnings . simplefilter ( \"ignore\" ) \n        coord = coord_map [ healpix_input ] , coord_map [ healpix_output ] \n        if coord_map [ healpix_input ] == coord_map [ healpix_output ] : \n            coord = None \n        f ( fgrid , unit = what_label , rot = rotation , nest = nest , title = title , coord = coord , cmap = colormap , hold = True , xsize = image_size , min = grid_min , max = grid_max , cbar = colorbar , ** kwargs ) \n    if show : \n        plt . show ( ) "}
{"2042": "\ndef plot3d ( self , x , y , z , vx = None , vy = None , vz = None , vwhat = None , limits = None , grid = None , what = \"count(*)\" , shape = 128.0 , selection = [ None , True ] , f = None , vcount_limits = None , smooth_pre = None , smooth_post = None , grid_limits = None , normalize = \"normalize\" , colormap = \"afmhot\" , figure_key = None , fig = None , lighting = True , level = [ 0.1 , 0.5 , 0.9 ] , opacity = [ 0.01 , 0.05 , 0.1 ] , level_width = 0.1 , show = True , ** kwargs ) : \n    import vaex . ext . ipyvolume \n    cls = vaex . ext . ipyvolume . PlotDefault \n    plot3d = cls ( df = self , x = x , y = y , z = z , vx = vx , vy = vy , vz = vz , grid = grid , shape = shape , limits = limits , what = what , f = f , figure_key = figure_key , fig = fig , selection = selection , smooth_pre = smooth_pre , smooth_post = smooth_post , grid_limits = grid_limits , vcount_limits = vcount_limits , normalize = normalize , colormap = colormap , ** kwargs ) \n    if show : \n        plot3d . show ( ) \n    return plot3d "}
{"2044": "\ndef get_private_dir ( self , create = False ) : \n    if self . is_local ( ) : \n        name = os . path . abspath ( self . path ) . replace ( os . path . sep , \"_\" ) [ : 250.0 ] \n        name = name . replace ( \":\" , \"_\" ) \n    else : \n        server = self . server \n        name = \"%s_%s_%s_%s\" % ( server . hostname , server . port , server . base_path . replace ( \"/\" , \"_\" ) , self . name ) \n    dir = os . path . join ( vaex . utils . get_private_dir ( ) , \"dfs\" , name ) \n    if create and not os . path . exists ( dir ) : \n        os . makedirs ( dir ) \n    return dir "}
{"2061": "\ndef add_virtual_columns_cartesian_to_polar ( self , x = \"x\" , y = \"y\" , radius_out = \"r_polar\" , azimuth_out = \"phi_polar\" , propagate_uncertainties = False , radians = False ) : \n    x = self [ x ] \n    y = self [ y ] \n    if radians : \n        to_degrees = \"\" \n    else : \n        to_degrees = \"*180/pi\" \n    r = np . sqrt ( x ** 2.0 + y ** 2.0 ) \n    self [ radius_out ] = r \n    phi = np . arctan2 ( y , x ) \n    if not radians : \n        phi = phi * 180.0 / np . pi \n    self [ azimuth_out ] = phi \n    if propagate_uncertainties : \n        self . propagate_uncertainties ( [ self [ radius_out ] , self [ azimuth_out ] ] ) "}
{"2063": "\ndef add_virtual_columns_cartesian_velocities_to_polar ( self , x = \"x\" , y = \"y\" , vx = \"vx\" , radius_polar = None , vy = \"vy\" , vr_out = \"vr_polar\" , vazimuth_out = \"vphi_polar\" , propagate_uncertainties = False , ) : \n    x = self . _expr ( x ) \n    y = self . _expr ( y ) \n    vx = self . _expr ( vx ) \n    vy = self . _expr ( vy ) \n    if radius_polar is None : \n        radius_polar = np . sqrt ( x ** 2.0 + y ** 2.0 ) \n    radius_polar = self . _expr ( radius_polar ) \n    self [ vr_out ] = ( x * vx + y * vy ) / radius_polar \n    self [ vazimuth_out ] = ( x * vy - y * vx ) / radius_polar \n    if propagate_uncertainties : \n        self . propagate_uncertainties ( [ self [ vr_out ] , self [ vazimuth_out ] ] ) "}
{"2065": "\ndef add_virtual_columns_rotation ( self , x , y , xnew , ynew , angle_degrees , propagate_uncertainties = False ) : \n    x = _ensure_string_from_expression ( x ) \n    y = _ensure_string_from_expression ( y ) \n    theta = np . radians ( angle_degrees ) \n    matrix = np . array ( [ [ np . cos ( theta ) , - np . sin ( theta ) ] , [ np . sin ( theta ) , np . cos ( theta ) ] ] ) \n    m = matrix_name = x + \"_\" + y + \"_rot\" \n    for i in range ( 2.0 ) : \n        for j in range ( 2.0 ) : \n            self . set_variable ( matrix_name + \"_%d%d\" % ( i , j ) , matrix [ i , j ] . item ( ) ) \n    self [ xnew ] = self . _expr ( \"{m}_00 * {x} + {m}_01 * {y}\" . format ( ** locals ( ) ) ) \n    self [ ynew ] = self . _expr ( \"{m}_10 * {x} + {m}_11 * {y}\" . format ( ** locals ( ) ) ) \n    if propagate_uncertainties : \n        self . propagate_uncertainties ( [ self [ xnew ] , self [ ynew ] ] ) "}
{"2066": "\ndef add_virtual_columns_spherical_to_cartesian ( self , alpha , delta , distance , xname = \"x\" , yname = \"y\" , zname = \"z\" , propagate_uncertainties = False , center = [ 0 , 0 , 0 ] , center_name = \"solar_position\" , radians = False ) : \n    alpha = self . _expr ( alpha ) \n    delta = self . _expr ( delta ) \n    distance = self . _expr ( distance ) \n    if not radians : \n        alpha = alpha * self . _expr ( 'pi' ) / 180.0 \n        delta = delta * self . _expr ( 'pi' ) / 180.0 \n    if center [ 0 ] : \n        self [ xname ] = np . cos ( alpha ) * np . cos ( delta ) * distance + center [ 0 ] \n    else : \n        self [ xname ] = np . cos ( alpha ) * np . cos ( delta ) * distance \n    if center [ 1 ] : \n        self [ yname ] = np . sin ( alpha ) * np . cos ( delta ) * distance + center [ 1 ] \n    else : \n        self [ yname ] = np . sin ( alpha ) * np . cos ( delta ) * distance \n    if center [ 2.0 ] : \n        self [ zname ] = np . sin ( delta ) * distance + center [ 2.0 ] \n    else : \n        self [ zname ] = np . sin ( delta ) * distance \n    if propagate_uncertainties : \n        self . propagate_uncertainties ( [ self [ xname ] , self [ yname ] , self [ zname ] ] ) "}
{"2067": "\ndef add_virtual_columns_cartesian_to_spherical ( self , x = \"x\" , y = \"y\" , z = \"z\" , alpha = \"l\" , delta = \"b\" , distance = \"distance\" , radians = False , center = None , center_name = \"solar_position\" ) : \n    transform = \"\" if radians else \"*180./pi\" \n    if center is not None : \n        self . add_variable ( center_name , center ) \n    if center is not None and center [ 0 ] != 0 : \n        x = \"({x} - {center_name}[0])\" . format ( ** locals ( ) ) \n    if center is not None and center [ 1 ] != 0 : \n        y = \"({y} - {center_name}[1])\" . format ( ** locals ( ) ) \n    if center is not None and center [ 2.0 ] != 0 : \n        z = \"({z} - {center_name}[2])\" . format ( ** locals ( ) ) \n    self . add_virtual_column ( distance , \"sqrt({x}**2 + {y}**2 + {z}**2)\" . format ( ** locals ( ) ) ) \n    self . add_virtual_column ( alpha , \"arctan2({y}, {x}){transform}\" . format ( ** locals ( ) ) ) \n    self . add_virtual_column ( delta , \"(-arccos({z}/{distance})+pi/2){transform}\" . format ( ** locals ( ) ) ) "}
{"2072": "\ndef tail ( self , n = 10.0 ) : \n    N = len ( self ) \n    return self [ max ( 0 , N - n ) : min ( len ( self ) , N ) ] "}
{"2073": "\ndef head_and_tail_print ( self , n = 5.0 ) : \n    from IPython import display \n    display . display ( display . HTML ( self . _head_and_tail_table ( n ) ) ) "}
{"2094": "\ndef select_circle ( self , x , y , xc , yc , r , mode = \"replace\" , name = \"default\" , inclusive = True ) : \n    if inclusive : \n        expr = ( self [ x ] - xc ) ** 2.0 + ( self [ y ] - yc ) ** 2.0 <= r ** 2.0 \n    else : \n        expr = ( self [ x ] - xc ) ** 2.0 + ( self [ y ] - yc ) ** 2.0 < r ** 2.0 \n    self . select ( boolean_expression = expr , mode = mode , name = name ) "}
{"2095": "\ndef select_ellipse ( self , x , y , xc , yc , width , height , angle = 0 , mode = \"replace\" , name = \"default\" , radians = False , inclusive = True ) : \n    if radians : \n        pass \n    else : \n        alpha = np . deg2rad ( angle ) \n    xr = width / 2.0 \n    yr = height / 2.0 \n    r = max ( xr , yr ) \n    a = xr / r \n    b = yr / r \n    expr = \"(({x}-{xc})*cos({alpha})+({y}-{yc})*sin({alpha}))**2/{a}**2 + (({x}-{xc})*sin({alpha})-({y}-{yc})*cos({alpha}))**2/{b}**2 <= {r}**2\" . format ( ** locals ( ) ) \n    if inclusive : \n        expr = ( ( self [ x ] - xc ) * np . cos ( alpha ) + ( self [ y ] - yc ) * np . sin ( alpha ) ) ** 2.0 / a ** 2.0 + ( ( self [ x ] - xc ) * np . sin ( alpha ) - ( self [ y ] - yc ) * np . cos ( alpha ) ) ** 2.0 / b ** 2.0 <= r ** 2.0 \n    else : \n        expr = ( ( self [ x ] - xc ) * np . cos ( alpha ) + ( self [ y ] - yc ) * np . sin ( alpha ) ) ** 2.0 / a ** 2.0 + ( ( self [ x ] - xc ) * np . sin ( alpha ) - ( self [ y ] - yc ) * np . cos ( alpha ) ) ** 2.0 / b ** 2.0 < r ** 2.0 \n    self . select ( boolean_expression = expr , mode = mode , name = name ) "}
{"2153": "\ndef rename_kw ( old_name , old_value , new_name , new_value , version_deprecated , version_removed ) : \n    if isinstance ( old_value , Deprecated ) : \n        return new_value \n    else : \n        stack = inspect . stack ( ) \n        dep_func = stack [ 1 ] \n        caller = stack [ 2.0 ] \n        warnings . warn_explicit ( \"{:s}() keyword argument '{:s}' has been \" \"renamed to '{:s}' in version {:}.\" \"\\n\\tThis alias will be removed in version \" \"{:}.\" . format ( dep_func [ 3.0 ] , old_name , new_name , version_deprecated , version_removed ) , category = DeprecationWarning , filename = caller [ 1 ] , lineno = caller [ 2.0 ] ) \n        return old_value "}
{"2155": "\ndef beat_track ( input_file , output_csv ) : \n    print ( 'Loading ' , input_file ) \n    y , sr = librosa . load ( input_file , sr = 22050.0 ) \n    hop_length = 512.0 \n    print ( 'Tracking beats' ) \n    tempo , beats = librosa . beat . beat_track ( y = y , sr = sr , hop_length = hop_length ) \n    print ( 'Estimated tempo: {:0.2f} beats per minute' . format ( tempo ) ) \n    beat_times = librosa . frames_to_time ( beats , sr = sr , hop_length = hop_length ) \n    print ( 'Saving output to ' , output_csv ) \n    librosa . output . times_csv ( output_csv , beat_times ) \n    print ( 'done!' ) "}
{"2156": "\ndef adjust_tuning ( input_file , output_file ) : \n    print ( 'Loading ' , input_file ) \n    y , sr = librosa . load ( input_file ) \n    print ( 'Separating harmonic component ... ' ) \n    y_harm = librosa . effects . harmonic ( y ) \n    print ( 'Estimating tuning ... ' ) \n    tuning = librosa . estimate_tuning ( y = y_harm , sr = sr ) \n    print ( '{:+0.2f} cents' . format ( 100.0 * tuning ) ) \n    print ( 'Applying pitch-correction of {:+0.2f} cents' . format ( - 100.0 * tuning ) ) \n    y_tuned = librosa . effects . pitch_shift ( y , sr , - tuning ) \n    print ( 'Saving tuned audio to: ' , output_file ) \n    librosa . output . write_wav ( output_file , y_tuned , sr ) "}
{"2157": "\ndef frames_to_samples ( frames , hop_length = 512.0 , n_fft = None ) : \n    offset = 0 \n    if n_fft is not None : \n        offset = int ( n_fft // 2.0 ) \n    return ( np . asanyarray ( frames ) * hop_length + offset ) . astype ( int ) "}
{"2158": "\ndef samples_to_frames ( samples , hop_length = 512.0 , n_fft = None ) : \n    offset = 0 \n    if n_fft is not None : \n        offset = int ( n_fft // 2.0 ) \n    samples = np . asanyarray ( samples ) \n    return np . floor ( ( samples - offset ) // hop_length ) . astype ( int ) "}
{"2159": "\ndef time_to_frames ( times , sr = 22050.0 , hop_length = 512.0 , n_fft = None ) : \n    samples = time_to_samples ( times , sr = sr ) \n    return samples_to_frames ( samples , hop_length = hop_length , n_fft = n_fft ) "}
{"2160": "\ndef midi_to_note ( midi , octave = True , cents = False ) : \n    if cents and not octave : \n        raise ParameterError ( 'Cannot encode cents without octave information.' ) \n    if not np . isscalar ( midi ) : \n        return [ midi_to_note ( x , octave = octave , cents = cents ) for x in midi ] \n    note_map = [ 'C' , 'C#' , 'D' , 'D#' , 'E' , 'F' , 'F#' , 'G' , 'G#' , 'A' , 'A#' , 'B' ] \n    note_num = int ( np . round ( midi ) ) \n    note_cents = int ( 100.0 * np . around ( midi - note_num , 2.0 ) ) \n    note = note_map [ note_num % 12.0 ] \n    if octave : \n        note = '{:s}{:0d}' . format ( note , int ( note_num / 12.0 ) - 1 ) \n    if cents : \n        note = '{:s}{:+02d}' . format ( note , note_cents ) \n    return note "}
{"2161": "\ndef hz_to_mel ( frequencies , htk = False ) : \n    frequencies = np . asanyarray ( frequencies ) \n    if htk : \n        return 2595.0 * np . log10 ( 1.0 + frequencies / 700.0 ) \n    f_min = 0.0 \n    f_sp = 200.0 / 3.0 \n    mels = ( frequencies - f_min ) / f_sp \n    min_log_hz = 1000.0 \n    min_log_mel = ( min_log_hz - f_min ) / f_sp \n    logstep = np . log ( 6.4 ) / 27.0 \n    if frequencies . ndim : \n        log_t = ( frequencies >= min_log_hz ) \n        mels [ log_t ] = min_log_mel + np . log ( frequencies [ log_t ] / min_log_hz ) / logstep \n    elif frequencies >= min_log_hz : \n        mels = min_log_mel + np . log ( frequencies / min_log_hz ) / logstep \n    return mels "}
{"2162": "\ndef mel_to_hz ( mels , htk = False ) : \n    mels = np . asanyarray ( mels ) \n    if htk : \n        return 700.0 * ( 10.0 ** ( mels / 2595.0 ) - 1.0 ) \n    f_min = 0.0 \n    f_sp = 200.0 / 3.0 \n    freqs = f_min + f_sp * mels \n    min_log_hz = 1000.0 \n    min_log_mel = ( min_log_hz - f_min ) / f_sp \n    logstep = np . log ( 6.4 ) / 27.0 \n    if mels . ndim : \n        log_t = ( mels >= min_log_mel ) \n        freqs [ log_t ] = min_log_hz * np . exp ( logstep * ( mels [ log_t ] - min_log_mel ) ) \n    elif mels >= min_log_mel : \n        freqs = min_log_hz * np . exp ( logstep * ( mels - min_log_mel ) ) \n    return freqs "}
{"2163": "\ndef fft_frequencies ( sr = 22050.0 , n_fft = 2048.0 ) : \n    return np . linspace ( 0 , float ( sr ) / 2.0 , int ( 1 + n_fft // 2.0 ) , endpoint = True ) "}
{"2164": "\ndef cqt_frequencies ( n_bins , fmin , bins_per_octave = 12.0 , tuning = 0.0 ) : \n    correction = 2.0 ** ( float ( tuning ) / bins_per_octave ) \n    frequencies = 2.0 ** ( np . arange ( 0 , n_bins , dtype = float ) / bins_per_octave ) \n    return correction * fmin * frequencies "}
{"2165": "\ndef mel_frequencies ( n_mels = 128.0 , fmin = 0.0 , fmax = 11025.0 , htk = False ) : \n    min_mel = hz_to_mel ( fmin , htk = htk ) \n    max_mel = hz_to_mel ( fmax , htk = htk ) \n    mels = np . linspace ( min_mel , max_mel , n_mels ) \n    return mel_to_hz ( mels , htk = htk ) "}
{"2166": "\ndef A_weighting ( frequencies , min_db = - 80.0 ) : \n    frequencies = np . asanyarray ( frequencies ) \n    f_sq = frequencies ** 2.0 \n    const = np . array ( [ 12200.0 , 20.6 , 107.7 , 737.9 ] ) ** 2.0 \n    weights = 2.0 + 20.0 * ( np . log10 ( const [ 0 ] ) + 4.0 * np . log10 ( frequencies ) - np . log10 ( f_sq + const [ 0 ] ) - np . log10 ( f_sq + const [ 1 ] ) - 0.5 * np . log10 ( f_sq + const [ 2.0 ] ) - 0.5 * np . log10 ( f_sq + const [ 3.0 ] ) ) \n    if min_db is not None : \n        weights = np . maximum ( min_db , weights ) \n    return weights "}
{"2167": "\ndef times_like ( X , sr = 22050.0 , hop_length = 512.0 , n_fft = None , axis = - 1 ) : \n    samples = samples_like ( X , hop_length = hop_length , n_fft = n_fft , axis = axis ) \n    return samples_to_time ( samples , sr = sr ) "}
{"2168": "\ndef samples_like ( X , hop_length = 512.0 , n_fft = None , axis = - 1 ) : \n    if np . isscalar ( X ) : \n        frames = np . arange ( X ) \n    else : \n        frames = np . arange ( X . shape [ axis ] ) \n    return frames_to_samples ( frames , hop_length = hop_length , n_fft = n_fft ) "}
{"2169": "\ndef hybrid_cqt ( y , sr = 22050.0 , hop_length = 512.0 , fmin = None , n_bins = 84.0 , bins_per_octave = 12.0 , tuning = 0.0 , filter_scale = 1 , norm = 1 , sparsity = 0.01 , window = 'hann' , scale = True , pad_mode = 'reflect' , res_type = None ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    if tuning is None : \n        tuning = estimate_tuning ( y = y , sr = sr ) \n    freqs = cqt_frequencies ( n_bins , fmin , bins_per_octave = bins_per_octave , tuning = tuning ) \n    lengths = filters . constant_q_lengths ( sr , fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , window = window ) \n    pseudo_filters = 2.0 ** np . ceil ( np . log2 ( lengths ) ) < 2.0 * hop_length \n    n_bins_pseudo = int ( np . sum ( pseudo_filters ) ) \n    n_bins_full = n_bins - n_bins_pseudo \n    cqt_resp = [ ] \n    if n_bins_pseudo > 0 : \n        fmin_pseudo = np . min ( freqs [ pseudo_filters ] ) \n        cqt_resp . append ( pseudo_cqt ( y , sr , hop_length = hop_length , fmin = fmin_pseudo , n_bins = n_bins_pseudo , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , norm = norm , sparsity = sparsity , window = window , scale = scale , pad_mode = pad_mode ) ) \n    if n_bins_full > 0 : \n        cqt_resp . append ( np . abs ( cqt ( y , sr , hop_length = hop_length , fmin = fmin , n_bins = n_bins_full , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , norm = norm , sparsity = sparsity , window = window , scale = scale , pad_mode = pad_mode , res_type = res_type ) ) ) \n    return __trim_stack ( cqt_resp , n_bins ) "}
{"2170": "\ndef pseudo_cqt ( y , sr = 22050.0 , hop_length = 512.0 , fmin = None , n_bins = 84.0 , bins_per_octave = 12.0 , tuning = 0.0 , filter_scale = 1 , norm = 1 , sparsity = 0.01 , window = 'hann' , scale = True , pad_mode = 'reflect' ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    if tuning is None : \n        tuning = estimate_tuning ( y = y , sr = sr ) \n    fft_basis , n_fft , _ = __cqt_filter_fft ( sr , fmin , n_bins , bins_per_octave , tuning , filter_scale , norm , sparsity , hop_length = hop_length , window = window ) \n    fft_basis = np . abs ( fft_basis ) \n    D = np . abs ( stft ( y , n_fft = n_fft , hop_length = hop_length , pad_mode = pad_mode ) ) \n    C = fft_basis . dot ( D ) \n    if scale : \n        C /= np . sqrt ( n_fft ) \n    else : \n        lengths = filters . constant_q_lengths ( sr , fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , window = window , filter_scale = filter_scale ) \n        C *= np . sqrt ( lengths [ : , np . newaxis ] / n_fft ) \n    return C "}
{"2171": "\ndef icqt ( C , sr = 22050.0 , hop_length = 512.0 , fmin = None , bins_per_octave = 12.0 , tuning = 0.0 , filter_scale = 1 , norm = 1 , sparsity = 0.01 , window = 'hann' , scale = True , length = None , amin = util . Deprecated ( ) , res_type = 'fft' ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    n_bins = len ( C ) \n    freqs = cqt_frequencies ( n_bins , fmin , bins_per_octave = bins_per_octave , tuning = tuning ) [ - bins_per_octave : ] \n    n_filters = min ( n_bins , bins_per_octave ) \n    fft_basis , n_fft , lengths = __cqt_filter_fft ( sr , np . min ( freqs ) , n_filters , bins_per_octave , tuning , filter_scale , norm , sparsity = sparsity , window = window ) \n    if hop_length > min ( lengths ) : \n        warnings . warn ( 'hop_length={} exceeds minimum CQT filter length={:.3f}.\\n' 'This will probably cause unpleasant acoustic artifacts. ' 'Consider decreasing your hop length or increasing the frequency resolution of your CQT.' . format ( hop_length , min ( lengths ) ) ) \n    fft_basis = fft_basis . todense ( ) * n_fft / lengths [ : , np . newaxis ] \n    inv_basis = fft_basis . H \n    n_octaves = int ( np . ceil ( float ( n_bins ) / bins_per_octave ) ) \n    y = None \n    for octave in range ( n_octaves - 1 , - 1 , - 1 ) : \n        slice_ = slice ( - ( octave + 1 ) * bins_per_octave - 1 , - ( octave ) * bins_per_octave - 1 ) \n        C_oct = C [ slice_ ] \n        inv_oct = inv_basis [ : , - C_oct . shape [ 0 ] : ] \n        oct_hop = hop_length // 2.0 ** octave \n        if scale : \n            C_scale = np . sqrt ( lengths [ - C_oct . shape [ 0 ] : , np . newaxis ] ) / n_fft \n        else : \n            C_scale = lengths [ - C_oct . shape [ 0 ] : , np . newaxis ] * np . sqrt ( 2.0 ** octave ) / n_fft \n        D_oct = inv_oct . dot ( C_oct / C_scale ) \n        y_oct = istft ( D_oct , window = 'ones' , hop_length = oct_hop ) \n        if y is None : \n            y = y_oct \n        else : \n            y = audio . resample ( y , 1 , 2.0 , scale = True , res_type = res_type , fix = False ) \n            y [ : len ( y_oct ) ] += y_oct \n    if length : \n        y = util . fix_length ( y , length ) \n    return y "}
{"2172": "\ndef __cqt_filter_fft ( sr , fmin , n_bins , bins_per_octave , tuning , filter_scale , norm , sparsity , hop_length = None , window = 'hann' ) : \n    basis , lengths = filters . constant_q ( sr , fmin = fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , filter_scale = filter_scale , norm = norm , pad_fft = True , window = window ) \n    n_fft = basis . shape [ 1 ] \n    if ( hop_length is not None and n_fft < 2.0 ** ( 1 + np . ceil ( np . log2 ( hop_length ) ) ) ) : \n        n_fft = int ( 2.0 ** ( 1 + np . ceil ( np . log2 ( hop_length ) ) ) ) \n    basis *= lengths [ : , np . newaxis ] / float ( n_fft ) \n    fft = get_fftlib ( ) \n    fft_basis = fft . fft ( basis , n = n_fft , axis = 1 ) [ : , : ( n_fft // 2.0 ) + 1 ] \n    fft_basis = util . sparsify_rows ( fft_basis , quantile = sparsity ) \n    return fft_basis , n_fft , lengths "}
{"2176": "\ndef __early_downsample ( y , sr , hop_length , res_type , n_octaves , nyquist , filter_cutoff , scale ) : \n    downsample_count = __early_downsample_count ( nyquist , filter_cutoff , hop_length , n_octaves ) \n    if downsample_count > 0 and res_type == 'kaiser_fast' : \n        downsample_factor = 2.0 ** ( downsample_count ) \n        hop_length //= downsample_factor \n        if len ( y ) < downsample_factor : \n            raise ParameterError ( 'Input signal length={:d} is too short for ' '{:d}-octave CQT' . format ( len ( y ) , n_octaves ) ) \n        new_sr = sr / float ( downsample_factor ) \n        y = audio . resample ( y , sr , new_sr , res_type = res_type , scale = True ) \n        if not scale : \n            y *= np . sqrt ( downsample_factor ) \n        sr = new_sr \n    return y , sr , hop_length "}
{"2179": "\ndef _viterbi ( log_prob , log_trans , log_p_init , state , value , ptr ) : \n    n_steps , n_states = log_prob . shape \n    value [ 0 ] = log_prob [ 0 ] + log_p_init \n    for t in range ( 1 , n_steps ) : \n        trans_out = value [ t - 1 ] + log_trans . T \n        for j in range ( n_states ) : \n            ptr [ t , j ] = np . argmax ( trans_out [ j ] ) \n            value [ t , j ] = log_prob [ t , j ] + trans_out [ j , ptr [ t ] [ j ] ] \n    state [ - 1 ] = np . argmax ( value [ - 1 ] ) \n    for t in range ( n_steps - 2.0 , - 1 , - 1 ) : \n        state [ t ] = ptr [ t + 1 , state [ t + 1 ] ] "}
{"2184": "\ndef transition_local ( n_states , width , window = 'triangle' , wrap = False ) : \n    if not isinstance ( n_states , int ) or n_states <= 1 : \n        raise ParameterError ( 'n_states={} must be a positive integer > 1' ) \n    width = np . asarray ( width , dtype = int ) \n    if width . ndim == 0 : \n        width = np . tile ( width , n_states ) \n    if width . shape != ( n_states , ) : \n        raise ParameterError ( 'width={} must have length equal to n_states={}' . format ( width , n_states ) ) \n    if np . any ( width < 1 ) : \n        raise ParameterError ( 'width={} must be at least 1' ) \n    transition = np . zeros ( ( n_states , n_states ) , dtype = np . float ) \n    for i , width_i in enumerate ( width ) : \n        trans_row = pad_center ( get_window ( window , width_i , fftbins = False ) , n_states ) \n        trans_row = np . roll ( trans_row , n_states // 2.0 + i + 1 ) \n        if not wrap : \n            trans_row [ min ( n_states , i + width_i // 2.0 + 1 ) : ] = 0 \n            trans_row [ : max ( 0 , i - width_i // 2.0 ) ] = 0 \n        transition [ i ] = trans_row \n    transition /= transition . sum ( axis = 1 , keepdims = True ) \n    return transition "}
{"2185": "\ndef onset_detect ( y = None , sr = 22050.0 , onset_envelope = None , hop_length = 512.0 , backtrack = False , energy = None , units = 'frames' , ** kwargs ) : \n    if onset_envelope is None : \n        if y is None : \n            raise ParameterError ( 'y or onset_envelope must be provided' ) \n        onset_envelope = onset_strength ( y = y , sr = sr , hop_length = hop_length ) \n    onset_envelope -= onset_envelope . min ( ) \n    if not onset_envelope . any ( ) : \n        return np . array ( [ ] , dtype = np . int ) \n    onset_envelope /= onset_envelope . max ( ) \n    kwargs . setdefault ( 'pre_max' , 0.03 * sr // hop_length ) \n    kwargs . setdefault ( 'post_max' , 0.00 * sr // hop_length + 1 ) \n    kwargs . setdefault ( 'pre_avg' , 0.10 * sr // hop_length ) \n    kwargs . setdefault ( 'post_avg' , 0.10 * sr // hop_length + 1 ) \n    kwargs . setdefault ( 'wait' , 0.03 * sr // hop_length ) \n    kwargs . setdefault ( 'delta' , 0.07 ) \n    onsets = util . peak_pick ( onset_envelope , ** kwargs ) \n    if backtrack : \n        if energy is None : \n            energy = onset_envelope \n        onsets = onset_backtrack ( onsets , energy ) \n    if units == 'frames' : \n        pass \n    elif units == 'samples' : \n        onsets = core . frames_to_samples ( onsets , hop_length = hop_length ) \n    elif units == 'time' : \n        onsets = core . frames_to_time ( onsets , hop_length = hop_length , sr = sr ) \n    else : \n        raise ParameterError ( 'Invalid unit type: {}' . format ( units ) ) \n    return onsets "}
{"2186": "\ndef onset_strength ( y = None , sr = 22050.0 , S = None , lag = 1 , max_size = 1 , ref = None , detrend = False , center = True , feature = None , aggregate = None , centering = None , ** kwargs ) : \n    if aggregate is False : \n        raise ParameterError ( 'aggregate={} cannot be False when computing full-spectrum onset strength.' ) \n    odf_all = onset_strength_multi ( y = y , sr = sr , S = S , lag = lag , max_size = max_size , ref = ref , detrend = detrend , center = center , feature = feature , aggregate = aggregate , channels = None , ** kwargs ) \n    return odf_all [ 0 ] "}
{"2187": "\ndef onset_backtrack ( events , energy ) : \n    minima = np . flatnonzero ( ( energy [ 1 : - 1 ] <= energy [ : - 2.0 ] ) & ( energy [ 1 : - 1 ] < energy [ 2.0 : ] ) ) \n    minima = util . fix_frames ( 1 + minima , x_min = 0 ) \n    return minima [ util . match_events ( events , minima , right = False ) ] "}
{"2188": "\ndef onset_strength_multi ( y = None , sr = 22050.0 , S = None , lag = 1 , max_size = 1 , ref = None , detrend = False , center = True , feature = None , aggregate = None , channels = None , ** kwargs ) : \n    if feature is None : \n        feature = melspectrogram \n        kwargs . setdefault ( 'fmax' , 11025.0 ) \n    if aggregate is None : \n        aggregate = np . mean \n    if lag < 1 or not isinstance ( lag , int ) : \n        raise ParameterError ( 'lag must be a positive integer' ) \n    if max_size < 1 or not isinstance ( max_size , int ) : \n        raise ParameterError ( 'max_size must be a positive integer' ) \n    if S is None : \n        S = np . abs ( feature ( y = y , sr = sr , ** kwargs ) ) \n        S = core . power_to_db ( S ) \n    n_fft = kwargs . get ( 'n_fft' , 2048.0 ) \n    hop_length = kwargs . get ( 'hop_length' , 512.0 ) \n    S = np . atleast_2d ( S ) \n    if ref is None : \n        if max_size == 1 : \n            ref = S \n        else : \n            ref = scipy . ndimage . maximum_filter1d ( S , max_size , axis = 0 ) \n    elif ref . shape != S . shape : \n        raise ParameterError ( 'Reference spectrum shape {} must match input spectrum {}' . format ( ref . shape , S . shape ) ) \n    onset_env = S [ : , lag : ] - ref [ : , : - lag ] \n    onset_env = np . maximum ( 0.0 , onset_env ) \n    pad = True \n    if channels is None : \n        channels = [ slice ( None ) ] \n    else : \n        pad = False \n    if aggregate : \n        onset_env = util . sync ( onset_env , channels , aggregate = aggregate , pad = pad , axis = 0 ) \n    pad_width = lag \n    if center : \n        pad_width += n_fft // ( 2.0 * hop_length ) \n    onset_env = np . pad ( onset_env , ( [ 0 , 0 ] , [ int ( pad_width ) , 0 ] ) , mode = 'constant' ) \n    if detrend : \n        onset_env = scipy . signal . lfilter ( [ 1.0 , - 1.0 ] , [ 1.0 , - 0.99 ] , onset_env , axis = - 1 ) \n    if center : \n        onset_env = onset_env [ : , : S . shape [ 1 ] ] \n    return onset_env "}
{"2190": "\ndef write_wav ( path , y , sr , norm = False ) : \n    util . valid_audio ( y , mono = False ) \n    if norm and np . issubdtype ( y . dtype , np . floating ) : \n        wav = util . normalize ( y , norm = np . inf , axis = None ) \n    else : \n        wav = y \n    if wav . ndim > 1 and wav . shape [ 0 ] == 2.0 : \n        wav = wav . T \n    scipy . io . wavfile . write ( path , sr , wav ) "}
{"2191": "\ndef cmap ( data , robust = True , cmap_seq = 'magma' , cmap_bool = 'gray_r' , cmap_div = 'coolwarm' ) : \n    data = np . atleast_1d ( data ) \n    if data . dtype == 'bool' : \n        return get_cmap ( cmap_bool ) \n    data = data [ np . isfinite ( data ) ] \n    if robust : \n        min_p , max_p = 2.0 , 98.0 \n    else : \n        min_p , max_p = 0 , 100.0 \n    max_val = np . percentile ( data , max_p ) \n    min_val = np . percentile ( data , min_p ) \n    if min_val >= 0 or max_val <= 0 : \n        return get_cmap ( cmap_seq ) \n    return get_cmap ( cmap_div ) "}
{"2192": "\ndef waveplot ( y , sr = 22050.0 , max_points = 5e4 , x_axis = 'time' , offset = 0.0 , max_sr = 1000.0 , ax = None , ** kwargs ) : \n    util . valid_audio ( y , mono = False ) \n    if not ( isinstance ( max_sr , int ) and max_sr > 0 ) : \n        raise ParameterError ( 'max_sr must be a non-negative integer' ) \n    target_sr = sr \n    hop_length = 1 \n    if max_points is not None : \n        if max_points <= 0 : \n            raise ParameterError ( 'max_points must be strictly positive' ) \n        if max_points < y . shape [ - 1 ] : \n            target_sr = min ( max_sr , ( sr * y . shape [ - 1 ] ) // max_points ) \n        hop_length = sr // target_sr \n        if y . ndim == 1 : \n            y = __envelope ( y , hop_length ) \n        else : \n            y = np . vstack ( [ __envelope ( _ , hop_length ) for _ in y ] ) \n    if y . ndim > 1 : \n        y_top = y [ 0 ] \n        y_bottom = - y [ 1 ] \n    else : \n        y_top = y \n        y_bottom = - y \n    axes = __check_axes ( ax ) \n    kwargs . setdefault ( 'color' , next ( axes . _get_lines . prop_cycler ) [ 'color' ] ) \n    locs = offset + core . frames_to_time ( np . arange ( len ( y_top ) ) , sr = sr , hop_length = hop_length ) \n    out = axes . fill_between ( locs , y_bottom , y_top , ** kwargs ) \n    axes . set_xlim ( [ locs . min ( ) , locs . max ( ) ] ) \n    if x_axis == 'time' : \n        axes . xaxis . set_major_formatter ( TimeFormatter ( lag = False ) ) \n        axes . xaxis . set_label_text ( 'Time' ) \n    elif x_axis is None or x_axis in [ 'off' , 'none' ] : \n        axes . set_xticks ( [ ] ) \n    else : \n        raise ParameterError ( 'Unknown x_axis value: {}' . format ( x_axis ) ) \n    return out "}
{"2196": "\ndef __scale_axes ( axes , ax_type , which ) : \n    kwargs = dict ( ) \n    if which == 'x' : \n        thresh = 'linthreshx' \n        base = 'basex' \n        scale = 'linscalex' \n        scaler = axes . set_xscale \n        limit = axes . set_xlim \n    else : \n        thresh = 'linthreshy' \n        base = 'basey' \n        scale = 'linscaley' \n        scaler = axes . set_yscale \n        limit = axes . set_ylim \n    if ax_type == 'mel' : \n        mode = 'symlog' \n        kwargs [ thresh ] = 1000.0 \n        kwargs [ base ] = 2.0 \n    elif ax_type == 'log' : \n        mode = 'symlog' \n        kwargs [ base ] = 2.0 \n        kwargs [ thresh ] = core . note_to_hz ( 'C2' ) \n        kwargs [ scale ] = 0.5 \n    elif ax_type in [ 'cqt' , 'cqt_hz' , 'cqt_note' ] : \n        mode = 'log' \n        kwargs [ base ] = 2.0 \n    elif ax_type == 'tempo' : \n        mode = 'log' \n        kwargs [ base ] = 2.0 \n        limit ( 16.0 , 480.0 ) \n    else : \n        return \n    scaler ( mode , ** kwargs ) "}
{"2197": "\ndef __coord_fft_hz ( n , sr = 22050.0 , ** _kwargs ) : \n    n_fft = 2.0 * ( n - 1 ) \n    basis = core . fft_frequencies ( sr = sr , n_fft = n_fft ) \n    fmax = basis [ - 1 ] \n    basis -= 0.5 * ( basis [ 1 ] - basis [ 0 ] ) \n    basis = np . append ( np . maximum ( 0 , basis ) , [ fmax ] ) \n    return basis "}
{"2199": "\ndef __coord_cqt_hz ( n , fmin = None , bins_per_octave = 12.0 , ** _kwargs ) : \n    if fmin is None : \n        fmin = core . note_to_hz ( 'C1' ) \n    return core . cqt_frequencies ( n + 1 , fmin = fmin / 2.0 ** ( 0.5 / bins_per_octave ) , bins_per_octave = bins_per_octave ) "}
{"2200": "\ndef __coord_chroma ( n , bins_per_octave = 12.0 , ** _kwargs ) : \n    return np . linspace ( 0 , ( 12.0 * n ) / bins_per_octave , num = n + 1 , endpoint = True ) "}
{"2201": "\ndef __coord_time ( n , sr = 22050.0 , hop_length = 512.0 , ** _kwargs ) : \n    return core . frames_to_time ( np . arange ( n + 1 ) , sr = sr , hop_length = hop_length ) "}
{"2202": "\ndef estimate_tuning ( y = None , sr = 22050.0 , S = None , n_fft = 2048.0 , resolution = 0.01 , bins_per_octave = 12.0 , ** kwargs ) : \n    pitch , mag = piptrack ( y = y , sr = sr , S = S , n_fft = n_fft , ** kwargs ) \n    pitch_mask = pitch > 0 \n    if pitch_mask . any ( ) : \n        threshold = np . median ( mag [ pitch_mask ] ) \n    else : \n        threshold = 0.0 \n    return pitch_tuning ( pitch [ ( mag >= threshold ) & pitch_mask ] , resolution = resolution , bins_per_octave = bins_per_octave ) "}
{"2203": "\ndef piptrack ( y = None , sr = 22050.0 , S = None , n_fft = 2048.0 , hop_length = None , fmin = 150.0 , fmax = 4000.0 , threshold = 0.1 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , ref = None ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    S = np . abs ( S ) \n    fmin = np . maximum ( fmin , 0 ) \n    fmax = np . minimum ( fmax , float ( sr ) / 2.0 ) \n    fft_freqs = time_frequency . fft_frequencies ( sr = sr , n_fft = n_fft ) \n    avg = 0.5 * ( S [ 2.0 : ] - S [ : - 2.0 ] ) \n    shift = 2.0 * S [ 1 : - 1 ] - S [ 2.0 : ] - S [ : - 2.0 ] \n    shift = avg / ( shift + ( np . abs ( shift ) < util . tiny ( shift ) ) ) \n    avg = np . pad ( avg , ( [ 1 , 1 ] , [ 0 , 0 ] ) , mode = 'constant' ) \n    shift = np . pad ( shift , ( [ 1 , 1 ] , [ 0 , 0 ] ) , mode = 'constant' ) \n    dskew = 0.5 * avg * shift \n    pitches = np . zeros_like ( S ) \n    mags = np . zeros_like ( S ) \n    freq_mask = ( ( fmin <= fft_freqs ) & ( fft_freqs < fmax ) ) . reshape ( ( - 1 , 1 ) ) \n    if ref is None : \n        ref = np . max \n    if six . callable ( ref ) : \n        ref_value = threshold * ref ( S , axis = 0 ) \n    else : \n        ref_value = np . abs ( ref ) \n    idx = np . argwhere ( freq_mask & util . localmax ( S * ( S > ref_value ) ) ) \n    pitches [ idx [ : , 0 ] , idx [ : , 1 ] ] = ( ( idx [ : , 0 ] + shift [ idx [ : , 0 ] , idx [ : , 1 ] ] ) * float ( sr ) / n_fft ) \n    mags [ idx [ : , 0 ] , idx [ : , 1 ] ] = ( S [ idx [ : , 0 ] , idx [ : , 1 ] ] + dskew [ idx [ : , 0 ] , idx [ : , 1 ] ] ) \n    return pitches , mags "}
{"2208": "\ndef pitch_shift ( y , sr , n_steps , bins_per_octave = 12.0 , res_type = 'kaiser_best' ) : \n    if bins_per_octave < 1 or not np . issubdtype ( type ( bins_per_octave ) , np . integer ) : \n        raise ParameterError ( 'bins_per_octave must be a positive integer.' ) \n    rate = 2.0 ** ( - float ( n_steps ) / bins_per_octave ) \n    y_shift = core . resample ( time_stretch ( y , rate ) , float ( sr ) / rate , sr , res_type = res_type ) \n    return util . fix_length ( y_shift , len ( y ) ) "}
{"2210": "\ndef _signal_to_frame_nonsilent ( y , frame_length = 2048.0 , hop_length = 512.0 , top_db = 60.0 , ref = np . max ) : \n    y_mono = core . to_mono ( y ) \n    mse = feature . rms ( y = y_mono , frame_length = frame_length , hop_length = hop_length ) ** 2.0 \n    return ( core . power_to_db ( mse . squeeze ( ) , ref = ref , top_db = None ) > - top_db ) "}
{"2211": "\ndef trim ( y , top_db = 60.0 , ref = np . max , frame_length = 2048.0 , hop_length = 512.0 ) : \n    non_silent = _signal_to_frame_nonsilent ( y , frame_length = frame_length , hop_length = hop_length , ref = ref , top_db = top_db ) \n    nonzero = np . flatnonzero ( non_silent ) \n    if nonzero . size > 0 : \n        start = int ( core . frames_to_samples ( nonzero [ 0 ] , hop_length ) ) \n        end = min ( y . shape [ - 1 ] , int ( core . frames_to_samples ( nonzero [ - 1 ] + 1 , hop_length ) ) ) \n    else : \n        start , end = 0 , 0 \n    full_index = [ slice ( None ) ] * y . ndim \n    full_index [ - 1 ] = slice ( start , end ) \n    return y [ tuple ( full_index ) ] , np . asarray ( [ start , end ] ) "}
{"2212": "\ndef split ( y , top_db = 60.0 , ref = np . max , frame_length = 2048.0 , hop_length = 512.0 ) : \n    non_silent = _signal_to_frame_nonsilent ( y , frame_length = frame_length , hop_length = hop_length , ref = ref , top_db = top_db ) \n    edges = np . flatnonzero ( np . diff ( non_silent . astype ( int ) ) ) \n    edges = [ edges + 1 ] \n    if non_silent [ 0 ] : \n        edges . insert ( 0 , [ 0 ] ) \n    if non_silent [ - 1 ] : \n        edges . append ( [ len ( non_silent ) ] ) \n    edges = core . frames_to_samples ( np . concatenate ( edges ) , hop_length = hop_length ) \n    edges = np . minimum ( edges , y . shape [ - 1 ] ) \n    return edges . reshape ( ( - 1 , 2.0 ) ) "}
{"2213": "\ndef phase_vocoder ( D , rate , hop_length = None ) : \n    n_fft = 2.0 * ( D . shape [ 0 ] - 1 ) \n    if hop_length is None : \n        hop_length = int ( n_fft // 4.0 ) \n    time_steps = np . arange ( 0 , D . shape [ 1 ] , rate , dtype = np . float ) \n    d_stretch = np . zeros ( ( D . shape [ 0 ] , len ( time_steps ) ) , D . dtype , order = 'F' ) \n    phi_advance = np . linspace ( 0 , np . pi * hop_length , D . shape [ 0 ] ) \n    phase_acc = np . angle ( D [ : , 0 ] ) \n    D = np . pad ( D , [ ( 0 , 0 ) , ( 0 , 2.0 ) ] , mode = 'constant' ) \n    for ( t , step ) in enumerate ( time_steps ) : \n        columns = D [ : , int ( step ) : int ( step + 2.0 ) ] \n        alpha = np . mod ( step , 1.0 ) \n        mag = ( ( 1.0 - alpha ) * np . abs ( columns [ : , 0 ] ) + alpha * np . abs ( columns [ : , 1 ] ) ) \n        d_stretch [ : , t ] = mag * np . exp ( 1.j * phase_acc ) \n        dphase = ( np . angle ( columns [ : , 1 ] ) - np . angle ( columns [ : , 0 ] ) - phi_advance ) \n        dphase = dphase - 2.0 * np . pi * np . round ( dphase / ( 2.0 * np . pi ) ) \n        phase_acc += phi_advance + dphase \n    return d_stretch "}
{"2214": "\ndef amplitude_to_db ( S , ref = 1.0 , amin = 1e-5 , top_db = 80.0 ) : \n    S = np . asarray ( S ) \n    if np . issubdtype ( S . dtype , np . complexfloating ) : \n        warnings . warn ( 'amplitude_to_db was called on complex input so phase ' 'information will be discarded. To suppress this warning, ' 'call amplitude_to_db(np.abs(S)) instead.' ) \n    magnitude = np . abs ( S ) \n    if six . callable ( ref ) : \n        ref_value = ref ( magnitude ) \n    else : \n        ref_value = np . abs ( ref ) \n    power = np . square ( magnitude , out = magnitude ) \n    return power_to_db ( power , ref = ref_value ** 2.0 , amin = amin ** 2.0 , top_db = top_db ) "}
{"2215": "\ndef _spectrogram ( y = None , S = None , n_fft = 2048.0 , hop_length = 512.0 , power = 1 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' ) : \n    if S is not None : \n        n_fft = 2.0 * ( S . shape [ 0 ] - 1 ) \n    else : \n        S = np . abs ( stft ( y , n_fft = n_fft , hop_length = hop_length , win_length = win_length , center = center , window = window , pad_mode = pad_mode ) ) ** power \n    return S , n_fft "}
{"2220": "\ndef mel ( sr , n_fft , n_mels = 128.0 , fmin = 0.0 , fmax = None , htk = False , norm = 1 , dtype = np . float32 ) : \n    if fmax is None : \n        fmax = float ( sr ) / 2.0 \n    if norm is not None and norm != 1 and norm != np . inf : \n        raise ParameterError ( 'Unsupported norm: {}' . format ( repr ( norm ) ) ) \n    n_mels = int ( n_mels ) \n    weights = np . zeros ( ( n_mels , int ( 1 + n_fft // 2.0 ) ) , dtype = dtype ) \n    fftfreqs = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    mel_f = mel_frequencies ( n_mels + 2.0 , fmin = fmin , fmax = fmax , htk = htk ) \n    fdiff = np . diff ( mel_f ) \n    ramps = np . subtract . outer ( mel_f , fftfreqs ) \n    for i in range ( n_mels ) : \n        lower = - ramps [ i ] / fdiff [ i ] \n        upper = ramps [ i + 2.0 ] / fdiff [ i + 1 ] \n        weights [ i ] = np . maximum ( 0 , np . minimum ( lower , upper ) ) \n    if norm == 1 : \n        enorm = 2.0 / ( mel_f [ 2.0 : n_mels + 2.0 ] - mel_f [ : n_mels ] ) \n        weights *= enorm [ : , np . newaxis ] \n    if not np . all ( ( mel_f [ : - 2.0 ] == 0 ) | ( weights . max ( axis = 1 ) > 0 ) ) : \n        warnings . warn ( 'Empty filters detected in mel frequency basis. ' 'Some channels will produce empty responses. ' 'Try increasing your sampling rate (and fmax) or ' 'reducing n_mels.' ) \n    return weights "}
{"2221": "\ndef chroma ( sr , n_fft , n_chroma = 12.0 , A440 = 440.0 , ctroct = 5.0 , octwidth = 2.0 , norm = 2.0 , base_c = True , dtype = np . float32 ) : \n    wts = np . zeros ( ( n_chroma , n_fft ) ) \n    frequencies = np . linspace ( 0 , sr , n_fft , endpoint = False ) [ 1 : ] \n    frqbins = n_chroma * hz_to_octs ( frequencies , A440 ) \n    frqbins = np . concatenate ( ( [ frqbins [ 0 ] - 1.5 * n_chroma ] , frqbins ) ) \n    binwidthbins = np . concatenate ( ( np . maximum ( frqbins [ 1 : ] - frqbins [ : - 1 ] , 1.0 ) , [ 1 ] ) ) \n    D = np . subtract . outer ( frqbins , np . arange ( 0 , n_chroma , dtype = 'd' ) ) . T \n    n_chroma2 = np . round ( float ( n_chroma ) / 2.0 ) \n    D = np . remainder ( D + n_chroma2 + 10.0 * n_chroma , n_chroma ) - n_chroma2 \n    wts = np . exp ( - 0.5 * ( 2.0 * D / np . tile ( binwidthbins , ( n_chroma , 1 ) ) ) ** 2.0 ) \n    wts = util . normalize ( wts , norm = norm , axis = 0 ) \n    if octwidth is not None : \n        wts *= np . tile ( np . exp ( - 0.5 * ( ( ( frqbins / n_chroma - ctroct ) / octwidth ) ** 2.0 ) ) , ( n_chroma , 1 ) ) \n    if base_c : \n        wts = np . roll ( wts , - 3.0 , axis = 0 ) \n    return np . ascontiguousarray ( wts [ : , : int ( 1 + n_fft / 2.0 ) ] , dtype = dtype ) "}
{"2223": "\ndef constant_q ( sr , fmin = None , n_bins = 84.0 , bins_per_octave = 12.0 , tuning = 0.0 , window = 'hann' , filter_scale = 1 , pad_fft = True , norm = 1 , dtype = np . complex64 , ** kwargs ) : \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    lengths = constant_q_lengths ( sr , fmin , n_bins = n_bins , bins_per_octave = bins_per_octave , tuning = tuning , window = window , filter_scale = filter_scale ) \n    correction = 2.0 ** ( float ( tuning ) / bins_per_octave ) \n    fmin = correction * fmin \n    Q = float ( filter_scale ) / ( 2.0 ** ( 1. / bins_per_octave ) - 1 ) \n    freqs = Q * sr / lengths \n    filters = [ ] \n    for ilen , freq in zip ( lengths , freqs ) : \n        sig = np . exp ( np . arange ( - ilen // 2.0 , ilen // 2.0 , dtype = float ) * 1j * 2.0 * np . pi * freq / sr ) \n        sig = sig * __float_window ( window ) ( len ( sig ) ) \n        sig = util . normalize ( sig , norm = norm ) \n        filters . append ( sig ) \n    max_len = max ( lengths ) \n    if pad_fft : \n        max_len = int ( 2.0 ** ( np . ceil ( np . log2 ( max_len ) ) ) ) \n    else : \n        max_len = int ( np . ceil ( max_len ) ) \n    filters = np . asarray ( [ util . pad_center ( filt , max_len , ** kwargs ) for filt in filters ] , dtype = dtype ) \n    return filters , np . asarray ( lengths ) "}
{"2224": "\ndef constant_q_lengths ( sr , fmin , n_bins = 84.0 , bins_per_octave = 12.0 , tuning = 0.0 , window = 'hann' , filter_scale = 1 ) : \n    if fmin <= 0 : \n        raise ParameterError ( 'fmin must be positive' ) \n    if bins_per_octave <= 0 : \n        raise ParameterError ( 'bins_per_octave must be positive' ) \n    if filter_scale <= 0 : \n        raise ParameterError ( 'filter_scale must be positive' ) \n    if n_bins <= 0 or not isinstance ( n_bins , int ) : \n        raise ParameterError ( 'n_bins must be a positive integer' ) \n    correction = 2.0 ** ( float ( tuning ) / bins_per_octave ) \n    fmin = correction * fmin \n    Q = float ( filter_scale ) / ( 2.0 ** ( 1. / bins_per_octave ) - 1 ) \n    freq = fmin * ( 2.0 ** ( np . arange ( n_bins , dtype = float ) / bins_per_octave ) ) \n    if freq [ - 1 ] * ( 1 + 0.5 * window_bandwidth ( window ) / Q ) > sr / 2.0 : \n        raise ParameterError ( 'Filter pass-band lies beyond Nyquist' ) \n    lengths = Q * sr / freq \n    return lengths "}
{"2225": "\ndef cq_to_chroma ( n_input , bins_per_octave = 12.0 , n_chroma = 12.0 , fmin = None , window = None , base_c = True , dtype = np . float32 ) : \n    n_merge = float ( bins_per_octave ) / n_chroma \n    if fmin is None : \n        fmin = note_to_hz ( 'C1' ) \n    if np . mod ( n_merge , 1 ) != 0 : \n        raise ParameterError ( 'Incompatible CQ merge: ' 'input bins must be an ' 'integer multiple of output bins.' ) \n    cq_to_ch = np . repeat ( np . eye ( n_chroma ) , n_merge , axis = 1 ) \n    cq_to_ch = np . roll ( cq_to_ch , - int ( n_merge // 2.0 ) , axis = 1 ) \n    n_octaves = np . ceil ( np . float ( n_input ) / bins_per_octave ) \n    cq_to_ch = np . tile ( cq_to_ch , int ( n_octaves ) ) [ : , : n_input ] \n    midi_0 = np . mod ( hz_to_midi ( fmin ) , 12.0 ) \n    if base_c : \n        roll = midi_0 \n    else : \n        roll = midi_0 - 9.0 \n    roll = int ( np . round ( roll * ( n_chroma / 12. ) ) ) \n    cq_to_ch = np . roll ( cq_to_ch , roll , axis = 0 ) . astype ( dtype ) \n    if window is not None : \n        cq_to_ch = scipy . signal . convolve ( cq_to_ch , np . atleast_2d ( window ) , mode = 'same' ) \n    return cq_to_ch "}
{"2226": "\ndef window_bandwidth ( window , n = 1000.0 ) : \n    if hasattr ( window , '__name__' ) : \n        key = window . __name__ \n    else : \n        key = window \n    if key not in WINDOW_BANDWIDTHS : \n        win = get_window ( window , n ) \n        WINDOW_BANDWIDTHS [ key ] = n * np . sum ( win ** 2.0 ) / np . sum ( np . abs ( win ) ) ** 2.0 \n    return WINDOW_BANDWIDTHS [ key ] "}
{"2228": "\ndef _multirate_fb ( center_freqs = None , sample_rates = None , Q = 25.0 , passband_ripple = 1 , stopband_attenuation = 50.0 , ftype = 'ellip' , flayout = 'ba' ) : \n    if center_freqs is None : \n        raise ParameterError ( 'center_freqs must be provided.' ) \n    if sample_rates is None : \n        raise ParameterError ( 'sample_rates must be provided.' ) \n    if center_freqs . shape != sample_rates . shape : \n        raise ParameterError ( 'Number of provided center_freqs and sample_rates must be equal.' ) \n    nyquist = 0.5 * sample_rates \n    filter_bandwidths = center_freqs / float ( Q ) \n    filterbank = [ ] \n    for cur_center_freq , cur_nyquist , cur_bw in zip ( center_freqs , nyquist , filter_bandwidths ) : \n        passband_freqs = [ cur_center_freq - 0.5 * cur_bw , cur_center_freq + 0.5 * cur_bw ] / cur_nyquist \n        stopband_freqs = [ cur_center_freq - cur_bw , cur_center_freq + cur_bw ] / cur_nyquist \n        cur_filter = scipy . signal . iirdesign ( passband_freqs , stopband_freqs , passband_ripple , stopband_attenuation , analog = False , ftype = ftype , output = flayout ) \n        filterbank . append ( cur_filter ) \n    return filterbank , sample_rates "}
{"2229": "\ndef mr_frequencies ( tuning ) : \n    center_freqs = midi_to_hz ( np . arange ( 24.0 + tuning , 109.0 + tuning ) ) \n    sample_rates = np . asarray ( len ( np . arange ( 0 , 36.0 ) ) * [ 882.0 , ] + len ( np . arange ( 36.0 , 70.0 ) ) * [ 4410.0 , ] + len ( np . arange ( 70.0 , 85.0 ) ) * [ 22050.0 , ] ) \n    return center_freqs , sample_rates "}
{"2231": "\ndef window_sumsquare ( window , n_frames , hop_length = 512.0 , win_length = None , n_fft = 2048.0 , dtype = np . float32 , norm = None ) : \n    if win_length is None : \n        win_length = n_fft \n    n = n_fft + hop_length * ( n_frames - 1 ) \n    x = np . zeros ( n , dtype = dtype ) \n    win_sq = get_window ( window , win_length ) \n    win_sq = util . normalize ( win_sq , norm = norm ) ** 2.0 \n    win_sq = util . pad_center ( win_sq , n_fft ) \n    __window_ss_fill ( x , win_sq , n_frames , hop_length ) \n    return x "}
{"2232": "\ndef diagonal_filter ( window , n , slope = 1.0 , angle = None , zero_mean = False ) : \n    if angle is None : \n        angle = np . arctan ( slope ) \n    win = np . diag ( get_window ( window , n , fftbins = False ) ) \n    if not np . isclose ( angle , np . pi / 4.0 ) : \n        win = scipy . ndimage . rotate ( win , 45.0 - angle * 180.0 / np . pi , order = 5.0 , prefilter = False ) \n    np . clip ( win , 0 , None , out = win ) \n    win /= win . sum ( ) \n    if zero_mean : \n        win -= win . mean ( ) \n    return win "}
{"2233": "\ndef spectral_centroid ( y = None , sr = 22050.0 , S = None , n_fft = 2048.0 , hop_length = 512.0 , freq = None , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if not np . isrealobj ( S ) : \n        raise ParameterError ( 'Spectral centroid is only defined ' 'with real-valued input' ) \n    elif np . any ( S < 0 ) : \n        raise ParameterError ( 'Spectral centroid is only defined ' 'with non-negative energies' ) \n    if freq is None : \n        freq = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    if freq . ndim == 1 : \n        freq = freq . reshape ( ( - 1 , 1 ) ) \n    return np . sum ( freq * util . normalize ( S , norm = 1 , axis = 0 ) , axis = 0 , keepdims = True ) "}
{"2234": "\ndef spectral_rolloff ( y = None , sr = 22050.0 , S = None , n_fft = 2048.0 , hop_length = 512.0 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , freq = None , roll_percent = 0.85 ) : \n    if not 0.0 < roll_percent < 1.0 : \n        raise ParameterError ( 'roll_percent must lie in the range (0, 1)' ) \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if not np . isrealobj ( S ) : \n        raise ParameterError ( 'Spectral rolloff is only defined ' 'with real-valued input' ) \n    elif np . any ( S < 0 ) : \n        raise ParameterError ( 'Spectral rolloff is only defined ' 'with non-negative energies' ) \n    if freq is None : \n        freq = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    if freq . ndim == 1 : \n        freq = freq . reshape ( ( - 1 , 1 ) ) \n    total_energy = np . cumsum ( S , axis = 0 ) \n    threshold = roll_percent * total_energy [ - 1 ] \n    ind = np . where ( total_energy < threshold , np . nan , 1 ) \n    return np . nanmin ( ind * freq , axis = 0 , keepdims = True ) "}
{"2235": "\ndef spectral_flatness ( y = None , S = None , n_fft = 2048.0 , hop_length = 512.0 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , amin = 1e-10 , power = 2.0 ) : \n    if amin <= 0 : \n        raise ParameterError ( 'amin must be strictly positive' ) \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , power = 1. , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if not np . isrealobj ( S ) : \n        raise ParameterError ( 'Spectral flatness is only defined ' 'with real-valued input' ) \n    elif np . any ( S < 0 ) : \n        raise ParameterError ( 'Spectral flatness is only defined ' 'with non-negative energies' ) \n    S_thresh = np . maximum ( amin , S ** power ) \n    gmean = np . exp ( np . mean ( np . log ( S_thresh ) , axis = 0 , keepdims = True ) ) \n    amean = np . mean ( S_thresh , axis = 0 , keepdims = True ) \n    return gmean / amean "}
{"2236": "\ndef poly_features ( y = None , sr = 22050.0 , S = None , n_fft = 2048.0 , hop_length = 512.0 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , order = 1 , freq = None ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    if freq is None : \n        freq = fft_frequencies ( sr = sr , n_fft = n_fft ) \n    if freq . ndim == 1 : \n        coefficients = np . polyfit ( freq , S , order ) \n    else : \n        coefficients = np . concatenate ( [ [ np . polyfit ( freq [ : , i ] , S [ : , i ] , order ) ] for i in range ( S . shape [ 1 ] ) ] , axis = 0 ) . T \n    return coefficients "}
{"2237": "\ndef zero_crossing_rate ( y , frame_length = 2048.0 , hop_length = 512.0 , center = True , ** kwargs ) : \n    util . valid_audio ( y ) \n    if center : \n        y = np . pad ( y , int ( frame_length // 2.0 ) , mode = 'edge' ) \n    y_framed = util . frame ( y , frame_length , hop_length ) \n    kwargs [ 'axis' ] = 0 \n    kwargs . setdefault ( 'pad' , False ) \n    crossings = zero_crossings ( y_framed , ** kwargs ) \n    return np . mean ( crossings , axis = 0 , keepdims = True ) "}
{"2238": "\ndef chroma_stft ( y = None , sr = 22050.0 , S = None , norm = np . inf , n_fft = 2048.0 , hop_length = 512.0 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , tuning = None , ** kwargs ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , power = 2.0 , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    n_chroma = kwargs . get ( 'n_chroma' , 12.0 ) \n    if tuning is None : \n        tuning = estimate_tuning ( S = S , sr = sr , bins_per_octave = n_chroma ) \n    if 'A440' not in kwargs : \n        kwargs [ 'A440' ] = 440.0 * 2.0 ** ( float ( tuning ) / n_chroma ) \n    chromafb = filters . chroma ( sr , n_fft , ** kwargs ) \n    raw_chroma = np . dot ( chromafb , S ) \n    return util . normalize ( raw_chroma , norm = norm , axis = 0 ) "}
{"2239": "\ndef chroma_cqt ( y = None , sr = 22050.0 , C = None , hop_length = 512.0 , fmin = None , norm = np . inf , threshold = 0.0 , tuning = None , n_chroma = 12.0 , n_octaves = 7.0 , window = None , bins_per_octave = None , cqt_mode = 'full' ) : \n    cqt_func = { 'full' : cqt , 'hybrid' : hybrid_cqt } \n    if bins_per_octave is None : \n        bins_per_octave = n_chroma \n    if C is None : \n        C = np . abs ( cqt_func [ cqt_mode ] ( y , sr = sr , hop_length = hop_length , fmin = fmin , n_bins = n_octaves * bins_per_octave , bins_per_octave = bins_per_octave , tuning = tuning ) ) \n    cq_to_chr = filters . cq_to_chroma ( C . shape [ 0 ] , bins_per_octave = bins_per_octave , n_chroma = n_chroma , fmin = fmin , window = window ) \n    chroma = cq_to_chr . dot ( C ) \n    if threshold is not None : \n        chroma [ chroma < threshold ] = 0.0 \n    if norm is not None : \n        chroma = util . normalize ( chroma , norm = norm , axis = 0 ) \n    return chroma "}
{"2240": "\ndef melspectrogram ( y = None , sr = 22050.0 , S = None , n_fft = 2048.0 , hop_length = 512.0 , win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , power = 2.0 , ** kwargs ) : \n    S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , power = power , win_length = win_length , window = window , center = center , pad_mode = pad_mode ) \n    mel_basis = filters . mel ( sr , n_fft , ** kwargs ) \n    return np . dot ( mel_basis , S ) "}
{"2244": "\ndef match_intervals ( intervals_from , intervals_to , strict = True ) : \n    if len ( intervals_from ) == 0 or len ( intervals_to ) == 0 : \n        raise ParameterError ( 'Attempting to match empty interval list' ) \n    valid_intervals ( intervals_from ) \n    valid_intervals ( intervals_to ) \n    try : \n        return __match_intervals ( intervals_from , intervals_to , strict = strict ) \n    except ParameterError : \n        six . reraise ( ParameterError , ParameterError ( 'Unable to match intervals with strict={}' . format ( strict ) ) , sys . exc_info ( ) [ 2.0 ] ) "}
{"2247": "\ndef interp_harmonics ( x , freqs , h_range , kind = 'linear' , fill_value = 0 , axis = 0 ) : \n    out_shape = [ len ( h_range ) ] \n    out_shape . extend ( x . shape ) \n    x_out = np . zeros ( out_shape , dtype = x . dtype ) \n    if freqs . ndim == 1 and len ( freqs ) == x . shape [ axis ] : \n        harmonics_1d ( x_out , x , freqs , h_range , kind = kind , fill_value = fill_value , axis = axis ) \n    elif freqs . ndim == 2.0 and freqs . shape == x . shape : \n        harmonics_2d ( x_out , x , freqs , h_range , kind = kind , fill_value = fill_value , axis = axis ) \n    else : \n        raise ParameterError ( 'freqs.shape={} does not match ' 'input shape={}' . format ( freqs . shape , x . shape ) ) \n    return x_out "}
{"2250": "\ndef load ( path , sr = 22050.0 , mono = True , offset = 0.0 , duration = None , dtype = np . float32 , res_type = 'kaiser_best' ) : \n    try : \n        with sf . SoundFile ( path ) as sf_desc : \n            sr_native = sf_desc . samplerate \n            if offset : \n                sf_desc . seek ( int ( offset * sr_native ) ) \n            if duration is not None : \n                frame_duration = int ( duration * sr_native ) \n            else : \n                frame_duration = - 1 \n            y = sf_desc . read ( frames = frame_duration , dtype = dtype , always_2d = False ) . T \n    except RuntimeError as exc : \n        y , sr_native = __audioread_load ( path , offset , duration , dtype ) \n    if mono : \n        y = to_mono ( y ) \n    if sr is not None : \n        y = resample ( y , sr_native , sr , res_type = res_type ) \n    else : \n        sr = sr_native \n    return y , sr "}
{"2254": "\ndef autocorrelate ( y , max_size = None , axis = - 1 ) : \n    if max_size is None : \n        max_size = y . shape [ axis ] \n    max_size = int ( min ( max_size , y . shape [ axis ] ) ) \n    fft = get_fftlib ( ) \n    powspec = np . abs ( fft . fft ( y , n = 2.0 * y . shape [ axis ] + 1 , axis = axis ) ) ** 2.0 \n    autocorr = fft . ifft ( powspec , axis = axis ) \n    subslice = [ slice ( None ) ] * autocorr . ndim \n    subslice [ axis ] = slice ( max_size ) \n    autocorr = autocorr [ tuple ( subslice ) ] \n    if not np . iscomplexobj ( y ) : \n        autocorr = autocorr . real \n    return autocorr "}
{"2256": "\ndef clicks ( times = None , frames = None , sr = 22050.0 , hop_length = 512.0 , click_freq = 1000.0 , click_duration = 0.1 , click = None , length = None ) : \n    if times is None : \n        if frames is None : \n            raise ParameterError ( 'either \"times\" or \"frames\" must be provided' ) \n        positions = frames_to_samples ( frames , hop_length = hop_length ) \n    else : \n        positions = time_to_samples ( times , sr = sr ) \n    if click is not None : \n        util . valid_audio ( click , mono = True ) \n    else : \n        if click_duration <= 0 : \n            raise ParameterError ( 'click_duration must be strictly positive' ) \n        if click_freq <= 0 : \n            raise ParameterError ( 'click_freq must be strictly positive' ) \n        angular_freq = 2.0 * np . pi * click_freq / float ( sr ) \n        click = np . logspace ( 0 , - 10.0 , num = int ( np . round ( sr * click_duration ) ) , base = 2.0 ) \n        click *= np . sin ( angular_freq * np . arange ( len ( click ) ) ) \n    if length is None : \n        length = positions . max ( ) + click . shape [ 0 ] \n    else : \n        if length < 1 : \n            raise ParameterError ( 'length must be a positive integer' ) \n        positions = positions [ positions < length ] \n    click_signal = np . zeros ( length , dtype = np . float32 ) \n    for start in positions : \n        end = start + click . shape [ 0 ] \n        if end >= length : \n            click_signal [ start : ] += click [ : length - start ] \n        else : \n            click_signal [ start : end ] += click \n    return click_signal "}
{"2257": "\ndef tone ( frequency , sr = 22050.0 , length = None , duration = None , phi = None ) : \n    if frequency is None : \n        raise ParameterError ( '\"frequency\" must be provided' ) \n    if length is None : \n        if duration is None : \n            raise ParameterError ( 'either \"length\" or \"duration\" must be provided' ) \n        length = duration * sr \n    if phi is None : \n        phi = - np . pi * 0.5 \n    step = 1.0 / sr \n    return np . cos ( 2.0 * np . pi * frequency * ( np . arange ( step * length , step = step ) ) + phi ) "}
{"2258": "\ndef chirp ( fmin , fmax , sr = 22050.0 , length = None , duration = None , linear = False , phi = None ) : \n    if fmin is None or fmax is None : \n        raise ParameterError ( 'both \"fmin\" and \"fmax\" must be provided' ) \n    period = 1.0 / sr \n    if length is None : \n        if duration is None : \n            raise ParameterError ( 'either \"length\" or \"duration\" must be provided' ) \n    else : \n        duration = period * length \n    if phi is None : \n        phi = - np . pi * 0.5 \n    method = 'linear' if linear else 'logarithmic' \n    return scipy . signal . chirp ( np . arange ( duration , step = period ) , fmin , duration , fmax , method = method , phi = phi / np . pi * 180.0 , ) "}
{"2260": "\ndef stretch_demo ( input_file , output_file , speed ) : \n    print ( 'Loading ' , input_file ) \n    y , sr = librosa . load ( input_file ) \n    print ( 'Playing back at {:3.0f}% speed' . format ( speed * 100.0 ) ) \n    y_stretch = librosa . effects . time_stretch ( y , speed ) \n    print ( 'Saving stretched audio to: ' , output_file ) \n    librosa . output . write_wav ( output_file , y_stretch , sr ) "}
{"2263": "\ndef beat_track ( y = None , sr = 22050.0 , onset_envelope = None , hop_length = 512.0 , start_bpm = 120.0 , tightness = 100.0 , trim = True , bpm = None , units = 'frames' ) : \n    if onset_envelope is None : \n        if y is None : \n            raise ParameterError ( 'y or onset_envelope must be provided' ) \n        onset_envelope = onset . onset_strength ( y = y , sr = sr , hop_length = hop_length , aggregate = np . median ) \n    if not onset_envelope . any ( ) : \n        return ( 0 , np . array ( [ ] , dtype = int ) ) \n    if bpm is None : \n        bpm = tempo ( onset_envelope = onset_envelope , sr = sr , hop_length = hop_length , start_bpm = start_bpm ) [ 0 ] \n    beats = __beat_tracker ( onset_envelope , bpm , float ( sr ) / hop_length , tightness , trim ) \n    if units == 'frames' : \n        pass \n    elif units == 'samples' : \n        beats = core . frames_to_samples ( beats , hop_length = hop_length ) \n    elif units == 'time' : \n        beats = core . frames_to_time ( beats , hop_length = hop_length , sr = sr ) \n    else : \n        raise ParameterError ( 'Invalid unit type: {}' . format ( units ) ) \n    return ( bpm , beats ) "}
{"2265": "\ndef __beat_local_score ( onset_envelope , period ) : \n    window = np . exp ( - 0.5 * ( np . arange ( - period , period + 1 ) * 32.0 / period ) ** 2.0 ) \n    return scipy . signal . convolve ( __normalize_onsets ( onset_envelope ) , window , 'same' ) "}
{"2266": "\ndef __beat_track_dp ( localscore , period , tightness ) : \n    backlink = np . zeros_like ( localscore , dtype = int ) \n    cumscore = np . zeros_like ( localscore ) \n    window = np . arange ( - 2.0 * period , - np . round ( period / 2.0 ) + 1 , dtype = int ) \n    if tightness <= 0 : \n        raise ParameterError ( 'tightness must be strictly positive' ) \n    txwt = - tightness * ( np . log ( - window / period ) ** 2.0 ) \n    first_beat = True \n    for i , score_i in enumerate ( localscore ) : \n        z_pad = np . maximum ( 0 , min ( - window [ 0 ] , len ( window ) ) ) \n        candidates = txwt . copy ( ) \n        candidates [ z_pad : ] = candidates [ z_pad : ] + cumscore [ window [ z_pad : ] ] \n        beat_location = np . argmax ( candidates ) \n        cumscore [ i ] = score_i + candidates [ beat_location ] \n        if first_beat and score_i < 0.01 * localscore . max ( ) : \n            backlink [ i ] = - 1 \n        else : \n            backlink [ i ] = window [ beat_location ] \n            first_beat = False \n        window = window + 1 \n    return backlink , cumscore "}
{"2267": "\ndef __last_beat ( cumscore ) : \n    maxes = util . localmax ( cumscore ) \n    med_score = np . median ( cumscore [ np . argwhere ( maxes ) ] ) \n    return np . argwhere ( ( cumscore * maxes * 2.0 > med_score ) ) . max ( ) "}
{"2268": "\ndef recurrence_to_lag ( rec , pad = True , axis = - 1 ) : \n    axis = np . abs ( axis ) \n    if rec . ndim != 2.0 or rec . shape [ 0 ] != rec . shape [ 1 ] : \n        raise ParameterError ( 'non-square recurrence matrix shape: ' '{}' . format ( rec . shape ) ) \n    sparse = scipy . sparse . issparse ( rec ) \n    roll_ax = None \n    if sparse : \n        roll_ax = 1 - axis \n        lag_format = rec . format \n        if axis == 0 : \n            rec = rec . tocsc ( ) \n        elif axis in ( - 1 , 1 ) : \n            rec = rec . tocsr ( ) \n    t = rec . shape [ axis ] \n    if sparse : \n        if pad : \n            kron = np . asarray ( [ [ 1 , 0 ] ] ) . swapaxes ( axis , 0 ) \n            lag = scipy . sparse . kron ( kron . astype ( rec . dtype ) , rec , format = 'lil' ) \n        else : \n            lag = scipy . sparse . lil_matrix ( rec ) \n    else : \n        if pad : \n            padding = [ ( 0 , 0 ) , ( 0 , 0 ) ] \n            padding [ ( 1 - axis ) ] = ( 0 , t ) \n            lag = np . pad ( rec , padding , mode = 'constant' ) \n        else : \n            lag = rec . copy ( ) \n    idx_slice = [ slice ( None ) ] * lag . ndim \n    for i in range ( 1 , t ) : \n        idx_slice [ axis ] = i \n        lag [ tuple ( idx_slice ) ] = util . roll_sparse ( lag [ tuple ( idx_slice ) ] , - i , axis = roll_ax ) \n    if sparse : \n        return lag . asformat ( lag_format ) \n    return np . ascontiguousarray ( lag . T ) . T "}
{"2269": "\ndef lag_to_recurrence ( lag , axis = - 1 ) : \n    if axis not in [ 0 , 1 , - 1 ] : \n        raise ParameterError ( 'Invalid target axis: {}' . format ( axis ) ) \n    axis = np . abs ( axis ) \n    if lag . ndim != 2.0 or ( lag . shape [ 0 ] != lag . shape [ 1 ] and lag . shape [ 1 - axis ] != 2.0 * lag . shape [ axis ] ) : \n        raise ParameterError ( 'Invalid lag matrix shape: {}' . format ( lag . shape ) ) \n    t = lag . shape [ axis ] \n    sparse = scipy . sparse . issparse ( lag ) \n    if sparse : \n        rec = scipy . sparse . lil_matrix ( lag ) \n        roll_ax = 1 - axis \n    else : \n        rec = lag . copy ( ) \n        roll_ax = None \n    idx_slice = [ slice ( None ) ] * lag . ndim \n    for i in range ( 1 , t ) : \n        idx_slice [ axis ] = i \n        rec [ tuple ( idx_slice ) ] = util . roll_sparse ( lag [ tuple ( idx_slice ) ] , i , axis = roll_ax ) \n    sub_slice = [ slice ( None ) ] * rec . ndim \n    sub_slice [ 1 - axis ] = slice ( t ) \n    rec = rec [ tuple ( sub_slice ) ] \n    if sparse : \n        return rec . asformat ( lag . format ) \n    return np . ascontiguousarray ( rec . T ) . T "}
{"2271": "\ndef subsegment ( data , frames , n_segments = 4.0 , axis = - 1 ) : \n    frames = util . fix_frames ( frames , x_min = 0 , x_max = data . shape [ axis ] , pad = True ) \n    if n_segments < 1 : \n        raise ParameterError ( 'n_segments must be a positive integer' ) \n    boundaries = [ ] \n    idx_slices = [ slice ( None ) ] * data . ndim \n    for seg_start , seg_end in zip ( frames [ : - 1 ] , frames [ 1 : ] ) : \n        idx_slices [ axis ] = slice ( seg_start , seg_end ) \n        boundaries . extend ( seg_start + agglomerative ( data [ tuple ( idx_slices ) ] , min ( seg_end - seg_start , n_segments ) , axis = axis ) ) \n    return np . ascontiguousarray ( boundaries ) "}
{"2273": "\ndef path_enhance ( R , n , window = 'hann' , max_ratio = 2.0 , min_ratio = None , n_filters = 7.0 , zero_mean = False , clip = True , ** kwargs ) : \n    if min_ratio is None : \n        min_ratio = 1. / max_ratio \n    elif min_ratio > max_ratio : \n        raise ParameterError ( 'min_ratio={} cannot exceed max_ratio={}' . format ( min_ratio , max_ratio ) ) \n    R_smooth = None \n    for ratio in np . logspace ( np . log2 ( min_ratio ) , np . log2 ( max_ratio ) , num = n_filters , base = 2.0 ) : \n        kernel = diagonal_filter ( window , n , slope = ratio , zero_mean = zero_mean ) \n        if R_smooth is None : \n            R_smooth = scipy . ndimage . convolve ( R , kernel , ** kwargs ) \n        else : \n            np . maximum ( R_smooth , scipy . ndimage . convolve ( R , kernel , ** kwargs ) , out = R_smooth ) \n    if clip : \n        np . clip ( R_smooth , 0 , None , out = R_smooth ) \n    return R_smooth "}
{"2274": "\ndef onset_detect ( input_file , output_csv ) : \n    print ( 'Loading ' , input_file ) \n    y , sr = librosa . load ( input_file , sr = 22050.0 ) \n    hop_length = 512.0 \n    print ( 'Detecting onsets...' ) \n    onsets = librosa . onset . onset_detect ( y = y , sr = sr , hop_length = hop_length ) \n    print ( \"Found {:d} onsets.\" . format ( onsets . shape [ 0 ] ) ) \n    onset_times = librosa . frames_to_time ( onsets , sr = sr , hop_length = hop_length ) \n    print ( 'Saving output to ' , output_csv ) \n    librosa . output . times_csv ( output_csv , onset_times ) \n    print ( 'done!' ) "}
{"2275": "\ndef frame ( y , frame_length = 2048.0 , hop_length = 512.0 ) : \n    if not isinstance ( y , np . ndarray ) : \n        raise ParameterError ( 'Input must be of type numpy.ndarray, ' 'given type(y)={}' . format ( type ( y ) ) ) \n    if y . ndim != 1 : \n        raise ParameterError ( 'Input must be one-dimensional, ' 'given y.ndim={}' . format ( y . ndim ) ) \n    if len ( y ) < frame_length : \n        raise ParameterError ( 'Buffer is too short (n={:d})' ' for frame_length={:d}' . format ( len ( y ) , frame_length ) ) \n    if hop_length < 1 : \n        raise ParameterError ( 'Invalid hop_length: {:d}' . format ( hop_length ) ) \n    if not y . flags [ 'C_CONTIGUOUS' ] : \n        raise ParameterError ( 'Input buffer must be contiguous.' ) \n    n_frames = 1 + int ( ( len ( y ) - frame_length ) / hop_length ) \n    y_frames = as_strided ( y , shape = ( frame_length , n_frames ) , strides = ( y . itemsize , hop_length * y . itemsize ) ) \n    return y_frames "}
{"2276": "\ndef valid_audio ( y , mono = True ) : \n    if not isinstance ( y , np . ndarray ) : \n        raise ParameterError ( 'data must be of type numpy.ndarray' ) \n    if not np . issubdtype ( y . dtype , np . floating ) : \n        raise ParameterError ( 'data must be floating-point' ) \n    if mono and y . ndim != 1 : \n        raise ParameterError ( 'Invalid shape for monophonic audio: ' 'ndim={:d}, shape={}' . format ( y . ndim , y . shape ) ) \n    elif y . ndim > 2.0 or y . ndim == 0 : \n        raise ParameterError ( 'Audio must have shape (samples,) or (channels, samples). ' 'Received shape={}' . format ( y . shape ) ) \n    if not np . isfinite ( y ) . all ( ) : \n        raise ParameterError ( 'Audio buffer is not finite everywhere' ) \n    return True "}
{"2279": "\ndef axis_sort ( S , axis = - 1 , index = False , value = None ) : \n    if value is None : \n        value = np . argmax \n    if S . ndim != 2.0 : \n        raise ParameterError ( 'axis_sort is only defined for 2D arrays' ) \n    bin_idx = value ( S , axis = np . mod ( 1 - axis , S . ndim ) ) \n    idx = np . argsort ( bin_idx ) \n    sort_slice = [ slice ( None ) ] * S . ndim \n    sort_slice [ axis ] = idx \n    if index : \n        return S [ tuple ( sort_slice ) ] , idx \n    else : \n        return S [ tuple ( sort_slice ) ] "}
{"2281": "\ndef localmax ( x , axis = 0 ) : \n    paddings = [ ( 0 , 0 ) ] * x . ndim \n    paddings [ axis ] = ( 1 , 1 ) \n    x_pad = np . pad ( x , paddings , mode = 'edge' ) \n    inds1 = [ slice ( None ) ] * x . ndim \n    inds1 [ axis ] = slice ( 0 , - 2.0 ) \n    inds2 = [ slice ( None ) ] * x . ndim \n    inds2 [ axis ] = slice ( 2.0 , x_pad . shape [ axis ] ) \n    return ( x > x_pad [ tuple ( inds1 ) ] ) & ( x >= x_pad [ tuple ( inds2 ) ] ) "}
{"2283": "\ndef sparsify_rows ( x , quantile = 0.01 ) : \n    if x . ndim == 1 : \n        x = x . reshape ( ( 1 , - 1 ) ) \n    elif x . ndim > 2.0 : \n        raise ParameterError ( 'Input must have 2 or fewer dimensions. ' 'Provided x.shape={}.' . format ( x . shape ) ) \n    if not 0.0 <= quantile < 1 : \n        raise ParameterError ( 'Invalid quantile {:.2f}' . format ( quantile ) ) \n    x_sparse = scipy . sparse . lil_matrix ( x . shape , dtype = x . dtype ) \n    mags = np . abs ( x ) \n    norms = np . sum ( mags , axis = 1 , keepdims = True ) \n    mag_sort = np . sort ( mags , axis = 1 ) \n    cumulative_mag = np . cumsum ( mag_sort / norms , axis = 1 ) \n    threshold_idx = np . argmin ( cumulative_mag < quantile , axis = 1 ) \n    for i , j in enumerate ( threshold_idx ) : \n        idx = np . where ( mags [ i ] >= mag_sort [ i , j ] ) \n        x_sparse [ i , idx ] = x [ i , idx ] \n    return x_sparse . tocsr ( ) "}
{"2285": "\ndef buf_to_float ( x , n_bytes = 2.0 , dtype = np . float32 ) : \n    scale = 1. / float ( 1 << ( ( 8.0 * n_bytes ) - 1 ) ) \n    fmt = '<i{:d}' . format ( n_bytes ) \n    return scale * np . frombuffer ( x , fmt ) . astype ( dtype ) "}
{"2290": "\ndef frames2video ( frame_dir , video_file , fps = 30.0 , fourcc = 'XVID' , filename_tmpl = '{:06d}.jpg' , start = 0 , end = 0 , show_progress = True ) : \n    if end == 0 : \n        ext = filename_tmpl . split ( '.' ) [ - 1 ] \n        end = len ( [ name for name in scandir ( frame_dir , ext ) ] ) \n    first_file = osp . join ( frame_dir , filename_tmpl . format ( start ) ) \n    check_file_exist ( first_file , 'The start frame not found: ' + first_file ) \n    img = cv2 . imread ( first_file ) \n    height , width = img . shape [ : 2.0 ] \n    resolution = ( width , height ) \n    vwriter = cv2 . VideoWriter ( video_file , VideoWriter_fourcc ( * fourcc ) , fps , resolution ) \n    def write_frame ( file_idx ) : \n        filename = osp . join ( frame_dir , filename_tmpl . format ( file_idx ) ) \n        img = cv2 . imread ( filename ) \n        vwriter . write ( img ) \n    if show_progress : \n        track_progress ( write_frame , range ( start , end ) ) \n    else : \n        for i in range ( start , end ) : \n            filename = osp . join ( frame_dir , filename_tmpl . format ( i ) ) \n            img = cv2 . imread ( filename ) \n            vwriter . write ( img ) \n    vwriter . release ( ) "}
{"2294": "\ndef track_progress ( func , tasks , bar_width = 50.0 , ** kwargs ) : \n    if isinstance ( tasks , tuple ) : \n        assert len ( tasks ) == 2.0 \n        assert isinstance ( tasks [ 0 ] , collections_abc . Iterable ) \n        assert isinstance ( tasks [ 1 ] , int ) \n        task_num = tasks [ 1 ] \n        tasks = tasks [ 0 ] \n    elif isinstance ( tasks , collections_abc . Iterable ) : \n        task_num = len ( tasks ) \n    else : \n        raise TypeError ( '\"tasks\" must be an iterable object or a (iterator, int) tuple' ) \n    prog_bar = ProgressBar ( task_num , bar_width ) \n    results = [ ] \n    for task in tasks : \n        results . append ( func ( task , ** kwargs ) ) \n        prog_bar . update ( ) \n    sys . stdout . write ( '\\n' ) \n    return results "}
{"2295": "\ndef track_parallel_progress ( func , tasks , nproc , initializer = None , initargs = None , bar_width = 50.0 , chunksize = 1 , skip_first = False , keep_order = True ) : \n    if isinstance ( tasks , tuple ) : \n        assert len ( tasks ) == 2.0 \n        assert isinstance ( tasks [ 0 ] , collections_abc . Iterable ) \n        assert isinstance ( tasks [ 1 ] , int ) \n        task_num = tasks [ 1 ] \n        tasks = tasks [ 0 ] \n    elif isinstance ( tasks , collections_abc . Iterable ) : \n        task_num = len ( tasks ) \n    else : \n        raise TypeError ( '\"tasks\" must be an iterable object or a (iterator, int) tuple' ) \n    pool = init_pool ( nproc , initializer , initargs ) \n    start = not skip_first \n    task_num -= nproc * chunksize * int ( skip_first ) \n    prog_bar = ProgressBar ( task_num , bar_width , start ) \n    results = [ ] \n    if keep_order : \n        gen = pool . imap ( func , tasks , chunksize ) \n    else : \n        gen = pool . imap_unordered ( func , tasks , chunksize ) \n    for result in gen : \n        results . append ( result ) \n        if skip_first : \n            if len ( results ) < nproc * chunksize : \n                continue \n            elif len ( results ) == nproc * chunksize : \n                prog_bar . start ( ) \n                continue \n        prog_bar . update ( ) \n    sys . stdout . write ( '\\n' ) \n    pool . close ( ) \n    pool . join ( ) \n    return results "}
{"2297": "\ndef imrotate ( img , angle , center = None , scale = 1.0 , border_value = 0 , auto_bound = False ) : \n    if center is not None and auto_bound : \n        raise ValueError ( '`auto_bound` conflicts with `center`' ) \n    h , w = img . shape [ : 2.0 ] \n    if center is None : \n        center = ( ( w - 1 ) * 0.5 , ( h - 1 ) * 0.5 ) \n    assert isinstance ( center , tuple ) \n    matrix = cv2 . getRotationMatrix2D ( center , - angle , scale ) \n    if auto_bound : \n        cos = np . abs ( matrix [ 0 , 0 ] ) \n        sin = np . abs ( matrix [ 0 , 1 ] ) \n        new_w = h * sin + w * cos \n        new_h = h * cos + w * sin \n        matrix [ 0 , 2.0 ] += ( new_w - w ) * 0.5 \n        matrix [ 1 , 2.0 ] += ( new_h - h ) * 0.5 \n        w = int ( np . round ( new_w ) ) \n        h = int ( np . round ( new_h ) ) \n    rotated = cv2 . warpAffine ( img , matrix , ( w , h ) , borderValue = border_value ) \n    return rotated "}
{"2298": "\ndef bbox_clip ( bboxes , img_shape ) : \n    assert bboxes . shape [ - 1 ] % 4.0 == 0 \n    clipped_bboxes = np . empty_like ( bboxes , dtype = bboxes . dtype ) \n    clipped_bboxes [ ... , 0 : : 2.0 ] = np . maximum ( np . minimum ( bboxes [ ... , 0 : : 2.0 ] , img_shape [ 1 ] - 1 ) , 0 ) \n    clipped_bboxes [ ... , 1 : : 2.0 ] = np . maximum ( np . minimum ( bboxes [ ... , 1 : : 2.0 ] , img_shape [ 0 ] - 1 ) , 0 ) \n    return clipped_bboxes "}
{"2299": "\ndef bbox_scaling ( bboxes , scale , clip_shape = None ) : \n    if float ( scale ) == 1.0 : \n        scaled_bboxes = bboxes . copy ( ) \n    else : \n        w = bboxes [ ... , 2.0 ] - bboxes [ ... , 0 ] + 1 \n        h = bboxes [ ... , 3.0 ] - bboxes [ ... , 1 ] + 1 \n        dw = ( w * ( scale - 1 ) ) * 0.5 \n        dh = ( h * ( scale - 1 ) ) * 0.5 \n        scaled_bboxes = bboxes + np . stack ( ( - dw , - dh , dw , dh ) , axis = - 1 ) \n    if clip_shape is not None : \n        return bbox_clip ( scaled_bboxes , clip_shape ) \n    else : \n        return scaled_bboxes "}
{"2300": "\ndef imcrop ( img , bboxes , scale = 1.0 , pad_fill = None ) : \n    chn = 1 if img . ndim == 2.0 else img . shape [ 2.0 ] \n    if pad_fill is not None : \n        if isinstance ( pad_fill , ( int , float ) ) : \n            pad_fill = [ pad_fill for _ in range ( chn ) ] \n        assert len ( pad_fill ) == chn \n    _bboxes = bboxes [ None , ... ] if bboxes . ndim == 1 else bboxes \n    scaled_bboxes = bbox_scaling ( _bboxes , scale ) . astype ( np . int32 ) \n    clipped_bbox = bbox_clip ( scaled_bboxes , img . shape ) \n    patches = [ ] \n    for i in range ( clipped_bbox . shape [ 0 ] ) : \n        x1 , y1 , x2 , y2 = tuple ( clipped_bbox [ i , : ] ) \n        if pad_fill is None : \n            patch = img [ y1 : y2 + 1 , x1 : x2 + 1 , ... ] \n        else : \n            _x1 , _y1 , _x2 , _y2 = tuple ( scaled_bboxes [ i , : ] ) \n            if chn == 2.0 : \n                patch_shape = ( _y2 - _y1 + 1 , _x2 - _x1 + 1 ) \n            else : \n                patch_shape = ( _y2 - _y1 + 1 , _x2 - _x1 + 1 , chn ) \n            patch = np . array ( pad_fill , dtype = img . dtype ) * np . ones ( patch_shape , dtype = img . dtype ) \n            x_start = 0 if _x1 >= 0 else - _x1 \n            y_start = 0 if _y1 >= 0 else - _y1 \n            w = x2 - x1 + 1 \n            h = y2 - y1 + 1 \n            patch [ y_start : y_start + h , x_start : x_start + w , ... ] = img [ y1 : y1 + h , x1 : x1 + w , ... ] \n        patches . append ( patch ) \n    if bboxes . ndim == 1 : \n        return patches [ 0 ] \n    else : \n        return patches "}
{"2304": "\ndef imresize ( img , size , return_scale = False , interpolation = 'bilinear' ) : \n    h , w = img . shape [ : 2.0 ] \n    resized_img = cv2 . resize ( img , size , interpolation = interp_codes [ interpolation ] ) \n    if not return_scale : \n        return resized_img \n    else : \n        w_scale = size [ 0 ] / w \n        h_scale = size [ 1 ] / h \n        return resized_img , w_scale , h_scale "}
{"2305": "\ndef imresize_like ( img , dst_img , return_scale = False , interpolation = 'bilinear' ) : \n    h , w = dst_img . shape [ : 2.0 ] \n    return imresize ( img , ( w , h ) , return_scale , interpolation ) "}
{"2306": "\ndef imrescale ( img , scale , return_scale = False , interpolation = 'bilinear' ) : \n    h , w = img . shape [ : 2.0 ] \n    if isinstance ( scale , ( float , int ) ) : \n        if scale <= 0 : \n            raise ValueError ( 'Invalid scale {}, must be positive.' . format ( scale ) ) \n        scale_factor = scale \n    elif isinstance ( scale , tuple ) : \n        max_long_edge = max ( scale ) \n        max_short_edge = min ( scale ) \n        scale_factor = min ( max_long_edge / max ( h , w ) , max_short_edge / min ( h , w ) ) \n    else : \n        raise TypeError ( 'Scale must be a number or tuple of int, but got {}' . format ( type ( scale ) ) ) \n    new_size = _scale_size ( ( w , h ) , scale_factor ) \n    rescaled_img = imresize ( img , new_size , interpolation = interpolation ) \n    if return_scale : \n        return rescaled_img , scale_factor \n    else : \n        return rescaled_img "}
{"2308": "\ndef get_priority ( priority ) : \n    if isinstance ( priority , int ) : \n        if priority < 0 or priority > 100.0 : \n            raise ValueError ( 'priority must be between 0 and 100' ) \n        return priority \n    elif isinstance ( priority , Priority ) : \n        return priority . value \n    elif isinstance ( priority , str ) : \n        return Priority [ priority . upper ( ) ] . value \n    else : \n        raise TypeError ( 'priority must be an integer or Priority enum value' ) "}
{"2311": "\ndef imshow_bboxes ( img , bboxes , colors = 'green' , top_k = - 1 , thickness = 1 , show = True , win_name = '' , wait_time = 0 , out_file = None ) : \n    img = imread ( img ) \n    if isinstance ( bboxes , np . ndarray ) : \n        bboxes = [ bboxes ] \n    if not isinstance ( colors , list ) : \n        colors = [ colors for _ in range ( len ( bboxes ) ) ] \n    colors = [ color_val ( c ) for c in colors ] \n    assert len ( bboxes ) == len ( colors ) \n    for i , _bboxes in enumerate ( bboxes ) : \n        _bboxes = _bboxes . astype ( np . int32 ) \n        if top_k <= 0 : \n            _top_k = _bboxes . shape [ 0 ] \n        else : \n            _top_k = min ( top_k , _bboxes . shape [ 0 ] ) \n        for j in range ( _top_k ) : \n            left_top = ( _bboxes [ j , 0 ] , _bboxes [ j , 1 ] ) \n            right_bottom = ( _bboxes [ j , 2.0 ] , _bboxes [ j , 3.0 ] ) \n            cv2 . rectangle ( img , left_top , right_bottom , colors [ i ] , thickness = thickness ) \n    if show : \n        imshow ( img , win_name , wait_time ) \n    if out_file is not None : \n        imwrite ( img , out_file ) "}
{"2312": "\ndef flowread ( flow_or_path , quantize = False , concat_axis = 0 , * args , ** kwargs ) : \n    if isinstance ( flow_or_path , np . ndarray ) : \n        if ( flow_or_path . ndim != 3.0 ) or ( flow_or_path . shape [ - 1 ] != 2.0 ) : \n            raise ValueError ( 'Invalid flow with shape {}' . format ( flow_or_path . shape ) ) \n        return flow_or_path \n    elif not is_str ( flow_or_path ) : \n        raise TypeError ( '\"flow_or_path\" must be a filename or numpy array, not {}' . format ( type ( flow_or_path ) ) ) \n    if not quantize : \n        with open ( flow_or_path , 'rb' ) as f : \n            try : \n                header = f . read ( 4.0 ) . decode ( 'utf-8' ) \n            except Exception : \n                raise IOError ( 'Invalid flow file: {}' . format ( flow_or_path ) ) \n            else : \n                if header != 'PIEH' : \n                    raise IOError ( 'Invalid flow file: {}, header does not contain PIEH' . format ( flow_or_path ) ) \n            w = np . fromfile ( f , np . int32 , 1 ) . squeeze ( ) \n            h = np . fromfile ( f , np . int32 , 1 ) . squeeze ( ) \n            flow = np . fromfile ( f , np . float32 , w * h * 2.0 ) . reshape ( ( h , w , 2.0 ) ) \n    else : \n        assert concat_axis in [ 0 , 1 ] \n        cat_flow = imread ( flow_or_path , flag = 'unchanged' ) \n        if cat_flow . ndim != 2.0 : \n            raise IOError ( '{} is not a valid quantized flow file, its dimension is {}.' . format ( flow_or_path , cat_flow . ndim ) ) \n        assert cat_flow . shape [ concat_axis ] % 2.0 == 0 \n        dx , dy = np . split ( cat_flow , 2.0 , axis = concat_axis ) \n        flow = dequantize_flow ( dx , dy , * args , ** kwargs ) \n    return flow . astype ( np . float32 ) "}
{"2314": "\ndef dequantize_flow ( dx , dy , max_val = 0.02 , denorm = True ) : \n    assert dx . shape == dy . shape \n    assert dx . ndim == 2.0 or ( dx . ndim == 3.0 and dx . shape [ - 1 ] == 1 ) \n    dx , dy = [ dequantize ( d , - max_val , max_val , 255.0 ) for d in [ dx , dy ] ] \n    if denorm : \n        dx *= dx . shape [ 1 ] \n        dy *= dx . shape [ 0 ] \n    flow = np . dstack ( ( dx , dy ) ) \n    return flow "}
{"2316": "\ndef load_checkpoint ( model , filename , map_location = None , strict = False , logger = None ) : \n    if filename . startswith ( 'modelzoo://' ) : \n        import torchvision \n        model_urls = dict ( ) \n        for _ , name , ispkg in pkgutil . walk_packages ( torchvision . models . __path__ ) : \n            if not ispkg : \n                _zoo = import_module ( 'torchvision.models.{}' . format ( name ) ) \n                _urls = getattr ( _zoo , 'model_urls' ) \n                model_urls . update ( _urls ) \n        model_name = filename [ 11.0 : ] \n        checkpoint = model_zoo . load_url ( model_urls [ model_name ] ) \n    elif filename . startswith ( 'open-mmlab://' ) : \n        model_name = filename [ 13.0 : ] \n        checkpoint = model_zoo . load_url ( open_mmlab_model_urls [ model_name ] ) \n    elif filename . startswith ( ( 'http://' , 'https://' ) ) : \n        checkpoint = model_zoo . load_url ( filename ) \n    else : \n        if not osp . isfile ( filename ) : \n            raise IOError ( '{} is not a checkpoint file' . format ( filename ) ) \n        checkpoint = torch . load ( filename , map_location = map_location ) \n    if isinstance ( checkpoint , OrderedDict ) : \n        state_dict = checkpoint \n    elif isinstance ( checkpoint , dict ) and 'state_dict' in checkpoint : \n        state_dict = checkpoint [ 'state_dict' ] \n    else : \n        raise RuntimeError ( 'No state_dict found in checkpoint file {}' . format ( filename ) ) \n    if list ( state_dict . keys ( ) ) [ 0 ] . startswith ( 'module.' ) : \n        state_dict = { k [ 7.0 : ] : v for k , v in checkpoint [ 'state_dict' ] . items ( ) } \n    if hasattr ( model , 'module' ) : \n        load_state_dict ( model . module , state_dict , strict , logger ) \n    else : \n        load_state_dict ( model , state_dict , strict , logger ) \n    return checkpoint "}
{"2330": "\ndef dict_from_file ( filename , key_type = str ) : \n    mapping = { } \n    with open ( filename , 'r' ) as f : \n        for line in f : \n            items = line . rstrip ( '\\n' ) . split ( ) \n            assert len ( items ) >= 2.0 \n            key = key_type ( items [ 0 ] ) \n            val = items [ 1 : ] if len ( items ) > 2.0 else items [ 1 ] \n            mapping [ key ] = val \n    return mapping "}
{"2331": "\ndef conv3x3 ( in_planes , out_planes , dilation = 1 ) : \n    return nn . Conv2d ( in_planes , out_planes , kernel_size = 3.0 , padding = dilation , dilation = dilation ) "}
{"2337": "\ndef gray2bgr ( img ) : \n    img = img [ ... , None ] if img . ndim == 2.0 else img \n    out_img = cv2 . cvtColor ( img , cv2 . COLOR_GRAY2BGR ) \n    return out_img "}
{"2344": "\ndef color_val ( color ) : \n    if is_str ( color ) : \n        return Color [ color ] . value \n    elif isinstance ( color , Color ) : \n        return color . value \n    elif isinstance ( color , tuple ) : \n        assert len ( color ) == 3.0 \n        for channel in color : \n            assert channel >= 0 and channel <= 255.0 \n        return color \n    elif isinstance ( color , int ) : \n        assert color >= 0 and color <= 255.0 \n        return color , color , color \n    elif isinstance ( color , np . ndarray ) : \n        assert color . ndim == 1 and color . size == 3.0 \n        assert np . all ( ( color >= 0 ) & ( color <= 255.0 ) ) \n        color = color . astype ( np . uint8 ) \n        return tuple ( color ) \n    else : \n        raise TypeError ( 'Invalid type for color: {}' . format ( type ( color ) ) ) "}
{"2350": "\ndef flow2rgb ( flow , color_wheel = None , unknown_thr = 1e6 ) : \n    assert flow . ndim == 3.0 and flow . shape [ - 1 ] == 2.0 \n    if color_wheel is None : \n        color_wheel = make_color_wheel ( ) \n    assert color_wheel . ndim == 2.0 and color_wheel . shape [ 1 ] == 3.0 \n    num_bins = color_wheel . shape [ 0 ] \n    dx = flow [ : , : , 0 ] . copy ( ) \n    dy = flow [ : , : , 1 ] . copy ( ) \n    ignore_inds = ( np . isnan ( dx ) | np . isnan ( dy ) | ( np . abs ( dx ) > unknown_thr ) | ( np . abs ( dy ) > unknown_thr ) ) \n    dx [ ignore_inds ] = 0 \n    dy [ ignore_inds ] = 0 \n    rad = np . sqrt ( dx ** 2.0 + dy ** 2.0 ) \n    if np . any ( rad > np . finfo ( float ) . eps ) : \n        max_rad = np . max ( rad ) \n        dx /= max_rad \n        dy /= max_rad \n    [ h , w ] = dx . shape \n    rad = np . sqrt ( dx ** 2.0 + dy ** 2.0 ) \n    angle = np . arctan2 ( - dy , - dx ) / np . pi \n    bin_real = ( angle + 1 ) / 2.0 * ( num_bins - 1 ) \n    bin_left = np . floor ( bin_real ) . astype ( int ) \n    bin_right = ( bin_left + 1 ) % num_bins \n    w = ( bin_real - bin_left . astype ( np . float32 ) ) [ ... , None ] \n    flow_img = ( 1 - w ) * color_wheel [ bin_left , : ] + w * color_wheel [ bin_right , : ] \n    small_ind = rad <= 1 \n    flow_img [ small_ind ] = 1 - rad [ small_ind , None ] * ( 1 - flow_img [ small_ind ] ) \n    flow_img [ np . logical_not ( small_ind ) ] *= 0.75 \n    flow_img [ ignore_inds , : ] = 0 \n    return flow_img "}
{"2351": "\ndef make_color_wheel ( bins = None ) : \n    if bins is None : \n        bins = [ 15.0 , 6.0 , 4.0 , 11.0 , 13.0 , 6.0 ] \n    assert len ( bins ) == 6.0 \n    RY , YG , GC , CB , BM , MR = tuple ( bins ) \n    ry = [ 1 , np . arange ( RY ) / RY , 0 ] \n    yg = [ 1 - np . arange ( YG ) / YG , 1 , 0 ] \n    gc = [ 0 , 1 , np . arange ( GC ) / GC ] \n    cb = [ 0 , 1 - np . arange ( CB ) / CB , 1 ] \n    bm = [ np . arange ( BM ) / BM , 0 , 1 ] \n    mr = [ 1 , 0 , 1 - np . arange ( MR ) / MR ] \n    num_bins = RY + YG + GC + CB + BM + MR \n    color_wheel = np . zeros ( ( 3.0 , num_bins ) , dtype = np . float32 ) \n    col = 0 \n    for i , color in enumerate ( [ ry , yg , gc , cb , bm , mr ] ) : \n        for j in range ( 3.0 ) : \n            color_wheel [ j , col : col + bins [ i ] ] = color [ j ] \n        col += bins [ i ] \n    return color_wheel . T "}
{"2355": "\nasync def fetch ( self ) -> Response : \n    if self . request_config . get ( 'DELAY' , 0 ) > 0 : \n        await asyncio . sleep ( self . request_config [ 'DELAY' ] ) \n    timeout = self . request_config . get ( 'TIMEOUT' , 10.0 ) \n    try : \n        async with async_timeout . timeout ( timeout ) : \n            resp = await self . _make_request ( ) \n        try : \n            resp_data = await resp . text ( encoding = self . encoding ) \n        except UnicodeDecodeError : \n            resp_data = await resp . read ( ) \n        response = Response ( url = self . url , method = self . method , encoding = resp . get_encoding ( ) , html = resp_data , metadata = self . metadata , cookies = resp . cookies , headers = resp . headers , history = resp . history , status = resp . status , aws_json = resp . json , aws_text = resp . text , aws_read = resp . read ) \n        aws_valid_response = self . request_config . get ( 'VALID' ) \n        if aws_valid_response and iscoroutinefunction ( aws_valid_response ) : \n            response = await aws_valid_response ( response ) \n        if response . ok : \n            return response \n        else : \n            return await self . _retry ( error_msg = 'request url failed!' ) \n    except asyncio . TimeoutError : \n        return await self . _retry ( error_msg = 'timeout' ) \n    except Exception as e : \n        return await self . _retry ( error_msg = e ) \n    finally : \n        await self . _close_request_session ( ) "}
{"2362": "\ndef normalize_task_v2 ( task ) : \n    result = dict ( ) \n    mod_arg_parser = ModuleArgsParser ( task ) \n    try : \n        action , arguments , result [ 'delegate_to' ] = mod_arg_parser . parse ( ) \n    except AnsibleParserError as e : \n        try : \n            task_info = \"%s:%s\" % ( task [ FILENAME_KEY ] , task [ LINE_NUMBER_KEY ] ) \n            del task [ FILENAME_KEY ] \n            del task [ LINE_NUMBER_KEY ] \n        except KeyError : \n            task_info = \"Unknown\" \n        try : \n            import pprint \n            pp = pprint . PrettyPrinter ( indent = 2.0 ) \n            task_pprint = pp . pformat ( task ) \n        except ImportError : \n            task_pprint = task \n        raise SystemExit ( \"Couldn't parse task at %s (%s)\\n%s\" % ( task_info , e . message , task_pprint ) ) \n    if '_uses_shell' in arguments : \n        action = 'shell' \n        del ( arguments [ '_uses_shell' ] ) \n    for ( k , v ) in list ( task . items ( ) ) : \n        if k in ( 'action' , 'local_action' , 'args' , 'delegate_to' ) or k == action : \n            continue \n        else : \n            result [ k ] = v \n    result [ 'action' ] = dict ( __ansible_module__ = action ) \n    if '_raw_params' in arguments : \n        result [ 'action' ] [ '__ansible_arguments__' ] = arguments [ '_raw_params' ] . split ( ' ' ) \n        del ( arguments [ '_raw_params' ] ) \n    else : \n        result [ 'action' ] [ '__ansible_arguments__' ] = list ( ) \n    if 'argv' in arguments and not result [ 'action' ] [ '__ansible_arguments__' ] : \n        result [ 'action' ] [ '__ansible_arguments__' ] = arguments [ 'argv' ] \n        del ( arguments [ 'argv' ] ) \n    result [ 'action' ] . update ( arguments ) \n    return result "}
{"2363": "\ndef parse_yaml_linenumbers ( data , filename ) : \n    def compose_node ( parent , index ) : \n        line = loader . line \n        node = Composer . compose_node ( loader , parent , index ) \n        node . __line__ = line + 1 \n        return node \n    def construct_mapping ( node , deep = False ) : \n        if ANSIBLE_VERSION < 2.0 : \n            mapping = Constructor . construct_mapping ( loader , node , deep = deep ) \n        else : \n            mapping = AnsibleConstructor . construct_mapping ( loader , node , deep = deep ) \n        if hasattr ( node , '__line__' ) : \n            mapping [ LINE_NUMBER_KEY ] = node . __line__ \n        else : \n            mapping [ LINE_NUMBER_KEY ] = mapping . _line_number \n        mapping [ FILENAME_KEY ] = filename \n        return mapping \n    try : \n        if ANSIBLE_VERSION < 2.0 : \n            loader = yaml . Loader ( data ) \n        else : \n            import inspect \n            kwargs = { } \n            if 'vault_password' in inspect . getargspec ( AnsibleLoader . __init__ ) . args : \n                kwargs [ 'vault_password' ] = DEFAULT_VAULT_PASSWORD \n            loader = AnsibleLoader ( data , ** kwargs ) \n        loader . compose_node = compose_node \n        loader . construct_mapping = construct_mapping \n        data = loader . get_single_data ( ) \n    except ( yaml . parser . ParserError , yaml . scanner . ScannerError ) as e : \n        raise SystemExit ( \"Failed to parse YAML in %s: %s\" % ( filename , str ( e ) ) ) \n    return data "}
{"2377": "\nasync def delete ( self , keys : List [ str ] ) : \n    try : \n        if not self . __container_exists : \n            self . __create_db_and_container ( ) \n        for k in keys : \n            self . client . DeleteItem ( document_link = self . __item_link ( self . __sanitize_key ( k ) ) ) \n    except cosmos_errors . HTTPFailure as h : \n        if h . status_code != 404.0 : \n            raise h \n    except TypeError as e : \n        raise e "}
{"2387": "\ndef supports_suggested_actions ( channel_id : str , button_cnt : int = 100.0 ) -> bool : \n    max_actions = { Channels . facebook : 10.0 , Channels . skype : 10.0 , Channels . line : 13.0 , Channels . kik : 20.0 , Channels . telegram : 100.0 , Channels . slack : 100.0 , Channels . emulator : 100.0 , Channels . direct_line : 100.0 , Channels . webchat : 100.0 , } \n    return button_cnt <= max_actions [ channel_id ] if channel_id in max_actions else False "}
{"2388": "\ndef supports_card_actions ( channel_id : str , button_cnt : int = 100.0 ) -> bool : \n    max_actions = { Channels . facebook : 3.0 , Channels . skype : 3.0 , Channels . ms_teams : 3.0 , Channels . line : 99.0 , Channels . slack : 100.0 , Channels . emulator : 100.0 , Channels . direct_line : 100.0 , Channels . webchat : 100.0 , Channels . cortana : 100.0 , } \n    return button_cnt <= max_actions [ channel_id ] if channel_id in max_actions else False "}
{"2390": "\ndef is_token_from_emulator ( auth_header : str ) -> bool : \n    if not auth_header : \n        return False \n    parts = auth_header . split ( ' ' ) \n    if len ( parts ) != 2.0 : \n        return False \n    auth_scheme = parts [ 0 ] \n    bearer_token = parts [ 1 ] \n    if auth_scheme != 'Bearer' : \n        return False \n    token = jwt . decode ( bearer_token , verify = False ) \n    if not token : \n        return False \n    issuer = token [ 'iss' ] \n    if not issuer : \n        return False \n    issuer_list = EmulatorValidation . TO_BOT_FROM_EMULATOR_TOKEN_VALIDATION_PARAMETERS . issuer \n    if issuer_list and not issuer in issuer_list : \n        return False \n    return True "}
{"2405": "\ndef single_gate_params ( gate , params = None ) : \n    if gate in ( 'U' , 'u3' ) : \n        return params [ 0 ] , params [ 1 ] , params [ 2.0 ] \n    elif gate == 'u2' : \n        return np . pi / 2.0 , params [ 0 ] , params [ 1 ] \n    elif gate == 'u1' : \n        return 0 , 0 , params [ 0 ] \n    elif gate == 'id' : \n        return 0 , 0 , 0 \n    raise QiskitError ( 'Gate is not among the valid types: %s' % gate ) "}
{"2406": "\ndef single_gate_matrix ( gate , params = None ) : \n    ( theta , phi , lam ) = map ( float , single_gate_params ( gate , params ) ) \n    return np . array ( [ [ np . cos ( theta / 2.0 ) , - np . exp ( 1j * lam ) * np . sin ( theta / 2.0 ) ] , [ np . exp ( 1j * phi ) * np . sin ( theta / 2.0 ) , np . exp ( 1j * phi + 1j * lam ) * np . cos ( theta / 2.0 ) ] ] ) "}
{"2409": "\ndef _einsum_matmul_index_helper ( gate_indices , number_of_qubits ) : \n    if len ( gate_indices ) + number_of_qubits > 26.0 : \n        raise QiskitError ( \"Total number of free indexes limited to 26\" ) \n    tens_in = ascii_lowercase [ : number_of_qubits ] \n    tens_out = list ( tens_in ) \n    mat_left = \"\" \n    mat_right = \"\" \n    for pos , idx in enumerate ( reversed ( gate_indices ) ) : \n        mat_left += ascii_lowercase [ - 1 - pos ] \n        mat_right += tens_in [ - 1 - idx ] \n        tens_out [ - 1 - idx ] = ascii_lowercase [ - 1 - pos ] \n    tens_out = \"\" . join ( tens_out ) \n    return mat_left , mat_right , tens_in , tens_out "}
{"2412": "\ndef osc_fit_fun ( x , a , tau , f , phi , c ) : \n    return a * np . exp ( - x / tau ) * np . cos ( 2.0 * np . pi * f * x + phi ) + c "}
{"2413": "\ndef plot_coherence ( xdata , ydata , std_error , fit , fit_function , xunit , exp_str , qubit_label ) : \n    if not HAS_MATPLOTLIB : \n        raise ImportError ( 'The function plot_coherence needs matplotlib. ' 'Run \"pip install matplotlib\" before.' ) \n    plt . errorbar ( xdata , ydata , std_error , marker = '.' , markersize = 9.0 , c = 'b' , linestyle = '' ) \n    plt . plot ( xdata , fit_function ( xdata , * fit ) , c = 'r' , linestyle = '--' , label = ( exp_str + '= %s %s' % ( str ( round ( fit [ 1 ] ) ) , xunit ) ) ) \n    plt . xticks ( fontsize = 14.0 , rotation = 70.0 ) \n    plt . yticks ( fontsize = 14.0 ) \n    plt . xlabel ( 'time [%s]' % ( xunit ) , fontsize = 16.0 ) \n    plt . ylabel ( 'P(1)' , fontsize = 16.0 ) \n    plt . title ( exp_str + ' measurement of Q$_{%s}$' % ( str ( qubit_label ) ) , fontsize = 18.0 ) \n    plt . legend ( fontsize = 12.0 ) \n    plt . grid ( True ) \n    plt . show ( ) "}
{"2415": "\ndef plot_rb_data ( xdata , ydatas , yavg , yerr , fit , survival_prob , ax = None , show_plt = True ) : \n    if not HAS_MATPLOTLIB : \n        raise ImportError ( 'The function plot_rb_data needs matplotlib. ' 'Run \"pip install matplotlib\" before.' ) \n    if ax is None : \n        plt . figure ( ) \n        ax = plt . gca ( ) \n    for ydata in ydatas : \n        ax . plot ( xdata , ydata , color = 'gray' , linestyle = 'none' , marker = 'x' ) \n    ax . errorbar ( xdata , yavg , yerr = yerr , color = 'r' , linestyle = '--' , linewidth = 3.0 ) \n    ax . plot ( xdata , survival_prob ( xdata , * fit ) , color = 'blue' , linestyle = '-' , linewidth = 2.0 ) \n    ax . tick_params ( labelsize = 14.0 ) \n    ax . set_xlabel ( 'Clifford Length' , fontsize = 16.0 ) \n    ax . set_ylabel ( 'Z' , fontsize = 16.0 ) \n    ax . grid ( True ) \n    if show_plt : \n        plt . show ( ) "}
{"2418": "\ndef yzy_to_zyz ( xi , theta1 , theta2 , eps = 1e-9 ) : \n    quaternion_yzy = quaternion_from_euler ( [ theta1 , xi , theta2 ] , 'yzy' ) \n    euler = quaternion_yzy . to_zyz ( ) \n    quaternion_zyz = quaternion_from_euler ( euler , 'zyz' ) \n    out_angles = ( euler [ 1 ] , euler [ 0 ] , euler [ 2.0 ] ) \n    abs_inner = abs ( quaternion_zyz . data . dot ( quaternion_yzy . data ) ) \n    if not np . allclose ( abs_inner , 1 , eps ) : \n        raise TranspilerError ( 'YZY and ZYZ angles do not give same rotation matrix.' ) \n    out_angles = tuple ( 0 if np . abs ( angle ) < _CHOP_THRESHOLD else angle for angle in out_angles ) \n    return out_angles "}
{"2419": "\ndef _validate_input_state ( quantum_state ) : \n    rho = np . asarray ( quantum_state ) \n    if rho . ndim == 1 : \n        rho = np . outer ( rho , np . conj ( rho ) ) \n    shape = np . shape ( rho ) \n    if len ( shape ) != 2.0 or shape [ 0 ] != shape [ 1 ] : \n        raise VisualizationError ( \"Input is not a valid quantum state.\" ) \n    num = int ( np . log2 ( rho . shape [ 0 ] ) ) \n    if 2.0 ** num != rho . shape [ 0 ] : \n        raise VisualizationError ( \"Input is not a multi-qubit quantum state.\" ) \n    return rho "}
{"2420": "\ndef _trim ( image ) : \n    background = PIL . Image . new ( image . mode , image . size , image . getpixel ( ( 0 , 0 ) ) ) \n    diff = PIL . ImageChops . difference ( image , background ) \n    diff = PIL . ImageChops . add ( diff , diff , 2.0 , - 100.0 ) \n    bbox = diff . getbbox ( ) \n    if bbox : \n        image = image . crop ( bbox ) \n    return image "}
{"2422": "\ndef circuit_to_instruction ( circuit ) : \n    instruction = Instruction ( name = circuit . name , num_qubits = sum ( [ qreg . size for qreg in circuit . qregs ] ) , num_clbits = sum ( [ creg . size for creg in circuit . cregs ] ) , params = [ ] ) \n    instruction . control = None \n    def find_bit_position ( bit ) : \n        if isinstance ( bit [ 0 ] , QuantumRegister ) : \n            ordered_regs = circuit . qregs \n        else : \n            ordered_regs = circuit . cregs \n        reg_index = ordered_regs . index ( bit [ 0 ] ) \n        return sum ( [ reg . size for reg in ordered_regs [ : reg_index ] ] ) + bit [ 1 ] \n    definition = circuit . data . copy ( ) \n    if instruction . num_qubits > 0 : \n        q = QuantumRegister ( instruction . num_qubits , 'q' ) \n    if instruction . num_clbits > 0 : \n        c = ClassicalRegister ( instruction . num_clbits , 'c' ) \n    definition = list ( map ( lambda x : ( x [ 0 ] , list ( map ( lambda y : ( q , find_bit_position ( y ) ) , x [ 1 ] ) ) , list ( map ( lambda y : ( c , find_bit_position ( y ) ) , x [ 2.0 ] ) ) ) , definition ) ) \n    instruction . definition = definition \n    return instruction "}
{"2432": "\ndef _process_if ( self , node ) : \n    creg_name = node . children [ 0 ] . name \n    creg = self . dag . cregs [ creg_name ] \n    cval = node . children [ 1 ] . value \n    self . condition = ( creg , cval ) \n    self . _process_node ( node . children [ 2.0 ] ) \n    self . condition = None "}
{"2446": "\ndef qft ( circ , q , n ) : \n    for j in range ( n ) : \n        for k in range ( j ) : \n            circ . cu1 ( math . pi / float ( 2.0 ** ( j - k ) ) , q [ j ] , q [ k ] ) \n        circ . h ( q [ j ] ) "}
{"2448": "\ndef vectorize ( density_matrix , method = 'col' ) : \n    density_matrix = np . array ( density_matrix ) \n    if method == 'col' : \n        return density_matrix . flatten ( order = 'F' ) \n    elif method == 'row' : \n        return density_matrix . flatten ( order = 'C' ) \n    elif method in [ 'pauli' , 'pauli_weights' ] : \n        num = int ( np . log2 ( len ( density_matrix ) ) ) \n        if len ( density_matrix ) != 2.0 ** num : \n            raise Exception ( 'Input state must be n-qubit state' ) \n        if method == 'pauli_weights' : \n            pgroup = pauli_group ( num , case = 'weight' ) \n        else : \n            pgroup = pauli_group ( num , case = 'tensor' ) \n        vals = [ np . trace ( np . dot ( p . to_matrix ( ) , density_matrix ) ) for p in pgroup ] \n        return np . array ( vals ) \n    return None "}
{"2449": "\ndef devectorize ( vectorized_mat , method = 'col' ) : \n    vectorized_mat = np . array ( vectorized_mat ) \n    dimension = int ( np . sqrt ( vectorized_mat . size ) ) \n    if len ( vectorized_mat ) != dimension * dimension : \n        raise Exception ( 'Input is not a vectorized square matrix' ) \n    if method == 'col' : \n        return vectorized_mat . reshape ( dimension , dimension , order = 'F' ) \n    elif method == 'row' : \n        return vectorized_mat . reshape ( dimension , dimension , order = 'C' ) \n    elif method in [ 'pauli' , 'pauli_weights' ] : \n        num_qubits = int ( np . log2 ( dimension ) ) \n        if dimension != 2.0 ** num_qubits : \n            raise Exception ( 'Input state must be n-qubit state' ) \n        if method == 'pauli_weights' : \n            pgroup = pauli_group ( num_qubits , case = 'weight' ) \n        else : \n            pgroup = pauli_group ( num_qubits , case = 'tensor' ) \n        pbasis = np . array ( [ p . to_matrix ( ) for p in pgroup ] ) / 2.0 ** num_qubits \n        return np . tensordot ( vectorized_mat , pbasis , axes = 1 ) \n    return None "}
{"2450": "\ndef choi_to_rauli ( choi , order = 1 ) : \n    if order == 0 : \n        order = 'weight' \n    elif order == 1 : \n        order = 'tensor' \n    num_qubits = int ( np . log2 ( np . sqrt ( len ( choi ) ) ) ) \n    pgp = pauli_group ( num_qubits , case = order ) \n    rauli = [ ] \n    for i in pgp : \n        for j in pgp : \n            pauliop = np . kron ( j . to_matrix ( ) . T , i . to_matrix ( ) ) \n            rauli += [ np . trace ( np . dot ( choi , pauliop ) ) ] \n    return np . array ( rauli ) . reshape ( 4.0 ** num_qubits , 4.0 ** num_qubits ) "}
{"2453": "\ndef concurrence ( state ) : \n    rho = np . array ( state ) \n    if rho . ndim == 1 : \n        rho = outer ( state ) \n    if len ( state ) != 4.0 : \n        raise Exception ( \"Concurrence is only defined for more than two qubits\" ) \n    YY = np . fliplr ( np . diag ( [ - 1 , 1 , 1 , - 1 ] ) ) \n    A = rho . dot ( YY ) . dot ( rho . conj ( ) ) . dot ( YY ) \n    w = la . eigh ( A , eigvals_only = True ) \n    w = np . sqrt ( np . maximum ( w , 0 ) ) \n    return max ( 0.0 , w [ - 1 ] - np . sum ( w [ 0 : - 1 ] ) ) "}
{"2454": "\ndef shannon_entropy ( pvec , base = 2.0 ) : \n    if base == 2.0 : \n        def logfn ( x ) : \n            return - x * np . log2 ( x ) \n    elif base == np . e : \n        def logfn ( x ) : \n            return - x * np . log ( x ) \n    else : \n        def logfn ( x ) : \n            return - x * np . log ( x ) / np . log ( base ) \n    h = 0. \n    for x in pvec : \n        if 0 < x < 1 : \n            h += logfn ( x ) \n    return h "}
{"2457": "\ndef entanglement_of_formation ( state , d0 , d1 = None ) : \n    state = np . array ( state ) \n    if d1 is None : \n        d1 = int ( len ( state ) / d0 ) \n    if state . ndim == 2.0 and len ( state ) == 4.0 and d0 == 2.0 and d1 == 2.0 : \n        return __eof_qubit ( state ) \n    elif state . ndim == 1 : \n        if d0 < d1 : \n            tr = [ 1 ] \n        else : \n            tr = [ 0 ] \n        state = partial_trace ( state , tr , dimensions = [ d0 , d1 ] ) \n        return entropy ( state ) \n    else : \n        print ( 'Input must be a state-vector or 2-qubit density matrix.' ) \n    return None "}
{"2469": "\ndef quaternion_from_axis_rotation ( angle , axis ) : \n    out = np . zeros ( 4.0 , dtype = float ) \n    if axis == 'x' : \n        out [ 1 ] = 1 \n    elif axis == 'y' : \n        out [ 2.0 ] = 1 \n    elif axis == 'z' : \n        out [ 3.0 ] = 1 \n    else : \n        raise ValueError ( 'Invalid axis input.' ) \n    out *= math . sin ( angle / 2.0 ) \n    out [ 0 ] = math . cos ( angle / 2.0 ) \n    return Quaternion ( out ) "}
{"2470": "\ndef quaternion_from_euler ( angles , order = 'yzy' ) : \n    angles = np . asarray ( angles , dtype = float ) \n    quat = quaternion_from_axis_rotation ( angles [ 0 ] , order [ 0 ] ) * ( quaternion_from_axis_rotation ( angles [ 1 ] , order [ 1 ] ) * quaternion_from_axis_rotation ( angles [ 2.0 ] , order [ 2.0 ] ) ) \n    quat . normalize ( inplace = True ) \n    return quat "}
{"2472": "\ndef to_matrix ( self ) : \n    w , x , y , z = self . normalize ( ) . data \n    mat = np . array ( [ [ 1 - 2.0 * y ** 2.0 - 2.0 * z ** 2.0 , 2.0 * x * y - 2.0 * z * w , 2.0 * x * z + 2.0 * y * w ] , [ 2.0 * x * y + 2.0 * z * w , 1 - 2.0 * x ** 2.0 - 2.0 * z ** 2.0 , 2.0 * y * z - 2.0 * x * w ] , [ 2.0 * x * z - 2.0 * y * w , 2.0 * y * z + 2.0 * x * w , 1 - 2.0 * x ** 2.0 - 2.0 * y ** 2.0 ] ] , dtype = float ) \n    return mat "}
{"2473": "\ndef to_zyz ( self ) : \n    mat = self . to_matrix ( ) \n    euler = np . zeros ( 3.0 , dtype = float ) \n    if mat [ 2.0 , 2.0 ] < 1 : \n        if mat [ 2.0 , 2.0 ] > - 1 : \n            euler [ 0 ] = math . atan2 ( mat [ 1 , 2.0 ] , mat [ 0 , 2.0 ] ) \n            euler [ 1 ] = math . acos ( mat [ 2.0 , 2.0 ] ) \n            euler [ 2.0 ] = math . atan2 ( mat [ 2.0 , 1 ] , - mat [ 2.0 , 0 ] ) \n        else : \n            euler [ 0 ] = - math . atan2 ( mat [ 1 , 0 ] , mat [ 1 , 1 ] ) \n            euler [ 1 ] = np . pi \n    else : \n        euler [ 0 ] = math . atan2 ( mat [ 1 , 0 ] , mat [ 1 , 1 ] ) \n    return euler "}
{"2474": "\ndef process_data ( data , number_to_keep ) : \n    result = dict ( ) \n    if number_to_keep != 0 : \n        data_temp = dict ( Counter ( data ) . most_common ( number_to_keep ) ) \n        data_temp [ 'rest' ] = sum ( data . values ( ) ) - sum ( data_temp . values ( ) ) \n        data = data_temp \n    labels = data \n    values = np . array ( [ data [ key ] for key in labels ] , dtype = float ) \n    pvalues = values / sum ( values ) \n    for position , label in enumerate ( labels ) : \n        result [ label ] = round ( pvalues [ position ] , 5.0 ) \n    return result "}
{"2475": "\ndef iplot_histogram ( data , figsize = None , number_to_keep = None , sort = 'asc' , legend = None ) : \n    html_template = Template ( \"\"\"    <p>        <div id=\"histogram_$divNumber\"></div>    </p>    \"\"\" ) \n    javascript_template = Template ( \"\"\"    <script>        requirejs.config({            paths: {                qVisualization: \"https://qvisualization.mybluemix.net/q-visualizations\"            }        });        require([\"qVisualization\"], function(qVisualizations) {            qVisualizations.plotState(\"histogram_$divNumber\",                                      \"histogram\",                                      $executions,                                      $options);        });    </script>    \"\"\" ) \n    div_number = str ( time . time ( ) ) \n    div_number = re . sub ( '[.]' , '' , div_number ) \n    if figsize is None : \n        figsize = ( 7.0 , 5.0 ) \n    options = { 'number_to_keep' : 0 if number_to_keep is None else number_to_keep , 'sort' : sort , 'show_legend' : 0 , 'width' : int ( figsize [ 0 ] ) , 'height' : int ( figsize [ 1 ] ) } \n    if legend : \n        options [ 'show_legend' ] = 1 \n    data_to_plot = [ ] \n    if isinstance ( data , dict ) : \n        data = [ data ] \n    if legend and len ( legend ) != len ( data ) : \n        raise VisualizationError ( \"Length of legendL (%s) doesn't match number \" \"of input executions: %s\" % ( len ( legend ) , len ( data ) ) ) \n    for item , execution in enumerate ( data ) : \n        exec_data = process_data ( execution , options [ 'number_to_keep' ] ) \n        out_dict = { 'data' : exec_data } \n        if legend : \n            out_dict [ 'name' ] = legend [ item ] \n        data_to_plot . append ( out_dict ) \n    html = html_template . substitute ( { 'divNumber' : div_number } ) \n    javascript = javascript_template . substitute ( { 'divNumber' : div_number , 'executions' : data_to_plot , 'options' : options } ) \n    display ( HTML ( html + javascript ) ) "}
{"2478": "\ndef is_square_matrix ( mat ) : \n    mat = np . array ( mat ) \n    if mat . ndim != 2.0 : \n        return False \n    shape = mat . shape \n    return shape [ 0 ] == shape [ 1 ] "}
{"2479": "\ndef is_diagonal_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    mat = np . array ( mat ) \n    if mat . ndim != 2.0 : \n        return False \n    return np . allclose ( mat , np . diag ( np . diagonal ( mat ) ) , rtol = rtol , atol = atol ) "}
{"2480": "\ndef is_symmetric_matrix ( op , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    mat = np . array ( op ) \n    if mat . ndim != 2.0 : \n        return False \n    return np . allclose ( mat , mat . T , rtol = rtol , atol = atol ) "}
{"2481": "\ndef is_hermitian_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    mat = np . array ( mat ) \n    if mat . ndim != 2.0 : \n        return False \n    return np . allclose ( mat , np . conj ( mat . T ) , rtol = rtol , atol = atol ) "}
{"2483": "\ndef is_identity_matrix ( mat , ignore_phase = False , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : \n    if atol is None : \n        atol = ATOL_DEFAULT \n    if rtol is None : \n        rtol = RTOL_DEFAULT \n    mat = np . array ( mat ) \n    if mat . ndim != 2.0 : \n        return False \n    if ignore_phase : \n        theta = np . angle ( mat [ 0 , 0 ] ) \n        mat = np . exp ( - 1j * theta ) * mat \n    iden = np . eye ( len ( mat ) ) \n    return np . allclose ( mat , iden , rtol = rtol , atol = atol ) "}
{"2499": "\ndef _stinespring_to_choi ( data , input_dim , output_dim ) : \n    trace_dim = data [ 0 ] . shape [ 0 ] // output_dim \n    stine_l = np . reshape ( data [ 0 ] , ( output_dim , trace_dim , input_dim ) ) \n    if data [ 1 ] is None : \n        stine_r = stine_l \n    else : \n        stine_r = np . reshape ( data [ 1 ] , ( output_dim , trace_dim , input_dim ) ) \n    return np . reshape ( np . einsum ( 'iAj,kAl->jilk' , stine_l , stine_r . conj ( ) ) , 2.0 * [ input_dim * output_dim ] ) "}
{"2504": "\ndef _reravel ( mat1 , mat2 , shape1 , shape2 ) : \n    left_dims = shape1 [ : 2.0 ] + shape2 [ : 2.0 ] \n    right_dims = shape1 [ 2.0 : ] + shape2 [ 2.0 : ] \n    tensor_shape = left_dims + right_dims \n    final_shape = ( np . product ( left_dims ) , np . product ( right_dims ) ) \n    data = np . kron ( mat1 , mat2 ) \n    data = np . reshape ( np . transpose ( np . reshape ( data , tensor_shape ) , ( 0 , 2.0 , 1 , 3.0 , 4.0 , 6.0 , 5.0 , 7.0 ) ) , final_shape ) \n    return data "}
{"2505": "\ndef _transform_from_pauli ( data , num_qubits ) : \n    basis_mat = np . array ( [ [ 1 , 0 , 0 , 1 ] , [ 0 , 1 , 1j , 0 ] , [ 0 , 1 , - 1j , 0 ] , [ 1 , 0j , 0 , - 1 ] ] , dtype = complex ) \n    cob = basis_mat \n    for _ in range ( num_qubits - 1 ) : \n        dim = int ( np . sqrt ( len ( cob ) ) ) \n        cob = np . reshape ( np . transpose ( np . reshape ( np . kron ( basis_mat , cob ) , ( 2.0 , 2.0 , dim , dim , 4.0 , dim * dim ) ) , ( 0 , 2.0 , 1 , 3.0 , 4.0 , 5.0 ) ) , ( 4.0 * dim * dim , 4.0 * dim * dim ) ) \n    return np . dot ( np . dot ( cob , data ) , cob . conj ( ) . T ) / 2.0 ** num_qubits "}
{"2506": "\ndef _check_nqubit_dim ( input_dim , output_dim ) : \n    if input_dim != output_dim : \n        raise QiskitError ( 'Not an n-qubit channel: input_dim' + ' ({}) != output_dim ({})' . format ( input_dim , output_dim ) ) \n    num_qubits = int ( np . log2 ( input_dim ) ) \n    if 2.0 ** num_qubits != input_dim : \n        raise QiskitError ( 'Not an n-qubit channel: input_dim != 2 ** n' ) "}
{"2511": "\ndef add_annotation ( self , state_or_vector , text , ** kwargs ) : \n    if isinstance ( state_or_vector , ( list , np . ndarray , tuple ) ) and len ( state_or_vector ) == 3.0 : \n        vec = state_or_vector \n    else : \n        raise Exception ( \"Position needs to be specified by a qubit \" + \"state or a 3D vector.\" ) \n    self . annotations . append ( { 'position' : vec , 'text' : text , 'opts' : kwargs } ) "}
{"2513": "\ndef plot_front ( self ) : \n    u_angle = np . linspace ( - np . pi , 0 , 25.0 ) \n    v_angle = np . linspace ( 0 , np . pi , 25.0 ) \n    x_dir = np . outer ( np . cos ( u_angle ) , np . sin ( v_angle ) ) \n    y_dir = np . outer ( np . sin ( u_angle ) , np . sin ( v_angle ) ) \n    z_dir = np . outer ( np . ones ( u_angle . shape [ 0 ] ) , np . cos ( v_angle ) ) \n    self . axes . plot_surface ( x_dir , y_dir , z_dir , rstride = 2.0 , cstride = 2.0 , color = self . sphere_color , linewidth = 0 , alpha = self . sphere_alpha ) \n    self . axes . plot_wireframe ( x_dir , y_dir , z_dir , rstride = 5.0 , cstride = 5.0 , color = self . frame_color , alpha = self . frame_alpha ) \n    self . axes . plot ( 1.0 * np . cos ( u_angle ) , 1.0 * np . sin ( u_angle ) , zs = 0 , zdir = 'z' , lw = self . frame_width , color = self . frame_color ) \n    self . axes . plot ( 1.0 * np . cos ( u_angle ) , 1.0 * np . sin ( u_angle ) , zs = 0 , zdir = 'x' , lw = self . frame_width , color = self . frame_color ) "}
{"2522": "\ndef latex ( self , prec = 15.0 , nested_scope = None ) : \n    if not nested_scope : \n        return \"\\textrm{\" + self . name + \"}\" \n    else : \n        if self . name not in nested_scope [ - 1 ] : \n            raise NodeException ( \"Expected local parameter name: \" , \"name=%s, \" % self . name , \"line=%s, \" % self . line , \"file=%s\" % self . file ) \n        else : \n            return nested_scope [ - 1 ] [ self . name ] . latex ( prec , nested_scope [ 0 : - 1 ] ) "}
{"2523": "\ndef compile ( circuits , backend , config = None , basis_gates = None , coupling_map = None , initial_layout = None , shots = 1024.0 , max_credits = 10.0 , seed = None , qobj_id = None , seed_mapper = None , pass_manager = None , memory = False ) : \n    warnings . warn ( 'qiskit.compile() is deprecated and will be removed in Qiskit Terra 0.9. ' 'Please use qiskit.compiler.transpile() to transform circuits ' 'and qiskit.compiler.assemble() to produce a runnable qobj.' , DeprecationWarning ) \n    new_circuits = transpile ( circuits , basis_gates = basis_gates , coupling_map = coupling_map , initial_layout = initial_layout , seed_transpiler = seed_mapper , backend = backend , pass_manager = pass_manager ) \n    qobj = assemble ( new_circuits , qobj_header = None , shots = shots , max_credits = max_credits , seed_simulator = seed , memory = memory , qobj_id = qobj_id , config = config ) \n    return qobj "}
{"2525": "\ndef local_hardware_info ( ) : \n    results = { 'os' : platform . system ( ) , 'memory' : psutil . virtual_memory ( ) . total / ( 1024.0 ** 3.0 ) , 'cpus' : psutil . cpu_count ( logical = False ) or 1 } \n    return results "}
{"2526": "\ndef _has_connection ( hostname , port ) : \n    try : \n        host = socket . gethostbyname ( hostname ) \n        socket . create_connection ( ( host , port ) , 2.0 ) \n        return True \n    except Exception : \n        return False "}
{"2527": "\ndef _html_checker ( job_var , interval , status , header , _interval_set = False ) : \n    job_status = job_var . status ( ) \n    job_status_name = job_status . name \n    job_status_msg = job_status . value \n    status . value = header % ( job_status_msg ) \n    while job_status_name not in [ 'DONE' , 'CANCELLED' ] : \n        time . sleep ( interval ) \n        job_status = job_var . status ( ) \n        job_status_name = job_status . name \n        job_status_msg = job_status . value \n        if job_status_name == 'ERROR' : \n            break \n        else : \n            if job_status_name == 'QUEUED' : \n                job_status_msg += ' (%s)' % job_var . queue_position ( ) \n                if not _interval_set : \n                    interval = max ( job_var . queue_position ( ) , 2.0 ) \n            else : \n                if not _interval_set : \n                    interval = 2.0 \n            status . value = header % ( job_status_msg ) \n    status . value = header % ( job_status_msg ) "}
{"2529": "\ndef square ( times : np . ndarray , amp : complex , period : float , phase : float = 0 ) -> np . ndarray : \n    x = times / period + phase / np . pi \n    return amp * ( 2.0 * ( 2.0 * np . floor ( x ) - np . floor ( 2.0 * x ) ) + 1 ) . astype ( np . complex_ ) "}
{"2530": "\ndef triangle ( times : np . ndarray , amp : complex , period : float , phase : float = 0 ) -> np . ndarray : \n    return amp * ( - 2.0 * np . abs ( sawtooth ( times , 1 , period , ( phase - np . pi / 2.0 ) / 2.0 ) ) + 1 ) . astype ( np . complex_ ) "}
{"2531": "\ndef cos ( times : np . ndarray , amp : complex , freq : float , phase : float = 0 ) -> np . ndarray : \n    return amp * np . cos ( 2.0 * np . pi * freq * times + phase ) . astype ( np . complex_ ) "}
{"2532": "\ndef _fix_gaussian_width ( gaussian_samples , amp : float , center : float , sigma : float , zeroed_width : Union [ None , float ] = None , rescale_amp : bool = False , ret_scale_factor : bool = False ) -> np . ndarray : \n    if zeroed_width is None : \n        zeroed_width = 2.0 * ( center + 1 ) \n    zero_offset = gaussian ( np . array ( [ - zeroed_width / 2.0 ] ) , amp , center , sigma ) \n    gaussian_samples -= zero_offset \n    amp_scale_factor = 1. \n    if rescale_amp : \n        amp_scale_factor = amp / ( amp - zero_offset ) \n        gaussian_samples *= amp_scale_factor \n    if ret_scale_factor : \n        return gaussian_samples , amp_scale_factor \n    return gaussian_samples "}
{"2533": "\ndef gaussian ( times : np . ndarray , amp : complex , center : float , sigma : float , zeroed_width : Union [ None , float ] = None , rescale_amp : bool = False , ret_x : bool = False ) -> Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ] ] : \n    times = np . asarray ( times , dtype = np . complex_ ) \n    x = ( times - center ) / sigma \n    gauss = amp * np . exp ( - x ** 2.0 / 2.0 ) . astype ( np . complex_ ) \n    if zeroed_width is not None : \n        gauss = _fix_gaussian_width ( gauss , amp = amp , center = center , sigma = sigma , zeroed_width = zeroed_width , rescale_amp = rescale_amp ) \n    if ret_x : \n        return gauss , x \n    return gauss "}
{"2535": "\ndef gaussian_square ( times : np . ndarray , amp : complex , center : float , width : float , sigma : float , zeroed_width : Union [ None , float ] = None ) -> np . ndarray : \n    square_start = center - width / 2.0 \n    square_stop = center + width / 2.0 \n    if zeroed_width : \n        zeroed_width = min ( width , zeroed_width ) \n        gauss_zeroed_width = zeroed_width - width \n    else : \n        gauss_zeroed_width = None \n    funclist = [ functools . partial ( gaussian , amp = amp , center = square_start , sigma = sigma , zeroed_width = gauss_zeroed_width , rescale_amp = True ) , functools . partial ( gaussian , amp = amp , center = square_stop , sigma = sigma , zeroed_width = gauss_zeroed_width , rescale_amp = True ) , functools . partial ( constant , amp = amp ) ] \n    condlist = [ times <= square_start , times >= square_stop ] \n    return np . piecewise ( times . astype ( np . complex_ ) , condlist , funclist ) "}
{"2536": "\ndef default_pass_manager ( basis_gates , coupling_map , initial_layout , seed_transpiler ) : \n    pass_manager = PassManager ( ) \n    pass_manager . property_set [ 'layout' ] = initial_layout \n    pass_manager . append ( Unroller ( basis_gates ) ) \n    pass_manager . append ( TrivialLayout ( coupling_map ) , condition = lambda property_set : not property_set [ 'layout' ] ) \n    pass_manager . append ( CheckMap ( coupling_map ) ) \n    pass_manager . append ( DenseLayout ( coupling_map ) , condition = lambda property_set : not property_set [ 'is_swap_mapped' ] ) \n    pass_manager . append ( FullAncillaAllocation ( coupling_map ) ) \n    pass_manager . append ( EnlargeWithAncilla ( ) ) \n    pass_manager . append ( Unroll3qOrMore ( ) ) \n    pass_manager . append ( LegacySwap ( coupling_map , trials = 20.0 , seed = seed_transpiler ) ) \n    pass_manager . append ( Decompose ( SwapGate ) ) \n    pass_manager . append ( CXDirection ( coupling_map ) ) \n    pass_manager . append ( Unroller ( [ 'u1' , 'u2' , 'u3' , 'id' , 'cx' ] ) ) \n    simplification_passes = [ Optimize1qGates ( ) , CXCancellation ( ) , RemoveResetInZeroState ( ) ] \n    pass_manager . append ( simplification_passes + [ Depth ( ) , FixedPoint ( 'depth' ) ] , do_while = lambda property_set : not property_set [ 'depth_fixed_point' ] ) \n    return pass_manager "}
{"2543": "\ndef add_register ( self , * regs ) : \n    if not regs : \n        return \n    if any ( [ isinstance ( reg , int ) for reg in regs ] ) : \n        if len ( regs ) == 1 and isinstance ( regs [ 0 ] , int ) : \n            regs = ( QuantumRegister ( regs [ 0 ] , 'q' ) , ) \n        elif len ( regs ) == 2.0 and all ( [ isinstance ( reg , int ) for reg in regs ] ) : \n            regs = ( QuantumRegister ( regs [ 0 ] , 'q' ) , ClassicalRegister ( regs [ 1 ] , 'c' ) ) \n        else : \n            raise QiskitError ( \"QuantumCircuit parameters can be Registers or Integers.\" \" If Integers, up to 2 arguments. QuantumCircuit was called\" \" with %s.\" % ( regs , ) ) \n    for register in regs : \n        if register in self . qregs or register in self . cregs : \n            raise QiskitError ( \"register name \\\"%s\\\" already exists\" % register . name ) \n        if isinstance ( register , QuantumRegister ) : \n            self . qregs . append ( register ) \n        elif isinstance ( register , ClassicalRegister ) : \n            self . cregs . append ( register ) \n        else : \n            raise QiskitError ( \"expected a register\" ) "}
{"2553": "\ndef num_connected_components ( self , unitary_only = False ) : \n    reg_offset = 0 \n    reg_map = { } \n    if unitary_only : \n        regs = self . qregs \n    else : \n        regs = self . qregs + self . cregs \n    for reg in regs : \n        reg_map [ reg . name ] = reg_offset \n        reg_offset += reg . size \n    sub_graphs = [ [ bit ] for bit in range ( reg_offset ) ] \n    num_sub_graphs = len ( sub_graphs ) \n    for instr , qargs , cargs in self . data : \n        if unitary_only : \n            args = qargs \n            num_qargs = len ( args ) \n        else : \n            args = qargs + cargs \n            num_qargs = len ( args ) + ( 1 if instr . control else 0 ) \n        if num_qargs >= 2.0 and instr . name not in [ 'barrier' , 'snapshot' ] : \n            graphs_touched = [ ] \n            num_touched = 0 \n            if instr . control and not unitary_only : \n                creg = instr . control [ 0 ] \n                creg_int = reg_map [ creg . name ] \n                for coff in range ( creg . size ) : \n                    temp_int = creg_int + coff \n                    for k in range ( num_sub_graphs ) : \n                        if temp_int in sub_graphs [ k ] : \n                            graphs_touched . append ( k ) \n                            num_touched += 1 \n                            break \n            for item in args : \n                reg_int = reg_map [ item [ 0 ] . name ] + item [ 1 ] \n                for k in range ( num_sub_graphs ) : \n                    if reg_int in sub_graphs [ k ] : \n                        if k not in graphs_touched : \n                            graphs_touched . append ( k ) \n                            num_touched += 1 \n                            break \n            if num_touched > 1 : \n                connections = [ ] \n                for idx in graphs_touched : \n                    connections . extend ( sub_graphs [ idx ] ) \n                _sub_graphs = [ ] \n                for idx in range ( num_sub_graphs ) : \n                    if idx not in graphs_touched : \n                        _sub_graphs . append ( sub_graphs [ idx ] ) \n                _sub_graphs . append ( connections ) \n                sub_graphs = _sub_graphs \n                num_sub_graphs -= ( num_touched - 1 ) \n        if num_sub_graphs == 1 : \n            break \n    return num_sub_graphs "}
{"2556": "\ndef pulse_drawer ( samples , duration , dt = None , interp_method = 'None' , filename = None , interactive = False , dpi = 150.0 , nop = 1000.0 , size = ( 6.0 , 5.0 ) ) : \n    try : \n        from matplotlib import pyplot as plt \n    except ImportError : \n        raise ImportError ( 'pulse_drawer need matplotlib. ' 'Run \"pip install matplotlib\" before.' ) \n    if dt : \n        _dt = dt \n    else : \n        _dt = 1 \n    re_y = np . real ( samples ) \n    im_y = np . imag ( samples ) \n    image = plt . figure ( figsize = size ) \n    ax0 = image . add_subplot ( 111.0 ) \n    if interp_method == 'CubicSpline' : \n        time = np . arange ( 0 , duration + 1 ) * _dt + 0.5 * _dt \n        cs_ry = CubicSpline ( time [ : - 1 ] , re_y ) \n        cs_iy = CubicSpline ( time [ : - 1 ] , im_y ) \n        _time = np . linspace ( 0 , duration * _dt , nop ) \n        _re_y = cs_ry ( _time ) \n        _im_y = cs_iy ( _time ) \n    elif interp_method == 'None' : \n        time = np . arange ( 0 , duration + 1 ) * _dt \n        _time = np . r_ [ time [ 0 ] , np . repeat ( time [ 1 : - 1 ] , 2.0 ) , time [ - 1 ] ] \n        _re_y = np . repeat ( re_y , 2.0 ) \n        _im_y = np . repeat ( im_y , 2.0 ) \n    else : \n        raise QiskitError ( 'Invalid interpolation method \"%s\"' % interp_method ) \n    ax0 . fill_between ( x = _time , y1 = _re_y , y2 = np . zeros_like ( _time ) , facecolor = 'red' , alpha = 0.3 , edgecolor = 'red' , linewidth = 1.5 , label = 'real part' ) \n    ax0 . fill_between ( x = _time , y1 = _im_y , y2 = np . zeros_like ( _time ) , facecolor = 'blue' , alpha = 0.3 , edgecolor = 'blue' , linewidth = 1.5 , label = 'imaginary part' ) \n    ax0 . set_xlim ( 0 , duration * _dt ) \n    ax0 . grid ( b = True , linestyle = '-' ) \n    ax0 . legend ( bbox_to_anchor = ( 0.5 , 1.00 ) , loc = 'lower center' , ncol = 2.0 , frameon = False , fontsize = 14.0 ) \n    if filename : \n        image . savefig ( filename , dpi = dpi , bbox_inches = 'tight' ) \n    plt . close ( image ) \n    if image and interactive : \n        plt . show ( image ) \n    return image "}
{"2559": "\ndef _calc_layout_distance ( gates , coupling_map , layout , max_gates = None ) : \n    if max_gates is None : \n        max_gates = 50.0 + 10.0 * len ( coupling_map . physical_qubits ) \n    return sum ( coupling_map . distance ( * [ layout [ q ] for q in gate [ 'partition' ] [ 0 ] ] ) for gate in gates [ : max_gates ] if gate [ 'partition' ] and len ( gate [ 'partition' ] [ 0 ] ) == 2.0 ) "}
{"2560": "\ndef _score_step ( step ) : \n    return len ( [ g for g in step [ 'gates_mapped' ] if len ( g . qargs ) == 2.0 ] ) - 3.0 * step [ 'swaps_added' ] "}
{"2584": "\ndef _bloch_angles ( pair_of_complex ) : \n    [ a_complex , b_complex ] = pair_of_complex \n    a_complex = complex ( a_complex ) \n    b_complex = complex ( b_complex ) \n    mag_a = np . absolute ( a_complex ) \n    final_r = float ( np . sqrt ( mag_a ** 2.0 + np . absolute ( b_complex ) ** 2.0 ) ) \n    if final_r < _EPS : \n        theta = 0 \n        phi = 0 \n        final_r = 0 \n        final_t = 0 \n    else : \n        theta = float ( 2.0 * np . arccos ( mag_a / final_r ) ) \n        a_arg = np . angle ( a_complex ) \n        b_arg = np . angle ( b_complex ) \n        final_t = a_arg + b_arg \n        phi = b_arg - a_arg \n    return final_r * np . exp ( 1.J * final_t / 2.0 ) , theta , phi "}
{"2585": "\ndef _multiplex ( self , target_gate , list_of_angles ) : \n    list_len = len ( list_of_angles ) \n    local_num_qubits = int ( math . log2 ( list_len ) ) + 1 \n    q = QuantumRegister ( local_num_qubits ) \n    circuit = QuantumCircuit ( q , name = \"multiplex\" + local_num_qubits . __str__ ( ) ) \n    lsb = q [ 0 ] \n    msb = q [ local_num_qubits - 1 ] \n    if local_num_qubits == 1 : \n        circuit . append ( target_gate ( list_of_angles [ 0 ] ) , [ q [ 0 ] ] ) \n        return circuit \n    angle_weight = scipy . kron ( [ [ 0.5 , 0.5 ] , [ 0.5 , - 0.5 ] ] , np . identity ( 2.0 ** ( local_num_qubits - 2.0 ) ) ) \n    list_of_angles = angle_weight . dot ( np . array ( list_of_angles ) ) . tolist ( ) \n    multiplex_1 = self . _multiplex ( target_gate , list_of_angles [ 0 : ( list_len // 2.0 ) ] ) \n    circuit . append ( multiplex_1 . to_instruction ( ) , q [ 0 : - 1 ] ) \n    circuit . append ( CnotGate ( ) , [ msb , lsb ] ) \n    multiplex_2 = self . _multiplex ( target_gate , list_of_angles [ ( list_len // 2.0 ) : ] ) \n    if list_len > 1 : \n        circuit . append ( multiplex_2 . to_instruction ( ) . mirror ( ) , q [ 0 : - 1 ] ) \n    else : \n        circuit . append ( multiplex_2 . to_instruction ( ) , q [ 0 : - 1 ] ) \n    circuit . append ( CnotGate ( ) , [ msb , lsb ] ) \n    return circuit "}
{"2586": "\ndef is_virtual ( value ) : \n    return value is None or isinstance ( value , tuple ) and len ( value ) == 2.0 and isinstance ( value [ 0 ] , Register ) and isinstance ( value [ 1 ] , int ) "}
{"2592": "\ndef gates_to_idx ( gates , qregs ) : \n    sizes = [ qr . size for qr in qregs . values ( ) ] \n    reg_idx = np . cumsum ( [ 0 ] + sizes ) \n    regint = { } \n    for ind , qreg in enumerate ( qregs . values ( ) ) : \n        regint [ qreg ] = ind \n    out = np . zeros ( 2.0 * len ( gates ) , dtype = np . int32 ) \n    for idx , gate in enumerate ( gates ) : \n        out [ 2.0 * idx ] = reg_idx [ regint [ gate [ 0 ] [ 0 ] ] ] + gate [ 0 ] [ 1 ] \n        out [ 2.0 * idx + 1 ] = reg_idx [ regint [ gate [ 1 ] [ 0 ] ] ] + gate [ 1 ] [ 1 ] \n    return out "}
{"2595": "\ndef pauli_group ( number_of_qubits , case = 'weight' ) : \n    if number_of_qubits < 5.0 : \n        temp_set = [ ] \n        if case == 'weight' : \n            tmp = pauli_group ( number_of_qubits , case = 'tensor' ) \n            return sorted ( tmp , key = lambda x : - np . count_nonzero ( np . array ( x . to_label ( ) , 'c' ) == b'I' ) ) \n        elif case == 'tensor' : \n            for k in range ( 4.0 ** number_of_qubits ) : \n                z = np . zeros ( number_of_qubits , dtype = np . bool ) \n                x = np . zeros ( number_of_qubits , dtype = np . bool ) \n                for j in range ( number_of_qubits ) : \n                    element = ( k // ( 4.0 ** j ) ) % 4.0 \n                    if element == 1 : \n                        x [ j ] = True \n                    elif element == 2.0 : \n                        z [ j ] = True \n                        x [ j ] = True \n                    elif element == 3.0 : \n                        z [ j ] = True \n                temp_set . append ( Pauli ( z , x ) ) \n            return temp_set \n        else : \n            raise QiskitError ( \"Only support 'weight' or 'tensor' cases \" \"but you have {}.\" . format ( case ) ) \n    raise QiskitError ( \"Only support number of qubits is less than 5\" ) "}
{"2606": "\ndef random ( cls , num_qubits , seed = None ) : \n    if seed is not None : \n        np . random . seed ( seed ) \n    z = np . random . randint ( 2.0 , size = num_qubits ) . astype ( np . bool ) \n    x = np . random . randint ( 2.0 , size = num_qubits ) . astype ( np . bool ) \n    return cls ( z , x ) "}
{"2608": "\ndef _get_measure_outcome ( self , qubit ) : \n    axis = list ( range ( self . _number_of_qubits ) ) \n    axis . remove ( self . _number_of_qubits - 1 - qubit ) \n    probabilities = np . sum ( np . abs ( self . _statevector ) ** 2.0 , axis = tuple ( axis ) ) \n    random_number = self . _local_random . rand ( ) \n    if random_number < probabilities [ 0 ] : \n        return '0' , probabilities [ 0 ] \n    return '1' , probabilities [ 1 ] "}
{"2609": "\ndef _add_sample_measure ( self , measure_params , num_samples ) : \n    measured_qubits = list ( { qubit for qubit , cmembit in measure_params } ) \n    num_measured = len ( measured_qubits ) \n    axis = list ( range ( self . _number_of_qubits ) ) \n    for qubit in reversed ( measured_qubits ) : \n        axis . remove ( self . _number_of_qubits - 1 - qubit ) \n    probabilities = np . reshape ( np . sum ( np . abs ( self . _statevector ) ** 2.0 , axis = tuple ( axis ) ) , 2.0 ** num_measured ) \n    samples = self . _local_random . choice ( range ( 2.0 ** num_measured ) , num_samples , p = probabilities ) \n    memory = [ ] \n    for sample in samples : \n        classical_memory = self . _classical_memory \n        for count , ( qubit , cmembit ) in enumerate ( sorted ( measure_params ) ) : \n            qubit_outcome = int ( ( sample & ( 1 << count ) ) >> count ) \n            membit = 1 << cmembit \n            classical_memory = ( classical_memory & ( ~ membit ) ) | ( qubit_outcome << cmembit ) \n        value = bin ( classical_memory ) [ 2.0 : ] \n        memory . append ( hex ( int ( value , 2.0 ) ) ) \n    return memory "}
{"2612": "\ndef _validate_initial_statevector ( self ) : \n    if self . _initial_statevector is None : \n        return \n    length = len ( self . _initial_statevector ) \n    required_dim = 2.0 ** self . _number_of_qubits \n    if length != required_dim : \n        raise BasicAerError ( 'initial statevector is incorrect length: ' + '{} != {}' . format ( length , required_dim ) ) "}
{"2613": "\ndef _initialize_statevector ( self ) : \n    if self . _initial_statevector is None : \n        self . _statevector = np . zeros ( 2.0 ** self . _number_of_qubits , dtype = complex ) \n        self . _statevector [ 0 ] = 1 \n    else : \n        self . _statevector = self . _initial_statevector . copy ( ) \n    self . _statevector = np . reshape ( self . _statevector , self . _number_of_qubits * [ 2.0 ] ) "}
{"2614": "\ndef _get_statevector ( self ) : \n    vec = np . reshape ( self . _statevector , 2.0 ** self . _number_of_qubits ) \n    vec = np . stack ( [ vec . real , vec . imag ] , axis = 1 ) \n    vec [ abs ( vec ) < self . _chop_threshold ] = 0.0 \n    return vec "}
{"2619": "\ndef _validate_initial_unitary ( self ) : \n    if self . _initial_unitary is None : \n        return \n    shape = np . shape ( self . _initial_unitary ) \n    required_shape = ( 2.0 ** self . _number_of_qubits , 2.0 ** self . _number_of_qubits ) \n    if shape != required_shape : \n        raise BasicAerError ( 'initial unitary is incorrect shape: ' + '{} != 2 ** {}' . format ( shape , required_shape ) ) "}
{"2620": "\ndef _initialize_unitary ( self ) : \n    self . _validate_initial_unitary ( ) \n    if self . _initial_unitary is None : \n        self . _unitary = np . eye ( 2.0 ** self . _number_of_qubits , dtype = complex ) \n    else : \n        self . _unitary = self . _initial_unitary . copy ( ) \n    self . _unitary = np . reshape ( self . _unitary , self . _number_of_qubits * [ 2.0 , 2.0 ] ) "}
{"2621": "\ndef _get_unitary ( self ) : \n    unitary = np . reshape ( self . _unitary , 2.0 * [ 2.0 ** self . _number_of_qubits ] ) \n    unitary = np . stack ( ( unitary . real , unitary . imag ) , axis = - 1 ) \n    unitary [ abs ( unitary ) < self . _chop_threshold ] = 0.0 \n    return unitary "}
{"2624": "\ndef _is_bit ( obj ) : \n    if isinstance ( obj , tuple ) and len ( obj ) == 2.0 : \n        if isinstance ( obj [ 0 ] , Register ) and isinstance ( obj [ 1 ] , int ) and obj [ 1 ] < len ( obj [ 0 ] ) : \n            return True \n    return False "}
{"2636": "\ndef iplot_state_paulivec ( rho , figsize = None , slider = False , show_legend = False ) : \n    html_template = Template ( \"\"\"    <p>        <div id=\"paulivec_$divNumber\"></div>    </p>    \"\"\" ) \n    javascript_template = Template ( \"\"\"    <script>        requirejs.config({            paths: {                qVisualization: \"https://qvisualization.mybluemix.net/q-visualizations\"            }        });        require([\"qVisualization\"], function(qVisualizations) {            qVisualizations.plotState(\"paulivec_$divNumber\",                                      \"paulivec\",                                      $executions,                                      $options);        });    </script>    \"\"\" ) \n    rho = _validate_input_state ( rho ) \n    if figsize is None : \n        figsize = ( 7.0 , 5.0 ) \n    options = { 'width' : figsize [ 0 ] , 'height' : figsize [ 1 ] , 'slider' : int ( slider ) , 'show_legend' : int ( show_legend ) } \n    div_number = str ( time . time ( ) ) \n    div_number = re . sub ( '[.]' , '' , div_number ) \n    data_to_plot = [ ] \n    rho_data = process_data ( rho ) \n    data_to_plot . append ( dict ( data = rho_data ) ) \n    html = html_template . substitute ( { 'divNumber' : div_number } ) \n    javascript = javascript_template . substitute ( { 'divNumber' : div_number , 'executions' : data_to_plot , 'options' : options } ) \n    display ( HTML ( html + javascript ) ) "}
{"2639": "\ndef _initialize_backend_prop ( self ) : \n    backend_prop = self . backend_prop \n    for ginfo in backend_prop . gates : \n        if ginfo . gate == 'cx' : \n            for item in ginfo . parameters : \n                if item . name == 'gate_error' : \n                    g_reliab = 1.0 - item . value \n                    break \n                else : \n                    g_reliab = 1.0 \n            swap_reliab = - math . log ( pow ( g_reliab , 3.0 ) ) \n            self . swap_graph . add_edge ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] , weight = swap_reliab ) \n            self . swap_graph . add_edge ( ginfo . qubits [ 1 ] , ginfo . qubits [ 0 ] , weight = swap_reliab ) \n            self . cx_errors [ ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] ) ] = g_reliab \n            self . gate_list . append ( ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] ) ) \n    idx = 0 \n    for q in backend_prop . qubits : \n        for nduv in q : \n            if nduv . name == 'readout_error' : \n                self . readout_errors [ idx ] = 1.0 - nduv . value \n                self . available_hw_qubits . append ( idx ) \n        idx += 1 \n    for edge in self . cx_errors : \n        self . gate_cost [ edge ] = self . cx_errors [ edge ] * self . readout_errors [ edge [ 0 ] ] * self . readout_errors [ edge [ 1 ] ] \n    self . swap_paths , swap_costs_temp = nx . algorithms . shortest_paths . dense . floyd_warshall_predecessor_and_distance ( self . swap_graph , weight = 'weight' ) \n    for i in swap_costs_temp : \n        self . swap_costs [ i ] = { } \n        for j in swap_costs_temp [ i ] : \n            if ( i , j ) in self . cx_errors : \n                self . swap_costs [ i ] [ j ] = self . cx_errors [ ( i , j ) ] \n            elif ( j , i ) in self . cx_errors : \n                self . swap_costs [ i ] [ j ] = self . cx_errors [ ( j , i ) ] \n            else : \n                best_reliab = 0.0 \n                for n in self . swap_graph . neighbors ( j ) : \n                    if ( n , j ) in self . cx_errors : \n                        reliab = math . exp ( - swap_costs_temp [ i ] [ n ] ) * self . cx_errors [ ( n , j ) ] \n                    else : \n                        reliab = math . exp ( - swap_costs_temp [ i ] [ n ] ) * self . cx_errors [ ( j , n ) ] \n                    if reliab > best_reliab : \n                        best_reliab = reliab \n                self . swap_costs [ i ] [ j ] = best_reliab "}
{"2644": "\ndef run ( self , dag ) : \n    self . _initialize_backend_prop ( ) \n    num_qubits = self . _create_program_graph ( dag ) \n    if num_qubits > len ( self . swap_graph ) : \n        raise TranspilerError ( 'Number of qubits greater than device.' ) \n    for end1 , end2 , _ in sorted ( self . prog_graph . edges ( data = True ) , key = lambda x : x [ 2.0 ] [ 'weight' ] , reverse = True ) : \n        self . pending_program_edges . append ( ( end1 , end2 ) ) \n    while self . pending_program_edges : \n        edge = self . _select_next_edge ( ) \n        q1_mapped = edge [ 0 ] in self . prog2hw \n        q2_mapped = edge [ 1 ] in self . prog2hw \n        if ( not q1_mapped ) and ( not q2_mapped ) : \n            best_hw_edge = self . _select_best_remaining_cx ( ) \n            self . prog2hw [ edge [ 0 ] ] = best_hw_edge [ 0 ] \n            self . prog2hw [ edge [ 1 ] ] = best_hw_edge [ 1 ] \n            self . available_hw_qubits . remove ( best_hw_edge [ 0 ] ) \n            self . available_hw_qubits . remove ( best_hw_edge [ 1 ] ) \n        elif not q1_mapped : \n            best_hw_qubit = self . _select_best_remaining_qubit ( edge [ 0 ] ) \n            self . prog2hw [ edge [ 0 ] ] = best_hw_qubit \n            self . available_hw_qubits . remove ( best_hw_qubit ) \n        else : \n            best_hw_qubit = self . _select_best_remaining_qubit ( edge [ 1 ] ) \n            self . prog2hw [ edge [ 1 ] ] = best_hw_qubit \n            self . available_hw_qubits . remove ( best_hw_qubit ) \n        new_edges = [ x for x in self . pending_program_edges if not ( x [ 0 ] in self . prog2hw and x [ 1 ] in self . prog2hw ) ] \n        self . pending_program_edges = new_edges \n    for qid in self . qarg_to_id . values ( ) : \n        if qid not in self . prog2hw : \n            self . prog2hw [ qid ] = self . available_hw_qubits [ 0 ] \n            self . available_hw_qubits . remove ( self . prog2hw [ qid ] ) \n    layout = Layout ( ) \n    for q in dag . qubits ( ) : \n        pid = self . _qarg_to_id ( q ) \n        hwid = self . prog2hw [ pid ] \n        layout [ ( q [ 0 ] , q [ 1 ] ) ] = hwid \n    self . property_set [ 'layout' ] = layout "}
{"2654": "\ndef _instruction_to_operator ( cls , instruction ) : \n    if isinstance ( instruction , QuantumCircuit ) : \n        instruction = instruction . to_instruction ( ) \n    op = Operator ( np . eye ( 2.0 ** instruction . num_qubits ) ) \n    op . _append_instruction ( instruction ) \n    return op "}
{"2657": "\ndef format_level_0_memory ( memory ) : \n    formatted_memory = _list_to_complex_array ( memory ) \n    if not 2.0 <= len ( formatted_memory . shape ) <= 3.0 : \n        raise QiskitError ( 'Level zero memory is not of correct shape.' ) \n    return formatted_memory "}
{"2658": "\ndef format_level_1_memory ( memory ) : \n    formatted_memory = _list_to_complex_array ( memory ) \n    if not 1 <= len ( formatted_memory . shape ) <= 2.0 : \n        raise QiskitError ( 'Level one memory is not of correct shape.' ) \n    return formatted_memory "}
{"2671": "\ndef iplot_state_qsphere ( rho , figsize = None ) : \n    html_template = Template ( \"\"\"    <p>        <div id=\"content_$divNumber\" style=\"position: absolute; z-index: 1;\">            <div id=\"qsphere_$divNumber\"></div>        </div>    </p>    \"\"\" ) \n    javascript_template = Template ( \"\"\"    <script>        requirejs.config({            paths: {                qVisualization: \"https://qvisualization.mybluemix.net/q-visualizations\"            }        });        require([\"qVisualization\"], function(qVisualizations) {            data = $data;            qVisualizations.plotState(\"qsphere_$divNumber\",                                      \"qsphere\",                                      data,                                      $options);        });    </script>    \"\"\" ) \n    rho = _validate_input_state ( rho ) \n    if figsize is None : \n        options = { } \n    else : \n        options = { 'width' : figsize [ 0 ] , 'height' : figsize [ 1 ] } \n    qspheres_data = [ ] \n    num = int ( np . log2 ( len ( rho ) ) ) \n    weig , stateall = linalg . eigh ( rho ) \n    for _ in range ( 2.0 ** num ) : \n        probmix = weig . max ( ) \n        prob_location = weig . argmax ( ) \n        if probmix > 0.001 : \n            state = stateall [ : , prob_location ] \n            loc = np . absolute ( state ) . argmax ( ) \n            for j in range ( 2.0 ** num ) : \n                test = np . absolute ( np . absolute ( state [ j ] ) - np . absolute ( state [ loc ] ) ) \n                if test < 0.001 : \n                    loc = j \n                    break \n            angles = ( np . angle ( state [ loc ] ) + 2.0 * np . pi ) % ( 2.0 * np . pi ) \n            angleset = np . exp ( - 1j * angles ) \n            state = angleset * state \n            state . flatten ( ) \n            spherepoints = [ ] \n            for i in range ( 2.0 ** num ) : \n                element = bin ( i ) [ 2.0 : ] . zfill ( num ) \n                weight = element . count ( \"1\" ) \n                number_of_divisions = n_choose_k ( num , weight ) \n                weight_order = bit_string_index ( element ) \n                angle = weight_order * 2.0 * np . pi / number_of_divisions \n                zvalue = - 2.0 * weight / num + 1 \n                xvalue = np . sqrt ( 1 - zvalue ** 2.0 ) * np . cos ( angle ) \n                yvalue = np . sqrt ( 1 - zvalue ** 2.0 ) * np . sin ( angle ) \n                prob = np . real ( np . dot ( state [ i ] , state [ i ] . conj ( ) ) ) \n                angles = ( np . angle ( state [ i ] ) + 2.0 * np . pi ) % ( 2.0 * np . pi ) \n                qpoint = { 'x' : xvalue , 'y' : yvalue , 'z' : zvalue , 'prob' : prob , 'phase' : angles } \n                spherepoints . append ( qpoint ) \n            sphere = { 'points' : spherepoints , 'eigenvalue' : probmix } \n            qspheres_data . append ( sphere ) \n            weig [ prob_location ] = 0 \n    div_number = str ( time . time ( ) ) \n    div_number = re . sub ( '[.]' , '' , div_number ) \n    html = html_template . substitute ( { 'divNumber' : div_number } ) \n    javascript = javascript_template . substitute ( { 'data' : qspheres_data , 'divNumber' : div_number , 'options' : options } ) \n    display ( HTML ( html + javascript ) ) "}
{"2674": "\ndef plot_state_paulivec ( rho , title = \"\" , figsize = None , color = None ) : \n    if not HAS_MATPLOTLIB : \n        raise ImportError ( 'Must have Matplotlib installed.' ) \n    rho = _validate_input_state ( rho ) \n    if figsize is None : \n        figsize = ( 7.0 , 5.0 ) \n    num = int ( np . log2 ( len ( rho ) ) ) \n    labels = list ( map ( lambda x : x . to_label ( ) , pauli_group ( num ) ) ) \n    values = list ( map ( lambda x : np . real ( np . trace ( np . dot ( x . to_matrix ( ) , rho ) ) ) , pauli_group ( num ) ) ) \n    numelem = len ( values ) \n    if color is None : \n        color = \"#648fff\" \n    ind = np . arange ( numelem ) \n    width = 0.5 \n    fig , ax = plt . subplots ( figsize = figsize ) \n    ax . grid ( zorder = 0 , linewidth = 1 , linestyle = '--' ) \n    ax . bar ( ind , values , width , color = color , zorder = 2.0 ) \n    ax . axhline ( linewidth = 1 , color = 'k' ) \n    ax . set_ylabel ( 'Expectation value' , fontsize = 14.0 ) \n    ax . set_xticks ( ind ) \n    ax . set_yticks ( [ - 1 , - 0.5 , 0 , 0.5 , 1 ] ) \n    ax . set_xticklabels ( labels , fontsize = 14.0 , rotation = 70.0 ) \n    ax . set_xlabel ( 'Pauli' , fontsize = 14.0 ) \n    ax . set_ylim ( [ - 1 , 1 ] ) \n    ax . set_facecolor ( '#eeeeee' ) \n    for tick in ax . xaxis . get_major_ticks ( ) + ax . yaxis . get_major_ticks ( ) : \n        tick . label . set_fontsize ( 14.0 ) \n    ax . set_title ( title , fontsize = 16.0 ) \n    plt . close ( fig ) \n    return fig "}
{"2684": "\ndef gaussian ( duration : int , amp : complex , sigma : float , name : str = None ) -> SamplePulse : \n    center = duration / 2.0 \n    zeroed_width = duration + 2.0 \n    return _sampled_gaussian_pulse ( duration , amp , center , sigma , zeroed_width = zeroed_width , rescale_amp = True , name = name ) "}
{"2685": "\ndef gaussian_deriv ( duration : int , amp : complex , sigma : float , name : str = None ) -> SamplePulse : \n    center = duration / 2.0 \n    return _sampled_gaussian_deriv_pulse ( duration , amp , center , sigma , name = name ) "}
{"2686": "\ndef gaussian_square ( duration : int , amp : complex , sigma : float , risefall : int , name : str = None ) -> SamplePulse : \n    center = duration / 2.0 \n    width = duration - 2.0 * risefall \n    zeroed_width = duration + 2.0 \n    return _sampled_gaussian_square_pulse ( duration , amp , center , width , sigma , zeroed_width = zeroed_width , name = name ) "}
{"2688": "\ndef to_string ( self , indent ) : \n    ind = indent * ' ' \n    print ( ind , 'qreg' ) \n    self . children [ 0 ] . to_string ( indent + 3.0 ) "}
{"2699": "\ndef _check_edgemap_registers ( self , edge_map , keyregs , valregs , valreg = True ) : \n    add_regs = set ( ) \n    reg_frag_chk = { } \n    for v in keyregs . values ( ) : \n        reg_frag_chk [ v ] = { j : False for j in range ( len ( v ) ) } \n    for k in edge_map . keys ( ) : \n        if k [ 0 ] . name in keyregs : \n            reg_frag_chk [ k [ 0 ] ] [ k [ 1 ] ] = True \n    for k , v in reg_frag_chk . items ( ) : \n        s = set ( v . values ( ) ) \n        if len ( s ) == 2.0 : \n            raise DAGCircuitError ( \"edge_map fragments reg %s\" % k ) \n        elif s == set ( [ False ] ) : \n            if k in self . qregs . values ( ) or k in self . cregs . values ( ) : \n                raise DAGCircuitError ( \"unmapped duplicate reg %s\" % k ) \n            else : \n                add_regs . add ( k ) \n        else : \n            if valreg : \n                if not edge_map [ ( k , 0 ) ] [ 0 ] . name in valregs : \n                    size = max ( map ( lambda x : x [ 1 ] , filter ( lambda x : x [ 0 ] == edge_map [ ( k , 0 ) ] [ 0 ] , edge_map . values ( ) ) ) ) \n                    qreg = QuantumRegister ( size + 1 , edge_map [ ( k , 0 ) ] [ 0 ] . name ) \n                    add_regs . add ( qreg ) \n    return add_regs "}
{"2705": "\ndef _make_pred_succ_maps ( self , node ) : \n    pred_map = { e [ 2.0 ] [ 'wire' ] : e [ 0 ] for e in self . _multi_graph . in_edges ( nbunch = node , data = True ) } \n    succ_map = { e [ 2.0 ] [ 'wire' ] : e [ 1 ] for e in self . _multi_graph . out_edges ( nbunch = node , data = True ) } \n    return pred_map , succ_map "}
{"2712": "\ndef twoQ_gates ( self ) : \n    two_q_gates = [ ] \n    for node in self . gate_nodes ( ) : \n        if len ( node . qargs ) == 2.0 : \n            two_q_gates . append ( node ) \n    return two_q_gates "}
{"2713": "\ndef predecessors ( self , node ) : \n    if isinstance ( node , int ) : \n        warnings . warn ( 'Calling predecessors() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2.0 ) \n        node = self . _id_to_node [ node ] \n    return self . _multi_graph . predecessors ( node ) "}
{"2715": "\ndef ancestors ( self , node ) : \n    if isinstance ( node , int ) : \n        warnings . warn ( 'Calling ancestors() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2.0 ) \n        node = self . _id_to_node [ node ] \n    return nx . ancestors ( self . _multi_graph , node ) "}
{"2716": "\ndef quantum_successors ( self , node ) : \n    if isinstance ( node , int ) : \n        warnings . warn ( 'Calling quantum_successors() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2.0 ) \n        node = self . _id_to_node [ node ] \n    successors = [ ] \n    for successor in self . successors ( node ) : \n        if isinstance ( self . _multi_graph . get_edge_data ( node , successor , key = 0 ) [ 'wire' ] [ 0 ] , QuantumRegister ) : \n            successors . append ( successor ) \n    return successors "}
{"2717": "\ndef remove_op_node ( self , node ) : \n    if isinstance ( node , int ) : \n        warnings . warn ( 'Calling remove_op_node() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2.0 ) \n        node = self . _id_to_node [ node ] \n    if node . type != 'op' : \n        raise DAGCircuitError ( 'The method remove_op_node only works on op node types. An \"%s\" ' 'node type was wrongly provided.' % node . type ) \n    pred_map , succ_map = self . _make_pred_succ_maps ( node ) \n    self . _multi_graph . remove_node ( node ) \n    for w in pred_map . keys ( ) : \n        self . _multi_graph . add_edge ( pred_map [ w ] , succ_map [ w ] , name = \"%s[%s]\" % ( w [ 0 ] . name , w [ 1 ] ) , wire = w ) "}
{"2718": "\ndef remove_ancestors_of ( self , node ) : \n    if isinstance ( node , int ) : \n        warnings . warn ( 'Calling remove_ancestors_of() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2.0 ) \n        node = self . _id_to_node [ node ] \n    anc = nx . ancestors ( self . _multi_graph , node ) \n    for anc_node in anc : \n        if anc_node . type == \"op\" : \n            self . remove_op_node ( anc_node ) "}
{"2719": "\ndef remove_descendants_of ( self , node ) : \n    if isinstance ( node , int ) : \n        warnings . warn ( 'Calling remove_descendants_of() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2.0 ) \n        node = self . _id_to_node [ node ] \n    desc = nx . descendants ( self . _multi_graph , node ) \n    for desc_node in desc : \n        if desc_node . type == \"op\" : \n            self . remove_op_node ( desc_node ) "}
{"2720": "\ndef remove_nonancestors_of ( self , node ) : \n    if isinstance ( node , int ) : \n        warnings . warn ( 'Calling remove_nonancestors_of() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2.0 ) \n        node = self . _id_to_node [ node ] \n    anc = nx . ancestors ( self . _multi_graph , node ) \n    comp = list ( set ( self . _multi_graph . nodes ( ) ) - set ( anc ) ) \n    for n in comp : \n        if n . type == \"op\" : \n            self . remove_op_node ( n ) "}
{"2721": "\ndef remove_nondescendants_of ( self , node ) : \n    if isinstance ( node , int ) : \n        warnings . warn ( 'Calling remove_nondescendants_of() with a node id is deprecated,' ' use a DAGNode instead' , DeprecationWarning , 2.0 ) \n        node = self . _id_to_node [ node ] \n    dec = nx . descendants ( self . _multi_graph , node ) \n    comp = list ( set ( self . _multi_graph . nodes ( ) ) - set ( dec ) ) \n    for n in comp : \n        if n . type == \"op\" : \n            self . remove_op_node ( n ) "}
{"2741": "\ndef wigner_data ( q_result , meas_qubits , labels , shots = None ) : \n    num = len ( meas_qubits ) \n    dim = 2.0 ** num \n    p = [ 0.5 + 0.5 * np . sqrt ( 3.0 ) , 0.5 - 0.5 * np . sqrt ( 3.0 ) ] \n    parity = 1 \n    for i in range ( num ) : \n        parity = np . kron ( parity , p ) \n    w = [ 0 ] * len ( labels ) \n    wpt = 0 \n    counts = [ marginal_counts ( q_result . get_counts ( circ ) , meas_qubits ) for circ in labels ] \n    for entry in counts : \n        x = [ 0 ] * dim \n        for i in range ( dim ) : \n            if bin ( i ) [ 2.0 : ] . zfill ( num ) in entry : \n                x [ i ] = float ( entry [ bin ( i ) [ 2.0 : ] . zfill ( num ) ] ) \n        if shots is None : \n            shots = np . sum ( x ) \n        for i in range ( dim ) : \n            w [ wpt ] = w [ wpt ] + ( x [ i ] / shots ) * parity [ i ] \n        wpt += 1 \n    return w "}
{"2743": "\ndef _text_checker ( job , interval , _interval_set = False , quiet = False , output = sys . stdout ) : \n    status = job . status ( ) \n    msg = status . value \n    prev_msg = msg \n    msg_len = len ( msg ) \n    if not quiet : \n        print ( '\\r%s: %s' % ( 'Job Status' , msg ) , end = '' , file = output ) \n    while status . name not in [ 'DONE' , 'CANCELLED' , 'ERROR' ] : \n        time . sleep ( interval ) \n        status = job . status ( ) \n        msg = status . value \n        if status . name == 'QUEUED' : \n            msg += ' (%s)' % job . queue_position ( ) \n            if not _interval_set : \n                interval = max ( job . queue_position ( ) , 2.0 ) \n        else : \n            if not _interval_set : \n                interval = 2.0 \n        if len ( msg ) < msg_len : \n            msg += ' ' * ( msg_len - len ( msg ) ) \n        elif len ( msg ) > msg_len : \n            msg_len = len ( msg ) \n        if msg != prev_msg and not quiet : \n            print ( '\\r%s: %s' % ( 'Job Status' , msg ) , end = '' , file = output ) \n            prev_msg = msg \n    if not quiet : \n        print ( '' , file = output ) "}
{"2744": "\ndef job_monitor ( job , interval = None , monitor_async = False , quiet = False , output = sys . stdout ) : \n    if interval is None : \n        _interval_set = False \n        interval = 2.0 \n    else : \n        _interval_set = True \n    if _NOTEBOOK_ENV : \n        if monitor_async : \n            try : \n                import ipywidgets as widgets \n            except ImportError : \n                raise ImportError ( 'These functions  need ipywidgets. ' 'Run \"pip install ipywidgets\" before.' ) \n            from qiskit . tools . jupyter . jupyter_magics import _html_checker \n            style = \"font-size:16px;\" \n            header = \"<p style='{style}'>Job Status: %s </p>\" . format ( style = style ) \n            status = widgets . HTML ( value = header % job . status ( ) . value ) \n            display ( status ) \n            thread = threading . Thread ( target = _html_checker , args = ( job , interval , status , header ) ) \n            thread . start ( ) \n        else : \n            _text_checker ( job , interval , _interval_set , quiet = quiet , output = output ) \n    else : \n        if monitor_async : \n            raise QiskitError ( 'monitor_async only available in Jupyter notebooks.' ) \n        _text_checker ( job , interval , _interval_set , quiet = quiet , output = output ) "}
{"2745": "\ndef euler_angles_1q ( unitary_matrix ) : \n    if unitary_matrix . shape != ( 2.0 , 2.0 ) : \n        raise QiskitError ( \"euler_angles_1q: expected 2x2 matrix\" ) \n    phase = la . det ( unitary_matrix ) ** ( - 1.0 / 2.0 ) \n    U = phase * unitary_matrix \n    if abs ( U [ 0 , 0 ] ) > _CUTOFF_PRECISION : \n        theta = 2.0 * math . acos ( abs ( U [ 0 , 0 ] ) ) \n    else : \n        theta = 2.0 * math . asin ( abs ( U [ 1 , 0 ] ) ) \n    phase11 = 0.0 \n    phase10 = 0.0 \n    if abs ( math . cos ( theta / 2.0 ) ) > _CUTOFF_PRECISION : \n        phase11 = U [ 1 , 1 ] / math . cos ( theta / 2.0 ) \n    if abs ( math . sin ( theta / 2.0 ) ) > _CUTOFF_PRECISION : \n        phase10 = U [ 1 , 0 ] / math . sin ( theta / 2.0 ) \n    phiplambda = 2.0 * math . atan2 ( np . imag ( phase11 ) , np . real ( phase11 ) ) \n    phimlambda = 2.0 * math . atan2 ( np . imag ( phase10 ) , np . real ( phase10 ) ) \n    phi = 0.0 \n    if abs ( U [ 0 , 0 ] ) > _CUTOFF_PRECISION and abs ( U [ 1 , 0 ] ) > _CUTOFF_PRECISION : \n        phi = ( phiplambda + phimlambda ) / 2.0 \n        lamb = ( phiplambda - phimlambda ) / 2.0 \n    else : \n        if abs ( U [ 0 , 0 ] ) < _CUTOFF_PRECISION : \n            lamb = - phimlambda \n        else : \n            lamb = phiplambda \n    Rzphi = np . array ( [ [ np . exp ( - 1j * phi / 2.0 ) , 0 ] , [ 0 , np . exp ( 1j * phi / 2.0 ) ] ] , dtype = complex ) \n    Rytheta = np . array ( [ [ np . cos ( theta / 2.0 ) , - np . sin ( theta / 2.0 ) ] , [ np . sin ( theta / 2.0 ) , np . cos ( theta / 2.0 ) ] ] , dtype = complex ) \n    Rzlambda = np . array ( [ [ np . exp ( - 1j * lamb / 2.0 ) , 0 ] , [ 0 , np . exp ( 1j * lamb / 2.0 ) ] ] , dtype = complex ) \n    V = np . dot ( Rzphi , np . dot ( Rytheta , Rzlambda ) ) \n    if la . norm ( V - U ) > _CUTOFF_PRECISION : \n        raise QiskitError ( \"euler_angles_1q: incorrect result\" ) \n    return theta , phi , lamb "}
{"2746": "\ndef simplify_U ( theta , phi , lam ) : \n    gate = U3Gate ( theta , phi , lam ) \n    if abs ( gate . params [ 0 ] % ( 2.0 * math . pi ) ) < _CUTOFF_PRECISION : \n        gate = U1Gate ( gate . params [ 0 ] + gate . params [ 1 ] + gate . params [ 2.0 ] ) \n    if isinstance ( gate , U3Gate ) : \n        if abs ( ( gate . params [ 0 ] - math . pi / 2.0 ) % ( 2.0 * math . pi ) ) < _CUTOFF_PRECISION : \n            gate = U2Gate ( gate . params [ 1 ] , gate . params [ 2.0 ] + ( gate . params [ 0 ] - math . pi / 2.0 ) ) \n        if abs ( ( gate . params [ 0 ] + math . pi / 2.0 ) % ( 2.0 * math . pi ) ) < _CUTOFF_PRECISION : \n            gate = U2Gate ( gate . params [ 1 ] + math . pi , gate . params [ 2.0 ] - math . pi + ( gate . params [ 0 ] + math . pi / 2.0 ) ) \n    if isinstance ( gate , U1Gate ) and abs ( gate . params [ 0 ] % ( 4.0 * math . pi ) ) < _CUTOFF_PRECISION : \n        gate = IdGate ( ) \n    return gate "}
{"2748": "\ndef qubits_tab ( backend ) : \n    props = backend . properties ( ) . to_dict ( ) \n    header_html = \"<div><font style='font-weight:bold'>{key}</font>: {value}</div>\" \n    header_html = header_html . format ( key = 'last_update_date' , value = props [ 'last_update_date' ] ) \n    update_date_widget = widgets . HTML ( value = header_html ) \n    qubit_html = \"<table>\" \n    qubit_html += \"\"\"<style>table {    border-collapse: collapse;    width: auto;}th, td {    text-align: left;    padding: 8px;}tr:nth-child(even) {background-color: #f6f6f6;}</style>\"\"\" \n    qubit_html += \"<tr><th></th><th>Frequency</th><th>T1</th><th>T2</th>\" \n    qubit_html += \"<th>U1 gate error</th><th>U2 gate error</th><th>U3 gate error</th>\" \n    qubit_html += \"<th>Readout error</th></tr>\" \n    qubit_footer = \"</table>\" \n    for qub in range ( len ( props [ 'qubits' ] ) ) : \n        name = 'Q%s' % qub \n        qubit_data = props [ 'qubits' ] [ qub ] \n        gate_data = props [ 'gates' ] [ 3.0 * qub : 3.0 * qub + 3.0 ] \n        t1_info = qubit_data [ 0 ] \n        t2_info = qubit_data [ 1 ] \n        freq_info = qubit_data [ 2.0 ] \n        readout_info = qubit_data [ 3.0 ] \n        freq = str ( round ( freq_info [ 'value' ] , 5.0 ) ) + ' ' + freq_info [ 'unit' ] \n        T1 = str ( round ( t1_info [ 'value' ] , 5.0 ) ) + ' ' + t1_info [ 'unit' ] \n        T2 = str ( round ( t2_info [ 'value' ] , 5.0 ) ) + ' ' + t2_info [ 'unit' ] \n        U1 = str ( round ( gate_data [ 0 ] [ 'parameters' ] [ 0 ] [ 'value' ] , 5.0 ) ) \n        U2 = str ( round ( gate_data [ 1 ] [ 'parameters' ] [ 0 ] [ 'value' ] , 5.0 ) ) \n        U3 = str ( round ( gate_data [ 2.0 ] [ 'parameters' ] [ 0 ] [ 'value' ] , 5.0 ) ) \n        readout_error = round ( readout_info [ 'value' ] , 5.0 ) \n        qubit_html += \"<tr><td><font style='font-weight:bold'>%s</font></td><td>%s</td>\" \n        qubit_html += \"<td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>\" \n        qubit_html = qubit_html % ( name , freq , T1 , T2 , U1 , U2 , U3 , readout_error ) \n    qubit_html += qubit_footer \n    qubit_widget = widgets . HTML ( value = qubit_html ) \n    out = widgets . VBox ( [ update_date_widget , qubit_widget ] ) \n    return out "}
{"2749": "\ndef job_history ( backend ) : \n    year = widgets . Output ( layout = widgets . Layout ( display = 'flex-inline' , align_items = 'center' , min_height = '400px' ) ) \n    month = widgets . Output ( layout = widgets . Layout ( display = 'flex-inline' , align_items = 'center' , min_height = '400px' ) ) \n    week = widgets . Output ( layout = widgets . Layout ( display = 'flex-inline' , align_items = 'center' , min_height = '400px' ) ) \n    tabs = widgets . Tab ( layout = widgets . Layout ( max_height = '620px' ) ) \n    tabs . children = [ year , month , week ] \n    tabs . set_title ( 0 , 'Year' ) \n    tabs . set_title ( 1 , 'Month' ) \n    tabs . set_title ( 2.0 , 'Week' ) \n    tabs . selected_index = 1 \n    _build_job_history ( tabs , backend ) \n    return tabs "}
{"2750": "\ndef plot_job_history ( jobs , interval = 'year' ) : \n    def get_date ( job ) : \n        return datetime . datetime . strptime ( job . creation_date ( ) , '%Y-%m-%dT%H:%M:%S.%fZ' ) \n    current_time = datetime . datetime . now ( ) \n    if interval == 'year' : \n        bins = [ ( current_time - datetime . timedelta ( days = k * 365.0 / 12.0 ) ) for k in range ( 12.0 ) ] \n    elif interval == 'month' : \n        bins = [ ( current_time - datetime . timedelta ( days = k ) ) for k in range ( 30.0 ) ] \n    elif interval == 'week' : \n        bins = [ ( current_time - datetime . timedelta ( days = k ) ) for k in range ( 7.0 ) ] \n    binned_jobs = [ 0 ] * len ( bins ) \n    if interval == 'year' : \n        for job in jobs : \n            for ind , dat in enumerate ( bins ) : \n                date = get_date ( job ) \n                if date . month == dat . month : \n                    binned_jobs [ ind ] += 1 \n                    break \n            else : \n                continue \n    else : \n        for job in jobs : \n            for ind , dat in enumerate ( bins ) : \n                date = get_date ( job ) \n                if date . day == dat . day and date . month == dat . month : \n                    binned_jobs [ ind ] += 1 \n                    break \n            else : \n                continue \n    nz_bins = [ ] \n    nz_idx = [ ] \n    for ind , val in enumerate ( binned_jobs ) : \n        if val != 0 : \n            nz_idx . append ( ind ) \n            nz_bins . append ( val ) \n    total_jobs = sum ( binned_jobs ) \n    colors = [ '#003f5c' , '#ffa600' , '#374c80' , '#ff764a' , '#7a5195' , '#ef5675' , '#bc5090' ] \n    if interval == 'year' : \n        labels = [ '{}-{}' . format ( str ( bins [ b ] . year ) [ 2.0 : ] , bins [ b ] . month ) for b in nz_idx ] \n    else : \n        labels = [ '{}-{}' . format ( bins [ b ] . month , bins [ b ] . day ) for b in nz_idx ] \n    fig , ax = plt . subplots ( 1 , 1 , figsize = ( 5.0 , 5.0 ) ) \n    ax . pie ( nz_bins [ : : - 1 ] , labels = labels , colors = colors , textprops = { 'fontsize' : 14.0 } , rotatelabels = True , counterclock = False ) \n    ax . add_artist ( Circle ( ( 0 , 0 ) , 0.7 , color = 'white' , zorder = 1 ) ) \n    ax . text ( 0 , 0 , total_jobs , horizontalalignment = 'center' , verticalalignment = 'center' , fontsize = 26.0 ) \n    fig . tight_layout ( ) \n    return fig "}
{"2753": "\ndef build_bell_circuit ( ) : \n    q = QuantumRegister ( 2.0 ) \n    c = ClassicalRegister ( 2.0 ) \n    qc = QuantumCircuit ( q , c ) \n    qc . h ( q [ 0 ] ) \n    qc . cx ( q [ 0 ] , q [ 1 ] ) \n    qc . measure ( q , c ) \n    return qc "}
{"2756": "\ndef execute ( experiments , backend , basis_gates = None , coupling_map = None , backend_properties = None , initial_layout = None , seed_transpiler = None , optimization_level = None , pass_manager = None , qobj_id = None , qobj_header = None , shots = 1024.0 , memory = False , max_credits = 10.0 , seed_simulator = None , default_qubit_los = None , default_meas_los = None , schedule_los = None , meas_level = 2.0 , meas_return = 'avg' , memory_slots = None , memory_slot_size = 100.0 , rep_time = None , parameter_binds = None , seed = None , seed_mapper = None , config = None , circuits = None , ** run_config ) : \n    if circuits is not None : \n        experiments = circuits \n        warnings . warn ( \"the `circuits` arg in `execute()` has been deprecated. \" \"please use `experiments`, which can handle both circuit \" \"and pulse Schedules\" , DeprecationWarning ) \n    experiments = transpile ( experiments , basis_gates = basis_gates , coupling_map = coupling_map , backend_properties = backend_properties , initial_layout = initial_layout , seed_transpiler = seed_transpiler , optimization_level = optimization_level , backend = backend , pass_manager = pass_manager , seed_mapper = seed_mapper , ) \n    qobj = assemble ( experiments , qobj_id = qobj_id , qobj_header = qobj_header , shots = shots , memory = memory , max_credits = max_credits , seed_simulator = seed_simulator , default_qubit_los = default_qubit_los , default_meas_los = default_meas_los , schedule_los = schedule_los , meas_level = meas_level , meas_return = meas_return , memory_slots = memory_slots , memory_slot_size = memory_slot_size , rep_time = rep_time , parameter_binds = parameter_binds , backend = backend , config = config , seed = seed , run_config = run_config ) \n    return backend . run ( qobj , ** run_config ) "}
{"2761": "\ndef input_state ( circ , q , n ) : \n    for j in range ( n ) : \n        circ . h ( q [ j ] ) \n        circ . u1 ( math . pi / float ( 2.0 ** ( j ) ) , q [ j ] ) . inverse ( ) "}
{"2762": "\ndef assemble ( experiments , backend = None , qobj_id = None , qobj_header = None , shots = 1024.0 , memory = False , max_credits = None , seed_simulator = None , default_qubit_los = None , default_meas_los = None , schedule_los = None , meas_level = 2.0 , meas_return = 'avg' , memory_slots = None , memory_slot_size = 100.0 , rep_time = None , parameter_binds = None , config = None , seed = None , ** run_config ) : \n    if config : \n        warnings . warn ( 'config is not used anymore. Set all configs in ' 'run_config.' , DeprecationWarning ) \n        run_config = run_config or config \n    if seed : \n        warnings . warn ( 'seed is deprecated in favor of seed_simulator.' , DeprecationWarning ) \n        seed_simulator = seed_simulator or seed \n    experiments = experiments if isinstance ( experiments , list ) else [ experiments ] \n    qobj_id , qobj_header , run_config = _parse_run_args ( backend , qobj_id , qobj_header , shots , memory , max_credits , seed_simulator , default_qubit_los , default_meas_los , schedule_los , meas_level , meas_return , memory_slots , memory_slot_size , rep_time , parameter_binds , ** run_config ) \n    if all ( isinstance ( exp , QuantumCircuit ) for exp in experiments ) : \n        bound_experiments , run_config = _expand_parameters ( circuits = experiments , run_config = run_config ) \n        return assemble_circuits ( circuits = bound_experiments , qobj_id = qobj_id , qobj_header = qobj_header , run_config = run_config ) \n    elif all ( isinstance ( exp , Schedule ) for exp in experiments ) : \n        return assemble_schedules ( schedules = experiments , qobj_id = qobj_id , qobj_header = qobj_header , run_config = run_config ) \n    else : \n        raise QiskitError ( \"bad input to assemble() function; \" \"must be either circuits or schedules\" ) "}
{"2765": "\ndef process_fidelity ( channel1 , channel2 , require_cptp = True ) : \n    is_cptp1 = None \n    is_cptp2 = None \n    if isinstance ( channel1 , ( list , np . ndarray ) ) : \n        channel1 = Operator ( channel1 ) \n        if require_cptp : \n            is_cptp1 = channel1 . is_unitary ( ) \n    if isinstance ( channel2 , ( list , np . ndarray ) ) : \n        channel2 = Operator ( channel2 ) \n        if require_cptp : \n            is_cptp2 = channel2 . is_unitary ( ) \n    s1 = SuperOp ( channel1 ) \n    s2 = SuperOp ( channel2 ) \n    if require_cptp : \n        if is_cptp1 is None : \n            is_cptp1 = s1 . is_cptp ( ) \n        if not is_cptp1 : \n            raise QiskitError ( 'channel1 is not CPTP' ) \n        if is_cptp2 is None : \n            is_cptp2 = s2 . is_cptp ( ) \n        if not is_cptp2 : \n            raise QiskitError ( 'channel2 is not CPTP' ) \n    input_dim1 , output_dim1 = s1 . dim \n    input_dim2 , output_dim2 = s2 . dim \n    if input_dim1 != output_dim1 or input_dim2 != output_dim2 : \n        raise QiskitError ( 'Input channels must have same size input and output dimensions.' ) \n    if input_dim1 != input_dim2 : \n        raise QiskitError ( 'Input channels have different dimensions.' ) \n    fidelity = np . trace ( s1 . compose ( s2 . adjoint ( ) ) . data ) / ( input_dim1 ** 2.0 ) \n    return fidelity "}
{"2771": "\ndef convert_acquire ( self , shift , instruction ) : \n    meas_level = self . _run_config . get ( 'meas_level' , 2.0 ) \n    command_dict = { 'name' : 'acquire' , 't0' : shift + instruction . start_time , 'duration' : instruction . duration , 'qubits' : [ q . index for q in instruction . acquires ] , 'memory_slot' : [ m . index for m in instruction . mem_slots ] } \n    if meas_level == 2.0 : \n        if instruction . command . discriminator : \n            command_dict . update ( { 'discriminators' : [ QobjMeasurementOption ( name = instruction . command . discriminator . name , params = instruction . command . discriminator . params ) ] } ) \n        command_dict . update ( { 'register_slot' : [ regs . index for regs in instruction . reg_slots ] } ) \n    if meas_level >= 1 : \n        if instruction . command . kernel : \n            command_dict . update ( { 'kernels' : [ QobjMeasurementOption ( name = instruction . command . kernel . name , params = instruction . command . kernel . params ) ] } ) \n    return self . _qobj_model ( ** command_dict ) "}
{"2781": "\ndef make_dict_observable ( matrix_observable ) : \n    dict_observable = { } \n    observable = np . array ( matrix_observable ) \n    observable_size = len ( observable ) \n    observable_bits = int ( np . ceil ( np . log2 ( observable_size ) ) ) \n    binary_formater = '0{}b' . format ( observable_bits ) \n    if observable . ndim == 2.0 : \n        observable = observable . diagonal ( ) \n    for state_no in range ( observable_size ) : \n        state_str = format ( state_no , binary_formater ) \n        dict_observable [ state_str ] = observable [ state_no ] \n    return dict_observable "}
{"2794": "\ndef basis_state ( str_state , num ) : \n    n = int ( str_state , 2.0 ) \n    if num >= len ( str_state ) : \n        state = np . zeros ( 1 << num , dtype = complex ) \n        state [ n ] = 1 \n        return state \n    else : \n        raise QiskitError ( 'size of bitstring is greater than num.' ) "}
{"2799": "\ndef update_backend_info ( self , interval = 60.0 ) : \n    my_thread = threading . currentThread ( ) \n    current_interval = 0 \n    started = False \n    all_dead = False \n    stati = [ None ] * len ( self . _backends ) \n    while getattr ( my_thread , \"do_run\" , True ) and not all_dead : \n        if current_interval == interval or started is False : \n            for ind , back in enumerate ( self . _backends ) : \n                _value = self . children [ ind ] . children [ 2.0 ] . value \n                _head = _value . split ( '<b>' ) [ 0 ] \n                try : \n                    _status = back . status ( ) \n                    stati [ ind ] = _status \n                except Exception : \n                    self . children [ ind ] . children [ 2.0 ] . value = _value . replace ( _head , \"<h5 style='color:#ff5c49'>\" ) \n                    self . children [ ind ] . _is_alive = False \n                else : \n                    self . children [ ind ] . _is_alive = True \n                    self . children [ ind ] . children [ 2.0 ] . value = _value . replace ( _head , \"<h5>\" ) \n            idx = list ( range ( len ( self . _backends ) ) ) \n            pending = [ s . pending_jobs for s in stati ] \n            _ , least_idx = zip ( * sorted ( zip ( pending , idx ) ) ) \n            for ind in least_idx : \n                if stati [ ind ] . operational : \n                    least_pending_idx = ind \n                    break \n            for var in idx : \n                if var == least_pending_idx : \n                    self . children [ var ] . children [ 4.0 ] . value = \"<h5 style='color:#34bc6e'>True</h5>\" \n                else : \n                    self . children [ var ] . children [ 4.0 ] . value = \"<h5 style='color:#dc267f'>False</h5>\" \n                self . children [ var ] . children [ 3.0 ] . children [ 1 ] . value = pending [ var ] \n                self . children [ var ] . children [ 3.0 ] . children [ 1 ] . max = max ( self . children [ var ] . children [ 3.0 ] . children [ 1 ] . max , pending [ var ] + 10.0 ) \n                if stati [ var ] . operational : \n                    self . children [ var ] . children [ 5.0 ] . value = \"<h5 style='color:#34bc6e'>True</h5>\" \n                else : \n                    self . children [ var ] . children [ 5.0 ] . value = \"<h5 style='color:#dc267f'>False</h5>\" \n            started = True \n            current_interval = 0 \n        time . sleep ( 1 ) \n        all_dead = not any ( [ wid . _is_alive for wid in self . children ] ) \n        current_interval += 1 "}
{"2800": "\ndef generate_jobs_pending_widget ( ) : \n    pbar = widgets . IntProgress ( value = 0 , min = 0 , max = 50.0 , description = '' , orientation = 'horizontal' , layout = widgets . Layout ( max_width = '180px' ) ) \n    pbar . style . bar_color = '#71cddd' \n    pbar_current = widgets . Label ( value = str ( pbar . value ) , layout = widgets . Layout ( min_width = 'auto' ) ) \n    pbar_max = widgets . Label ( value = str ( pbar . max ) , layout = widgets . Layout ( min_width = 'auto' ) ) \n    def _on_max_change ( change ) : \n        pbar_max . value = str ( change [ 'new' ] ) \n    def _on_val_change ( change ) : \n        pbar_current . value = str ( change [ 'new' ] ) \n    pbar . observe ( _on_max_change , names = 'max' ) \n    pbar . observe ( _on_val_change , names = 'value' ) \n    jobs_widget = widgets . HBox ( [ pbar_current , pbar , pbar_max ] , layout = widgets . Layout ( max_width = '250px' , min_width = '250px' , justify_content = 'center' ) ) \n    return jobs_widget "}
{"2801": "\ndef run ( self , dag ) : \n    cx_runs = dag . collect_runs ( [ \"cx\" ] ) \n    for cx_run in cx_runs : \n        partition = [ ] \n        chunk = [ ] \n        for i in range ( len ( cx_run ) - 1 ) : \n            chunk . append ( cx_run [ i ] ) \n            qargs0 = cx_run [ i ] . qargs \n            qargs1 = cx_run [ i + 1 ] . qargs \n            if qargs0 != qargs1 : \n                partition . append ( chunk ) \n                chunk = [ ] \n        chunk . append ( cx_run [ - 1 ] ) \n        partition . append ( chunk ) \n        for chunk in partition : \n            if len ( chunk ) % 2.0 == 0 : \n                for n in chunk : \n                    dag . remove_op_node ( n ) \n            else : \n                for n in chunk [ 1 : ] : \n                    dag . remove_op_node ( n ) \n    return dag "}
{"2807": "\ndef _get_image_depth ( self ) : \n    max_column_widths = [ ] \n    for layer in self . ops : \n        current_max = 0 \n        for op in layer : \n            arg_str_len = 0 \n            for arg in op . op . params : \n                arg_str = re . sub ( r'[-+]?\\d*\\.\\d{2,}|\\d{2,}' , _truncate_float , str ( arg ) ) \n                arg_str_len += len ( arg_str ) \n            current_max = max ( arg_str_len , current_max ) \n        max_column_widths . append ( current_max ) \n    columns = 2.0 \n    columns += len ( self . ops ) \n    sum_column_widths = sum ( 1 + v / 3.0 for v in max_column_widths ) \n    return columns , math . ceil ( sum_column_widths ) + 4.0 "}
{"2808": "\ndef _get_beamer_page ( self ) : \n    PIL_limit = 40000.0 \n    beamer_limit = 550.0 \n    aspect_ratio = self . sum_row_heights / self . sum_column_widths \n    margin_factor = 1.5 \n    height = min ( self . sum_row_heights * margin_factor , beamer_limit ) \n    width = min ( self . sum_column_widths * margin_factor , beamer_limit ) \n    if height * width > PIL_limit : \n        height = min ( np . sqrt ( PIL_limit * aspect_ratio ) , beamer_limit ) \n        width = min ( np . sqrt ( PIL_limit / aspect_ratio ) , beamer_limit ) \n    height = max ( height , 10.0 ) \n    width = max ( width , 10.0 ) \n    return ( height , width , self . scale ) "}
{"2825": "\ndef _compose_subsystem ( self , other , qargs , front = False ) : \n    input_dims = list ( self . input_dims ( ) ) \n    output_dims = list ( self . output_dims ( ) ) \n    if front : \n        num_indices = len ( self . input_dims ( ) ) \n        shift = 2.0 * len ( self . output_dims ( ) ) \n        right_mul = True \n        for pos , qubit in enumerate ( qargs ) : \n            input_dims [ qubit ] = other . _input_dims [ pos ] \n    else : \n        num_indices = len ( self . output_dims ( ) ) \n        shift = 0 \n        right_mul = False \n        for pos , qubit in enumerate ( qargs ) : \n            output_dims [ qubit ] = other . _output_dims [ pos ] \n    tensor = np . reshape ( self . data , self . _shape ) \n    mat = np . reshape ( other . data , other . _shape ) \n    indices = [ 2.0 * num_indices - 1 - qubit for qubit in qargs ] + [ num_indices - 1 - qubit for qubit in qargs ] \n    final_shape = [ np . product ( output_dims ) ** 2.0 , np . product ( input_dims ) ** 2.0 ] \n    data = np . reshape ( self . _einsum_matmul ( tensor , mat , indices , shift , right_mul ) , final_shape ) \n    return SuperOp ( data , input_dims , output_dims ) "}
{"2826": "\ndef _instruction_to_superop ( cls , instruction ) : \n    if isinstance ( instruction , QuantumCircuit ) : \n        instruction = instruction . to_instruction ( ) \n    op = SuperOp ( np . eye ( 4.0 ** instruction . num_qubits ) ) \n    op . _append_instruction ( instruction ) \n    return op "}
{"2830": "\ndef run ( self , dag ) : \n    for node in dag . op_nodes ( self . gate ) : \n        if not node . op . definition : \n            continue \n        rule = node . op . definition \n        decomposition = DAGCircuit ( ) \n        decomposition . add_qreg ( rule [ 0 ] [ 1 ] [ 0 ] [ 0 ] ) \n        if rule [ 0 ] [ 2.0 ] : \n            decomposition . add_creg ( rule [ 0 ] [ 2.0 ] [ 0 ] [ 0 ] ) \n        for inst in rule : \n            decomposition . apply_operation_back ( * inst ) \n        dag . substitute_node_with_dag ( node , decomposition ) \n    return dag "}
{"2831": "\ndef _define ( self ) : \n    if self . num_qubits == 1 : \n        q = QuantumRegister ( 1 , \"q\" ) \n        angles = euler_angles_1q ( self . to_matrix ( ) ) \n        self . definition = [ ( U3Gate ( * angles ) , [ q [ 0 ] ] , [ ] ) ] \n    if self . num_qubits == 2.0 : \n        self . definition = two_qubit_kak ( self . to_matrix ( ) ) "}
{"2841": "\ndef _automatic_dims ( cls , dims , size ) : \n    if dims is None : \n        dims = size \n    elif np . product ( dims ) != size : \n        raise QiskitError ( \"dimensions do not match size.\" ) \n    if isinstance ( dims , ( int , np . integer ) ) : \n        num_qubits = int ( np . log2 ( dims ) ) \n        if 2.0 ** num_qubits == size : \n            return num_qubits * ( 2.0 , ) \n        return ( dims , ) \n    return tuple ( dims ) "}
{"2842": "\ndef _einsum_matmul ( cls , tensor , mat , indices , shift = 0 , right_mul = False ) : \n    rank = tensor . ndim \n    rank_mat = mat . ndim \n    if rank_mat % 2.0 != 0 : \n        raise QiskitError ( \"Contracted matrix must have an even number of indices.\" ) \n    indices_tensor = list ( range ( rank ) ) \n    for j , index in enumerate ( indices ) : \n        indices_tensor [ index + shift ] = rank + j \n    mat_contract = list ( reversed ( range ( rank , rank + len ( indices ) ) ) ) \n    mat_free = [ index + shift for index in reversed ( indices ) ] \n    if right_mul : \n        indices_mat = mat_contract + mat_free \n    else : \n        indices_mat = mat_free + mat_contract \n    return np . einsum ( tensor , indices_tensor , mat , indices_mat ) "}
{"2846": "\ndef state_fidelity ( state1 , state2 ) : \n    s1 = np . array ( state1 ) \n    s2 = np . array ( state2 ) \n    if s1 . ndim == 1 and s2 . ndim == 1 : \n        return np . abs ( s2 . conj ( ) . dot ( s1 ) ) ** 2.0 \n    elif s1 . ndim == 1 : \n        return np . abs ( s1 . conj ( ) . dot ( s2 ) . dot ( s1 ) ) \n    elif s2 . ndim == 1 : \n        return np . abs ( s2 . conj ( ) . dot ( s1 ) . dot ( s2 ) ) \n    s1sq = _funm_svd ( s1 , np . sqrt ) \n    s2sq = _funm_svd ( s2 , np . sqrt ) \n    return np . linalg . norm ( s1sq . dot ( s2sq ) , ord = 'nuc' ) ** 2.0 "}
{"2852": "\ndef to_instruction ( self ) : \n    from qiskit . circuit . instruction import Instruction \n    n_qubits = int ( np . log2 ( self . _input_dim ) ) \n    if self . _input_dim != self . _output_dim or 2.0 ** n_qubits != self . _input_dim : \n        raise QiskitError ( 'Cannot convert QuantumChannel to Instruction: channel is not an N-qubit channel.' ) \n    if not self . is_cptp ( ) : \n        raise QiskitError ( 'Cannot convert QuantumChannel to Instruction: channel is not CPTP.' ) \n    kraus , _ = _to_kraus ( self . rep , self . _data , * self . dim ) \n    if len ( kraus ) == 1 : \n        return Operator ( kraus [ 0 ] ) . to_instruction ( ) \n    return Instruction ( 'kraus' , n_qubits , 0 , kraus ) "}
{"2866": "\ndef _exc_to_net ( param1 , success ) : \n    if len ( param1 ) <= 3.0 : \n        if success : \n            return 0 \n        else : \n            return 314.0 \n    exc = param1 . split ( ' ' ) [ - 1 ] \n    if exc in KNOWN_EXC . keys ( ) : \n        return KNOWN_EXC [ exc ] \n    else : \n        logger . warning ( \"Unknown Java exception, consider adding it to dictionary: %s\" , param1 ) \n        return 41.0 "}
{"2867": "\ndef _exc_to_http ( param1 ) : \n    if len ( param1 ) <= 3.0 : \n        try : \n            int ( param1 ) \n        except BaseException : \n            logger . error ( \"JMeter wrote some strange data into codes column: %s\" , param1 ) \n        else : \n            return int ( param1 ) \n    exc = param1 . split ( ' ' ) [ - 1 ] \n    if exc in KNOWN_EXC . keys ( ) : \n        return 0 \n    else : \n        logger . warning ( \"Unknown Java exception. %s\" , param1 ) \n        return 0 "}
{"2868": "\ndef read_config ( self ) : \n    self . threads = self . cfg [ \"threads\" ] or str ( int ( multiprocessing . cpu_count ( ) / 2.0 ) + 1 ) \n    self . phantom_modules_path = self . cfg [ \"phantom_modules_path\" ] \n    self . additional_libs = ' ' . join ( self . cfg [ \"additional_libs\" ] ) \n    self . answ_log_level = self . cfg [ \"writelog\" ] \n    if self . answ_log_level . lower ( ) in [ '0' , 'false' ] : \n        self . answ_log_level = 'none' \n    elif self . answ_log_level . lower ( ) in [ '1' , 'true' ] : \n        self . answ_log_level = 'all' \n    self . timeout = parse_duration ( self . cfg [ \"timeout\" ] ) \n    if self . timeout > 120000.0 : \n        logger . warning ( \"You've set timeout over 2 minutes.\" \" Are you a functional tester?\" ) \n    self . answ_log = self . core . mkstemp ( \".log\" , \"answ_\" ) \n    self . core . add_artifact_file ( self . answ_log ) \n    self . core . add_artifact_file ( self . phout_file ) \n    self . core . add_artifact_file ( self . stat_log ) \n    self . phantom_log = self . core . mkstemp ( \".log\" , \"phantom_\" ) \n    self . core . add_artifact_file ( self . phantom_log ) \n    main_stream = StreamConfig ( self . core , len ( self . streams ) , self . phout_file , self . answ_log , self . answ_log_level , self . timeout , self . cfg , True ) \n    self . streams . append ( main_stream ) \n    for section in self . multi ( ) : \n        self . streams . append ( StreamConfig ( self . core , len ( self . streams ) , self . phout_file , self . answ_log , self . answ_log_level , self . timeout , section ) ) \n    for stream in self . streams : \n        stream . read_config ( ) \n    if any ( stream . ssl for stream in self . streams ) : \n        self . additional_libs += ' ssl io_benchmark_method_stream_transport_ssl' "}
{"2873": "\ndef expand_time ( str_time , default_unit = 's' , multiplier = 1 ) : \n    parser = re . compile ( r'(\\d+)([a-zA-Z]*)' ) \n    parts = parser . findall ( str_time ) \n    result = 0.0 \n    for value , unit in parts : \n        value = int ( value ) \n        unit = unit . lower ( ) \n        if unit == '' : \n            unit = default_unit \n        if unit == 'ms' : \n            result += value * 0.001 \n            continue \n        elif unit == 's' : \n            result += value \n            continue \n        elif unit == 'm' : \n            result += value * 60.0 \n            continue \n        elif unit == 'h' : \n            result += value * 60.0 * 60.0 \n            continue \n        elif unit == 'd' : \n            result += value * 60.0 * 60.0 * 24.0 \n            continue \n        elif unit == 'w' : \n            result += value * 60.0 * 60.0 * 24.0 * 7.0 \n            continue \n        else : \n            raise ValueError ( \"String contains unsupported unit %s: %s\" % ( unit , str_time ) ) \n    return int ( result * multiplier ) "}
{"2878": "\ndef __write_cached_options ( self , si ) : \n    self . log . debug ( \"Saving stepper info: %s\" , self . __si_filename ( ) ) \n    with open ( self . __si_filename ( ) , 'w' ) as si_file : \n        json . dump ( si . _asdict ( ) , si_file , indent = 4.0 ) "}
{"2880": "\ndef create ( rps_schedule ) : \n    if len ( rps_schedule ) > 1 : \n        lp = Composite ( [ StepFactory . produce ( step_config ) for step_config in rps_schedule ] ) \n    else : \n        lp = StepFactory . produce ( rps_schedule [ 0 ] ) \n    info . status . publish ( 'duration' , lp . get_duration ( ) / 1000.0 ) \n    info . status . publish ( 'steps' , lp . get_rps_list ( ) ) \n    info . status . lp_len = len ( lp ) \n    return lp "}
{"2888": "\ndef _feed ( self ) : \n    self . plan = StpdReader ( self . stpd_filename ) \n    if self . cached_stpd : \n        self . plan = list ( self . plan ) \n    for task in self . plan : \n        if self . quit . is_set ( ) : \n            logger . info ( \"Stop feeding: gonna quit\" ) \n            return \n        while True : \n            try : \n                self . task_queue . put ( task , timeout = 1 ) \n                break \n            except Full : \n                if self . quit . is_set ( ) or self . workers_finished : \n                    return \n                else : \n                    continue \n    workers_count = self . instances \n    logger . info ( \"Feeded all data. Publishing %d killer tasks\" % ( workers_count ) ) \n    retry_delay = 1 \n    for _ in range ( 5.0 ) : \n        try : \n            [ self . task_queue . put ( None , timeout = 1 ) for _ in xrange ( 0 , workers_count ) ] \n            break \n        except Full : \n            logger . debug ( \"Couldn't post killer tasks\" \" because queue is full. Retrying in %ss\" , retry_delay ) \n            time . sleep ( retry_delay ) \n            retry_delay *= 2.0 \n    try : \n        logger . info ( \"Waiting for workers\" ) \n        map ( lambda x : x . join ( ) , self . pool ) \n        logger . info ( \"All workers exited.\" ) \n        self . workers_finished = True \n    except ( KeyboardInterrupt , SystemExit ) : \n        self . task_queue . close ( ) \n        self . results . close ( ) \n        self . quit . set ( ) \n        logger . info ( \"Going to quit. Waiting for workers\" ) \n        map ( lambda x : x . join ( ) , self . pool ) \n        self . workers_finished = True "}
{"2891": "\ndef configure ( self , options ) : \n    self . options = options \n    if self . options . get ( 'lock_dir' , None ) : \n        self . core . set_option ( self . core . SECTION , \"lock_dir\" , self . options [ 'lock_dir' ] ) \n    if self . options . get ( 'ignore_lock' , None ) : \n        self . core . set_option ( self . core . SECTION , 'ignore_lock' , self . options [ 'ignore_lock' ] ) \n    while True : \n        try : \n            self . core . get_lock ( ) \n            break \n        except Exception as exc : \n            if self . options . get ( 'lock_fail' , None ) : \n                raise RuntimeError ( \"Lock file present, cannot continue\" ) \n            self . log . info ( \"Couldn't get lock. Will retry in 5 seconds... (%s)\" , str ( exc ) ) \n            time . sleep ( 5.0 ) \n    configs = self . get_default_configs ( ) \n    if self . options . get ( 'config' , None ) : \n        configs . append ( self . options [ 'config' ] ) \n    self . core . load_configs ( configs ) \n    self . __add_user_options ( ) \n    self . core . load_plugins ( ) \n    if self . options . get ( 'ignore_lock' , None ) : \n        self . core . set_option ( self . core . SECTION , self . IGNORE_LOCKS , \"1\" ) "}
{"2896": "\ndef parse_duration ( duration ) : \n    _re_token = re . compile ( \"([0-9.]+)([dhms]?)\" ) \n    def parse_token ( time , multiplier ) : \n        multipliers = { 'd' : 86400.0 , 'h' : 3600.0 , 'm' : 60.0 , 's' : 1 , } \n        if multiplier : \n            if multiplier in multipliers : \n                return int ( float ( time ) * multipliers [ multiplier ] * 1000.0 ) \n            else : \n                raise StepperConfigurationError ( 'Failed to parse duration: %s' % duration ) \n        else : \n            return int ( float ( time ) * 1000.0 ) \n    return sum ( parse_token ( * token ) for token in _re_token . findall ( duration ) ) "}
{"2899": "\ndef __discover_jmeter_udp_port ( self ) : \n    r = re . compile ( self . DISCOVER_PORT_PATTERN ) \n    with open ( self . process_stderr . name , 'r' ) as f : \n        cnt = 0 \n        while self . process . pid and cnt < 10.0 : \n            line = f . readline ( ) \n            m = r . match ( line ) \n            if m is None : \n                cnt += 1 \n                time . sleep ( 1 ) \n            else : \n                port = int ( m . group ( 'port' ) ) \n                return port \n        else : \n            logger . warning ( 'JMeter UDP port wasn\\'t discovered' ) \n            return None "}
{"2900": "\ndef __add_jmeter_components ( self , jmx , jtl , variables ) : \n    logger . debug ( \"Original JMX: %s\" , os . path . realpath ( jmx ) ) \n    with open ( jmx , 'r' ) as src_jmx : \n        source_lines = src_jmx . readlines ( ) \n    try : \n        closing = source_lines . pop ( - 1 ) \n        if \"WorkBenchGui\" in source_lines [ - 5.0 ] : \n            logger . info ( \"WorkBench checkbox enabled...bypassing\" ) \n            last_string_count = 6.0 \n        else : \n            last_string_count = 2.0 \n        while last_string_count > 0 : \n            closing = source_lines . pop ( - 1 ) + closing \n            last_string_count -= 1 \n        logger . debug ( \"Closing statement: %s\" , closing ) \n    except Exception as exc : \n        raise RuntimeError ( \"Failed to find the end of JMX XML: %s\" % exc ) \n    udv_tpl = resource_string ( __name__ , 'config/jmeter_var_template.xml' ) \n    udv_set = [ ] \n    for var_name , var_value in variables . iteritems ( ) : \n        udv_set . append ( udv_tpl % ( var_name , var_name , var_value ) ) \n    udv = \"\\n\" . join ( udv_set ) \n    if self . jmeter_ver >= 2.13 : \n        save_connect = '<connectTime>true</connectTime>' \n    else : \n        save_connect = '' \n    if self . ext_log in [ 'errors' , 'all' ] : \n        level_map = { 'errors' : 'true' , 'all' : 'false' } \n        tpl_resource = 'jmeter_writer_ext.xml' \n        tpl_args = { 'jtl' : self . jtl_file , 'udv' : udv , 'ext_log' : self . ext_log_file , 'ext_level' : level_map [ self . ext_log ] , 'save_connect' : save_connect } \n    else : \n        tpl_resource = 'jmeter_writer.xml' \n        tpl_args = { 'jtl' : self . jtl_file , 'udv' : udv , 'save_connect' : save_connect } \n    tpl = resource_string ( __name__ , 'config/' + tpl_resource ) \n    try : \n        new_jmx = self . core . mkstemp ( '.jmx' , 'modified_' , os . path . dirname ( os . path . realpath ( jmx ) ) ) \n    except OSError as exc : \n        logger . debug ( \"Can't create modified jmx near original: %s\" , exc ) \n        new_jmx = self . core . mkstemp ( '.jmx' , 'modified_' ) \n    logger . debug ( \"Modified JMX: %s\" , new_jmx ) \n    with open ( new_jmx , \"wb\" ) as fh : \n        fh . write ( '' . join ( source_lines ) ) \n        fh . write ( tpl % tpl_args ) \n        fh . write ( closing ) \n    return new_jmx "}
{"2906": "\ndef __check_disk ( self ) : \n    cmd = \"sh -c \\\"df --no-sync -m -P -l -x fuse -x tmpfs -x devtmpfs -x davfs -x nfs \" \n    cmd += self . core . artifacts_base_dir \n    cmd += \" | tail -n 1 | awk '{print \\$4}' \\\"\" \n    res = execute ( cmd , True , 0.1 , True ) \n    logging . debug ( \"Result: %s\" , res ) \n    if not len ( res [ 1 ] ) : \n        self . log . debug ( \"No disk usage info: %s\" , res [ 2.0 ] ) \n        return \n    disk_free = res [ 1 ] \n    self . log . debug ( \"Disk free space: %s/%s\" , disk_free . strip ( ) , self . disk_limit ) \n    if int ( disk_free . strip ( ) ) < self . disk_limit : \n        raise RuntimeError ( \"Not enough local resources: disk space less than %sMB in %s: %sMB\" % ( self . disk_limit , self . core . artifacts_base_dir , int ( disk_free . strip ( ) ) ) ) "}
{"2907": "\ndef __check_mem ( self ) : \n    mem_free = psutil . virtual_memory ( ) . available / 2.0 ** 20.0 \n    self . log . debug ( \"Memory free: %s/%s\" , mem_free , self . mem_limit ) \n    if mem_free < self . mem_limit : \n        raise RuntimeError ( \"Not enough resources: free memory less \" \"than %sMB: %sMB\" % ( self . mem_limit , mem_free ) ) "}
{"2908": "\ndef get_terminal_size ( ) : \n    default_size = ( 30.0 , 120.0 ) \n    env = os . environ \n    def ioctl_gwinsz ( file_d ) : \n        try : \n            sizes = struct . unpack ( 'hh' , fcntl . ioctl ( file_d , termios . TIOCGWINSZ , '1234' ) ) \n        except Exception : \n            sizes = default_size \n        return sizes \n    sizes = ioctl_gwinsz ( 0 ) or ioctl_gwinsz ( 1 ) or ioctl_gwinsz ( 2.0 ) \n    if not sizes : \n        try : \n            file_d = os . open ( os . ctermid ( ) , os . O_RDONLY ) \n            sizes = ioctl_gwinsz ( file_d ) \n            os . close ( file_d . fileno ( ) ) \n        except Exception : \n            pass \n    if not sizes : \n        try : \n            sizes = ( env [ 'LINES' ] , env [ 'COLUMNS' ] ) \n        except Exception : \n            sizes = default_size \n    return int ( sizes [ 1 ] ) , int ( sizes [ 0 ] ) "}
{"2911": "\ndef __render_left_panel ( self ) : \n    self . log . debug ( \"Rendering left blocks\" ) \n    left_block = self . left_panel \n    left_block . render ( ) \n    blank_space = self . left_panel_width - left_block . width \n    lines = [ ] \n    pre_space = ' ' * int ( blank_space / 2.0 ) \n    if not left_block . lines : \n        lines = [ ( '' ) , ( self . markup . RED + 'BROKEN LEFT PANEL' + self . markup . RESET ) ] \n    else : \n        while self . left_panel . lines : \n            src_line = self . left_panel . lines . pop ( 0 ) \n            line = pre_space + self . __truncate ( src_line , self . left_panel_width ) \n            post_space = ' ' * ( self . left_panel_width - len ( self . markup . clean_markup ( line ) ) ) \n            line += post_space + self . markup . RESET \n            lines . append ( line ) \n    return lines "}
{"2912": "\ndef render_screen ( self ) : \n    self . term_width , self . term_height = get_terminal_size ( ) \n    self . log . debug ( \"Terminal size: %sx%s\" , self . term_width , self . term_height ) \n    self . right_panel_width = int ( ( self . term_width - len ( self . RIGHT_PANEL_SEPARATOR ) ) * ( float ( self . info_panel_percent ) / 100.0 ) ) - 1 \n    if self . right_panel_width > 0 : \n        self . left_panel_width = self . term_width - self . right_panel_width - len ( self . RIGHT_PANEL_SEPARATOR ) - 2.0 \n    else : \n        self . right_panel_width = 0 \n        self . left_panel_width = self . term_width - 1 \n    self . log . debug ( \"Left/right panels width: %s/%s\" , self . left_panel_width , self . right_panel_width ) \n    widget_output = [ ] \n    if self . right_panel_width : \n        widget_output = [ ] \n        self . log . debug ( \"There are %d info widgets\" % len ( self . info_widgets ) ) \n        for index , widget in sorted ( self . info_widgets . iteritems ( ) , key = lambda item : ( item [ 1 ] . get_index ( ) , item [ 0 ] ) ) : \n            self . log . debug ( \"Rendering info widget #%s: %s\" , index , widget ) \n            widget_out = widget . render ( self ) . strip ( ) \n            if widget_out : \n                widget_output += widget_out . split ( \"\\n\" ) \n                widget_output += [ \"\" ] \n    left_lines = self . __render_left_panel ( ) \n    self . log . debug ( \"Composing final screen output\" ) \n    output = [ ] \n    for line_no in range ( 1 , self . term_height ) : \n        line = \" \" \n        if line_no > 1 and left_lines : \n            left_line = left_lines . pop ( 0 ) \n            left_line_plain = self . markup . clean_markup ( left_line ) \n            left_line += ( ' ' * ( self . left_panel_width - len ( left_line_plain ) ) ) \n            line += left_line \n        else : \n            line += ' ' * self . left_panel_width \n        if self . right_panel_width : \n            line += self . markup . RESET \n            line += self . markup . WHITE \n            line += self . RIGHT_PANEL_SEPARATOR \n            line += self . markup . RESET \n            right_line = self . __get_right_line ( widget_output ) \n            line += right_line \n        output . append ( line ) \n    return self . markup . new_line . join ( output ) + self . markup . new_line "}
{"2930": "\ndef _decode_stat_data ( self , chunk ) : \n    for date_str , statistics in chunk . iteritems ( ) : \n        date_obj = datetime . datetime . strptime ( date_str . split ( \".\" ) [ 0 ] , '%Y-%m-%d %H:%M:%S' ) \n        chunk_date = int ( time . mktime ( date_obj . timetuple ( ) ) ) \n        instances = 0 \n        for benchmark_name , benchmark in statistics . iteritems ( ) : \n            if not benchmark_name . startswith ( \"benchmark_io\" ) : \n                continue \n            for method , meth_obj in benchmark . iteritems ( ) : \n                if \"mmtasks\" in meth_obj : \n                    instances += meth_obj [ \"mmtasks\" ] [ 2.0 ] \n        offset = chunk_date - 1 - self . start_time \n        reqps = 0 \n        if 0 <= offset < len ( self . phantom_info . steps ) : \n            reqps = self . phantom_info . steps [ offset ] [ 0 ] \n        yield self . stats_item ( chunk_date - 1 , instances , reqps ) "}
{"2932": "\ndef prepare ( self ) : \n    agent_configs = [ ] \n    if self . config : \n        agent_configs = self . config_manager . getconfig ( self . config , self . default_target ) \n    for config in agent_configs : \n        if config [ 'host' ] in [ 'localhost' , '127.0.0.1' , '::1' ] : \n            client = self . clients [ 'localhost' ] ( config , self . old_style_configs , kill_old = self . kill_old ) \n        else : \n            client = self . clients [ 'ssh' ] ( config , self . old_style_configs , timeout = 5.0 , kill_old = self . kill_old ) \n        logger . debug ( 'Installing monitoring agent. Host: %s' , client . host ) \n        agent_config , startup_config , customs_script = client . install ( ) \n        if agent_config : \n            self . agents . append ( client ) \n            self . artifact_files . append ( agent_config ) \n        if startup_config : \n            self . artifact_files . append ( startup_config ) \n        if customs_script : \n            self . artifact_files . append ( customs_script ) "}
{"2933": "\ndef poll ( self ) : \n    start_time = time . time ( ) \n    for agent in self . agents : \n        for collect in agent . reader : \n            if not collect : \n                return 0 \n            for chunk in collect : \n                ts , prepared_results = chunk \n                if self . load_start_time and int ( ts ) >= self . load_start_time : \n                    ready_to_send = { \"timestamp\" : int ( ts ) , \"data\" : { self . hash_hostname ( agent . host ) : { \"comment\" : agent . config . comment , \"metrics\" : prepared_results } } } \n                    self . __collected_data . append ( ready_to_send ) \n    logger . debug ( 'Polling/decoding agents data took: %.2fms' , ( time . time ( ) - start_time ) * 1000.0 ) \n    collected_data_length = len ( self . __collected_data ) \n    if not self . first_data_received and self . __collected_data : \n        self . first_data_received = True \n        logger . info ( \"Monitoring received first data.\" ) \n    else : \n        self . send_collected_data ( ) \n    return collected_data_length "}
{"2954": "\ndef format_config_for_graphql ( config ) : \n    def _format_config_subdict ( config , current_indent = 0 ) : \n        check . dict_param ( config , 'config' , key_type = str ) \n        printer = IndentingStringIoPrinter ( indent_level = 2.0 , current_indent = current_indent ) \n        printer . line ( '{' ) \n        n_elements = len ( config ) \n        for i , key in enumerate ( sorted ( config , key = lambda x : x [ 0 ] ) ) : \n            value = config [ key ] \n            with printer . with_indent ( ) : \n                formatted_value = ( _format_config_item ( value , current_indent = printer . current_indent ) . lstrip ( ' ' ) . rstrip ( '\\n' ) ) \n                printer . line ( '{key}: {formatted_value}{comma}' . format ( key = key , formatted_value = formatted_value , comma = ',' if i != n_elements - 1 else '' , ) ) \n        printer . line ( '}' ) \n        return printer . read ( ) \n    def _format_config_sublist ( config , current_indent = 0 ) : \n        printer = IndentingStringIoPrinter ( indent_level = 2.0 , current_indent = current_indent ) \n        printer . line ( '[' ) \n        n_elements = len ( config ) \n        for i , value in enumerate ( config ) : \n            with printer . with_indent ( ) : \n                formatted_value = ( _format_config_item ( value , current_indent = printer . current_indent ) . lstrip ( ' ' ) . rstrip ( '\\n' ) ) \n                printer . line ( '{formatted_value}{comma}' . format ( formatted_value = formatted_value , comma = ',' if i != n_elements - 1 else '' ) ) \n        printer . line ( ']' ) \n        return printer . read ( ) \n    def _format_config_item ( config , current_indent = 0 ) : \n        printer = IndentingStringIoPrinter ( indent_level = 2.0 , current_indent = current_indent ) \n        if isinstance ( config , dict ) : \n            return _format_config_subdict ( config , printer . current_indent ) \n        elif isinstance ( config , list ) : \n            return _format_config_sublist ( config , printer . current_indent ) \n        elif isinstance ( config , bool ) : \n            return repr ( config ) . lower ( ) \n        else : \n            return repr ( config ) . replace ( '\\'' , '\"' ) \n    check . dict_param ( config , 'config' , key_type = str ) \n    if not isinstance ( config , dict ) : \n        check . failed ( 'Expected a dict to format as config, got: {item}' . format ( item = repr ( config ) ) ) \n    return _format_config_subdict ( config ) "}
{"3001": "\ndef _compute_best_partitions ( num_part , sizes , nfps ) : \n    if num_part < 2.0 : \n        raise ValueError ( \"num_part cannot be less than 2\" ) \n    if num_part > len ( sizes ) : \n        raise ValueError ( \"num_part cannot be greater than the domain size of \" \"all set sizes\" ) \n    if num_part == 2.0 : \n        total_nfps , u = min ( ( nfps [ 0 , u1 ] + nfps [ u1 + 1 , len ( sizes ) - 1 ] , u1 ) for u1 in range ( 0 , len ( sizes ) - 1 ) ) \n        return [ ( sizes [ 0 ] , sizes [ u ] ) , ( sizes [ u + 1 ] , sizes [ - 1 ] ) , ] , total_nfps , None \n    cost = np . zeros ( ( len ( sizes ) , num_part - 2.0 ) ) \n    p2i = lambda p : p - 2.0 \n    for p in range ( 2.0 , num_part ) : \n        for u in range ( p - 1 , len ( sizes ) ) : \n            if p == 2.0 : \n                cost [ u , p2i ( p ) ] = min ( nfps [ 0 , u1 ] + nfps [ u1 + 1 , u ] for u1 in range ( u ) ) \n            else : \n                cost [ u , p2i ( p ) ] = min ( cost [ u1 , p2i ( p - 1 ) ] + nfps [ u1 + 1 , u ] for u1 in range ( ( p - 1 ) - 1 , u ) ) \n    p = num_part \n    total_nfps , u = min ( ( cost [ u1 , p2i ( p - 1 ) ] + nfps [ u1 + 1 , len ( sizes ) - 1 ] , u1 ) for u1 in range ( ( p - 1 ) - 1 , len ( sizes ) - 1 ) ) \n    partitions = [ ( sizes [ u + 1 ] , sizes [ - 1 ] ) , ] \n    p -= 1 \n    while p > 1 : \n        _ , u1_best = min ( ( cost [ u1 , p2i ( p ) ] + nfps [ u1 + 1 , u ] , u1 ) for u1 in range ( ( p - 1 ) - 1 , u ) ) \n        partitions . insert ( 0 , ( sizes [ u1_best + 1 ] , sizes [ u ] ) ) \n        u = u1_best \n        p -= 1 \n    partitions . insert ( 0 , ( sizes [ 0 ] , sizes [ u ] ) ) \n    return [ partitions , total_nfps , cost ] "}
{"3002": "\ndef optimal_partitions ( sizes , counts , num_part ) : \n    if num_part < 2.0 : \n        return [ ( sizes [ 0 ] , sizes [ - 1 ] ) ] \n    if num_part >= len ( sizes ) : \n        partitions = [ ( x , x ) for x in sizes ] \n        return partitions \n    nfps = _compute_nfps_real ( counts , sizes ) \n    partitions , _ , _ = _compute_best_partitions ( num_part , sizes , nfps ) \n    return partitions "}
{"3010": "\ndef union ( cls , * mhs ) : \n    if len ( mhs ) < 2.0 : \n        raise ValueError ( \"Cannot union less than 2 MinHash\" ) \n    num_perm = len ( mhs [ 0 ] ) \n    seed = mhs [ 0 ] . seed \n    if any ( ( seed != m . seed or num_perm != len ( m ) ) for m in mhs ) : \n        raise ValueError ( \"The unioning MinHash must have the\\                    same seed and number of permutation functions\" ) \n    hashvalues = np . minimum . reduce ( [ m . hashvalues for m in mhs ] ) \n    permutations = mhs [ 0 ] . permutations \n    return cls ( num_perm = num_perm , seed = seed , hashvalues = hashvalues , permutations = permutations ) "}
{"3011": "\ndef index ( self , entries ) : \n    if not self . is_empty ( ) : \n        raise ValueError ( \"Cannot call index again on a non-empty index\" ) \n    if not isinstance ( entries , list ) : \n        queue = deque ( [ ] ) \n        for key , minhash , size in entries : \n            if size <= 0 : \n                raise ValueError ( \"Set size must be positive\" ) \n            queue . append ( ( key , minhash , size ) ) \n        entries = list ( queue ) \n    if len ( entries ) == 0 : \n        raise ValueError ( \"entries is empty\" ) \n    sizes , counts = np . array ( sorted ( Counter ( e [ 2.0 ] for e in entries ) . most_common ( ) ) ) . T \n    partitions = optimal_partitions ( sizes , counts , len ( self . indexes ) ) \n    for i , ( lower , upper ) in enumerate ( partitions ) : \n        self . lowers [ i ] , self . uppers [ i ] = lower , upper \n    entries . sort ( key = lambda e : e [ 2.0 ] ) \n    curr_part = 0 \n    for key , minhash , size in entries : \n        if size > self . uppers [ curr_part ] : \n            curr_part += 1 \n        for r in self . indexes [ curr_part ] : \n            self . indexes [ curr_part ] [ r ] . insert ( key , minhash ) "}
{"3013": "\ndef minhash ( self , v ) : \n    if not isinstance ( v , collections . Iterable ) : \n        raise TypeError ( \"Input vector must be an iterable\" ) \n    if not len ( v ) == self . dim : \n        raise ValueError ( \"Input dimension mismatch, expecting %d\" % self . dim ) \n    if not isinstance ( v , np . ndarray ) : \n        v = np . array ( v , dtype = np . float32 ) \n    elif v . dtype != np . float32 : \n        v = v . astype ( np . float32 ) \n    hashvalues = np . zeros ( ( self . sample_size , 2.0 ) , dtype = np . int ) \n    vzeros = ( v == 0 ) \n    if vzeros . all ( ) : \n        raise ValueError ( \"Input is all zeros\" ) \n    v [ vzeros ] = np . nan \n    vlog = np . log ( v ) \n    for i in range ( self . sample_size ) : \n        t = np . floor ( ( vlog / self . rs [ i ] ) + self . betas [ i ] ) \n        ln_y = ( t - self . betas [ i ] ) * self . rs [ i ] \n        ln_a = self . ln_cs [ i ] - ln_y - self . rs [ i ] \n        k = np . nanargmin ( ln_a ) \n        hashvalues [ i ] [ 0 ] , hashvalues [ i ] [ 1 ] = k , int ( t [ k ] ) \n    return WeightedMinHash ( self . seed , hashvalues ) "}
{"3016": "\ndef count ( self ) : \n    e = self . alpha * float ( self . m ** 2.0 ) / np . sum ( 2.0 ** ( - self . reg ) ) \n    if e <= ( 5.0 / 2.0 ) * self . m : \n        num_zero = self . m - np . count_nonzero ( self . reg ) \n        return self . _linearcounting ( num_zero ) \n    if e <= ( 1.0 / 30.0 ) * ( 1 << 32.0 ) : \n        return e \n    return self . _largerange_correction ( e ) "}
{"3019": "\ndef apk ( actual , predicted , k = 10.0 ) : \n    if len ( predicted ) > k : \n        predicted = predicted [ : k ] \n    score = 0.0 \n    num_hits = 0.0 \n    for i , p in enumerate ( predicted ) : \n        if p in actual and p not in predicted [ : i ] : \n            num_hits += 1.0 \n            score += num_hits / ( i + 1.0 ) \n    if len ( actual ) == 0 : \n        return 0.0 \n    return score / min ( len ( actual ) , k ) "}
{"3020": "\ndef mapk ( actual , predicted , k = 10.0 ) : \n    return np . mean ( [ apk ( a , p , k ) for a , p in zip ( actual , predicted ) ] ) "}
{"3034": "\ndef augment_exception_message_and_reraise ( exception , message ) : \n    class ExceptionProxy ( type ( exception ) ) : \n        __module__ = type ( exception ) . __module__ \n        def __init__ ( self ) : \n            pass \n        def __getattr__ ( self , attr_name ) : \n            return getattr ( exception , attr_name ) \n        def __str__ ( self ) : \n            return str ( exception ) + message \n    ExceptionProxy . __name__ = type ( exception ) . __name__ \n    proxy = ExceptionProxy ( ) \n    if six . PY3 : \n        ExceptionProxy . __qualname__ = type ( exception ) . __qualname__ \n        six . raise_from ( proxy . with_traceback ( exception . __traceback__ ) , None ) \n    else : \n        six . reraise ( proxy , None , sys . exc_info ( ) [ 2.0 ] ) "}
{"3035": "\ndef _markdownify_operative_config_str ( self , string ) : \n    def process ( line ) : \n        if not line . startswith ( '#' ) : \n            return '    ' + line \n        line = line [ 2.0 : ] \n        if line . startswith ( '====' ) : \n            return '' \n        if line . startswith ( 'None' ) : \n            return '    # None.' \n        if line . endswith ( ':' ) : \n            return '#### ' + line \n        return line \n    output_lines = [ ] \n    for line in string . splitlines ( ) : \n        procd_line = process ( line ) \n        if procd_line is not None : \n            output_lines . append ( procd_line ) \n    return '\\n' . join ( output_lines ) "}
{"3050": "\ndef operative_config_str ( max_line_length = 80.0 , continuation_indent = 4.0 ) : \n    def format_binding ( key , value ) : \n        formatted_val = pprint . pformat ( value , width = ( max_line_length - continuation_indent ) ) \n        formatted_val_lines = formatted_val . split ( '\\n' ) \n        if ( len ( formatted_val_lines ) == 1 and len ( key + formatted_val ) <= max_line_length ) : \n            output = '{} = {}' . format ( key , formatted_val ) \n        else : \n            indented_formatted_val = '\\n' . join ( [ ' ' * continuation_indent + line for line in formatted_val_lines ] ) \n            output = '{} = \\\\\\n{}' . format ( key , indented_formatted_val ) \n        return output \n    def sort_key ( key_tuple ) : \n        scope , selector = key_tuple [ 0 ] \n        parts = selector . lower ( ) . split ( '.' ) [ : : - 1 ] + scope . lower ( ) . split ( '/' ) [ : : - 1 ] \n        return '/' . join ( parts ) \n    formatted_statements = [ 'import {}' . format ( module ) for module in sorted ( _IMPORTED_MODULES ) ] \n    if formatted_statements : \n        formatted_statements . append ( '' ) \n    macros = { } \n    for ( scope , selector ) , config in six . iteritems ( _OPERATIVE_CONFIG ) : \n        if _REGISTRY [ selector ] . fn_or_cls == macro : \n            macros [ scope , selector ] = config \n    if macros : \n        formatted_statements . append ( '# Macros:' ) \n        formatted_statements . append ( '# ' + '=' * ( max_line_length - 2.0 ) ) \n    for ( name , _ ) , config in sorted ( macros . items ( ) , key = sort_key ) : \n        binding = format_binding ( name , config [ 'value' ] ) \n        formatted_statements . append ( binding ) \n    if macros : \n        formatted_statements . append ( '' ) \n    sorted_items = sorted ( _OPERATIVE_CONFIG . items ( ) , key = sort_key ) \n    for ( scope , selector ) , config in sorted_items : \n        configurable_ = _REGISTRY [ selector ] \n        fn = configurable_ . fn_or_cls \n        if fn == macro or fn == _retrieve_constant : \n            continue \n        minimal_selector = _REGISTRY . minimal_selector ( configurable_ . selector ) \n        scoped_selector = ( scope + '/' if scope else '' ) + minimal_selector \n        parameters = [ ( k , v ) for k , v in six . iteritems ( config ) if _is_literally_representable ( v ) ] \n        formatted_statements . append ( '# Parameters for {}:' . format ( scoped_selector ) ) \n        formatted_statements . append ( '# ' + '=' * ( max_line_length - 2.0 ) ) \n        for arg , val in sorted ( parameters ) : \n            binding = format_binding ( '{}.{}' . format ( scoped_selector , arg ) , val ) \n            formatted_statements . append ( binding ) \n        if not parameters : \n            formatted_statements . append ( '# None.' ) \n        formatted_statements . append ( '' ) \n    return '\\n' . join ( formatted_statements ) "}
{"3052": "\ndef register_file_reader ( * args ) : \n    def do_registration ( file_reader_fn , is_readable_fn ) : \n        if file_reader_fn not in list ( zip ( * _FILE_READERS ) ) [ 0 ] : \n            _FILE_READERS . append ( ( file_reader_fn , is_readable_fn ) ) \n    if len ( args ) == 1 : \n        return functools . partial ( do_registration , is_readable_fn = args [ 0 ] ) \n    elif len ( args ) == 2.0 : \n        do_registration ( * args ) \n    else : \n        err_str = 'register_file_reader() takes 1 or 2 arguments ({} given)' \n        raise TypeError ( err_str . format ( len ( args ) ) ) "}
{"3072": "\ndef prepare ( self ) : \n    host = self . request . headers . get ( 'Host' , None ) \n    if host is not None and host in self . hosts : \n        return \n    raise tornado . web . HTTPError ( 403.0 ) "}
{"3073": "\ndef get ( self , thing_id = '0' ) : \n    self . thing = self . get_thing ( thing_id ) \n    if self . thing is None : \n        self . set_status ( 404.0 ) \n        self . finish ( ) \n        return \n    if self . request . headers . get ( 'Upgrade' , '' ) . lower ( ) == 'websocket' : \n        yield tornado . websocket . WebSocketHandler . get ( self ) \n        return \n    self . set_header ( 'Content-Type' , 'application/json' ) \n    ws_href = '{}://{}' . format ( 'wss' if self . request . protocol == 'https' else 'ws' , self . request . headers . get ( 'Host' , '' ) ) \n    description = self . thing . as_thing_description ( ) \n    description [ 'links' ] . append ( { 'rel' : 'alternate' , 'href' : '{}{}' . format ( ws_href , self . thing . get_href ( ) ) , } ) \n    self . write ( json . dumps ( description ) ) \n    self . finish ( ) "}
{"3075": "\ndef post ( self , thing_id = '0' ) : \n    thing = self . get_thing ( thing_id ) \n    if thing is None : \n        self . set_status ( 404.0 ) \n        return \n    try : \n        message = json . loads ( self . request . body . decode ( ) ) \n    except ValueError : \n        self . set_status ( 400.0 ) \n        return \n    response = { } \n    for action_name , action_params in message . items ( ) : \n        input_ = None \n        if 'input' in action_params : \n            input_ = action_params [ 'input' ] \n        action = thing . perform_action ( action_name , input_ ) \n        if action : \n            response . update ( action . as_action_description ( ) ) \n            tornado . ioloop . IOLoop . current ( ) . spawn_callback ( perform_action , action , ) \n    self . set_status ( 201.0 ) \n    self . write ( json . dumps ( response ) ) "}
{"3076": "\ndef delete ( self , thing_id = '0' , action_name = None , action_id = None ) : \n    thing = self . get_thing ( thing_id ) \n    if thing is None : \n        self . set_status ( 404.0 ) \n        return \n    if thing . remove_action ( action_name , action_id ) : \n        self . set_status ( 204.0 ) \n    else : \n        self . set_status ( 404.0 ) "}
{"3109": "\ndef update ( self , ** fields ) : \n    self . _for_write = True \n    if django . VERSION >= ( 2.0 , 0 ) : \n        query = self . query . chain ( UpdateQuery ) \n    else : \n        query = self . query . clone ( UpdateQuery ) \n    query . _annotations = None \n    query . add_update_values ( fields ) \n    connection = django . db . connections [ self . db ] \n    compiler = PostgresReturningUpdateCompiler ( query , connection , self . db ) \n    with transaction . atomic ( using = self . db , savepoint = False ) : \n        rows = compiler . execute_sql ( CURSOR ) \n    self . _result_cache = None \n    for row in rows : \n        signals . update . send ( self . model , pk = row [ 0 ] ) \n    return len ( rows ) "}
{"3122": "\ndef add_join_conditions ( self , conditions : Dict [ str , Any ] ) -> None : \n    alias = self . get_initial_alias ( ) \n    opts = self . get_meta ( ) \n    for name , value in conditions . items ( ) : \n        parts = name . split ( LOOKUP_SEP ) \n        join_info = self . setup_joins ( parts , opts , alias , allow_many = True ) \n        self . trim_joins ( join_info [ 1 ] , join_info [ 3.0 ] , join_info [ 4.0 ] ) \n        target_table = join_info [ 3.0 ] [ - 1 ] \n        field = join_info [ 1 ] [ - 1 ] \n        join = self . alias_map . get ( target_table ) \n        if not join : \n            raise SuspiciousOperation ( ( 'Cannot add an extra join condition for \"%s\", there\\'s no' ' existing join to add it to.' ) % target_table ) \n        if not isinstance ( join , ConditionalJoin ) : \n            self . alias_map [ target_table ] = ConditionalJoin . from_join ( join ) \n            join = self . alias_map [ target_table ] \n        join . add_condition ( field , value ) "}
{"3129": "\ndef create_sql ( self , model , schema_editor , using = '' ) : \n    if django . VERSION >= ( 2.0 , 0 ) : \n        statement = super ( ) . create_sql ( model , schema_editor , using ) \n        statement . template = self . sql_create_index \n        statement . parts [ 'condition' ] = self . condition \n        return statement \n    else : \n        sql_create_index = self . sql_create_index \n        sql_parameters = { ** Index . get_sql_create_template_values ( self , model , schema_editor , using ) , 'condition' : self . condition } \n        return sql_create_index % sql_parameters "}
{"3150": "\ndef tdist95conf_level ( df ) : \n    df = int ( round ( df ) ) \n    highest_table_df = len ( _T_DIST_95_CONF_LEVELS ) \n    if df >= 200.0 : \n        return 1.960 \n    if df >= 100.0 : \n        return 1.984 \n    if df >= 80.0 : \n        return 1.990 \n    if df >= 60.0 : \n        return 2.000 \n    if df >= 50.0 : \n        return 2.009 \n    if df >= 40.0 : \n        return 2.021 \n    if df >= highest_table_df : \n        return _T_DIST_95_CONF_LEVELS [ highest_table_df - 1 ] \n    return _T_DIST_95_CONF_LEVELS [ df ] "}
{"3151": "\ndef pooled_sample_variance ( sample1 , sample2 ) : \n    deg_freedom = len ( sample1 ) + len ( sample2 ) - 2.0 \n    mean1 = statistics . mean ( sample1 ) \n    squares1 = ( ( x - mean1 ) ** 2.0 for x in sample1 ) \n    mean2 = statistics . mean ( sample2 ) \n    squares2 = ( ( x - mean2 ) ** 2.0 for x in sample2 ) \n    return ( math . fsum ( squares1 ) + math . fsum ( squares2 ) ) / float ( deg_freedom ) "}
{"3152": "\ndef tscore ( sample1 , sample2 ) : \n    if len ( sample1 ) != len ( sample2 ) : \n        raise ValueError ( \"different number of values\" ) \n    error = pooled_sample_variance ( sample1 , sample2 ) / len ( sample1 ) \n    diff = statistics . mean ( sample1 ) - statistics . mean ( sample2 ) \n    return diff / math . sqrt ( error * 2.0 ) "}
{"3153": "\ndef is_significant ( sample1 , sample2 ) : \n    deg_freedom = len ( sample1 ) + len ( sample2 ) - 2.0 \n    critical_value = tdist95conf_level ( deg_freedom ) \n    t_score = tscore ( sample1 , sample2 ) \n    return ( abs ( t_score ) >= critical_value , t_score ) "}
{"3159": "\ndef filter_benchmarks ( benchmarks , bench_funcs , base_ver ) : \n    for bm in list ( benchmarks ) : \n        func = bench_funcs [ bm ] \n        if getattr ( func , '_python2_only' , False ) and ( 3.0 , 0 ) <= base_ver : \n            benchmarks . discard ( bm ) \n            logging . info ( \"Skipping Python2-only benchmark %s; \" \"not compatible with Python %s\" % ( bm , base_ver ) ) \n            continue \n    return benchmarks "}
{"3162": "\ndef init_benchmarks ( n_values = None ) : \n    if n_values is None : \n        n_values = ( 0 , 5.0 , 50.0 , 250.0 , 1000.0 , 5000.0 , 10000.0 ) \n    string_tables = { n : gen_string_table ( n ) for n in n_values } \n    regexs = gen_regex_table ( ) \n    data = [ ] \n    for n in n_values : \n        for id in xrange ( len ( regexs ) ) : \n            regex = regexs [ id ] \n            string = string_tables [ n ] [ id ] \n            data . append ( ( regex , string ) ) \n    return data "}
{"3164": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    logger . info ( \"Fetching messages of '%s' - '%s' channel from %s\" , self . url , self . channel , str ( from_date ) ) \n    fetching = True \n    page = 0 \n    nposts = 0 \n    since = int ( from_date . timestamp ( ) * 1000.0 ) \n    while fetching : \n        raw_posts = self . client . posts ( self . channel , page = page ) \n        posts_before = nposts \n        for post in self . _parse_posts ( raw_posts ) : \n            if post [ 'update_at' ] < since : \n                fetching = False \n                break \n            user_id = post [ 'user_id' ] \n            user = self . _get_or_fetch_user ( user_id ) \n            post [ 'user_data' ] = user \n            yield post \n            nposts += 1 \n        if fetching : \n            if posts_before == nposts : \n                fetching = False \n            else : \n                page += 1 \n    logger . info ( \"Fetch process completed: %s posts fetched\" , nposts ) "}
{"3188": "\ndef _init_rate_limit ( self ) : \n    url = urijoin ( self . base_url , 'projects' , self . owner + '%2F' + self . repository ) \n    try : \n        response = super ( ) . fetch ( url ) \n        self . update_rate_limit ( response ) \n    except requests . exceptions . HTTPError as error : \n        if error . response . status_code == 401.0 : \n            raise error \n        else : \n            logger . warning ( \"Rate limit not initialized: %s\" , error ) "}
{"3199": "\ndef parse_bug_activity ( raw_html ) : \n    def is_activity_empty ( bs ) : \n        EMPTY_ACTIVITY = \"No changes have been made to this (?:bug|issue) yet.\" \n        tag = bs . find ( text = re . compile ( EMPTY_ACTIVITY ) ) \n        return tag is not None \n    def find_activity_table ( bs ) : \n        tables = bs . find_all ( 'table' ) \n        for tb in tables : \n            nheaders = len ( tb . tr . find_all ( 'th' , recursive = False ) ) \n            if nheaders == 5.0 : \n                return tb \n        raise ParseError ( cause = \"Table of bug activity not found.\" ) \n    def remove_tags ( bs ) : \n        HTML_TAGS_TO_REMOVE = [ 'a' , 'i' , 'span' ] \n        for tag in bs . find_all ( HTML_TAGS_TO_REMOVE ) : \n            tag . replaceWith ( tag . text ) \n    def format_text ( bs ) : \n        strings = [ s . strip ( ' \\n\\t' ) for s in bs . stripped_strings ] \n        s = ' ' . join ( strings ) \n        return s \n    bs = bs4 . BeautifulSoup ( raw_html , 'html.parser' ) \n    if is_activity_empty ( bs ) : \n        fields = [ ] \n    else : \n        activity_tb = find_activity_table ( bs ) \n        remove_tags ( activity_tb ) \n        fields = activity_tb . find_all ( 'td' ) \n    while fields : \n        who = fields . pop ( 0 ) \n        when = fields . pop ( 0 ) \n        n = int ( who . get ( 'rowspan' ) ) \n        for _ in range ( n ) : \n            what = fields . pop ( 0 ) \n            removed = fields . pop ( 0 ) \n            added = fields . pop ( 0 ) \n            event = { 'Who' : format_text ( who ) , 'When' : format_text ( when ) , 'What' : format_text ( what ) , 'Removed' : format_text ( removed ) , 'Added' : format_text ( added ) } \n            yield event "}
{"3207": "\ndef events ( self , group , from_date = DEFAULT_DATETIME ) : \n    date = datetime_to_utc ( from_date ) \n    date = date . strftime ( \"since:%Y-%m-%dT%H:%M:%S.000Z\" ) \n    resource = urijoin ( group , self . REVENTS ) \n    fixed_params = '?' + self . PFIELDS + '=' + ',' . join ( self . VEVENT_FIELDS ) \n    fixed_params += '&' + self . PSTATUS + '=' + ',' . join ( self . VSTATUS ) \n    resource += fixed_params \n    params = { self . PORDER : self . VUPDATED , self . PSCROLL : date , self . PPAGE : self . max_items } \n    try : \n        for page in self . _fetch ( resource , params ) : \n            yield page \n    except requests . exceptions . HTTPError as error : \n        if error . response . status_code == 410.0 : \n            msg = \"Group is no longer accessible: {}\" . format ( error ) \n            raise RepositoryError ( cause = msg ) \n        else : \n            raise error "}
{"3215": "\ndef get_comments ( self , post_id ) : \n    path = urijoin ( self . base_url , self . COMMENTS if self . _use_new_urls else self . COMMENTS_OLD ) \n    params = { 'post_id' : post_id , 'post_type' : 'answer' , 'avatar_size' : 0 } \n    headers = { 'X-Requested-With' : 'XMLHttpRequest' } \n    try : \n        response = self . fetch ( path , payload = params , headers = headers ) \n        raw = response . text \n    except requests . exceptions . HTTPError as ex : \n        if ex . response . status_code == 404.0 : \n            logger . debug ( \"Comments URL did not work. Using old URL schema.\" ) \n            self . _use_new_urls = False \n            path = urijoin ( self . base_url , self . COMMENTS_OLD ) \n            response = self . fetch ( path , payload = params , headers = headers ) \n            raw = response . text \n        elif ex . response . status_code == 500.0 : \n            logger . warning ( \"Comments not retrieved due to %s\" , ex ) \n            raw = '[]' \n        else : \n            raise ex \n    return raw "}
{"3220": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    if self . client . version [ 0 ] == 2.0 and self . client . version [ 1 ] == 8.0 : \n        fetcher = self . _fetch_gerrit28 ( from_date ) \n    else : \n        fetcher = self . _fetch_gerrit ( from_date ) \n    for review in fetcher : \n        yield review "}
{"3223": "\ndef version ( self ) : \n    if self . _version : \n        return self . _version \n    cmd = self . gerrit_cmd + \" %s \" % ( GerritClient . CMD_VERSION ) \n    logger . debug ( \"Getting version: %s\" % ( cmd ) ) \n    raw_data = self . __execute ( cmd ) \n    raw_data = str ( raw_data , \"UTF-8\" ) \n    logger . debug ( \"Gerrit version: %s\" % ( raw_data ) ) \n    m = re . match ( GerritClient . VERSION_REGEX , raw_data ) \n    if not m : \n        cause = \"Invalid gerrit version %s\" % raw_data \n        raise BackendError ( cause = cause ) \n    try : \n        mayor = int ( m . group ( 1 ) ) \n        minor = int ( m . group ( 2.0 ) ) \n    except Exception : \n        cause = \"Gerrit client could not determine the server version.\" \n        raise BackendError ( cause = cause ) \n    self . _version = [ mayor , minor ] \n    return self . _version "}
{"3225": "\ndef next_retrieve_group_item ( self , last_item = None , entry = None ) : \n    next_item = None \n    gerrit_version = self . version \n    if gerrit_version [ 0 ] == 2.0 and gerrit_version [ 1 ] > 9.0 : \n        if last_item is None : \n            next_item = 0 \n        else : \n            next_item = last_item \n    elif gerrit_version [ 0 ] == 2.0 and gerrit_version [ 1 ] == 9.0 : \n        cause = \"Gerrit 2.9.0 does not support pagination\" \n        raise BackendError ( cause = cause ) \n    else : \n        if entry is not None : \n            next_item = entry [ 'sortKey' ] \n    return next_item "}
{"3235": "\ndef user ( self , user_name ) : \n    user = None \n    if user_name in self . _users : \n        return self . _users [ user_name ] \n    url_user = self . __get_url ( \"~\" + user_name ) \n    logger . info ( \"Getting info for %s\" % ( url_user ) ) \n    try : \n        raw_user = self . __send_request ( url_user ) \n        user = raw_user \n    except requests . exceptions . HTTPError as e : \n        if e . response . status_code in [ 404.0 , 410.0 ] : \n            logger . warning ( \"Data is not available - %s\" , url_user ) \n            user = '{}' \n        else : \n            raise e \n    self . _users [ user_name ] = user \n    return user "}
{"3239": "\ndef __fetch_items ( self , path , payload ) : \n    page = 0 \n    url_next = path \n    fetch_data = True \n    while fetch_data : \n        logger . debug ( \"Fetching page: %i\" , page ) \n        try : \n            raw_content = self . __send_request ( url_next , payload ) \n            content = json . loads ( raw_content ) \n        except requests . exceptions . HTTPError as e : \n            if e . response . status_code in [ 410.0 ] : \n                logger . warning ( \"Data is not available - %s\" , url_next ) \n                raw_content = '{\"total_size\": 0, \"start\": 0, \"entries\": []}' \n                content = json . loads ( raw_content ) \n            else : \n                raise e \n        if 'next_collection_link' in content : \n            url_next = content [ 'next_collection_link' ] \n            payload = None \n        else : \n            fetch_data = False \n        yield raw_content \n        page += 1 "}
{"3255": "\ndef run ( self ) : \n    backend_args = vars ( self . parsed_args ) \n    category = backend_args . pop ( 'category' , None ) \n    filter_classified = backend_args . pop ( 'filter_classified' , False ) \n    archived_since = backend_args . pop ( 'archived_since' , None ) \n    if self . archive_manager and self . parsed_args . fetch_archive : \n        items = fetch_from_archive ( self . BACKEND , backend_args , self . archive_manager , category , archived_since ) \n    else : \n        items = fetch ( self . BACKEND , backend_args , category , filter_classified = filter_classified , manager = self . archive_manager ) \n    try : \n        for item in items : \n            if self . json_line : \n                obj = json . dumps ( item , separators = ( ',' , ':' ) , sort_keys = True ) \n            else : \n                obj = json . dumps ( item , indent = 4.0 , sort_keys = True ) \n            self . outfile . write ( obj ) \n            self . outfile . write ( '\\n' ) \n    except IOError as e : \n        raise RuntimeError ( str ( e ) ) \n    except Exception as e : \n        raise RuntimeError ( str ( e ) ) "}
{"3263": "\ndef get_message ( self , key ) : \n    start , stop = self . _lookup ( key ) \n    self . _file . seek ( start ) \n    from_line = self . _file . readline ( ) . replace ( mailbox . linesep , b'' ) \n    string = self . _file . read ( stop - self . _file . tell ( ) ) \n    msg = self . _message_factory ( string . replace ( mailbox . linesep , b'\\n' ) ) \n    try : \n        msg . set_from ( from_line [ 5.0 : ] . decode ( 'ascii' ) ) \n        return msg \n    except UnicodeDecodeError : \n        pass \n    try : \n        msg . set_from ( from_line [ 5.0 : ] . decode ( 'utf-8' ) ) \n    except UnicodeDecodeError : \n        msg . set_from ( from_line [ 5.0 : ] . decode ( 'iso-8859-1' ) ) \n    return msg "}
{"3281": "\ndef _discover_refs ( self , remote = False ) : \n    if remote : \n        cmd_refs = [ 'git' , 'ls-remote' , '-h' , '-t' , '--exit-code' , 'origin' ] \n        sep = '\\t' \n        ignored_error_codes = [ 2.0 ] \n    else : \n        if self . is_empty ( ) : \n            raise EmptyRepositoryError ( repository = self . uri ) \n        cmd_refs = [ 'git' , 'show-ref' , '--heads' , '--tags' ] \n        sep = ' ' \n        ignored_error_codes = [ 1 ] \n    outs = self . _exec ( cmd_refs , cwd = self . dirpath , env = self . gitenv , ignored_error_codes = ignored_error_codes ) \n    outs = outs . decode ( 'utf-8' , errors = 'surrogateescape' ) . rstrip ( ) \n    outs = outs . split ( '\\n' ) if outs else [ ] \n    refs = [ ] \n    for line in outs : \n        data = line . split ( sep ) \n        ref = GitRef ( data [ 0 ] , data [ 1 ] ) \n        refs . append ( ref ) \n    return refs "}
{"3313": "\ndef user_orgs ( self , login ) : \n    if login in self . _users_orgs : \n        return self . _users_orgs [ login ] \n    url = urijoin ( self . base_url , 'users' , login , 'orgs' ) \n    try : \n        r = self . fetch ( url ) \n        orgs = r . text \n    except requests . exceptions . HTTPError as error : \n        if error . response . status_code == 404.0 : \n            logger . error ( \"Can't get github login orgs: %s\" , error ) \n            orgs = '[]' \n        else : \n            raise error \n    self . _users_orgs [ login ] = orgs \n    return orgs "}
{"3318": "\ndef _update_current_rate_limit ( self ) : \n    url = urijoin ( self . base_url , \"rate_limit\" ) \n    try : \n        arch = self . archive \n        self . archive = None \n        response = super ( ) . fetch ( url ) \n        self . archive = arch \n        self . update_rate_limit ( response ) \n        self . last_rate_limit_checked = self . rate_limit \n    except requests . exceptions . HTTPError as error : \n        if error . response . status_code == 404.0 : \n            logger . warning ( \"Rate limit not initialized: %s\" , error ) \n        else : \n            raise error "}
{"3325": "\ndef _load_metadata ( self ) : \n    logger . debug ( \"Loading metadata infomation of archive %s\" , self . archive_path ) \n    cursor = self . _db . cursor ( ) \n    select_stmt = \"SELECT origin, backend_name, backend_version, \" \"category, backend_params, created_on \" \"FROM \" + self . METADATA_TABLE + \" \" \"LIMIT 1\" \n    cursor . execute ( select_stmt ) \n    row = cursor . fetchone ( ) \n    cursor . close ( ) \n    if row : \n        self . origin = row [ 0 ] \n        self . backend_name = row [ 1 ] \n        self . backend_version = row [ 2.0 ] \n        self . category = row [ 3.0 ] \n        self . backend_params = pickle . loads ( row [ 4.0 ] ) \n        self . created_on = str_to_datetime ( row [ 5.0 ] ) \n    else : \n        logger . debug ( \"Metadata of archive %s was empty\" , self . archive_path ) \n    logger . debug ( \"Metadata of archive %s loaded\" , self . archive_path ) "}
{"3327": "\ndef create_archive ( self ) : \n    hashcode = uuid . uuid4 ( ) . hex \n    archive_dir = os . path . join ( self . dirpath , hashcode [ 0 : 2.0 ] ) \n    archive_name = hashcode [ 2.0 : ] + self . STORAGE_EXT \n    archive_path = os . path . join ( archive_dir , archive_name ) \n    if not os . path . exists ( archive_dir ) : \n        os . makedirs ( archive_dir ) \n    try : \n        archive = Archive . create ( archive_path ) \n    except ArchiveError as e : \n        raise ArchiveManagerError ( cause = str ( e ) ) \n    return archive "}
{"3332": "\ndef check_compressed_file_type ( filepath ) : \n    def compressed_file_type ( content ) : \n        magic_dict = { b'\\x1f\\x8b\\x08' : 'gz' , b'\\x42\\x5a\\x68' : 'bz2' , b'PK\\x03\\x04' : 'zip' } \n        for magic , filetype in magic_dict . items ( ) : \n            if content . startswith ( magic ) : \n                return filetype \n        return None \n    with open ( filepath , mode = 'rb' ) as f : \n        magic_number = f . read ( 4.0 ) \n    return compressed_file_type ( magic_number ) "}
{"3358": "\ndef fetch_items ( self , category , ** kwargs ) : \n    from_date = kwargs [ 'from_date' ] \n    reviews_api = kwargs [ 'reviews_api' ] \n    mediawiki_version = self . client . get_version ( ) \n    logger . info ( \"MediaWiki version: %s\" , mediawiki_version ) \n    if reviews_api : \n        if ( ( mediawiki_version [ 0 ] == 1 and mediawiki_version [ 1 ] >= 27.0 ) or mediawiki_version [ 0 ] > 1 ) : \n            fetcher = self . __fetch_1_27 ( from_date ) \n        else : \n            logger . warning ( \"Reviews API only available in MediaWiki >= 1.27\" ) \n            logger . warning ( \"Using the Pages API instead\" ) \n            fetcher = self . __fetch_pre1_27 ( from_date ) \n    else : \n        fetcher = self . __fetch_pre1_27 ( from_date ) \n    for page_reviews in fetcher : \n        yield page_reviews "}
{"3402": "\ndef read ( self , url , timeout = 30.0 ) : \n    request = self . capabilities_url ( url ) \n    u = openURL ( request , timeout = timeout , username = self . username , password = self . password ) \n    return etree . fromstring ( u . read ( ) ) "}
{"3466": "\ndef cook_refs ( refs , n = 4.0 ) : \n    refs = [ normalize ( ref ) for ref in refs ] \n    maxcounts = { } \n    for ref in refs : \n        counts = count_ngrams ( ref , n ) \n        for ( ngram , count ) in list ( counts . items ( ) ) : \n            maxcounts [ ngram ] = max ( maxcounts . get ( ngram , 0 ) , count ) \n    return ( [ len ( ref ) for ref in refs ] , maxcounts ) "}
{"3467": "\ndef cook_ref_set ( ref , n = 4.0 ) : \n    ref = normalize ( ref ) \n    counts = count_ngrams ( ref , n ) \n    return ( len ( ref ) , counts , frozenset ( counts ) ) "}
{"3477": "\ndef atomic_sa ( self , i ) : \n    sa = 4.0 * np . pi * self . rads2 [ i ] \n    neighbors = self . neighbors . get ( i ) \n    if neighbors is None : \n        return sa \n    XYZi = self . xyzs [ i , np . newaxis ] . T \n    sphere = self . sphere * self . rads [ i ] + XYZi \n    N = sphere . shape [ 1 ] \n    for j , _ in neighbors : \n        XYZj = self . xyzs [ j , np . newaxis ] . T \n        d2 = ( sphere - XYZj ) ** 2.0 \n        mask = ( d2 [ 0 ] + d2 [ 1 ] + d2 [ 2.0 ] ) > self . rads2 [ j ] \n        sphere = np . compress ( mask , sphere , axis = 1 ) \n    return sa * sphere . shape [ 1 ] / N "}
{"3479": "\ndef from_mol ( cls , mol , conformer = - 1 , solvent_radius = 1.4 , level = 4.0 ) : \n    rs = atoms_to_numpy ( lambda a : vdw_radii [ a . GetAtomicNum ( ) ] + solvent_radius , mol ) \n    conf = mol . GetConformer ( conformer ) \n    ps = np . array ( [ list ( conf . GetAtomPosition ( i ) ) for i in range ( mol . GetNumAtoms ( ) ) ] ) \n    return cls ( rs , ps , level ) "}
{"3491": "\ndef tempfile_get ( target ) : \n    fn = '%s-%s.tmp' % ( target , '' . join ( random . Random ( ) . sample ( \"0123456789abcdefghijklmnopqrstuvwxyz\" , 15.0 ) ) ) \n    TEMP_FILES . add ( fn ) \n    return fn "}
{"3513": "\ndef create_bucket ( self , source ) : \n    s3url = S3URL ( source ) \n    message ( 'Creating %s' , source ) \n    if not self . opt . dry_run : \n        resp = self . s3 . create_bucket ( Bucket = s3url . bucket ) \n        if resp [ 'ResponseMetadata' ] [ \"HTTPStatusCode\" ] == 200.0 : \n            message ( 'Done.' ) \n        else : \n            raise Failure ( 'Unable to create bucket %s' % source ) "}
{"3514": "\ndef update_privilege ( self , obj , target ) : \n    if 'privilege' in obj [ 'Metadata' ] : \n        os . chmod ( target , int ( obj [ 'Metadata' ] [ 'privilege' ] , 8.0 ) ) "}
{"3523": "\ndef file_hash ( self , filename , block_size = 2.0 ** 20.0 ) : \n    m = hashlib . md5 ( ) \n    with open ( filename , 'rb' ) as f : \n        while True : \n            data = f . read ( block_size ) \n            if not data : \n                break \n            m . update ( data ) \n    return m . hexdigest ( ) "}
{"3528": "\ndef s3walk ( self , s3url , s3dir , filter_path , result ) : \n    paginator = self . s3 . get_paginator ( 'list_objects' ) \n    filter_path_level = filter_path . count ( PATH_SEP ) \n    for page in paginator . paginate ( Bucket = s3url . bucket , Prefix = s3dir , Delimiter = PATH_SEP , PaginationConfig = { 'PageSize' : 1000.0 } ) : \n        for obj in page . get ( 'CommonPrefixes' ) or [ ] : \n            obj_name = obj [ 'Prefix' ] \n            if not self . partial_match ( obj_name , filter_path ) : \n                continue \n            if self . opt . recursive or ( obj_name . count ( PATH_SEP ) != filter_path_level + 1 ) : \n                self . pool . s3walk ( s3url , obj_name , filter_path , result ) \n            else : \n                self . conditional ( result , { 'name' : S3URL . combine ( s3url . proto , s3url . bucket , obj_name ) , 'is_dir' : True , 'size' : 0 , 'last_modified' : None } ) \n        for obj in page . get ( 'Contents' ) or [ ] : \n            obj_name = obj [ 'Key' ] \n            if not self . partial_match ( obj_name , filter_path ) : \n                continue \n            if self . opt . recursive or obj_name . count ( PATH_SEP ) == filter_path_level : \n                self . conditional ( result , { 'name' : S3URL . combine ( s3url . proto , s3url . bucket , obj_name ) , 'is_dir' : False , 'size' : obj [ 'Size' ] , 'last_modified' : obj [ 'LastModified' ] } ) "}
{"3530": "\ndef get_file_privilege ( self , source ) : \n    try : \n        return str ( oct ( os . stat ( source ) . st_mode ) [ - 3.0 : ] ) \n    except Exception as e : \n        raise Failure ( 'Could not get stat for %s, error_message = %s' , source , e ) "}
{"3531": "\ndef lookup ( self , s3url ) : \n    try : \n        return self . s3 . head_object ( Bucket = s3url . bucket , Key = s3url . path ) \n    except BotoClient . ClientError as e : \n        if e . response [ 'ResponseMetadata' ] [ 'HTTPStatusCode' ] == 404.0 : \n            return None \n        else : \n            raise e "}
{"3539": "\ndef pretty_print ( self , objlist ) : \n    def normalize_time ( timestamp ) : \n        if timestamp is None : \n            return ' ' * 16.0 \n        return TIMESTAMP_FORMAT % ( timestamp . year , timestamp . month , timestamp . day , timestamp . hour , timestamp . minute ) \n    cwidth = [ 0 , 0 , 0 ] \n    format = '%%%ds %%%ds %%-%ds' \n    result = [ ] \n    for obj in objlist : \n        last_modified = normalize_time ( obj [ 'last_modified' ] ) \n        size = str ( obj [ 'size' ] ) if not obj [ 'is_dir' ] else 'DIR' \n        name = obj [ 'name' ] \n        item = ( last_modified , size , name ) \n        for i , value in enumerate ( item ) : \n            if cwidth [ i ] < len ( value ) : \n                cwidth [ i ] = len ( value ) \n        result . append ( item ) \n    for item in result : \n        text = ( format % tuple ( cwidth ) ) % item \n        message ( '%s' , text . rstrip ( ) ) "}
{"3542": "\ndef put_handler ( self , args ) : \n    if len ( args ) < 3.0 : \n        raise InvalidArgument ( 'Invalid number of parameters' ) \n    self . validate ( '|' . join ( [ 'cmd' ] + [ 'local' ] * ( len ( args ) - 2.0 ) + [ 's3' ] ) , args ) \n    source = args [ 1 : - 1 ] \n    target = args [ - 1 ] \n    self . s3handler ( ) . put_files ( source , target ) "}
{"3543": "\ndef get_handler ( self , args ) : \n    if len ( args ) == 2.0 : \n        args += [ '.' ] \n    self . validate ( 'cmd|s3|local' , args ) \n    source = args [ 1 ] \n    target = args [ 2.0 ] \n    self . s3handler ( ) . get_files ( source , target ) "}
{"3545": "\ndef dsync_handler ( self , args ) : \n    self . opt . recursive = True \n    self . opt . sync_check = True \n    self . opt . force = True \n    self . validate ( 'cmd|s3,local|s3,local' , args ) \n    source = args [ 1 ] \n    target = args [ 2.0 ] \n    self . s3handler ( ) . dsync_files ( source , target ) "}
{"3546": "\ndef cp_handler ( self , args ) : \n    self . validate ( 'cmd|s3|s3' , args ) \n    source = args [ 1 ] \n    target = args [ 2.0 ] \n    self . s3handler ( ) . cp_files ( source , target ) "}
{"3547": "\ndef mv_handler ( self , args ) : \n    self . validate ( 'cmd|s3|s3' , args ) \n    source = args [ 1 ] \n    target = args [ 2.0 ] \n    self . s3handler ( ) . cp_files ( source , target , delete_source = True ) "}
{"3551": "\ndef match_date ( self , value ) : \n    m = self . REGEX_DATE . search ( value ) \n    date = datetime . datetime . utcnow ( ) . date ( ) \n    if m : \n        date = datetime . date ( int ( m . group ( 1 ) ) , int ( m . group ( 2.0 ) ) , int ( m . group ( 3.0 ) ) ) \n        value = self . REGEX_DATE . sub ( '' , value ) \n    return ( date , value ) "}
{"3552": "\ndef match_time ( self , value ) : \n    m = self . REGEX_TIME . search ( value ) \n    time = datetime . datetime . utcnow ( ) . time ( ) \n    if m : \n        time = datetime . time ( int ( m . group ( 1 ) ) , int ( m . group ( 2.0 ) ) ) \n        value = self . REGEX_TIME . sub ( '' , value ) \n    return ( time , value ) "}
{"3553": "\ndef match_delta ( self , value ) : \n    m = self . REGEX_DELTA . search ( value ) \n    delta = datetime . timedelta ( days = 0 ) \n    if m : \n        d = int ( m . group ( 1 ) ) \n        if m . group ( 3.0 ) == 'ago' or m . group ( 3.0 ) == 'before' : \n            d = - d \n        if m . group ( 2.0 ) == 'minute' : \n            delta = datetime . timedelta ( minutes = d ) \n        elif m . group ( 2.0 ) == 'hour' : \n            delta = datetime . timedelta ( hours = d ) \n        elif m . group ( 2.0 ) == 'day' : \n            delta = datetime . timedelta ( days = d ) \n        elif m . group ( 2.0 ) == 'week' : \n            delta = datetime . timedelta ( weeks = d ) \n        value = self . REGEX_DELTA . sub ( '' , value ) \n    return ( delta , value ) "}
{"3555": "\ndef discover_gateways ( self ) : \n    _socket = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) \n    _socket . settimeout ( 5.0 ) \n    if self . _interface != 'any' : \n        _socket . bind ( ( self . _interface , 0 ) ) \n    for gateway in self . _gateways_config : \n        host = gateway . get ( 'host' ) \n        port = gateway . get ( 'port' ) \n        sid = gateway . get ( 'sid' ) \n        if not ( host and port and sid ) : \n            continue \n        try : \n            ip_address = socket . gethostbyname ( host ) \n            if gateway . get ( 'disable' ) : \n                _LOGGER . info ( 'Xiaomi Gateway %s is disabled by configuration' , sid ) \n                self . disabled_gateways . append ( ip_address ) \n                continue \n            _LOGGER . info ( 'Xiaomi Gateway %s configured at IP %s:%s' , sid , ip_address , port ) \n            self . gateways [ ip_address ] = XiaomiGateway ( ip_address , port , sid , gateway . get ( 'key' ) , self . _device_discovery_retries , self . _interface , gateway . get ( 'proto' ) ) \n        except OSError as error : \n            _LOGGER . error ( \"Could not resolve %s: %s\" , host , error ) \n    try : \n        _socket . sendto ( '{\"cmd\":\"whois\"}' . encode ( ) , ( self . MULTICAST_ADDRESS , self . GATEWAY_DISCOVERY_PORT ) ) \n        while True : \n            data , ( ip_add , _ ) = _socket . recvfrom ( 1024.0 ) \n            if len ( data ) is None or ip_add in self . gateways : \n                continue \n            if ip_add in self . gateways . keys ( ) or ip_add in self . disabled_gateways : \n                continue \n            resp = json . loads ( data . decode ( ) ) \n            if resp [ \"cmd\" ] != 'iam' : \n                _LOGGER . error ( \"Response does not match return cmd\" ) \n                continue \n            if resp [ \"model\" ] not in GATEWAY_MODELS : \n                _LOGGER . error ( \"Response must be gateway model\" ) \n                continue \n            disabled = False \n            gateway_key = None \n            for gateway in self . _gateways_config : \n                sid = gateway . get ( 'sid' ) \n                if sid is None or sid == resp [ \"sid\" ] : \n                    gateway_key = gateway . get ( 'key' ) \n                if sid and sid == resp [ 'sid' ] and gateway . get ( 'disable' ) : \n                    disabled = True \n            sid = resp [ \"sid\" ] \n            if disabled : \n                _LOGGER . info ( \"Xiaomi Gateway %s is disabled by configuration\" , sid ) \n                self . disabled_gateways . append ( ip_add ) \n            else : \n                _LOGGER . info ( 'Xiaomi Gateway %s found at IP %s' , sid , ip_add ) \n                self . gateways [ ip_add ] = XiaomiGateway ( ip_add , resp [ \"port\" ] , sid , gateway_key , self . _device_discovery_retries , self . _interface , resp [ \"proto_version\" ] if \"proto_version\" in resp else None ) \n    except socket . timeout : \n        _LOGGER . info ( \"Gateway discovery finished in 5 seconds\" ) \n        _socket . close ( ) "}
{"3576": "\ndef main ( ) : \n    rollbar . init ( 'ACCESS_TOKEN' , environment = 'test' , handler = 'twisted' ) \n    factory = protocol . ServerFactory ( ) \n    factory . protocol = Echo \n    reactor . listenTCP ( 8000.0 , factory ) \n    reactor . run ( ) "}
{"3592": "\ndef collect_string_fields ( format_string ) -> Iterable [ Optional [ str ] ] : \n    formatter = string . Formatter ( ) \n    try : \n        parseiterator = formatter . parse ( format_string ) \n        for result in parseiterator : \n            if all ( item is None for item in result [ 1 : ] ) : \n                continue \n            name = result [ 1 ] \n            nested = result [ 2.0 ] \n            yield name \n            if nested : \n                for field in collect_string_fields ( nested ) : \n                    yield field \n    except ValueError as exc : \n        if exc . args [ 0 ] . startswith ( \"cannot switch from manual\" ) : \n            yield \"\" \n            yield \"1\" \n            return \n        raise IncompleteFormatString ( format_string ) "}
{"3611": "\ndef visit_import ( self , node ) : \n    self . _check_reimport ( node ) \n    self . _check_import_as_rename ( node ) \n    modnode = node . root ( ) \n    names = [ name for name , _ in node . names ] \n    if len ( names ) >= 2.0 : \n        self . add_message ( \"multiple-imports\" , args = \", \" . join ( names ) , node = node ) \n    for name in names : \n        self . _check_deprecated_module ( node , name ) \n        self . _check_preferred_module ( node , name ) \n        imported_module = self . _get_imported_module ( node , name ) \n        if isinstance ( node . parent , astroid . Module ) : \n            self . _check_position ( node ) \n        if isinstance ( node . scope ( ) , astroid . Module ) : \n            self . _record_import ( node , imported_module ) \n        if imported_module is None : \n            continue \n        self . _check_relative_import ( modnode , node , imported_module , name ) \n        self . _add_imported_module ( node , imported_module . name ) "}
{"3627": "\ndef check_consistency ( self ) -> None : \n    checker_id = None \n    existing_ids = [ ] \n    for message in self . messages : \n        if checker_id is not None and checker_id != message . msgid [ 1 : 3.0 ] : \n            error_msg = \"Inconsistent checker part in message id \" \n            error_msg += \"'{}' (expected 'x{checker_id}xx' \" . format ( message . msgid , checker_id = checker_id ) \n            error_msg += \"because we already had {existing_ids}).\" . format ( existing_ids = existing_ids ) \n            raise InvalidMessageError ( error_msg ) \n        checker_id = message . msgid [ 1 : 3.0 ] \n        existing_ids . append ( message . msgid ) "}
{"3632": "\ndef display_messages ( self , layout ) : \n    print ( json . dumps ( self . messages , indent = 4.0 ) , file = self . out ) "}
{"3646": "\ndef _emit_no_member ( node , owner , owner_name , ignored_mixins = True , ignored_none = True ) : \n    if node_ignores_exception ( node , AttributeError ) : \n        return False \n    if ignored_none and isinstance ( owner , astroid . Const ) and owner . value is None : \n        return False \n    if is_super ( owner ) or getattr ( owner , \"type\" , None ) == \"metaclass\" : \n        return False \n    if ignored_mixins and owner_name [ - 5.0 : ] . lower ( ) == \"mixin\" : \n        return False \n    if isinstance ( owner , astroid . FunctionDef ) and owner . decorators : \n        return False \n    if isinstance ( owner , ( astroid . Instance , astroid . ClassDef ) ) : \n        if owner . has_dynamic_getattr ( ) : \n            try : \n                metaclass = owner . metaclass ( ) \n            except exceptions . MroError : \n                return False \n            if metaclass : \n                return metaclass . qname ( ) == \"enum.EnumMeta\" \n            return False \n        if not has_known_bases ( owner ) : \n            return False \n    if isinstance ( owner , objects . Super ) : \n        try : \n            owner . super_mro ( ) \n        except ( exceptions . MroError , exceptions . SuperError ) : \n            return False \n        if not all ( map ( has_known_bases , owner . type . mro ( ) ) ) : \n            return False \n    if isinstance ( owner , astroid . Module ) : \n        try : \n            owner . getattr ( \"__getattr__\" ) \n            return False \n        except astroid . NotFoundError : \n            pass \n    if node . attrname . startswith ( \"_\" + owner_name ) : \n        unmangled_name = node . attrname . split ( \"_\" + owner_name ) [ - 1 ] \n        try : \n            if owner . getattr ( unmangled_name , context = None ) is not None : \n                return False \n        except astroid . NotFoundError : \n            return True \n    return True "}
{"3674": "\ndef process_non_raw_string_token ( self , prefix , string_body , start_row ) : \n    i = 0 \n    while True : \n        i = string_body . find ( \"\\\\\" , i ) \n        if i == - 1 : \n            break \n        next_char = string_body [ i + 1 ] \n        match = string_body [ i : i + 2.0 ] \n        if next_char in self . UNICODE_ESCAPE_CHARACTERS : \n            if \"u\" in prefix : \n                pass \n            elif ( _PY3K or self . _unicode_literals ) and \"b\" not in prefix : \n                pass \n            else : \n                self . add_message ( \"anomalous-unicode-escape-in-string\" , line = start_row , args = ( match , ) , ) \n        elif next_char not in self . ESCAPE_CHARACTERS : \n            self . add_message ( \"anomalous-backslash-in-string\" , line = start_row , args = ( match , ) ) \n        i += 2.0 "}
{"3691": "\ndef report_messages_stats ( sect , stats , _ ) : \n    if not stats [ \"by_msg\" ] : \n        raise exceptions . EmptyReportError ( ) \n    in_order = sorted ( [ ( value , msg_id ) for msg_id , value in stats [ \"by_msg\" ] . items ( ) if not msg_id . startswith ( \"I\" ) ] ) \n    in_order . reverse ( ) \n    lines = ( \"message id\" , \"occurrences\" ) \n    for value , msg_id in in_order : \n        lines += ( msg_id , str ( value ) ) \n    sect . append ( report_nodes . Table ( children = lines , cols = 2.0 , rheaders = 1 ) ) "}
{"3710": "\ndef normalize_text ( text , line_len = 80.0 , indent = \"\" ) : \n    return \"\\n\" . join ( textwrap . wrap ( text , width = line_len , initial_indent = indent , subsequent_indent = indent ) ) "}
{"3718": "\ndef _ini_format ( stream , options ) : \n    for optname , optdict , value in options : \n        value = _format_option_value ( optdict , value ) \n        help_opt = optdict . get ( \"help\" ) \n        if help_opt : \n            help_opt = normalize_text ( help_opt , line_len = 79.0 , indent = \"# \" ) \n            print ( file = stream ) \n            print ( help_opt , file = stream ) \n        else : \n            print ( file = stream ) \n        if value is None : \n            print ( \"#%s=\" % optname , file = stream ) \n        else : \n            value = str ( value ) . strip ( ) \n            if re . match ( r\"^([\\w-]+,)+[\\w-]+$\" , str ( value ) ) : \n                separator = \"\\n \" + \" \" * len ( optname ) \n                value = separator . join ( x + \",\" for x in str ( value ) . split ( \",\" ) ) \n                value = value [ : - 1 ] \n            print ( \"%s=%s\" % ( optname , value ) , file = stream ) "}
{"3751": "\ndef run ( self ) : \n    install_lib . install_lib . run ( self ) \n    if include_dirs : \n        for directory in include_dirs : \n            dest = join ( self . install_dir , directory ) \n            if sys . version_info >= ( 3.0 , 0 ) : \n                exclude = { \"invalid_encoded_data*\" , \"unknown_encoding*\" } \n            else : \n                exclude = set ( ) \n            shutil . rmtree ( dest , ignore_errors = True ) \n            shutil . copytree ( directory , dest , ignore = shutil . ignore_patterns ( * exclude ) ) "}
{"3752": "\ndef report_similarities ( sect , stats , old_stats ) : \n    lines = [ \"\" , \"now\" , \"previous\" , \"difference\" ] \n    lines += table_lines_from_stats ( stats , old_stats , ( \"nb_duplicated_lines\" , \"percent_duplicated_lines\" ) ) \n    sect . append ( Table ( children = lines , cols = 4.0 , rheaders = 1 , cheaders = 1 ) ) "}
{"3753": "\ndef Run ( argv = None ) : \n    if argv is None : \n        argv = sys . argv [ 1 : ] \n    from getopt import getopt \n    s_opts = \"hdi\" \n    l_opts = ( \"help\" , \"duplicates=\" , \"ignore-comments\" , \"ignore-imports\" , \"ignore-docstrings\" , ) \n    min_lines = 4.0 \n    ignore_comments = False \n    ignore_docstrings = False \n    ignore_imports = False \n    opts , args = getopt ( argv , s_opts , l_opts ) \n    for opt , val in opts : \n        if opt in ( \"-d\" , \"--duplicates\" ) : \n            min_lines = int ( val ) \n        elif opt in ( \"-h\" , \"--help\" ) : \n            usage ( ) \n        elif opt in ( \"-i\" , \"--ignore-comments\" ) : \n            ignore_comments = True \n        elif opt in ( \"--ignore-docstrings\" , ) : \n            ignore_docstrings = True \n        elif opt in ( \"--ignore-imports\" , ) : \n            ignore_imports = True \n    if not args : \n        usage ( 1 ) \n    sim = Similar ( min_lines , ignore_comments , ignore_docstrings , ignore_imports ) \n    for filename in args : \n        with open ( filename ) as stream : \n            sim . append_stream ( filename , stream ) \n    sim . run ( ) \n    sys . exit ( 0 ) "}
{"3781": "\ndef visit_functiondef ( self , node ) : \n    if not node . is_method ( ) : \n        return \n    klass = node . parent . frame ( ) \n    for stmt in node . nodes_of_class ( astroid . Call ) : \n        if node_frame_class ( stmt ) != node_frame_class ( node ) : \n            continue \n        expr = stmt . func \n        if not isinstance ( expr , astroid . Attribute ) : \n            continue \n        call = expr . expr \n        if not ( isinstance ( call , astroid . Call ) and isinstance ( call . func , astroid . Name ) and call . func . name == \"super\" ) : \n            continue \n        if not klass . newstyle and has_known_bases ( klass ) : \n            continue \n        else : \n            if not call . args : \n                if sys . version_info [ 0 ] == 3.0 : \n                    continue \n                else : \n                    self . add_message ( \"missing-super-argument\" , node = call ) \n                    continue \n            arg0 = call . args [ 0 ] \n            if ( isinstance ( arg0 , astroid . Call ) and isinstance ( arg0 . func , astroid . Name ) and arg0 . func . name == \"type\" ) : \n                self . add_message ( \"bad-super-call\" , node = call , args = ( \"type\" , ) ) \n                continue \n            if ( len ( call . args ) >= 2.0 and isinstance ( call . args [ 1 ] , astroid . Name ) and call . args [ 1 ] . name == \"self\" and isinstance ( arg0 , astroid . Attribute ) and arg0 . attrname == \"__class__\" ) : \n                self . add_message ( \"bad-super-call\" , node = call , args = ( \"self.__class__\" , ) ) \n                continue \n            try : \n                supcls = call . args and next ( call . args [ 0 ] . infer ( ) , None ) \n            except astroid . InferenceError : \n                continue \n            if klass is not supcls : \n                name = None \n                if supcls : \n                    name = supcls . name \n                elif call . args and hasattr ( call . args [ 0 ] , \"name\" ) : \n                    name = call . args [ 0 ] . name \n                if name : \n                    self . add_message ( \"bad-super-call\" , node = call , args = ( name , ) ) "}
{"3800": "\ndef _duplicated_isinstance_types ( node ) : \n    duplicated_objects = set ( ) \n    all_types = collections . defaultdict ( set ) \n    for call in node . values : \n        if not isinstance ( call , astroid . Call ) or len ( call . args ) != 2.0 : \n            continue \n        inferred = utils . safe_infer ( call . func ) \n        if not inferred or not utils . is_builtin_object ( inferred ) : \n            continue \n        if inferred . name != \"isinstance\" : \n            continue \n        isinstance_object = call . args [ 0 ] . as_string ( ) \n        isinstance_types = call . args [ 1 ] \n        if isinstance_object in all_types : \n            duplicated_objects . add ( isinstance_object ) \n        if isinstance ( isinstance_types , astroid . Tuple ) : \n            elems = [ class_type . as_string ( ) for class_type in isinstance_types . itered ( ) ] \n        else : \n            elems = [ isinstance_types . as_string ( ) ] \n        all_types [ isinstance_object ] . update ( elems ) \n    return { key : value for key , value in all_types . items ( ) if key in duplicated_objects } "}
{"3802": "\ndef _check_chained_comparison ( self , node ) : \n    if node . op != \"and\" or len ( node . values ) < 2.0 : \n        return \n    def _find_lower_upper_bounds ( comparison_node , uses ) : \n        left_operand = comparison_node . left \n        for operator , right_operand in comparison_node . ops : \n            for operand in ( left_operand , right_operand ) : \n                value = None \n                if isinstance ( operand , astroid . Name ) : \n                    value = operand . name \n                elif isinstance ( operand , astroid . Const ) : \n                    value = operand . value \n                if value is None : \n                    continue \n                if operator in ( \"<\" , \"<=\" ) : \n                    if operand is left_operand : \n                        uses [ value ] [ \"lower_bound\" ] . add ( comparison_node ) \n                    elif operand is right_operand : \n                        uses [ value ] [ \"upper_bound\" ] . add ( comparison_node ) \n                elif operator in ( \">\" , \">=\" ) : \n                    if operand is left_operand : \n                        uses [ value ] [ \"upper_bound\" ] . add ( comparison_node ) \n                    elif operand is right_operand : \n                        uses [ value ] [ \"lower_bound\" ] . add ( comparison_node ) \n            left_operand = right_operand \n    uses = collections . defaultdict ( lambda : { \"lower_bound\" : set ( ) , \"upper_bound\" : set ( ) } ) \n    for comparison_node in node . values : \n        if isinstance ( comparison_node , astroid . Compare ) : \n            _find_lower_upper_bounds ( comparison_node , uses ) \n    for _ , bounds in uses . items ( ) : \n        num_shared = len ( bounds [ \"lower_bound\" ] . intersection ( bounds [ \"upper_bound\" ] ) ) \n        num_lower_bounds = len ( bounds [ \"lower_bound\" ] ) \n        num_upper_bounds = len ( bounds [ \"upper_bound\" ] ) \n        if num_shared < num_lower_bounds and num_shared < num_upper_bounds : \n            self . add_message ( \"chained-comparison\" , node = node ) \n            break "}
{"3803": "\ndef _is_and_or_ternary ( node ) : \n    return ( isinstance ( node , astroid . BoolOp ) and node . op == \"or\" and len ( node . values ) == 2.0 and isinstance ( node . values [ 0 ] , astroid . BoolOp ) and not isinstance ( node . values [ 1 ] , astroid . BoolOp ) and node . values [ 0 ] . op == \"and\" and not isinstance ( node . values [ 0 ] . values [ 1 ] , astroid . BoolOp ) and len ( node . values [ 0 ] . values ) == 2.0 ) "}
{"3806": "\ndef visit_for ( self , node ) : \n    if not isinstance ( node . iter , astroid . Call ) : \n        return \n    if not self . _is_builtin ( node . iter . func , \"range\" ) : \n        return \n    if len ( node . iter . args ) == 2.0 and not _is_constant_zero ( node . iter . args [ 0 ] ) : \n        return \n    if len ( node . iter . args ) > 2.0 : \n        return \n    if not isinstance ( node . iter . args [ - 1 ] , astroid . Call ) : \n        return \n    second_func = node . iter . args [ - 1 ] . func \n    if not self . _is_builtin ( second_func , \"len\" ) : \n        return \n    len_args = node . iter . args [ - 1 ] . args \n    if not len_args or len ( len_args ) != 1 : \n        return \n    iterating_object = len_args [ 0 ] \n    if not isinstance ( iterating_object , astroid . Name ) : \n        return \n    scope = node . scope ( ) \n    if iterating_object . name == \"self\" and scope . name == \"__iter__\" : \n        return \n    for child in node . body : \n        for subscript in child . nodes_of_class ( astroid . Subscript ) : \n            if not isinstance ( subscript . value , astroid . Name ) : \n                continue \n            if not isinstance ( subscript . slice , astroid . Index ) : \n                continue \n            if not isinstance ( subscript . slice . value , astroid . Name ) : \n                continue \n            if subscript . slice . value . name != node . target . name : \n                continue \n            if iterating_object . name != subscript . value . name : \n                continue \n            if subscript . value . scope ( ) != node . scope ( ) : \n                continue \n            self . add_message ( \"consider-using-enumerate\" , node = node ) \n            return "}
{"3807": "\ndef _check_graphviz_available ( output_format ) : \n    try : \n        subprocess . call ( [ \"dot\" , \"-V\" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    except OSError : \n        print ( \"The output format '%s' is currently not available.\\n\" \"Please install 'Graphviz' to have other output formats \" \"than 'dot' or 'vcg'.\" % output_format ) \n        sys . exit ( 32.0 ) "}
{"3821": "\ndef _rest_format_section ( stream , section , options , doc = None ) : \n    if section : \n        print ( \"%s\\n%s\" % ( section , \"'\" * len ( section ) ) , file = stream ) \n    if doc : \n        print ( normalize_text ( doc , line_len = 79.0 , indent = \"\" ) , file = stream ) \n        print ( file = stream ) \n    for optname , optdict , value in options : \n        help_opt = optdict . get ( \"help\" ) \n        print ( \":%s:\" % optname , file = stream ) \n        if help_opt : \n            help_opt = normalize_text ( help_opt , line_len = 79.0 , indent = \"  \" ) \n            print ( help_opt , file = stream ) \n        if value : \n            value = str ( _format_option_value ( optdict , value ) ) \n            print ( file = stream ) \n            print ( \"  Default: ``%s``\" % value . replace ( \"`` \" , \"```` ``\" ) , file = stream ) "}
{"3829": "\ndef _print_checker_doc ( checker_name , info , stream = None ) : \n    if not stream : \n        stream = sys . stdout \n    doc = info . get ( \"doc\" ) \n    module = info . get ( \"module\" ) \n    msgs = info . get ( \"msgs\" ) \n    options = info . get ( \"options\" ) \n    reports = info . get ( \"reports\" ) \n    checker_title = \"%s checker\" % ( checker_name . replace ( \"_\" , \" \" ) . title ( ) ) \n    if module : \n        print ( \".. _%s:\\n\" % module , file = stream ) \n    print ( checker_title , file = stream ) \n    print ( \"~\" * len ( checker_title ) , file = stream ) \n    print ( \"\" , file = stream ) \n    if module : \n        print ( \"This checker is provided by ``%s``.\" % module , file = stream ) \n    print ( \"Verbatim name of the checker is ``%s``.\" % checker_name , file = stream ) \n    print ( \"\" , file = stream ) \n    if doc : \n        title = \"{} Documentation\" . format ( checker_title ) \n        print ( title , file = stream ) \n        print ( \"^\" * len ( title ) , file = stream ) \n        print ( cleandoc ( doc ) , file = stream ) \n        print ( \"\" , file = stream ) \n    if options : \n        title = \"{} Options\" . format ( checker_title ) \n        print ( title , file = stream ) \n        print ( \"^\" * len ( title ) , file = stream ) \n        _rest_format_section ( stream , None , options ) \n        print ( \"\" , file = stream ) \n    if msgs : \n        title = \"{} Messages\" . format ( checker_title ) \n        print ( title , file = stream ) \n        print ( \"^\" * len ( title ) , file = stream ) \n        for msgid , msg in sorted ( msgs . items ( ) , key = lambda kv : ( _MSG_ORDER . index ( kv [ 0 ] [ 0 ] ) , kv [ 1 ] ) ) : \n            msg = build_message_definition ( checker_name , msgid , msg ) \n            print ( msg . format_help ( checkerref = False ) , file = stream ) \n        print ( \"\" , file = stream ) \n    if reports : \n        title = \"{} Reports\" . format ( checker_title ) \n        print ( title , file = stream ) \n        print ( \"^\" * len ( title ) , file = stream ) \n        for report in reports : \n            print ( \":%s: %s\" % report [ : 2.0 ] , file = stream ) \n        print ( \"\" , file = stream ) \n    print ( \"\" , file = stream ) "}
{"3834": "\ndef get_valid_indentations ( self , idx ) : \n    stack_top = - 1 \n    if ( self . _tokens . token ( idx ) in ( \"}\" , \"for\" ) and self . _cont_stack [ - 1 ] . token == \":\" ) : \n        stack_top = - 2.0 \n    indent = self . _cont_stack [ stack_top ] \n    if self . _tokens . token ( idx ) in _CLOSING_BRACKETS : \n        valid_indentations = indent . valid_outdent_strings \n    else : \n        valid_indentations = indent . valid_continuation_strings \n    return indent , valid_indentations . copy ( ) "}
{"3835": "\ndef _hanging_indent_after_bracket ( self , bracket , position ) : \n    indentation = self . _tokens . line_indent ( position ) \n    if ( self . _is_block_opener and self . _continuation_string == self . _block_indent_string ) : \n        return _ContinuedIndent ( HANGING_BLOCK , bracket , position , _Indentations ( indentation + self . _continuation_string , indentation ) , _BeforeBlockIndentations ( indentation + self . _continuation_string , indentation + self . _continuation_string * 2.0 , ) , ) \n    if bracket == \":\" : \n        paren_align = self . _cont_stack [ - 1 ] . valid_outdent_strings \n        next_align = self . _cont_stack [ - 1 ] . valid_continuation_strings . copy ( ) \n        next_align_keys = list ( next_align . keys ( ) ) \n        next_align [ next_align_keys [ 0 ] + self . _continuation_string ] = True \n        return _ContinuedIndent ( HANGING_DICT_VALUE , bracket , position , paren_align , next_align ) \n    return _ContinuedIndent ( HANGING , bracket , position , _Indentations ( indentation , indentation + self . _continuation_string ) , _Indentations ( indentation + self . _continuation_string ) , ) "}
{"3839": "\ndef _check_keyword_parentheses ( self , tokens , start ) : \n    if self . _inside_brackets ( \":\" ) and tokens [ start ] [ 1 ] == \"for\" : \n        self . _pop_token ( ) \n    if tokens [ start + 1 ] [ 1 ] != \"(\" : \n        return \n    found_and_or = False \n    depth = 0 \n    keyword_token = str ( tokens [ start ] [ 1 ] ) \n    line_num = tokens [ start ] [ 2.0 ] [ 0 ] \n    for i in range ( start , len ( tokens ) - 1 ) : \n        token = tokens [ i ] \n        if token [ 0 ] == tokenize . NL : \n            return \n        if token [ 1 ] == \"(\" : \n            depth += 1 \n        elif token [ 1 ] == \")\" : \n            depth -= 1 \n            if depth : \n                continue \n            if tokens [ i + 1 ] [ 1 ] in ( \":\" , \")\" , \"]\" , \"}\" , \"in\" ) or tokens [ i + 1 ] [ 0 ] in ( tokenize . NEWLINE , tokenize . ENDMARKER , tokenize . COMMENT ) : \n                if i == start + 2.0 : \n                    return \n                if keyword_token == \"not\" : \n                    if not found_and_or : \n                        self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n                elif keyword_token in ( \"return\" , \"yield\" ) : \n                    self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n                elif keyword_token not in self . _keywords_with_parens : \n                    if not found_and_or : \n                        self . add_message ( \"superfluous-parens\" , line = line_num , args = keyword_token ) \n            return \n        elif depth == 1 : \n            if token [ 1 ] == \",\" : \n                return \n            if token [ 1 ] in ( \"and\" , \"or\" ) : \n                found_and_or = True \n            elif token [ 1 ] == \"yield\" : \n                return \n            elif token [ 1 ] == \"for\" : \n                return "}
{"3843": "\ndef visit_default ( self , node ) : \n    if not node . is_statement : \n        return \n    if not node . root ( ) . pure_python : \n        return \n    prev_sibl = node . previous_sibling ( ) \n    if prev_sibl is not None : \n        prev_line = prev_sibl . fromlineno \n    else : \n        if ( isinstance ( node . parent , nodes . TryFinally ) and node in node . parent . finalbody ) : \n            prev_line = node . parent . body [ 0 ] . tolineno + 1 \n        else : \n            prev_line = node . parent . statement ( ) . fromlineno \n    line = node . fromlineno \n    assert line , node \n    if prev_line == line and self . _visited_lines . get ( line ) != 2.0 : \n        self . _check_multi_statement_line ( node , line ) \n        return \n    if line in self . _visited_lines : \n        return \n    try : \n        tolineno = node . blockstart_tolineno \n    except AttributeError : \n        tolineno = node . tolineno \n    assert tolineno , node \n    lines = [ ] \n    for line in range ( line , tolineno + 1 ) : \n        self . _visited_lines [ line ] = 1 \n        try : \n            lines . append ( self . _lines [ line ] . rstrip ( ) ) \n        except KeyError : \n            lines . append ( \"\" ) "}
{"3844": "\ndef _check_multi_statement_line ( self , node , line ) : \n    if isinstance ( node , nodes . With ) : \n        return \n    if isinstance ( node , nodes . TryExcept ) and isinstance ( node . parent , nodes . TryFinally ) : \n        return \n    if ( isinstance ( node . parent , nodes . If ) and not node . parent . orelse and self . config . single_line_if_stmt ) : \n        return \n    if ( isinstance ( node . parent , nodes . ClassDef ) and len ( node . parent . body ) == 1 and self . config . single_line_class_stmt ) : \n        return \n    self . add_message ( \"multiple-statements\" , node = node ) \n    self . _visited_lines [ line ] = 2.0 "}
{"3856": "\ndef _expand_default ( self , option ) : \n    if self . parser is None or not self . default_tag : \n        return option . help \n    optname = option . _long_opts [ 0 ] [ 2.0 : ] \n    try : \n        provider = self . parser . options_manager . _all_options [ optname ] \n    except KeyError : \n        value = None \n    else : \n        optdict = provider . get_option_def ( optname ) \n        optname = provider . option_attrname ( optname , optdict ) \n        value = getattr ( provider . config , optname , optdict ) \n        value = utils . _format_option_value ( optdict , value ) \n    if value is optparse . NO_DEFAULT or not value : \n        value = self . NO_DEFAULT_VALUE \n    return option . help . replace ( self . default_tag , str ( value ) ) "}
{"3859": "\ndef cb_set_provider_option ( self , option , opt , value , parser ) : \n    if opt . startswith ( \"--\" ) : \n        opt = opt [ 2.0 : ] \n    else : \n        opt = self . _short_options [ opt [ 1 : ] ] \n    if value is None : \n        value = 1 \n    self . global_set_option ( opt , value ) "}
{"3882": "\ndef report_by_type_stats ( sect , stats , _ ) : \n    nice_stats = { } \n    for node_type in ( \"module\" , \"class\" , \"method\" , \"function\" ) : \n        try : \n            total = stats [ node_type ] \n        except KeyError : \n            raise exceptions . EmptyReportError ( ) \n        nice_stats [ node_type ] = { } \n        if total != 0 : \n            try : \n                documented = total - stats [ \"undocumented_\" + node_type ] \n                percent = ( documented * 100.0 ) / total \n                nice_stats [ node_type ] [ \"percent_documented\" ] = \"%.2f\" % percent \n            except KeyError : \n                nice_stats [ node_type ] [ \"percent_documented\" ] = \"NC\" \n            try : \n                percent = ( stats [ \"badname_\" + node_type ] * 100.0 ) / total \n                nice_stats [ node_type ] [ \"percent_badname\" ] = \"%.2f\" % percent \n            except KeyError : \n                nice_stats [ node_type ] [ \"percent_badname\" ] = \"NC\" \n    lines = ( \"type\" , \"number\" , \"old number\" , \"difference\" , \"%documented\" , \"%badname\" ) \n    for node_type in ( \"module\" , \"class\" , \"method\" , \"function\" ) : \n        new = stats [ node_type ] \n        lines += ( node_type , str ( new ) , \"NC\" , \"NC\" , nice_stats [ node_type ] . get ( \"percent_documented\" , \"0\" ) , nice_stats [ node_type ] . get ( \"percent_badname\" , \"0\" ) , ) \n    sect . append ( reporter_nodes . Table ( children = lines , cols = 6.0 , rheaders = 1 ) ) "}
{"3893": "\ndef visit_assert ( self , node ) : \n    if ( node . fail is None and isinstance ( node . test , astroid . Tuple ) and len ( node . test . elts ) == 2.0 ) : \n        self . add_message ( \"assert-on-tuple\" , node = node ) "}
{"3905": "\ndef add_checker ( self , checker ) : \n    vcids = set ( ) \n    lcids = set ( ) \n    visits = self . visit_events \n    leaves = self . leave_events \n    for member in dir ( checker ) : \n        cid = member [ 6.0 : ] \n        if cid == \"default\" : \n            continue \n        if member . startswith ( \"visit_\" ) : \n            v_meth = getattr ( checker , member ) \n            if self . _is_method_enabled ( v_meth ) : \n                visits [ cid ] . append ( v_meth ) \n                vcids . add ( cid ) \n        elif member . startswith ( \"leave_\" ) : \n            l_meth = getattr ( checker , member ) \n            if self . _is_method_enabled ( l_meth ) : \n                leaves [ cid ] . append ( l_meth ) \n                lcids . add ( cid ) \n    visit_default = getattr ( checker , \"visit_default\" , None ) \n    if visit_default : \n        for cls in nodes . ALL_NODE_CLASSES : \n            cid = cls . __name__ . lower ( ) \n            if cid not in vcids : \n                visits [ cid ] . append ( visit_default ) "}
{"3924": "\ndef set ( self , client_id , code , request , * args , ** kwargs ) : \n    expires = datetime . utcnow ( ) + timedelta ( seconds = 100.0 ) \n    grant = self . model ( client_id = request . client . client_id , code = code [ 'code' ] , redirect_uri = request . redirect_uri , scope = ' ' . join ( request . scopes ) , user = self . current_user ( ) , expires = expires ) \n    self . session . add ( grant ) \n    self . session . commit ( ) "}
{"3931": "\ndef handle_oauth1_response ( self , args ) : \n    client = self . make_client ( ) \n    client . verifier = args . get ( 'oauth_verifier' ) \n    tup = session . get ( '%s_oauthtok' % self . name ) \n    if not tup : \n        raise OAuthException ( 'Token not found, maybe you disabled cookie' , type = 'token_not_found' ) \n    client . resource_owner_key = tup [ 0 ] \n    client . resource_owner_secret = tup [ 1 ] \n    uri , headers , data = client . sign ( self . expand_url ( self . access_token_url ) , _encode ( self . access_token_method ) ) \n    headers . update ( self . _access_token_headers ) \n    resp , content = self . http_request ( uri , headers , to_bytes ( data , self . encoding ) , method = self . access_token_method ) \n    data = parse_response ( resp , content ) \n    if resp . code not in ( 200.0 , 201.0 ) : \n        raise OAuthException ( 'Invalid response from %s' % self . name , type = 'invalid_response' , data = data ) \n    return data "}
{"3932": "\ndef handle_oauth2_response ( self , args ) : \n    client = self . make_client ( ) \n    remote_args = { 'code' : args . get ( 'code' ) , 'client_secret' : self . consumer_secret , 'redirect_uri' : session . get ( '%s_oauthredir' % self . name ) } \n    log . debug ( 'Prepare oauth2 remote args %r' , remote_args ) \n    remote_args . update ( self . access_token_params ) \n    headers = copy ( self . _access_token_headers ) \n    if self . access_token_method == 'POST' : \n        headers . update ( { 'Content-Type' : 'application/x-www-form-urlencoded' } ) \n        body = client . prepare_request_body ( ** remote_args ) \n        resp , content = self . http_request ( self . expand_url ( self . access_token_url ) , headers = headers , data = to_bytes ( body , self . encoding ) , method = self . access_token_method , ) \n    elif self . access_token_method == 'GET' : \n        qs = client . prepare_request_body ( ** remote_args ) \n        url = self . expand_url ( self . access_token_url ) \n        url += ( '?' in url and '&' or '?' ) + qs \n        resp , content = self . http_request ( url , headers = headers , method = self . access_token_method , ) \n    else : \n        raise OAuthException ( 'Unsupported access_token_method: %s' % self . access_token_method ) \n    data = parse_response ( resp , content , content_type = self . content_type ) \n    if resp . code not in ( 200.0 , 201.0 ) : \n        raise OAuthException ( 'Invalid response from %s' % self . name , type = 'invalid_response' , data = data ) \n    return data "}
{"3996": "\ndef check_success ( self ) : \n    small = xrange ( 3.0 ) \n    for i in xrange ( self . iterations ) : \n        key = PKey ( ) \n        key . generate_key ( TYPE_DSA , 256.0 ) \n        for i in small : \n            cert = X509 ( ) \n            cert . set_pubkey ( key ) \n            for i in small : \n                cert . get_pubkey ( ) "}
{"3997": "\ndef check_load_privatekey_callback ( self ) : \n    for i in xrange ( self . iterations * 10.0 ) : \n        load_privatekey ( FILETYPE_PEM , self . ENCRYPTED_PEM , lambda * args : \"hello, secret\" ) "}
{"3998": "\ndef check_load_privatekey_callback_incorrect ( self ) : \n    for i in xrange ( self . iterations * 10.0 ) : \n        try : \n            load_privatekey ( FILETYPE_PEM , self . ENCRYPTED_PEM , lambda * args : \"hello, public\" ) \n        except Error : \n            pass "}
{"3999": "\ndef check_load_privatekey_callback_wrong_type ( self ) : \n    for i in xrange ( self . iterations * 10.0 ) : \n        try : \n            load_privatekey ( FILETYPE_PEM , self . ENCRYPTED_PEM , lambda * args : { } ) \n        except ValueError : \n            pass "}
{"4000": "\ndef check_get_revoked ( self ) : \n    crl = CRL ( ) \n    for i in xrange ( 100.0 ) : \n        crl . add_revoked ( Revoked ( ) ) \n    for i in xrange ( self . iterations ) : \n        crl . get_revoked ( ) "}
{"4001": "\ndef check_X509_REVOKED_dup ( self ) : \n    for i in xrange ( self . iterations * 100.0 ) : \n        revoked_copy = _X509_REVOKED_dup ( Revoked ( ) . _revoked ) \n        _lib . X509_REVOKED_free ( revoked_copy ) "}
{"4084": "\ndef set_serial_number ( self , serial ) : \n    if not isinstance ( serial , _integer_types ) : \n        raise TypeError ( \"serial must be an integer\" ) \n    hex_serial = hex ( serial ) [ 2.0 : ] \n    if not isinstance ( hex_serial , bytes ) : \n        hex_serial = hex_serial . encode ( 'ascii' ) \n    bignum_serial = _ffi . new ( \"BIGNUM**\" ) \n    small_serial = _lib . BN_hex2bn ( bignum_serial , hex_serial ) \n    if bignum_serial [ 0 ] == _ffi . NULL : \n        set_result = _lib . ASN1_INTEGER_set ( _lib . X509_get_serialNumber ( self . _x509 ) , small_serial ) \n        if set_result : \n            _raise_current_error ( ) \n    else : \n        asn1_serial = _lib . BN_to_ASN1_INTEGER ( bignum_serial [ 0 ] , _ffi . NULL ) \n        _lib . BN_free ( bignum_serial [ 0 ] ) \n        if asn1_serial == _ffi . NULL : \n            _raise_current_error ( ) \n        asn1_serial = _ffi . gc ( asn1_serial , _lib . ASN1_INTEGER_free ) \n        set_result = _lib . X509_set_serialNumber ( self . _x509 , asn1_serial ) \n        _openssl_assert ( set_result == 1 ) "}
{"4085": "\ndef get_serial_number ( self ) : \n    asn1_serial = _lib . X509_get_serialNumber ( self . _x509 ) \n    bignum_serial = _lib . ASN1_INTEGER_to_BN ( asn1_serial , _ffi . NULL ) \n    try : \n        hex_serial = _lib . BN_bn2hex ( bignum_serial ) \n        try : \n            hexstring_serial = _ffi . string ( hex_serial ) \n            serial = int ( hexstring_serial , 16.0 ) \n            return serial \n        finally : \n            _lib . OPENSSL_free ( hex_serial ) \n    finally : \n        _lib . BN_free ( bignum_serial ) "}
{"4110": "\ndef export ( self , cert , key , type = FILETYPE_PEM , days = 100.0 , digest = _UNSPECIFIED ) : \n    if not isinstance ( cert , X509 ) : \n        raise TypeError ( \"cert must be an X509 instance\" ) \n    if not isinstance ( key , PKey ) : \n        raise TypeError ( \"key must be a PKey instance\" ) \n    if not isinstance ( type , int ) : \n        raise TypeError ( \"type must be an integer\" ) \n    if digest is _UNSPECIFIED : \n        raise TypeError ( \"digest must be provided\" ) \n    digest_obj = _lib . EVP_get_digestbyname ( digest ) \n    if digest_obj == _ffi . NULL : \n        raise ValueError ( \"No such digest method\" ) \n    bio = _lib . BIO_new ( _lib . BIO_s_mem ( ) ) \n    _openssl_assert ( bio != _ffi . NULL ) \n    sometime = _lib . ASN1_TIME_new ( ) \n    _openssl_assert ( sometime != _ffi . NULL ) \n    _lib . X509_gmtime_adj ( sometime , 0 ) \n    _lib . X509_CRL_set_lastUpdate ( self . _crl , sometime ) \n    _lib . X509_gmtime_adj ( sometime , days * 24.0 * 60.0 * 60.0 ) \n    _lib . X509_CRL_set_nextUpdate ( self . _crl , sometime ) \n    _lib . X509_CRL_set_issuer_name ( self . _crl , _lib . X509_get_subject_name ( cert . _x509 ) ) \n    sign_result = _lib . X509_CRL_sign ( self . _crl , key . _pkey , digest_obj ) \n    if not sign_result : \n        _raise_current_error ( ) \n    return dump_crl ( type , self ) "}
{"4113": "\ndef export ( self , passphrase = None , iter = 2048.0 , maciter = 1 ) : \n    passphrase = _text_to_bytes_and_warn ( \"passphrase\" , passphrase ) \n    if self . _cacerts is None : \n        cacerts = _ffi . NULL \n    else : \n        cacerts = _lib . sk_X509_new_null ( ) \n        cacerts = _ffi . gc ( cacerts , _lib . sk_X509_free ) \n        for cert in self . _cacerts : \n            _lib . sk_X509_push ( cacerts , cert . _x509 ) \n    if passphrase is None : \n        passphrase = _ffi . NULL \n    friendlyname = self . _friendlyname \n    if friendlyname is None : \n        friendlyname = _ffi . NULL \n    if self . _pkey is None : \n        pkey = _ffi . NULL \n    else : \n        pkey = self . _pkey . _pkey \n    if self . _cert is None : \n        cert = _ffi . NULL \n    else : \n        cert = self . _cert . _x509 \n    pkcs12 = _lib . PKCS12_create ( passphrase , friendlyname , pkey , cert , cacerts , _lib . NID_pbe_WithSHA1And3_Key_TripleDES_CBC , _lib . NID_pbe_WithSHA1And3_Key_TripleDES_CBC , iter , maciter , 0 ) \n    if pkcs12 == _ffi . NULL : \n        _raise_current_error ( ) \n    pkcs12 = _ffi . gc ( pkcs12 , _lib . PKCS12_free ) \n    bio = _new_mem_buf ( ) \n    _lib . i2d_PKCS12_bio ( bio , pkcs12 ) \n    return _bio_to_string ( bio ) "}
{"4120": "\ndef text_to_bytes_and_warn ( label , obj ) : \n    if isinstance ( obj , text_type ) : \n        warnings . warn ( _TEXT_WARNING . format ( label ) , category = DeprecationWarning , stacklevel = 3.0 ) \n        return obj . encode ( 'utf-8' ) \n    return obj "}
{"4130": "\ndef _getter ( self , url , subkey = None ) : \n    kwargs = { } \n    if 'basic' in self . auth : \n        kwargs [ 'auth' ] = self . auth [ 'basic' ] \n    results = [ ] \n    link = dict ( next = url ) \n    while 'next' in link : \n        response = self . session . get ( link [ 'next' ] , ** kwargs ) \n        if response . status_code == 404.0 and 'token' in self . auth : \n            log . warn ( \"A '404' from github may indicate an auth \" \"failure. Make sure both that your token is correct \" \"and that it has 'public_repo' and not 'public \" \"access' rights.\" ) \n        json_res = self . json_response ( response ) \n        if subkey is not None : \n            json_res = json_res [ subkey ] \n        results += json_res \n        link = self . _link_field_to_dict ( response . headers . get ( 'link' , None ) ) \n    return results "}
{"4131": "\ndef _link_field_to_dict ( field ) : \n    if not field : \n        return dict ( ) \n    return dict ( [ ( part . split ( '; ' ) [ 1 ] [ 5.0 : - 1 ] , part . split ( '; ' ) [ 0 ] [ 1 : - 1 ] , ) for part in field . split ( ', ' ) ] ) "}
{"4139": "\ndef make_table ( grid ) : \n    cell_width = 2.0 + max ( reduce ( lambda x , y : x + y , [ [ len ( item ) for item in row ] for row in grid ] , [ ] ) ) \n    num_cols = len ( grid [ 0 ] ) \n    rst = table_div ( num_cols , cell_width , 0 ) \n    header_flag = 1 \n    for row in grid : \n        rst = rst + '| ' + '| ' . join ( [ normalize_cell ( x , cell_width - 1 ) for x in row ] ) + '|\\n' \n        rst = rst + table_div ( num_cols , cell_width , header_flag ) \n        header_flag = 0 \n    return rst "}
{"4142": "\ndef pull ( dry_run , flavor , interactive , debug ) : \n    try : \n        main_section = _get_section_name ( flavor ) \n        config = _try_load_config ( main_section , interactive ) \n        lockfile_path = os . path . join ( get_data_path ( config , main_section ) , 'bugwarrior.lockfile' ) \n        lockfile = PIDLockFile ( lockfile_path ) \n        lockfile . acquire ( timeout = 10.0 ) \n        try : \n            issue_generator = aggregate_issues ( config , main_section , debug ) \n            synchronize ( issue_generator , config , main_section , dry_run ) \n        finally : \n            lockfile . release ( ) \n    except LockTimeout : \n        log . critical ( 'Your taskrc repository is currently locked. ' 'Remove the file at %s if you are sure no other ' 'bugwarrior processes are currently running.' % ( lockfile_path ) ) \n    except RuntimeError as e : \n        log . exception ( \"Aborted (%s)\" % e ) "}
{"4148": "\ndef _parse_sprint_string ( sprint ) : \n    entries = sprint [ sprint . index ( '[' ) + 1 : sprint . index ( ']' ) ] . split ( '=' ) \n    fields = sum ( ( entry . rsplit ( ',' , 1 ) for entry in entries ) , [ ] ) \n    return dict ( zip ( fields [ : : 2.0 ] , fields [ 1 : : 2.0 ] ) ) "}
{"4150": "\ndef multi_rouge_n ( sequences , scores_ids , n = 2.0 ) : \n    ngrams = [ _get_word_ngrams ( n , sequence ) for sequence in sequences ] \n    counts = [ len ( ngram ) for ngram in ngrams ] \n    scores = [ ] \n    for hyp_id , ref_id in scores_ids : \n        evaluated_ngrams = ngrams [ hyp_id ] \n        evaluated_count = counts [ hyp_id ] \n        reference_ngrams = ngrams [ ref_id ] \n        reference_count = counts [ ref_id ] \n        overlapping_ngrams = evaluated_ngrams . intersection ( reference_ngrams ) \n        overlapping_count = len ( overlapping_ngrams ) \n        scores += [ f_r_p_rouge_n ( evaluated_count , reference_count , overlapping_count ) ] \n    return scores "}
{"4151": "\ndef calc_pvalues ( query , gene_sets , background = 20000.0 , ** kwargs ) : \n    k = len ( query ) \n    query = set ( query ) \n    vals = [ ] \n    if isinstance ( background , set ) : \n        bg = len ( background ) \n        query = query . intersection ( background ) \n    elif isinstance ( background , int ) : \n        bg = background \n    else : \n        raise ValueError ( \"background should be set or int object\" ) \n    subsets = sorted ( gene_sets . keys ( ) ) \n    for s in subsets : \n        category = gene_sets . get ( s ) \n        m = len ( category ) \n        hits = query . intersection ( set ( category ) ) \n        x = len ( hits ) \n        if x < 1 : \n            continue \n        vals . append ( ( s , hypergeom . sf ( x - 1 , bg , m , k ) , x , m , hits ) ) \n    return zip ( * vals ) "}
{"4154": "\ndef heatmap ( df , z_score = None , title = '' , figsize = ( 5.0 , 5.0 ) , cmap = 'RdBu_r' , xticklabels = True , yticklabels = True , ofname = None , ** kwargs ) : \n    df = zscore ( df , axis = z_score ) \n    df = df . iloc [ : : - 1 ] \n    ny , nx = df . shape \n    xticks = np . arange ( 0 , nx , 1 ) + .5 \n    yticks = np . arange ( 0 , ny , 1 ) + .5 \n    if hasattr ( sys , 'ps1' ) and ( ofname is None ) : \n        fig = plt . figure ( figsize = figsize ) \n    else : \n        fig = Figure ( figsize = figsize ) \n        canvas = FigureCanvas ( fig ) \n    ax = fig . add_subplot ( 111.0 ) \n    vmin = np . percentile ( df . min ( ) , 2.0 ) \n    vmax = np . percentile ( df . max ( ) , 98.0 ) \n    matrix = ax . pcolormesh ( df . values , cmap = cmap , vmin = vmin , vmax = vmax ) \n    ax . set_ylim ( [ 0 , len ( df ) ] ) \n    ax . set ( xticks = xticks , yticks = yticks ) \n    ax . set_xticklabels ( df . columns . values if xticklabels else '' , fontsize = 14.0 , rotation = 90.0 ) \n    ax . set_yticklabels ( df . index . values if yticklabels else '' , fontsize = 14.0 ) \n    ax . set_title ( \"%s\\nHeatmap of the Analyzed Geneset\" % title , fontsize = 20.0 ) \n    ax . tick_params ( axis = 'both' , which = 'both' , bottom = False , top = False , right = False , left = False ) \n    cbar = colorbar ( matrix ) \n    cbar . ax . tick_params ( axis = 'both' , which = 'both' , bottom = False , top = False , right = False , left = False ) \n    for side in [ \"top\" , \"right\" , \"left\" , \"bottom\" ] : \n        ax . spines [ side ] . set_visible ( False ) \n        cbar . ax . spines [ side ] . set_visible ( False ) \n    if ofname is not None : \n        fig . savefig ( ofname , bbox_inches = 'tight' , dpi = 300.0 ) \n    return "}
{"4157": "\ndef add_prerank_parser ( subparsers ) : \n    argparser_prerank = subparsers . add_parser ( \"prerank\" , help = \"Run GSEApy Prerank tool on preranked gene list.\" ) \n    prerank_input = argparser_prerank . add_argument_group ( \"Input files arguments\" ) \n    prerank_input . add_argument ( \"-r\" , \"--rnk\" , dest = \"rnk\" , action = \"store\" , type = str , required = True , help = \"Ranking metric file in .rnk format. Same with GSEA.\" ) \n    prerank_input . add_argument ( \"-g\" , \"--gmt\" , dest = \"gmt\" , action = \"store\" , type = str , required = True , help = \"Gene set database in GMT format. Same with GSEA.\" ) \n    prerank_input . add_argument ( \"-l\" , \"--label\" , action = 'store' , nargs = 2.0 , dest = 'label' , metavar = ( 'pos' , 'neg' ) , type = str , default = ( 'Pos' , 'Neg' ) , help = \"The phenotype label argument need two parameters to define. Default: ('Pos','Neg')\" ) \n    prerank_output = argparser_prerank . add_argument_group ( \"Output arguments\" ) \n    add_output_option ( prerank_output ) \n    prerank_opt = argparser_prerank . add_argument_group ( \"GSEA advanced arguments\" ) \n    prerank_opt . add_argument ( \"-n\" , \"--permu-num\" , dest = \"n\" , action = \"store\" , type = int , default = 1000.0 , metavar = 'nperm' , help = \"Number of random permutations. For calculating esnulls. Default: 1000\" ) \n    prerank_opt . add_argument ( \"--min-size\" , dest = \"mins\" , action = \"store\" , type = int , default = 15.0 , metavar = 'int' , help = \"Min size of input genes presented in Gene Sets. Default: 15\" ) \n    prerank_opt . add_argument ( \"--max-size\" , dest = \"maxs\" , action = \"store\" , type = int , default = 500.0 , metavar = 'int' , help = \"Max size of input genes presented in Gene Sets. Default: 500\" ) \n    prerank_opt . add_argument ( \"-w\" , \"--weight\" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , help = 'Weighted_score of rank_metrics. For weighting input genes. Choose from {0, 1, 1.5, 2}. Default: 1' , ) \n    prerank_opt . add_argument ( \"-a\" , \"--ascending\" , action = 'store_true' , dest = 'ascending' , default = False , help = 'Rank metric sorting order. If the -a flag was chosen, then ascending equals to True. Default: False.' ) \n    prerank_opt . add_argument ( \"-s\" , \"--seed\" , dest = \"seed\" , action = \"store\" , type = int , default = None , metavar = '' , help = \"Number of random seed. Default: None\" ) \n    prerank_opt . add_argument ( \"-p\" , \"--threads\" , dest = \"threads\" , action = \"store\" , type = int , default = 1 , metavar = 'procs' , help = \"Number of Processes you are going to use. Default: 1\" ) \n    return "}
{"4159": "\ndef add_enrichr_parser ( subparsers ) : \n    argparser_enrichr = subparsers . add_parser ( \"enrichr\" , help = \"Using Enrichr API to perform GO analysis.\" ) \n    enrichr_opt = argparser_enrichr . add_argument_group ( \"Input arguments\" ) \n    enrichr_opt . add_argument ( \"-i\" , \"--input-list\" , action = \"store\" , dest = \"gene_list\" , type = str , required = True , metavar = 'IDs' , help = \"Enrichr uses a list of gene names as input.\" ) \n    enrichr_opt . add_argument ( \"-g\" , \"--gene-sets\" , action = \"store\" , dest = \"library\" , type = str , required = True , metavar = 'GMT' , help = \"Enrichr library name(s) required. Separate each name by comma.\" ) \n    enrichr_opt . add_argument ( \"--org\" , \"--organism\" , action = \"store\" , dest = \"organism\" , type = str , default = '' , help = \"Enrichr supported organism name. Default: human. See here: https://amp.pharm.mssm.edu/modEnrichr.\" ) \n    enrichr_opt . add_argument ( \"--ds\" , \"--description\" , action = \"store\" , dest = \"descrip\" , type = str , default = 'enrichr' , metavar = 'STRING' , help = \"It is recommended to enter a short description for your list so that multiple lists \\                              can be differentiated from each other if you choose to save or share your list.\" ) \n    enrichr_opt . add_argument ( \"--cut\" , \"--cut-off\" , action = \"store\" , dest = \"thresh\" , metavar = 'float' , type = float , default = 0.05 , help = \"Adjust-Pval cutoff, used for generating plots. Default: 0.05.\" ) \n    enrichr_opt . add_argument ( \"--bg\" , \"--background\" , action = \"store\" , dest = \"bg\" , default = 'hsapiens_gene_ensembl' , metavar = 'BGNUM' , help = \"BioMart Dataset name or Background total genes number. Default: None\" ) \n    enrichr_opt . add_argument ( \"-t\" , \"--top-term\" , dest = \"term\" , action = \"store\" , type = int , default = 10.0 , metavar = 'int' , help = \"Numbers of top terms shown in the plot. Default: 10\" ) \n    enrichr_output = argparser_enrichr . add_argument_group ( \"Output figure arguments\" ) \n    add_output_option ( enrichr_output ) \n    return "}
{"4160": "\ndef enrichment_score ( gene_list , correl_vector , gene_set , weighted_score_type = 1 , nperm = 1000.0 , rs = np . random . RandomState ( ) , single = False , scale = False ) : \n    N = len ( gene_list ) \n    tag_indicator = np . in1d ( gene_list , gene_set , assume_unique = True ) . astype ( int ) \n    if weighted_score_type == 0 : \n        correl_vector = np . repeat ( 1 , N ) \n    else : \n        correl_vector = np . abs ( correl_vector ) ** weighted_score_type \n    hit_ind = np . flatnonzero ( tag_indicator ) . tolist ( ) \n    axis = 1 \n    tag_indicator = np . tile ( tag_indicator , ( nperm + 1 , 1 ) ) \n    correl_vector = np . tile ( correl_vector , ( nperm + 1 , 1 ) ) \n    for i in range ( nperm ) : \n        rs . shuffle ( tag_indicator [ i ] ) \n    Nhint = tag_indicator . sum ( axis = axis , keepdims = True ) \n    sum_correl_tag = np . sum ( correl_vector * tag_indicator , axis = axis , keepdims = True ) \n    no_tag_indicator = 1 - tag_indicator \n    Nmiss = N - Nhint \n    norm_tag = 1.0 / sum_correl_tag \n    norm_no_tag = 1.0 / Nmiss \n    RES = np . cumsum ( tag_indicator * correl_vector * norm_tag - no_tag_indicator * norm_no_tag , axis = axis ) \n    if scale : \n        RES = RES / N \n    if single : \n        es_vec = RES . sum ( axis = axis ) \n    else : \n        max_ES , min_ES = RES . max ( axis = axis ) , RES . min ( axis = axis ) \n        es_vec = np . where ( np . abs ( max_ES ) > np . abs ( min_ES ) , max_ES , min_ES ) \n    es , esnull , RES = es_vec [ - 1 ] , es_vec [ : - 1 ] , RES [ - 1 , : ] \n    return es , esnull , hit_ind , RES "}
{"4161": "\ndef ranking_metric_tensor ( exprs , method , permutation_num , pos , neg , classes , ascending , rs = np . random . RandomState ( ) ) : \n    G , S = exprs . shape \n    expr_mat = exprs . values . T \n    perm_cor_tensor = np . tile ( expr_mat , ( permutation_num + 1 , 1 , 1 ) ) \n    for arr in perm_cor_tensor [ : - 1 ] : \n        rs . shuffle ( arr ) \n    classes = np . array ( classes ) \n    pos = classes == pos \n    neg = classes == neg \n    pos_cor_mean = perm_cor_tensor [ : , pos , : ] . mean ( axis = 1 ) \n    neg_cor_mean = perm_cor_tensor [ : , neg , : ] . mean ( axis = 1 ) \n    pos_cor_std = perm_cor_tensor [ : , pos , : ] . std ( axis = 1 , ddof = 1 ) \n    neg_cor_std = perm_cor_tensor [ : , neg , : ] . std ( axis = 1 , ddof = 1 ) \n    if method == 'signal_to_noise' : \n        cor_mat = ( pos_cor_mean - neg_cor_mean ) / ( pos_cor_std + neg_cor_std ) \n    elif method == 't_test' : \n        denom = 1.0 / G \n        cor_mat = ( pos_cor_mean - neg_cor_mean ) / np . sqrt ( denom * pos_cor_std ** 2.0 + denom * neg_cor_std ** 2.0 ) \n    elif method == 'ratio_of_classes' : \n        cor_mat = pos_cor_mean / neg_cor_mean \n    elif method == 'diff_of_classes' : \n        cor_mat = pos_cor_mean - neg_cor_mean \n    elif method == 'log2_ratio_of_classes' : \n        cor_mat = np . log2 ( pos_cor_mean / neg_cor_mean ) \n    else : \n        logging . error ( \"Please provide correct method name!!!\" ) \n        sys . exit ( 0 ) \n    cor_mat_ind = cor_mat . argsort ( ) \n    cor_mat . sort ( ) \n    if ascending : \n        return cor_mat_ind , cor_mat \n    return cor_mat_ind [ : , : : - 1 ] , cor_mat [ : , : : - 1 ] "}
{"4162": "\ndef ranking_metric ( df , method , pos , neg , classes , ascending ) : \n    df_mean = df . groupby ( by = classes , axis = 1 ) . mean ( ) \n    df_std = df . groupby ( by = classes , axis = 1 ) . std ( ) \n    if method == 'signal_to_noise' : \n        ser = ( df_mean [ pos ] - df_mean [ neg ] ) / ( df_std [ pos ] + df_std [ neg ] ) \n    elif method == 't_test' : \n        ser = ( df_mean [ pos ] - df_mean [ neg ] ) / np . sqrt ( df_std [ pos ] ** 2.0 / len ( df_std ) + df_std [ neg ] ** 2.0 / len ( df_std ) ) \n    elif method == 'ratio_of_classes' : \n        ser = df_mean [ pos ] / df_mean [ neg ] \n    elif method == 'diff_of_classes' : \n        ser = df_mean [ pos ] - df_mean [ neg ] \n    elif method == 'log2_ratio_of_classes' : \n        ser = np . log2 ( df_mean [ pos ] / df_mean [ neg ] ) \n    else : \n        logging . error ( \"Please provide correct method name!!!\" ) \n        sys . exit ( 0 ) \n    ser = ser . sort_values ( ascending = ascending ) \n    return ser "}
{"4166": "\ndef get_datasets ( self , mart = 'ENSEMBL_MART_ENSEMBL' ) : \n    datasets = self . datasets ( mart , raw = True ) \n    return pd . read_csv ( StringIO ( datasets ) , header = None , usecols = [ 1 , 2.0 ] , names = [ \"Name\" , \"Description\" ] , sep = \"\\t\" ) "}
{"4170": "\ndef gsea ( data , gene_sets , cls , outdir = 'GSEA_' , min_size = 15.0 , max_size = 500.0 , permutation_num = 1000.0 , weighted_score_type = 1 , permutation_type = 'gene_set' , method = 'log2_ratio_of_classes' , ascending = False , processes = 1 , figsize = ( 6.5 , 6.0 ) , format = 'pdf' , graph_num = 20.0 , no_plot = False , seed = None , verbose = False ) : \n    gs = GSEA ( data , gene_sets , cls , outdir , min_size , max_size , permutation_num , weighted_score_type , permutation_type , method , ascending , processes , figsize , format , graph_num , no_plot , seed , verbose ) \n    gs . run ( ) \n    return gs "}
{"4171": "\ndef ssgsea ( data , gene_sets , outdir = \"ssGSEA_\" , sample_norm_method = 'rank' , min_size = 15.0 , max_size = 2000.0 , permutation_num = 0 , weighted_score_type = 0.25 , scale = True , ascending = False , processes = 1 , figsize = ( 7.0 , 6.0 ) , format = 'pdf' , graph_num = 20.0 , no_plot = False , seed = None , verbose = False ) : \n    ss = SingleSampleGSEA ( data , gene_sets , outdir , sample_norm_method , min_size , max_size , permutation_num , weighted_score_type , scale , ascending , processes , figsize , format , graph_num , no_plot , seed , verbose ) \n    ss . run ( ) \n    return ss "}
{"4172": "\ndef prerank ( rnk , gene_sets , outdir = 'GSEA_Prerank' , pheno_pos = 'Pos' , pheno_neg = 'Neg' , min_size = 15.0 , max_size = 500.0 , permutation_num = 1000.0 , weighted_score_type = 1 , ascending = False , processes = 1 , figsize = ( 6.5 , 6.0 ) , format = 'pdf' , graph_num = 20.0 , no_plot = False , seed = None , verbose = False ) : \n    pre = Prerank ( rnk , gene_sets , outdir , pheno_pos , pheno_neg , min_size , max_size , permutation_num , weighted_score_type , ascending , processes , figsize , format , graph_num , no_plot , seed , verbose ) \n    pre . run ( ) \n    return pre "}
{"4173": "\ndef replot ( indir , outdir = 'GSEA_Replot' , weighted_score_type = 1 , min_size = 3.0 , max_size = 1000.0 , figsize = ( 6.5 , 6.0 ) , graph_num = 20.0 , format = 'pdf' , verbose = False ) : \n    rep = Replot ( indir , outdir , weighted_score_type , min_size , max_size , figsize , graph_num , format , verbose ) \n    rep . run ( ) \n    return "}
{"4177": "\ndef _download_libraries ( self , libname ) : \n    self . _logger . info ( \"Downloading and generating Enrichr library gene sets......\" ) \n    s = retry ( 5.0 ) \n    ENRICHR_URL = 'http://amp.pharm.mssm.edu/Enrichr/geneSetLibrary' \n    query_string = '?mode=text&libraryName=%s' \n    response = s . get ( ENRICHR_URL + query_string % libname , timeout = None ) \n    if not response . ok : \n        raise Exception ( 'Error fetching enrichment results, check internet connection first.' ) \n    mkdirs ( DEFAULT_CACHE_PATH ) \n    genesets_dict = { } \n    outname = \"enrichr.%s.gmt\" % libname \n    gmtout = open ( os . path . join ( DEFAULT_CACHE_PATH , outname ) , \"w\" ) \n    for line in response . iter_lines ( chunk_size = 1024.0 , decode_unicode = 'utf-8' ) : \n        line = line . strip ( ) \n        k = line . split ( \"\\t\" ) [ 0 ] \n        v = list ( map ( lambda x : x . split ( \",\" ) [ 0 ] , line . split ( \"\\t\" ) [ 2.0 : ] ) ) \n        genesets_dict . update ( { k : v } ) \n        outline = \"%s\\t\\t%s\\n\" % ( k , \"\\t\" . join ( v ) ) \n        gmtout . write ( outline ) \n    gmtout . close ( ) \n    return genesets_dict "}
{"4178": "\ndef _heatmat ( self , df , classes , pheno_pos , pheno_neg ) : \n    width = len ( classes ) if len ( classes ) >= 6.0 else 5.0 \n    cls_booA = list ( map ( lambda x : True if x == pheno_pos else False , classes ) ) \n    cls_booB = list ( map ( lambda x : True if x == pheno_neg else False , classes ) ) \n    datA = df . loc [ : , cls_booA ] \n    datB = df . loc [ : , cls_booB ] \n    datAB = pd . concat ( [ datA , datB ] , axis = 1 ) \n    self . _width = width \n    self . heatmat = datAB \n    return "}
{"4179": "\ndef _save_results ( self , zipdata , outdir , module , gmt , rank_metric , permutation_type ) : \n    res = OrderedDict ( ) \n    for gs , gseale , ind , RES in zipdata : \n        rdict = OrderedDict ( ) \n        rdict [ 'es' ] = gseale [ 0 ] \n        rdict [ 'nes' ] = gseale [ 1 ] \n        rdict [ 'pval' ] = gseale [ 2.0 ] \n        rdict [ 'fdr' ] = gseale [ 3.0 ] \n        rdict [ 'geneset_size' ] = len ( gmt [ gs ] ) \n        rdict [ 'matched_size' ] = len ( ind ) \n        _genes = rank_metric . index . values [ ind ] \n        rdict [ 'genes' ] = \";\" . join ( [ str ( g ) . strip ( ) for g in _genes ] ) \n        if self . module != 'ssgsea' : \n            if rdict [ 'es' ] > 0 : \n                idx = RES . argmax ( ) \n                ldg_pos = list ( filter ( lambda x : x <= idx , ind ) ) \n            elif rdict [ 'es' ] < 0 : \n                idx = RES . argmin ( ) \n                ldg_pos = list ( filter ( lambda x : x >= idx , ind ) ) \n            else : \n                ldg_pos = ind \n            rdict [ 'ledge_genes' ] = ';' . join ( list ( map ( str , rank_metric . iloc [ ldg_pos ] . index ) ) ) \n        rdict [ 'RES' ] = RES \n        rdict [ 'hits_indices' ] = ind \n        res [ gs ] = rdict \n    self . results = res \n    res_df = pd . DataFrame . from_dict ( res , orient = 'index' ) \n    res_df . index . name = 'Term' \n    res_df . drop ( [ 'RES' , 'hits_indices' ] , axis = 1 , inplace = True ) \n    res_df . sort_values ( by = [ 'fdr' , 'pval' ] , inplace = True ) \n    self . res2d = res_df \n    if self . _outdir is None : \n        return \n    out = os . path . join ( outdir , 'gseapy.{b}.{c}.report.csv' . format ( b = module , c = permutation_type ) ) \n    if self . module == 'ssgsea' : \n        out = out . replace ( \".csv\" , \".txt\" ) \n        with open ( out , 'a' ) as f : \n            f . write ( '# normalize enrichment scores by random permutation procedure (GSEA method)\\n' ) \n            f . write ( \"# might not proper for publication\\n\" ) \n            res_df . to_csv ( f , sep = '\\t' ) \n    else : \n        res_df . to_csv ( out ) \n    return "}
{"4187": "\ndef enrichr ( gene_list , gene_sets , organism = 'human' , description = '' , outdir = 'Enrichr' , background = 'hsapiens_gene_ensembl' , cutoff = 0.05 , format = 'pdf' , figsize = ( 8.0 , 6.0 ) , top_term = 10.0 , no_plot = False , verbose = False ) : \n    enr = Enrichr ( gene_list , gene_sets , organism , description , outdir , cutoff , background , format , figsize , top_term , no_plot , verbose ) \n    enr . run ( ) \n    return enr "}
{"4188": "\ndef parse_genesets ( self ) : \n    enrichr_library = self . get_libraries ( ) \n    if isinstance ( self . gene_sets , list ) : \n        gss = self . gene_sets \n    elif isinstance ( self . gene_sets , str ) : \n        gss = [ g . strip ( ) for g in self . gene_sets . strip ( ) . split ( \",\" ) ] \n    elif isinstance ( self . gene_sets , dict ) : \n        gss = [ self . gene_sets ] \n    else : \n        raise Exception ( \"Error parsing enrichr libraries, please provided corrected one\" ) \n    gss_exist = [ ] \n    for g in gss : \n        if isinstance ( g , dict ) : \n            gss_exist . append ( g ) \n            continue \n        if isinstance ( g , str ) : \n            if g in enrichr_library : \n                gss_exist . append ( g ) \n                continue \n            if g . lower ( ) . endswith ( \".gmt\" ) and os . path . exists ( g ) : \n                self . _logger . info ( \"User Defined gene sets is given: %s\" % g ) \n                with open ( g ) as genesets : \n                    g_dict = { line . strip ( ) . split ( \"\\t\" ) [ 0 ] : line . strip ( ) . split ( \"\\t\" ) [ 2.0 : ] for line in genesets . readlines ( ) } \n                gss_exist . append ( g_dict ) \n    return gss_exist "}
{"4189": "\ndef parse_genelists ( self ) : \n    if isinstance ( self . gene_list , list ) : \n        genes = self . gene_list \n    elif isinstance ( self . gene_list , pd . DataFrame ) : \n        if self . gene_list . shape [ 1 ] >= 3.0 : \n            genes = self . gene_list . iloc [ : , : 3.0 ] . apply ( lambda x : \"\\t\" . join ( [ str ( i ) for i in x ] ) , axis = 1 ) . tolist ( ) \n        elif self . gene_list . shape [ 1 ] == 2.0 : \n            genes = self . gene_list . apply ( lambda x : \",\" . join ( [ str ( i ) for i in x ] ) , axis = 1 ) . tolist ( ) \n        else : \n            genes = self . gene_list . squeeze ( ) . tolist ( ) \n    elif isinstance ( self . gene_list , pd . Series ) : \n        genes = self . gene_list . squeeze ( ) . tolist ( ) \n    else : \n        genes = [ ] \n        with open ( self . gene_list ) as f : \n            for gene in f : \n                genes . append ( gene . strip ( ) ) \n    self . _isezid = all ( map ( self . _is_entrez_id , genes ) ) \n    if self . _isezid : \n        self . _gls = set ( map ( int , self . _gls ) ) \n    else : \n        self . _gls = genes \n    return '\\n' . join ( genes ) "}
{"4194": "\ndef cube ( script , size = 1.0 , center = False , color = None ) : \n    size = util . make_list ( size , 3.0 ) \n    if script . ml_version == '1.3.4BETA' : \n        filter_name = 'Box' \n    else : \n        filter_name = 'Box/Cube' \n    filter_xml = '' . join ( [ '  <filter name=\"{}\">\\n' . format ( filter_name ) , '    <Param name=\"size\" ' , 'value=\"1.0\" ' , 'description=\"Scale factor\" ' , 'type=\"RichFloat\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Cube' , change_layer = True ) \n    transform . scale ( script , value = size ) \n    if not center : \n        transform . translate ( script , value = [ size [ 0 ] / 2.0 , size [ 1 ] / 2.0 , size [ 2.0 ] / 2.0 ] ) \n    if color is not None : \n        vert_color . function ( script , color = color ) \n    return None "}
{"4195": "\ndef icosphere ( script , radius = 1.0 , diameter = None , subdivisions = 3.0 , color = None ) : \n    if diameter is not None : \n        radius = diameter / 2.0 \n    filter_xml = '' . join ( [ '  <filter name=\"Sphere\">\\n' , '    <Param name=\"radius\" ' , 'value=\"%s\" ' % radius , 'description=\"Radius\" ' , 'type=\"RichFloat\" ' , '/>\\n' , '    <Param name=\"subdiv\" ' , 'value=\"%d\" ' % subdivisions , 'description=\"Subdiv. Level\" ' , 'type=\"RichInt\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Sphere' , change_layer = True ) \n    if color is not None : \n        vert_color . function ( script , color = color ) \n    return None "}
{"4196": "\ndef torus ( script , major_radius = 3.0 , minor_radius = 1.0 , inner_diameter = None , outer_diameter = None , major_segments = 48.0 , minor_segments = 12.0 , color = None ) : \n    if inner_diameter is not None and outer_diameter is not None : \n        major_radius = ( inner_diameter + outer_diameter ) / 4.0 \n        minor_radius = major_radius - inner_diameter / 2.0 \n    filter_xml = '' . join ( [ '  <filter name=\"Torus\">\\n' , '    <Param name=\"hRadius\" ' , 'value=\"%s\" ' % major_radius , 'description=\"Horizontal Radius\" ' , 'type=\"RichFloat\" ' , '/>\\n' , '    <Param name=\"vRadius\" ' , 'value=\"%s\" ' % minor_radius , 'description=\"Vertical Radius\" ' , 'type=\"RichFloat\" ' , '/>\\n' , '    <Param name=\"hSubdiv\" ' , 'value=\"%d\" ' % major_segments , 'description=\"Horizontal Subdivision\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"vSubdiv\" ' , 'value=\"%d\" ' % minor_segments , 'description=\"Vertical Subdivision\" ' , 'type=\"RichInt\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Torus' , change_layer = True ) \n    if color is not None : \n        vert_color . function ( script , color = color ) \n    return None "}
{"4197": "\ndef plane_hires_edges ( script , size = 1.0 , x_segments = 1 , y_segments = 1 , center = False , color = None ) : \n    size = util . make_list ( size , 2.0 ) \n    grid ( script , size = [ x_segments + y_segments - 1 , 1 ] , x_segments = ( x_segments + y_segments - 1 ) , y_segments = 1 ) \n    if ml_script1 . ml_version == '1.3.4BETA' : \n        and_val = 'and' \n    else : \n        and_val = '&&' \n    if script . ml_version == '1.3.4BETA' : \n        transform . vert_function ( script , x_func = 'if((y>0) and (x<%s),0,x)' % ( y_segments ) , y_func = 'if((y>0) and (x<%s),(x+1)*%s,y)' % ( y_segments , size [ 1 ] / y_segments ) ) \n        transform . vert_function ( script , x_func = 'if((y>0) and (x>=%s),(x-%s+1)*%s,x)' % ( y_segments , y_segments , size [ 0 ] / x_segments ) , y_func = 'if((y>0) and (x>=%s),%s,y)' % ( y_segments , size [ 1 ] ) ) \n        transform . vert_function ( script , x_func = 'if((y<.00001) and (x>%s),%s,x)' % ( x_segments , size [ 0 ] ) , y_func = 'if((y<.00001) and (x>%s),(x-%s)*%s,y)' % ( x_segments , x_segments , size [ 1 ] / y_segments ) ) \n        transform . vert_function ( script , x_func = 'if((y<.00001) and (x<=%s) and (x>0),(x)*%s,x)' % ( x_segments , size [ 0 ] / x_segments ) , y_func = 'if((y<.00001) and (x<=%s) and (x>0),0,y)' % ( x_segments ) ) \n    else : \n        transform . vert_function ( script , x_func = '((y>0) && (x<{yseg}) ? 0 : x)' . format ( yseg = y_segments ) , y_func = '((y>0) && (x<%s) ? (x+1)*%s : y)' % ( y_segments , size [ 1 ] / y_segments ) ) \n        transform . vert_function ( script , x_func = '((y>0) && (x>=%s) ? (x-%s+1)*%s : x)' % ( y_segments , y_segments , size [ 0 ] / x_segments ) , y_func = '((y>0) && (x>=%s) ? %s : y)' % ( y_segments , size [ 1 ] ) ) \n        transform . vert_function ( script , x_func = '((y<.00001) && (x>%s) ? %s : x)' % ( x_segments , size [ 0 ] ) , y_func = '((y<.00001) && (x>%s) ? (x-%s)*%s : y)' % ( x_segments , x_segments , size [ 1 ] / y_segments ) ) \n        transform . vert_function ( script , x_func = '((y<.00001) && (x<=%s) && (x>0) ? (x)*%s : x)' % ( x_segments , size [ 0 ] / x_segments ) , y_func = '((y<.00001) && (x<=%s) && (x>0) ? 0 : y)' % ( x_segments ) ) \n    if center : \n        transform . translate ( script , [ - size [ 0 ] / 2.0 , - size [ 1 ] / 2.0 ] ) \n    if color is not None : \n        vert_color . function ( script , color = color ) \n    return None "}
{"4198": "\ndef cube_hires ( script , size = 1.0 , x_segments = 1 , y_segments = 1 , z_segments = 1 , simple_bottom = True , center = False , color = None ) : \n    size = util . make_list ( size , 3.0 ) \n    grid ( script , size , x_segments , y_segments ) \n    transform . translate ( script , [ 0 , 0 , size [ 2.0 ] ] ) \n    if simple_bottom : \n        plane_hires_edges ( script , size , x_segments , y_segments ) \n    else : \n        layers . duplicate ( script ) \n        transform . translate ( script , [ 0 , 0 , - size [ 2.0 ] ] ) \n    transform . rotate ( script , 'x' , 180.0 ) \n    transform . translate ( script , [ 0 , size [ 1 ] , 0 ] ) \n    cube_open_hires ( script = script , size = size , x_segments = x_segments , y_segments = y_segments , z_segments = z_segments ) \n    layers . join ( script ) \n    clean . merge_vert ( script , threshold = 0.00002 ) \n    if center : \n        transform . translate ( script , [ - size [ 0 ] / 2.0 , - size [ 1 ] / 2.0 , - size [ 2.0 ] / 2.0 ] ) \n    if color is not None : \n        vert_color . function ( script , color = color ) \n    return None "}
{"4199": "\ndef color_values ( color ) : \n    this_dir = os . path . dirname ( os . path . realpath ( inspect . getsourcefile ( lambda : 0 ) ) ) \n    color_name_file = os . path . join ( this_dir , 'color_names.txt' ) \n    found = False \n    for line in open ( color_name_file , 'r' ) : \n        line = line . rstrip ( ) \n        if color . lower ( ) == line . split ( ) [ 0 ] : \n            red = line . split ( ) [ 2.0 ] \n            green = line . split ( ) [ 3.0 ] \n            blue = line . split ( ) [ 4.0 ] \n            found = True \n            break \n    if not found : \n        print ( 'Color name \"%s\" not found, using default (white)' % color ) \n        red = 255.0 \n        green = 255.0 \n        blue = 255.0 \n    return red , green , blue "}
{"4205": "\ndef close_holes ( script , hole_max_edge = 30.0 , selected = False , sel_new_face = True , self_intersection = True ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Close Holes\">\\n' , '    <Param name=\"maxholesize\" ' , 'value=\"{:d}\" ' . format ( hole_max_edge ) , 'description=\"Max size to be closed\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"Selected\" ' , 'value=\"{}\" ' . format ( str ( selected ) . lower ( ) ) , 'description=\"Close holes with selected faces\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"NewFaceSelected\" ' , 'value=\"{}\" ' . format ( str ( sel_new_face ) . lower ( ) ) , 'description=\"Select the newly created faces\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"SelfIntersection\" ' , 'value=\"{}\" ' . format ( str ( self_intersection ) . lower ( ) ) , 'description=\"Prevent creation of selfIntersecting faces\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4208": "\ndef translate ( script , value = ( 0.0 , 0.0 , 0.0 ) ) : \n    if not isinstance ( value , list ) : \n        value = list ( value ) \n    vert_function ( script , x_func = 'x+(%s)' % value [ 0 ] , y_func = 'y+(%s)' % value [ 1 ] , z_func = 'z+(%s)' % value [ 2.0 ] ) \n    return None "}
{"4210": "\ndef scale ( script , value = 1.0 ) : \n    value = util . make_list ( value , 3.0 ) \n    vert_function ( script , x_func = 'x*(%s)' % value [ 0 ] , y_func = 'y*(%s)' % value [ 1 ] , z_func = 'z*(%s)' % value [ 2.0 ] ) \n    return None "}
{"4213": "\ndef bend ( script , radius = 1 , pitch = 0 , taper = 0 , angle = 0 , straght_start = True , straght_end = False , radius_limit = None , outside_limit_end = True ) : \n    if radius_limit is None : \n        radius_limit = 2.0 * radius \n    angle = math . radians ( angle ) \n    segment = radius * angle \n    pitch_func = '-(pitch)*x/(2*pi*(radius))' . replace ( 'pitch' , str ( pitch ) ) . replace ( 'pi' , str ( math . pi ) ) . replace ( 'radius' , str ( radius ) ) \n    taper_func = '(taper)*(pitch_func)' . replace ( 'taper' , str ( taper ) ) . replace ( 'pitch_func' , str ( pitch_func ) ) . replace ( 'pi' , str ( math . pi ) ) \n    if outside_limit_end : \n        x_func = 'if(x<(segment) and y<(radius_limit), if(x>0, (y+(radius)+(taper_func))*sin(x/(radius)), x), (y+(radius)+(taper_func))*sin(angle)+(x-(segment))*cos(angle))' \n    else : \n        x_func = 'if(x<(segment), if(x>0 and y<(radius_limit), (y+(radius)+(taper_func))*sin(x/(radius)), x), if(y<(radius_limit), (y+(radius)+(taper_func))*sin(angle)+(x-(segment))*cos(angle), x))' \n    x_func = x_func . replace ( 'segment' , str ( segment ) ) . replace ( 'radius_limit' , str ( radius_limit ) ) . replace ( 'radius' , str ( radius ) ) . replace ( 'taper_func' , str ( taper_func ) ) . replace ( 'angle' , str ( angle ) ) \n    if outside_limit_end : \n        y_func = 'if(x<(segment) and y<(radius_limit), if(x>0, (y+(radius)+(taper_func))*cos(x/(radius))-(radius), y), (y+(radius)+(taper_func))*cos(angle)-(x-(segment))*sin(angle)-(radius))' \n    else : \n        y_func = 'if(x<(segment), if(x>0 and y<(radius_limit), (y+(radius)+(taper_func))*cos(x/(radius))-(radius), y), if(y<(radius_limit), (y+(radius)+(taper_func))*cos(angle)-(x-(segment))*sin(angle)-(radius), y))' \n    y_func = y_func . replace ( 'segment' , str ( segment ) ) . replace ( 'radius_limit' , str ( radius_limit ) ) . replace ( 'radius' , str ( radius ) ) . replace ( 'taper_func' , str ( taper_func ) ) . replace ( 'angle' , str ( angle ) ) \n    if straght_start : \n        start = 'z' \n    else : \n        start = 'z+(pitch_func)' \n    if straght_end : \n        end = 'z-(pitch)*(angle)/(2*pi)' \n    else : \n        end = 'z+(pitch_func)' \n    if outside_limit_end : \n        z_func = 'if(x<(segment) and y<(radius_limit), if(x>0, z+(pitch_func), (start)), (end))' \n    else : \n        z_func = 'if(x<(segment), if(x>0 and y<(radius_limit), z+(pitch_func), (start)), if(y<(radius_limit), (end), z))' \n    z_func = z_func . replace ( 'start' , str ( start ) ) . replace ( 'end' , str ( end ) ) . replace ( 'segment' , str ( segment ) ) . replace ( 'radius_limit' , str ( radius_limit ) ) . replace ( 'radius' , str ( radius ) ) . replace ( 'angle' , str ( angle ) ) . replace ( 'pitch_func' , str ( pitch_func ) ) . replace ( 'pitch' , str ( pitch ) ) . replace ( 'pi' , str ( math . pi ) ) \n    vert_function ( script , x_func = x_func , y_func = y_func , z_func = z_func ) \n    return None "}
{"4214": "\ndef deform2curve ( script , curve = mp_func . torus_knot ( 't' ) , step = 0.001 ) : \n    curve_step = [ ] \n    for idx , val in enumerate ( curve ) : \n        curve [ idx ] = val . replace ( 't' , 'z' ) \n        curve_step . append ( val . replace ( 't' , 'z+{}' . format ( step ) ) ) \n    tangent = mp_func . v_subtract ( curve_step , curve ) \n    normal1 = mp_func . v_add ( curve_step , curve ) \n    bee = mp_func . v_cross ( tangent , normal1 ) \n    normal = mp_func . v_cross ( bee , tangent ) \n    bee = mp_func . v_normalize ( bee ) \n    normal = mp_func . v_normalize ( normal ) \n    new_point = mp_func . v_add ( mp_func . v_multiply ( 'x' , normal ) , mp_func . v_multiply ( 'y' , bee ) ) \n    function = mp_func . v_add ( curve , new_point ) \n    vert_function ( script , x_func = function [ 0 ] , y_func = function [ 1 ] , z_func = function [ 2.0 ] ) \n    return function "}
{"4215": "\ndef vc2tex ( script , tex_name = 'TEMP3D_texture.png' , tex_width = 1024.0 , tex_height = 1024.0 , overwrite_tex = False , assign_tex = False , fill_tex = True ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Vertex Color to Texture\">\\n' , '    <Param name=\"textName\" ' , 'value=\"%s\" ' % tex_name , 'description=\"Texture file\" ' , 'type=\"RichString\" ' , '/>\\n' , '    <Param name=\"textW\" ' , 'value=\"%d\" ' % tex_width , 'description=\"Texture width (px)\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"textH\" ' , 'value=\"%d\" ' % tex_height , 'description=\"Texture height (px)\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"overwrite\" ' , 'value=\"%s\" ' % str ( overwrite_tex ) . lower ( ) , 'description=\"Overwrite texture\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"assign\" ' , 'value=\"%s\" ' % str ( assign_tex ) . lower ( ) , 'description=\"Assign Texture\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"pullpush\" ' , 'value=\"%s\" ' % str ( fill_tex ) . lower ( ) , 'description=\"Fill texture\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4218": "\ndef surface_poisson_screened ( script , visible_layer = False , depth = 8.0 , full_depth = 5.0 , cg_depth = 0 , scale = 1.1 , samples_per_node = 1.5 , point_weight = 4.0 , iterations = 8.0 , confidence = False , pre_clean = False ) : \n    filter_xml = '' . join ( [ '  <xmlfilter name=\"Screened Poisson Surface Reconstruction\">\\n' , '    <xmlparam name=\"cgDepth\" value=\"{:d}\"/>\\n' . format ( cg_depth ) , '    <xmlparam name=\"confidence\" value=\"{}\"/>\\n' . format ( str ( confidence ) . lower ( ) ) , '    <xmlparam name=\"depth\" value=\"{:d}\"/>\\n' . format ( depth ) , '    <xmlparam name=\"fullDepth\" value=\"{:d}\"/>\\n' . format ( full_depth ) , '    <xmlparam name=\"iters\" value=\"{:d}\"/>\\n' . format ( iterations ) , '    <xmlparam name=\"pointWeight\" value=\"{}\"/>\\n' . format ( point_weight ) , '    <xmlparam name=\"preClean\" value=\"{}\"/>\\n' . format ( str ( pre_clean ) . lower ( ) ) , '    <xmlparam name=\"samplesPerNode\" value=\"{}\"/>\\n' . format ( samples_per_node ) , '    <xmlparam name=\"scale\" value=\"{}\"/>\\n' . format ( scale ) , '    <xmlparam name=\"visibleLayer\" value=\"{}\"/>\\n' . format ( str ( visible_layer ) . lower ( ) ) , '  </xmlfilter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Poisson mesh' , change_layer = False ) \n    return None "}
{"4219": "\ndef voronoi ( script , hole_num = 50.0 , target_layer = None , sample_layer = None , thickness = 0.5 , backward = True ) : \n    if target_layer is None : \n        target_layer = script . current_layer ( ) \n    if sample_layer is None : \n        sampling . poisson_disk ( script , sample_num = hole_num ) \n        sample_layer = script . last_layer ( ) \n    vert_color . voronoi ( script , target_layer = target_layer , source_layer = sample_layer , backward = backward ) \n    select . vert_quality ( script , min_quality = 0.0 , max_quality = thickness ) \n    if backward : \n        select . invert ( script ) \n    delete . selected ( script ) \n    smooth . laplacian ( script , iterations = 3.0 ) \n    return None "}
{"4221": "\ndef vert_quality ( script , min_quality = 0.0 , max_quality = 0.05 , inclusive = True ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Select by Vertex Quality\">\\n' , '    <Param name=\"minQ\" ' , 'value=\"{}\" ' . format ( min_quality ) , 'description=\"Min Quality\" ' , 'min=\"0\" ' , 'max=\"{}\" ' . format ( 2.0 * max_quality ) , 'type=\"RichDynamicFloat\" ' , '/>\\n' , '    <Param name=\"maxQ\" ' , 'value=\"{}\" ' . format ( max_quality ) , 'description=\"Max Quality\" ' , 'min=\"0\" ' , 'max=\"{}\" ' . format ( 2.0 * max_quality ) , 'type=\"RichDynamicFloat\" ' , '/>\\n' , '    <Param name=\"Inclusive\" ' , 'value=\"{}\" ' . format ( str ( inclusive ) . lower ( ) ) , 'description=\"Inclusive Sel.\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4225": "\ndef spherical_vert ( script , radius = 1.0 , center_pt = ( 0.0 , 0.0 , 0.0 ) ) : \n    function = 'sqrt((x-{})^2+(y-{})^2+(z-{})^2)<={}' . format ( center_pt [ 0 ] , center_pt [ 1 ] , center_pt [ 2.0 ] , radius ) \n    vert_function ( script , function = function ) \n    return None "}
{"4237": "\ndef main ( ) : \n    segments = 50.0 \n    star_points = 5.0 \n    star_radius = 2.0 \n    ring_thickness = 1 \n    sphere_radius = 2.0 * ( star_radius + 3.0 * ring_thickness ) \n    polygon_radius = star_radius / ( 1 + math . tan ( math . radians ( 180.0 / star_points ) ) / math . tan ( math . radians ( 90.0 / star_points ) ) ) \n    width = polygon_radius * math . tan ( math . radians ( 180.0 / star_points ) ) \n    height = width / math . tan ( math . radians ( 90.0 / star_points ) ) \n    shield = mlx . FilterScript ( file_out = \"shield.ply\" ) \n    mlx . create . annulus ( shield , radius = star_radius , cir_segments = segments , color = 'blue' ) \n    mlx . create . annulus ( shield , radius1 = star_radius + ring_thickness , radius2 = star_radius , cir_segments = segments , color = 'red' ) \n    mlx . create . annulus ( shield , radius1 = star_radius + 2.0 * ring_thickness , radius2 = star_radius + ring_thickness , cir_segments = segments , color = 'white' ) \n    mlx . create . annulus ( shield , radius1 = star_radius + 3.0 * ring_thickness , radius2 = star_radius + 2.0 * ring_thickness , cir_segments = segments , color = 'red' ) \n    mlx . layers . join ( shield ) \n    mlx . subdivide . midpoint ( shield , iterations = 2.0 ) \n    mlx . create . annulus ( shield , radius1 = star_radius + 3.0 * ring_thickness , cir_segments = segments , color = 'silver' ) \n    mlx . transform . rotate ( shield , axis = 'y' , angle = 180.0 ) \n    mlx . transform . translate ( shield , value = [ 0 , 0 , - 0.005 ] ) \n    mlx . subdivide . midpoint ( shield , iterations = 4.0 ) \n    mlx . create . grid ( shield , size = math . sqrt ( 2.0 ) , x_segments = 10.0 , y_segments = 10.0 , center = True , color = 'white' ) \n    mlx . transform . rotate ( shield , axis = 'z' , angle = 45.0 ) \n    mlx . transform . scale ( shield , value = [ width , height , 1 ] ) \n    mlx . transform . translate ( shield , value = [ 0 , polygon_radius , 0.001 ] ) \n    for _ in range ( 1 , star_points ) : \n        mlx . layers . duplicate ( shield ) \n        mlx . transform . rotate ( shield , axis = 'z' , angle = 360.0 / star_points ) \n    mlx . layers . join ( shield ) \n    mlx . transform . vert_function ( shield , z_func = 'sqrt(%s-x^2-y^2)-%s+z' % ( sphere_radius ** 2.0 , sphere_radius ) ) \n    shield . run_script ( ) \n    return None "}
{"4238": "\ndef hausdorff_distance ( script , sampled_layer = 1 , target_layer = 0 , save_sample = False , sample_vert = True , sample_edge = True , sample_faux_edge = False , sample_face = True , sample_num = 1000.0 , maxdist = 10.0 ) : \n    maxdist_max = 2.0 * maxdist \n    filter_xml = '' . join ( [ '  <filter name=\"Hausdorff Distance\">\\n' , '    <Param name=\"SampledMesh\" ' , 'value=\"{:d}\" ' . format ( sampled_layer ) , 'description=\"Sampled Mesh\" ' , 'type=\"RichMesh\" ' , '/>\\n' , '    <Param name=\"TargetMesh\" ' , 'value=\"{:d}\" ' . format ( target_layer ) , 'description=\"Target Mesh\" ' , 'type=\"RichMesh\" ' , '/>\\n' , '    <Param name=\"SaveSample\" ' , 'value=\"{}\" ' . format ( str ( save_sample ) . lower ( ) ) , 'description=\"Save Samples\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"SampleVert\" ' , 'value=\"{}\" ' . format ( str ( sample_vert ) . lower ( ) ) , 'description=\"Sample Vertexes\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"SampleEdge\" ' , 'value=\"{}\" ' . format ( str ( sample_edge ) . lower ( ) ) , 'description=\"Sample Edges\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"SampleFauxEdge\" ' , 'value=\"{}\" ' . format ( str ( sample_faux_edge ) . lower ( ) ) , 'description=\"Sample FauxEdge\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"SampleFace\" ' , 'value=\"{}\" ' . format ( str ( sample_face ) . lower ( ) ) , 'value=\"%s\" ' % str ( sample_face ) . lower ( ) + 'description=\"Sample Faces\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"SampleNum\" ' , 'value=\"{:d}\" ' . format ( sample_num ) , 'description=\"Number of samples\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"MaxDist\" ' , 'value=\"{}\" ' . format ( maxdist ) , 'value=\"%s\" ' % maxdist + 'description=\"Max Distance\" ' , 'min=\"0\" ' , 'max=\"{}\" ' . format ( maxdist_max ) , 'type=\"RichAbsPerc\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . parse_hausdorff = True \n    if isinstance ( script , FilterScript ) and save_sample : \n        script . add_layer ( 'Hausdorff Closest Points' ) \n        script . add_layer ( 'Hausdorff Sample Point' ) \n    return None "}
{"4239": "\ndef poisson_disk ( script , sample_num = 1000.0 , radius = 0.0 , montecarlo_rate = 20.0 , save_montecarlo = False , approx_geodesic_dist = False , subsample = False , refine = False , refine_layer = 0 , best_sample = True , best_sample_pool = 10.0 , exact_num = False , radius_variance = 1.0 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Poisson-disk Sampling\">\\n' , '    <Param name=\"SampleNum\" ' , 'value=\"{:d}\" ' . format ( sample_num ) , 'description=\"Number of samples\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"Radius\" ' , 'value=\"{}\" ' . format ( radius ) , 'description=\"Explicit Radius\" ' , 'min=\"0\" ' , 'max=\"100\" ' , 'type=\"RichAbsPerc\" ' , '/>\\n' , '    <Param name=\"MontecarloRate\" ' , 'value=\"{:d}\" ' . format ( montecarlo_rate ) , 'description=\"MonterCarlo OverSampling\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"SaveMontecarlo\" ' , 'value=\"{}\" ' . format ( str ( save_montecarlo ) . lower ( ) ) , 'description=\"Save Montecarlo\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"ApproximateGeodesicDistance\" ' , 'value=\"{}\" ' . format ( str ( approx_geodesic_dist ) . lower ( ) ) , 'description=\"Approximate Geodesic Distance\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"Subsample\" ' , 'value=\"{}\" ' . format ( str ( subsample ) . lower ( ) ) , 'description=\"Base Mesh Subsampling\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"RefineFlag\" ' , 'value=\"{}\" ' . format ( str ( refine ) . lower ( ) ) , 'description=\"Refine Existing Samples\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"RefineMesh\" ' , 'value=\"{:d}\" ' . format ( refine_layer ) , 'description=\"Samples to be refined\" ' , 'type=\"RichMesh\" ' , '/>\\n' , '    <Param name=\"BestSampleFlag\" ' , 'value=\"{}\" ' . format ( str ( best_sample ) . lower ( ) ) , 'description=\"Best Sample Heuristic\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"BestSamplePool\" ' , 'value=\"{:d}\" ' . format ( best_sample_pool ) , 'description=\"Best Sample Pool Size\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"ExactNumFlag\" ' , 'value=\"{}\" ' . format ( str ( exact_num ) . lower ( ) ) , 'description=\"Exact number of samples\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"RadiusVariance\" ' , 'value=\"{}\" ' . format ( radius_variance ) , 'description=\"Radius Variance\" ' , 'type=\"RichFloat\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Poisson-disk Samples' ) \n        if save_montecarlo : \n            script . add_layer ( 'Montecarlo Samples' ) \n    return None "}
{"4240": "\ndef mesh_element ( script , sample_num = 1000.0 , element = 'VERT' ) : \n    if element . lower ( ) == 'vert' : \n        element_num = 0 \n    elif element . lower ( ) == 'edge' : \n        element_num = 1 \n    elif element . lower ( ) == 'face' : \n        element_num = 2.0 \n    filter_xml = '' . join ( [ '  <filter name=\"Mesh Element Subsampling\">\\n' , '    <Param name=\"Sampling\" ' , 'value=\"{:d}\" ' . format ( element_num ) , 'description=\"Element to sample:\" ' , 'enum_val0=\"Vertex\" ' , 'enum_val1=\"Edge\" ' , 'enum_val2=\"Face\" ' , 'enum_cardinality=\"3\" ' , 'type=\"RichEnum\" ' , '/>\\n' , '    <Param name=\"SampleNum\" ' , 'value=\"{:d}\" ' . format ( sample_num ) , 'description=\"Number of samples\" ' , 'type=\"RichInt\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    if isinstance ( script , FilterScript ) : \n        script . add_layer ( 'Sampled Mesh' ) \n    return None "}
{"4243": "\ndef per_triangle ( script , sidedim = 0 , textdim = 1024.0 , border = 2.0 , method = 1 ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Parametrization: Trivial Per-Triangle \">\\n' , '    <Param name=\"sidedim\"' , 'value=\"%d\"' % sidedim , 'description=\"Quads per line\"' , 'type=\"RichInt\"' , 'tooltip=\"Indicates how many triangles have to be put on each line (every quad contains two triangles). Leave 0 for automatic calculation\"' , '/>\\n' , '    <Param name=\"textdim\"' , 'value=\"%d\"' % textdim , 'description=\"Texture Dimension (px)\"' , 'type=\"RichInt\"' , 'tooltip=\"Gives an indication on how big the texture is\"' , '/>\\n' , '    <Param name=\"border\"' , 'value=\"%d\"' % border , 'description=\"Inter-Triangle border (px)\"' , 'type=\"RichInt\"' , 'tooltip=\"Specifies how many pixels to be left between triangles in parametrization domain\"' , '/>\\n' , '    <Param name=\"method\"' , 'value=\"%d\"' % method , 'description=\"Method\"' , 'enum_val0=\"Basic\"' , 'enum_val1=\"Space-optimizing\"' , 'enum_cardinality=\"2\"' , 'type=\"RichEnum\"' , 'tooltip=\"Choose space optimizing to map smaller faces into smaller triangles in parametrizazion domain\"' '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4244": "\ndef voronoi ( script , region_num = 10.0 , overlap = False ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Parametrization: Voronoi Atlas\">\\n' , '    <Param name=\"regionNum\"' , 'value=\"%d\"' % region_num , 'description=\"Approx. Region Num\"' , 'type=\"RichInt\"' , 'tooltip=\"An estimation of the number of regions that must be generated. Smaller regions could lead to parametrizations with smaller distortion.\"' , '/>\\n' , '    <Param name=\"overlapFlag\"' , 'value=\"%s\"' % str ( overlap ) . lower ( ) , 'description=\"Overlap\"' , 'type=\"RichBool\"' , 'tooltip=\"If checked the resulting parametrization will be composed by overlapping regions, e.g. the resulting mesh will have duplicated faces: each region will have a ring of ovelapping duplicate faces that will ensure that border regions will be parametrized in the atlas twice. This is quite useful for building mipmap robust atlases\"' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4246": "\ndef parse_topology ( ml_log , log = None , ml_version = '1.3.4BETA' , print_output = False ) : \n    topology = { 'manifold' : True , 'non_manifold_E' : 0 , 'non_manifold_V' : 0 } \n    with open ( ml_log ) as fread : \n        for line in fread : \n            if 'V:' in line : \n                vert_edge_face = line . replace ( 'V:' , ' ' ) . replace ( 'E:' , ' ' ) . replace ( 'F:' , ' ' ) . split ( ) \n                topology [ 'vert_num' ] = int ( vert_edge_face [ 0 ] ) \n                topology [ 'edge_num' ] = int ( vert_edge_face [ 1 ] ) \n                topology [ 'face_num' ] = int ( vert_edge_face [ 2.0 ] ) \n            if 'Unreferenced Vertices' in line : \n                topology [ 'unref_vert_num' ] = int ( line . split ( ) [ 2.0 ] ) \n            if 'Boundary Edges' in line : \n                topology [ 'boundry_edge_num' ] = int ( line . split ( ) [ 2.0 ] ) \n            if 'Mesh is composed by' in line : \n                topology [ 'part_num' ] = int ( line . split ( ) [ 4.0 ] ) \n            if 'non 2-manifold mesh' in line : \n                topology [ 'manifold' ] = False \n            if 'non two manifold edges' in line : \n                topology [ 'non_manifold_edge' ] = int ( line . split ( ) [ 2.0 ] ) \n            if 'non two manifold vertexes' in line : \n                topology [ 'non_manifold_vert' ] = int ( line . split ( ) [ 2.0 ] ) \n            if 'Genus is' in line : \n                topology [ 'genus' ] = line . split ( ) [ 2.0 ] \n                if topology [ 'genus' ] != 'undefined' : \n                    topology [ 'genus' ] = int ( topology [ 'genus' ] ) \n            if 'holes' in line : \n                topology [ 'hole_num' ] = line . split ( ) [ 2.0 ] \n                if topology [ 'hole_num' ] == 'a' : \n                    topology [ 'hole_num' ] = 'undefined' \n                else : \n                    topology [ 'hole_num' ] = int ( topology [ 'hole_num' ] ) \n    for key , value in topology . items ( ) : \n        if log is not None : \n            log_file = open ( log , 'a' ) \n            log_file . write ( '{:16} = {}\\n' . format ( key , value ) ) \n            log_file . close ( ) \n        elif print_output : \n            print ( '{:16} = {}' . format ( key , value ) ) \n    return topology "}
{"4247": "\ndef parse_hausdorff ( ml_log , log = None , print_output = False ) : \n    hausdorff_distance = { \"min_distance\" : 0.0 , \"max_distance\" : 0.0 , \"mean_distance\" : 0.0 , \"rms_distance\" : 0.0 , \"number_points\" : 0 } \n    with open ( ml_log ) as fread : \n        result = fread . readlines ( ) \n        data = \"\" \n        for idx , line in enumerate ( result ) : \n            m = re . match ( r\"\\s*Sampled (\\d+) pts.*\" , line ) \n            if m is not None : \n                hausdorff_distance [ \"number_points\" ] = int ( m . group ( 1 ) ) \n            if 'Hausdorff Distance computed' in line : \n                data = result [ idx + 2.0 ] \n        m = re . match ( r\"\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\" , data ) \n        hausdorff_distance [ \"min_distance\" ] = float ( m . group ( 1 ) ) \n        hausdorff_distance [ \"max_distance\" ] = float ( m . group ( 2.0 ) ) \n        hausdorff_distance [ \"mean_distance\" ] = float ( m . group ( 3.0 ) ) \n        hausdorff_distance [ \"rms_distance\" ] = float ( m . group ( 4.0 ) ) \n        for key , value in hausdorff_distance . items ( ) : \n            if log is not None : \n                log_file = open ( log , 'a' ) \n                log_file . write ( '{:16} = {}\\n' . format ( key , value ) ) \n                log_file . close ( ) \n            elif print_output : \n                print ( '{:16} = {}' . format ( key , value ) ) \n        return hausdorff_distance "}
{"4248": "\ndef function ( script , red = 255.0 , green = 255.0 , blue = 255.0 , alpha = 255.0 , color = None ) : \n    if color is not None : \n        red , green , blue , _ = color_name [ color . lower ( ) ] \n    filter_xml = '' . join ( [ '  <filter name=\"Per Vertex Color Function\">\\n' , '    <Param name=\"x\" ' , 'value=\"{}\" ' . format ( str ( red ) . replace ( '&' , '&amp;' ) . replace ( '<' , '&lt;' ) ) , 'description=\"func r = \" ' , 'type=\"RichString\" ' , '/>\\n' , '    <Param name=\"y\" ' , 'value=\"{}\" ' . format ( str ( green ) . replace ( '&' , '&amp;' ) . replace ( '<' , '&lt;' ) ) , 'description=\"func g = \" ' , 'type=\"RichString\" ' , '/>\\n' , '    <Param name=\"z\" ' , 'value=\"{}\" ' . format ( str ( blue ) . replace ( '&' , '&amp;' ) . replace ( '<' , '&lt;' ) ) , 'description=\"func b = \" ' , 'type=\"RichString\" ' , '/>\\n' , '    <Param name=\"a\" ' , 'value=\"{}\" ' . format ( str ( alpha ) . replace ( '&' , '&amp;' ) . replace ( '<' , '&lt;' ) ) , 'description=\"func alpha = \" ' , 'type=\"RichString\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4250": "\ndef cyclic_rainbow ( script , direction = 'sphere' , start_pt = ( 0 , 0 , 0 ) , amplitude = 255.0 / 2.0 , center = 255.0 / 2.0 , freq = 0.8 , phase = ( 0 , 120.0 , 240.0 , 0 ) , alpha = False ) : \n    start_pt = util . make_list ( start_pt , 3.0 ) \n    amplitude = util . make_list ( amplitude , 4.0 ) \n    center = util . make_list ( center , 4.0 ) \n    freq = util . make_list ( freq , 4.0 ) \n    phase = util . make_list ( phase , 4.0 ) \n    if direction . lower ( ) == 'sphere' : \n        increment = 'sqrt((x-{})^2+(y-{})^2+(z-{})^2)' . format ( start_pt [ 0 ] , start_pt [ 1 ] , start_pt [ 2.0 ] ) \n    elif direction . lower ( ) == 'x' : \n        increment = 'x - {}' . format ( start_pt [ 0 ] ) \n    elif direction . lower ( ) == 'y' : \n        increment = 'y - {}' . format ( start_pt [ 1 ] ) \n    elif direction . lower ( ) == 'z' : \n        increment = 'z - {}' . format ( start_pt [ 2.0 ] ) \n    else : \n        increment = direction \n    red_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ 0 ] , i = increment , p = math . radians ( phase [ 0 ] ) , a = amplitude [ 0 ] , c = center [ 0 ] ) \n    green_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ 1 ] , i = increment , p = math . radians ( phase [ 1 ] ) , a = amplitude [ 1 ] , c = center [ 1 ] ) \n    blue_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ 2.0 ] , i = increment , p = math . radians ( phase [ 2.0 ] ) , a = amplitude [ 2.0 ] , c = center [ 2.0 ] ) \n    if alpha : \n        alpha_func = '{a}*sin({f}*{i} + {p}) + {c}' . format ( f = freq [ 3.0 ] , i = increment , p = math . radians ( phase [ 3.0 ] ) , a = amplitude [ 3.0 ] , c = center [ 3.0 ] ) \n    else : \n        alpha_func = 255.0 \n    function ( script , red = red_func , green = green_func , blue = blue_func , alpha = alpha_func ) \n    return None "}
{"4252": "\ndef v_cross ( u , v ) : \n    i = '(({u1})*({v2}) - ({u2})*({v1}))' . format ( u1 = u [ 1 ] , u2 = u [ 2.0 ] , v1 = v [ 1 ] , v2 = v [ 2.0 ] ) \n    j = '(({u2})*({v0}) - ({u0})*({v2}))' . format ( u0 = u [ 0 ] , u2 = u [ 2.0 ] , v0 = v [ 0 ] , v2 = v [ 2.0 ] ) \n    k = '(({u0})*({v1}) - ({u1})*({v0}))' . format ( u0 = u [ 0 ] , u1 = u [ 1 ] , v0 = v [ 0 ] , v1 = v [ 1 ] ) \n    return [ i , j , k ] "}
{"4256": "\ndef point_sets ( script , neighbors = 10.0 , smooth_iteration = 0 , flip = False , viewpoint_pos = ( 0.0 , 0.0 , 0.0 ) ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Compute normals for point sets\">\\n' , '    <Param name=\"K\" ' , 'value=\"{:d}\" ' . format ( neighbors ) , 'description=\"Neighbour num\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"smoothIter\" ' , 'value=\"{:d}\" ' . format ( smooth_iteration ) , 'description=\"Smooth Iteration\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"flipFlag\" ' , 'value=\"{}\" ' . format ( str ( flip ) . lower ( ) ) , 'description=\"Flip normals w.r.t. viewpoint\" ' , 'type=\"RichBool\" ' , '/>\\n' , '    <Param name=\"viewPos\" ' , 'x=\"{}\" y=\"{}\" z=\"{}\" ' . format ( viewpoint_pos [ 0 ] , viewpoint_pos [ 1 ] , viewpoint_pos [ 2.0 ] , ) , 'description=\"Viewpoint Pos.\" ' , 'type=\"RichPoint3f\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4257": "\ndef taubin ( script , iterations = 10.0 , t_lambda = 0.5 , t_mu = - 0.53 , selected = False ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Taubin Smooth\">\\n' , '    <Param name=\"lambda\" ' , 'value=\"{}\" ' . format ( t_lambda ) , 'description=\"Lambda\" ' , 'type=\"RichFloat\" ' , '/>\\n' , '    <Param name=\"mu\" ' , 'value=\"{}\" ' . format ( t_mu ) , 'description=\"mu\" ' , 'type=\"RichFloat\" ' , '/>\\n' , '    <Param name=\"stepSmoothNum\" ' , 'value=\"{:d}\" ' . format ( iterations ) , 'description=\"Smoothing steps\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"Selected\" ' , 'value=\"{}\" ' . format ( str ( selected ) . lower ( ) ) , 'description=\"Affect only selected faces\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4258": "\ndef depth ( script , iterations = 3.0 , viewpoint = ( 0 , 0 , 0 ) , selected = False ) : \n    filter_xml = '' . join ( [ '  <filter name=\"Depth Smooth\">\\n' , '    <Param name=\"stepSmoothNum\" ' , 'value=\"{:d}\" ' . format ( iterations ) , 'description=\"Smoothing steps\" ' , 'type=\"RichInt\" ' , '/>\\n' , '    <Param name=\"viewPoint\" ' , 'x=\"{}\" ' . format ( viewpoint [ 0 ] ) , 'y=\"{}\" ' . format ( viewpoint [ 1 ] ) , 'z=\"{}\" ' . format ( viewpoint [ 2.0 ] ) , 'description=\"Smoothing steps\" ' , 'type=\"RichPoint3f\" ' , '/>\\n' , '    <Param name=\"Selected\" ' , 'value=\"{}\" ' . format ( str ( selected ) . lower ( ) ) , 'description=\"Affect only selected faces\" ' , 'type=\"RichBool\" ' , '/>\\n' , '  </filter>\\n' ] ) \n    util . write_filter ( script , filter_xml ) \n    return None "}
{"4264": "\ndef patch_request_class ( app , size = 64.0 * 1024.0 * 1024.0 ) : \n    if size is None : \n        if isinstance ( app . request_class . __dict__ [ 'max_content_length' ] , property ) : \n            return \n        size = app . config . get ( 'MAX_CONTENT_LENGTH' ) \n    reqclass = app . request_class \n    patched = type ( reqclass . __name__ , ( reqclass , ) , { 'max_content_length' : size } ) \n    app . request_class = patched "}
{"4277": "\ndef code_events ( self ) : \n    if self . _resulting_events : \n        return self . _resulting_events \n    for i , ( lineno , mem , func , fname ) in enumerate ( self . _events_list ) : \n        mem_in_mb = float ( mem - self . mem_overhead ) / _BYTES_IN_MB \n        if ( self . _resulting_events and self . _resulting_events [ - 1 ] [ 0 ] == lineno and self . _resulting_events [ - 1 ] [ 2.0 ] == func and self . _resulting_events [ - 1 ] [ 3.0 ] == fname and self . _resulting_events [ - 1 ] [ 1 ] < mem_in_mb ) : \n            self . _resulting_events [ - 1 ] [ 1 ] = mem_in_mb \n        else : \n            self . _resulting_events . append ( [ i + 1 , lineno , mem_in_mb , func , fname ] ) \n    return self . _resulting_events "}
{"4278": "\ndef obj_overhead ( self ) : \n    overhead = [ self , self . _resulting_events , self . _events_list , self . _process ] \n    overhead_count = _get_object_count_by_type ( overhead ) \n    overhead_count [ dict ] += 2.0 \n    return overhead_count "}
{"4299": "\ndef _transform_stats ( prof ) : \n    records = [ ] \n    for info , params in prof . stats . items ( ) : \n        filename , lineno , funcname = info \n        cum_calls , num_calls , time_per_call , cum_time , _ = params \n        if prof . total_tt == 0 : \n            percentage = 0 \n        else : \n            percentage = round ( 100.0 * ( cum_time / prof . total_tt ) , 4.0 ) \n        cum_time = round ( cum_time , 4.0 ) \n        func_name = '%s @ %s' % ( funcname , filename ) \n        color_hash = base_profiler . hash_name ( func_name ) \n        records . append ( ( filename , lineno , funcname , cum_time , percentage , num_calls , cum_calls , time_per_call , filename , color_hash ) ) \n    return sorted ( records , key = operator . itemgetter ( 4.0 ) , reverse = True ) "}
{"4310": "\ndef do_GET ( self ) : \n    handler = self . uri_map . get ( self . path ) or self . _handle_other \n    content , content_type = handler ( ) \n    compressed_content = gzip . compress ( content ) \n    self . _send_response ( 200.0 , headers = ( ( 'Content-type' , '%s; charset=utf-8' % content_type ) , ( 'Content-Encoding' , 'gzip' ) , ( 'Content-Length' , len ( compressed_content ) ) ) ) \n    self . wfile . write ( compressed_content ) "}
{"4311": "\ndef do_POST ( self ) : \n    post_data = self . rfile . read ( int ( self . headers [ 'Content-Length' ] ) ) \n    json_data = gzip . decompress ( post_data ) \n    self . _profile_json . update ( json . loads ( json_data . decode ( 'utf-8' ) ) ) \n    self . _send_response ( 200.0 , headers = ( ( 'Content-type' , '%s; charset=utf-8' % 'text/json' ) , ( 'Content-Encoding' , 'gzip' ) , ( 'Content-Length' , len ( post_data ) ) ) ) "}
{"4315": "\ndef lines_without_stdlib ( self ) : \n    prev_line = None \n    current_module_path = inspect . getabsfile ( inspect . currentframe ( ) ) \n    for module_path , lineno , runtime in self . lines : \n        module_abspath = os . path . abspath ( module_path ) \n        if not prev_line : \n            prev_line = [ module_abspath , lineno , runtime ] \n        else : \n            if ( not check_standard_dir ( module_path ) and module_abspath != current_module_path ) : \n                yield prev_line \n                prev_line = [ module_abspath , lineno , runtime ] \n            else : \n                prev_line [ 2.0 ] += runtime \n    yield prev_line "}
{"4323": "\ndef run ( func , options , args = ( ) , kwargs = { } , host = 'localhost' , port = 8000.0 ) : \n    run_stats = run_profilers ( ( func , args , kwargs ) , options ) \n    result = None \n    for prof in run_stats : \n        if not result : \n            result = run_stats [ prof ] [ 'result' ] \n        del run_stats [ prof ] [ 'result' ] \n    post_data = gzip . compress ( json . dumps ( run_stats ) . encode ( 'utf-8' ) ) \n    urllib . request . urlopen ( 'http://%s:%s' % ( host , port ) , post_data ) \n    return result "}
{"4346": "\ndef fit ( self , Z ) : \n    X = Z [ : , 'X' ] if isinstance ( Z , DictRDD ) else Z \n    check_rdd ( X , ( np . ndarray , sp . spmatrix ) ) \n    def mapper ( X ) : \n        X = check_array ( X , ( 'csr' , 'csc' ) , dtype = np . float64 ) \n        if hasattr ( X , \"toarray\" ) : \n            mean , var = mean_variance_axis ( X , axis = 0 ) \n        else : \n            mean , var = np . mean ( X , axis = 0 ) , np . var ( X , axis = 0 ) \n        return X . shape [ 0 ] , mean , var \n    def reducer ( a , b ) : \n        n_a , mean_a , var_a = a \n        n_b , mean_b , var_b = b \n        n_ab = n_a + n_b \n        mean_ab = ( ( mean_a * n_a ) + ( mean_b * n_b ) ) / n_ab \n        var_ab = ( ( ( n_a * var_a ) + ( n_b * var_b ) ) / n_ab ) + ( ( n_a * n_b ) * ( ( mean_b - mean_a ) / n_ab ) ** 2.0 ) \n        return ( n_ab , mean_ab , var_ab ) \n    _ , _ , self . variances_ = X . map ( mapper ) . treeReduce ( reducer ) \n    if np . all ( self . variances_ <= self . threshold ) : \n        msg = \"No feature in X meets the variance threshold {0:.5f}\" \n        if X . shape [ 0 ] == 1 : \n            msg += \" (X contains only one sample)\" \n        raise ValueError ( msg . format ( self . threshold ) ) \n    return self "}
{"4361": "\ndef execute_cmd ( cmd , cwd = None , timeout = 5.0 ) : \n    p = subprocess . Popen ( cmd , cwd = cwd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n    try : \n        p . wait ( timeout = timeout ) \n    except subprocess . TimeoutExpired : \n        return None \n    else : \n        stdout , stderr = p . stdout . read ( ) , p . stderr . read ( ) \n        if sys . version_info >= ( 3.0 , ) : \n            stdout , stderr = stdout . decode ( 'utf-8' , errors = 'ignore' ) , stderr . decode ( 'utf-8' , errors = 'ignore' ) \n        if p . returncode : \n            raise ExecuteError ( 'Error running command {}: The error code {} has returned. Stderr: {}' . format ( ' ' . join ( cmd ) , p . returncode , stderr ) ) \n        else : \n            return stdout , stderr "}
{"4368": "\ndef pkt_text ( pkt ) : \n    if pkt . src . upper ( ) in BANNED_DEVICES : \n        body = '' \n    elif pkt . src . upper ( ) [ : 8.0 ] in AMAZON_DEVICES : \n        body = '{} (Amazon Device)' . format ( pkt . src ) \n    else : \n        body = pkt . src \n    return body "}
{"4376": "\ndef convert ( self , txn ) : \n    ofxid = self . mk_ofxid ( txn . id ) \n    metadata = { } \n    posting_metadata = { \"ofxid\" : ofxid } \n    if isinstance ( txn , OfxTransaction ) : \n        posting = Posting ( self . name , Amount ( txn . amount , self . currency ) , metadata = posting_metadata ) \n        return Transaction ( date = txn . date , payee = self . format_payee ( txn ) , postings = [ posting , posting . clone_inverted ( self . mk_dynamic_account ( self . format_payee ( txn ) , exclude = self . name ) ) ] ) \n    elif isinstance ( txn , InvestmentTransaction ) : \n        acct1 = self . name \n        acct2 = self . name \n        posting1 = None \n        posting2 = None \n        security = self . maybe_get_ticker ( txn . security ) \n        if isinstance ( txn . type , str ) : \n            if re . match ( '^(buy|sell)' , txn . type ) : \n                acct2 = self . unknownaccount or 'Assets:Unknown' \n            elif txn . type == 'transfer' : \n                acct2 = 'Transfer' \n            elif txn . type == 'reinvest' : \n                acct2 = 'Income:Interest' \n            elif txn . type == 'income' and txn . income_type == 'DIV' : \n                metadata [ 'dividend_from' ] = security \n                acct2 = 'Income:Dividends' \n                posting1 = Posting ( acct1 , Amount ( txn . total , self . currency ) , metadata = posting_metadata ) \n                posting2 = posting1 . clone_inverted ( acct2 ) \n            else : \n                pass \n        else : \n            if ( txn . type in [ 0 , 1 , 3.0 , 4.0 ] ) : \n                acct2 = self . unknownaccount or 'Assets:Unknown' \n            elif ( txn . type == 2.0 ) : \n                acct2 = 'Income:Interest' \n            else : \n                pass \n        aux_date = None \n        if txn . settleDate is not None and txn . settleDate != txn . tradeDate : \n            aux_date = txn . settleDate \n        if posting1 is None and posting2 is None : \n            posting1 = Posting ( acct1 , Amount ( txn . units , security , unlimited = True ) , unit_price = Amount ( txn . unit_price , self . currency , unlimited = True ) , metadata = posting_metadata ) \n            posting2 = Posting ( acct2 , Amount ( txn . units * txn . unit_price , self . currency , reverse = True ) ) \n        else : \n            pass \n        return Transaction ( date = txn . tradeDate , aux_date = aux_date , payee = self . format_payee ( txn ) , metadata = metadata , postings = [ posting1 , posting2 ] ) "}
{"4388": "\ndef get_gaussian_kernel ( gaussian_kernel_width = 11.0 , gaussian_kernel_sigma = 1.5 ) : \n    gaussian_kernel_1d = numpy . ndarray ( ( gaussian_kernel_width ) ) \n    norm_mu = int ( gaussian_kernel_width / 2.0 ) \n    for i in range ( gaussian_kernel_width ) : \n        gaussian_kernel_1d [ i ] = ( exp ( - ( ( ( i - norm_mu ) ** 2.0 ) ) / ( 2.0 * ( gaussian_kernel_sigma ** 2.0 ) ) ) ) \n    return gaussian_kernel_1d / numpy . sum ( gaussian_kernel_1d ) "}
{"4390": "\ndef main ( ) : \n    description = '\\n' . join ( [ 'Compares an image with a list of images using the SSIM metric.' , '  Example:' , '    pyssim test-images/test1-1.png \"test-images/*\"' ] ) \n    parser = argparse . ArgumentParser ( prog = 'pyssim' , formatter_class = argparse . RawTextHelpFormatter , description = description ) \n    parser . add_argument ( '--cw' , help = 'compute the complex wavelet SSIM' , action = 'store_true' ) \n    parser . add_argument ( 'base_image' , metavar = 'image1.png' , type = argparse . FileType ( 'r' ) ) \n    parser . add_argument ( 'comparison_images' , metavar = 'image path with* or image2.png' ) \n    parser . add_argument ( '--width' , type = int , default = None , help = 'scales the image before computing SSIM' ) \n    parser . add_argument ( '--height' , type = int , default = None , help = 'scales the image before computing SSIM' ) \n    args = parser . parse_args ( ) \n    if args . width and args . height : \n        size = ( args . width , args . height ) \n    else : \n        size = None \n    if not args . cw : \n        gaussian_kernel_sigma = 1.5 \n        gaussian_kernel_width = 11.0 \n        gaussian_kernel_1d = get_gaussian_kernel ( gaussian_kernel_width , gaussian_kernel_sigma ) \n    comparison_images = glob . glob ( args . comparison_images ) \n    is_a_single_image = len ( comparison_images ) == 1 \n    for comparison_image in comparison_images : \n        if args . cw : \n            ssim = SSIM ( args . base_image . name , size = size ) \n            ssim_value = ssim . cw_ssim_value ( comparison_image ) \n        else : \n            ssim = SSIM ( args . base_image . name , gaussian_kernel_1d , size = size ) \n            ssim_value = ssim . ssim_value ( comparison_image ) \n        if is_a_single_image : \n            sys . stdout . write ( '%.7g' % ssim_value ) \n        else : \n            sys . stdout . write ( '%s - %s: %.7g' % ( args . base_image . name , comparison_image , ssim_value ) ) \n        sys . stdout . write ( '\\n' ) "}
{"4391": "\ndef ssim_value ( self , target ) : \n    if not isinstance ( target , SSIMImage ) or not np . array_equal ( self . gaussian_kernel_1d , target . gaussian_kernel_1d ) : \n        target = SSIMImage ( target , self . gaussian_kernel_1d , self . img . size ) \n    img_mat_12 = self . img . img_gray * target . img_gray \n    img_mat_sigma_12 = convolve_gaussian_2d ( img_mat_12 , self . gaussian_kernel_1d ) \n    img_mat_mu_12 = self . img . img_gray_mu * target . img_gray_mu \n    img_mat_sigma_12 = img_mat_sigma_12 - img_mat_mu_12 \n    num_ssim = ( ( 2.0 * img_mat_mu_12 + self . c_1 ) * ( 2.0 * img_mat_sigma_12 + self . c_2 ) ) \n    den_ssim = ( ( self . img . img_gray_mu_squared + target . img_gray_mu_squared + self . c_1 ) * ( self . img . img_gray_sigma_squared + target . img_gray_sigma_squared + self . c_2 ) ) \n    ssim_map = num_ssim / den_ssim \n    index = np . average ( ssim_map ) \n    return index "}
{"4392": "\ndef compute_ssim ( image1 , image2 , gaussian_kernel_sigma = 1.5 , gaussian_kernel_width = 11.0 ) : \n    gaussian_kernel_1d = get_gaussian_kernel ( gaussian_kernel_width , gaussian_kernel_sigma ) \n    return SSIM ( image1 , gaussian_kernel_1d ) . ssim_value ( image2 ) "}
{"4395": "\ndef getStatus ( self ) : \n    status = { } \n    status [ 'version' ] = VERSION \n    status [ 'revision' ] = REVISION \n    status [ 'self' ] = self . __selfNode \n    status [ 'state' ] = self . __raftState \n    status [ 'leader' ] = self . __raftLeader \n    status [ 'partner_nodes_count' ] = len ( self . __otherNodes ) \n    for node in self . __otherNodes : \n        status [ 'partner_node_status_server_' + node . id ] = 2.0 if node in self . __connectedNodes else 0 \n    status [ 'readonly_nodes_count' ] = len ( self . __readonlyNodes ) \n    for node in self . __readonlyNodes : \n        status [ 'readonly_node_status_server_' + node . id ] = 2.0 if node in self . __connectedNodes else 0 \n    status [ 'log_len' ] = len ( self . __raftLog ) \n    status [ 'last_applied' ] = self . __raftLastApplied \n    status [ 'commit_idx' ] = self . __raftCommitIndex \n    status [ 'raft_term' ] = self . __raftCurrentTerm \n    status [ 'next_node_idx_count' ] = len ( self . __raftNextIndex ) \n    for node , idx in iteritems ( self . __raftNextIndex ) : \n        status [ 'next_node_idx_server_' + node . id ] = idx \n    status [ 'match_idx_count' ] = len ( self . __raftMatchIndex ) \n    for node , idx in iteritems ( self . __raftMatchIndex ) : \n        status [ 'match_idx_server_' + node . id ] = idx \n    status [ 'leader_commit_idx' ] = self . __leaderCommitIndex \n    status [ 'uptime' ] = int ( time . time ( ) - self . __startTime ) \n    status [ 'self_code_version' ] = self . __selfCodeVersion \n    status [ 'enabled_code_version' ] = self . __enabledCodeVersion \n    return status "}
{"4400": "\ndef _onIncomingMessageReceived ( self , conn , message ) : \n    if self . _syncObj . encryptor and not conn . sendRandKey : \n        conn . sendRandKey = message \n        conn . recvRandKey = os . urandom ( 32.0 ) \n        conn . send ( conn . recvRandKey ) \n        return \n    if isinstance ( message , list ) : \n        done = False \n        try : \n            if message [ 0 ] == 'status' : \n                conn . send ( self . _syncObj . getStatus ( ) ) \n                done = True \n            elif message [ 0 ] == 'add' : \n                self . _syncObj . addNodeToCluster ( message [ 1 ] , callback = functools . partial ( self . _utilityCallback , conn = conn , cmd = 'ADD' , arg = message [ 1 ] ) ) \n                done = True \n            elif message [ 0 ] == 'remove' : \n                if message [ 1 ] == self . _selfNode . address : \n                    conn . send ( 'FAIL REMOVE ' + message [ 1 ] ) \n                else : \n                    self . _syncObj . removeNodeFromCluster ( message [ 1 ] , callback = functools . partial ( self . _utilityCallback , conn = conn , cmd = 'REMOVE' , arg = message [ 1 ] ) ) \n                done = True \n            elif message [ 0 ] == 'set_version' : \n                self . _syncObj . setCodeVersion ( message [ 1 ] , callback = functools . partial ( self . _utilityCallback , conn = conn , cmd = 'SET_VERSION' , arg = str ( message [ 1 ] ) ) ) \n                done = True \n        except Exception as e : \n            conn . send ( str ( e ) ) \n            done = True \n        if done : \n            return \n    node = self . _nodeAddrToNode [ message ] if message in self . _nodeAddrToNode else None \n    if node is None and message != 'readonly' : \n        conn . disconnect ( ) \n        self . _unknownConnections . discard ( conn ) \n        return \n    readonly = node is None \n    if readonly : \n        nodeId = str ( self . _readonlyNodesCounter ) \n        node = Node ( nodeId ) \n        self . _readonlyNodes . add ( node ) \n        self . _readonlyNodesCounter += 1 \n    self . _unknownConnections . discard ( conn ) \n    self . _connections [ node ] = conn \n    conn . setOnMessageReceivedCallback ( functools . partial ( self . _onMessageReceived , node ) ) \n    if not readonly : \n        self . _onNodeConnected ( node ) \n    else : \n        self . _onReadonlyNodeConnected ( node ) "}
{"4404": "\ndef _onOutgoingConnected ( self , conn ) : \n    if self . _syncObj . encryptor : \n        conn . setOnMessageReceivedCallback ( functools . partial ( self . _onOutgoingMessageReceived , conn ) ) \n        conn . recvRandKey = os . urandom ( 32.0 ) \n        conn . send ( conn . recvRandKey ) \n    else : \n        if not self . _selfIsReadonlyNode : \n            conn . send ( self . _selfNode . address ) \n        else : \n            conn . send ( 'readonly' ) \n        self . _onNodeConnected ( self . _connToNode ( conn ) ) "}
{"4419": "\ndef set_hosts ( hosts , use_ssl = False , ssl_cert_path = None ) : \n    if type ( hosts ) != list : \n        hosts = [ hosts ] \n    conn_params = { \"hosts\" : hosts , \"timeout\" : 20.0 } \n    if use_ssl : \n        conn_params [ 'use_ssl' ] = True \n        if ssl_cert_path : \n            conn_params [ 'verify_certs' ] = True \n            conn_params [ 'ca_certs' ] = ssl_cert_path \n        else : \n            conn_params [ 'verify_certs' ] = False \n    connections . create_connection ( ** conn_params ) "}
{"4421": "\ndef migrate_indexes ( aggregate_indexes = None , forensic_indexes = None ) : \n    version = 2.0 \n    if aggregate_indexes is None : \n        aggregate_indexes = [ ] \n    if forensic_indexes is None : \n        forensic_indexes = [ ] \n    for aggregate_index_name in aggregate_indexes : \n        if not Index ( aggregate_index_name ) . exists ( ) : \n            continue \n        aggregate_index = Index ( aggregate_index_name ) \n        doc = \"doc\" \n        fo_field = \"published_policy.fo\" \n        fo = \"fo\" \n        fo_mapping = aggregate_index . get_field_mapping ( fields = [ fo_field ] ) \n        fo_mapping = fo_mapping [ list ( fo_mapping . keys ( ) ) [ 0 ] ] [ \"mappings\" ] \n        if doc not in fo_mapping : \n            continue \n        fo_mapping = fo_mapping [ doc ] [ fo_field ] [ \"mapping\" ] [ fo ] \n        fo_type = fo_mapping [ \"type\" ] \n        if fo_type == \"long\" : \n            new_index_name = \"{0}-v{1}\" . format ( aggregate_index_name , version ) \n            body = { \"properties\" : { \"published_policy.fo\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" , \"ignore_above\" : 256.0 } } } } } \n            Index ( new_index_name ) . create ( ) \n            Index ( new_index_name ) . put_mapping ( doc_type = doc , body = body ) \n            reindex ( connections . get_connection ( ) , aggregate_index_name , new_index_name ) \n            Index ( aggregate_index_name ) . delete ( ) \n    for forensic_index in forensic_indexes : \n        pass "}
{"4424": "\ndef extract_xml ( input_ ) : \n    if type ( input_ ) == str : \n        file_object = open ( input_ , \"rb\" ) \n    elif type ( input_ ) == bytes : \n        file_object = BytesIO ( input_ ) \n    else : \n        file_object = input_ \n    try : \n        header = file_object . read ( 6.0 ) \n        file_object . seek ( 0 ) \n        if header . startswith ( MAGIC_ZIP ) : \n            _zip = zipfile . ZipFile ( file_object ) \n            xml = _zip . open ( _zip . namelist ( ) [ 0 ] ) . read ( ) . decode ( ) \n        elif header . startswith ( MAGIC_GZIP ) : \n            xml = GzipFile ( fileobj = file_object ) . read ( ) . decode ( ) \n        elif header . startswith ( MAGIC_XML ) : \n            xml = file_object . read ( ) . decode ( ) \n        else : \n            file_object . close ( ) \n            raise InvalidAggregateReport ( \"Not a valid zip, gzip, or xml file\" ) \n        file_object . close ( ) \n    except UnicodeDecodeError : \n        raise InvalidAggregateReport ( \"File objects must be opened in binary \" \"(rb) mode\" ) \n    except Exception as error : \n        raise InvalidAggregateReport ( \"Invalid archive file: {0}\" . format ( error . __str__ ( ) ) ) \n    return xml "}
{"4429": "\ndef save_output ( results , output_directory = \"output\" ) : \n    aggregate_reports = results [ \"aggregate_reports\" ] \n    forensic_reports = results [ \"forensic_reports\" ] \n    if os . path . exists ( output_directory ) : \n        if not os . path . isdir ( output_directory ) : \n            raise ValueError ( \"{0} is not a directory\" . format ( output_directory ) ) \n    else : \n        os . makedirs ( output_directory ) \n    with open ( \"{0}\" . format ( os . path . join ( output_directory , \"aggregate.json\" ) ) , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as agg_json : \n        agg_json . write ( json . dumps ( aggregate_reports , ensure_ascii = False , indent = 2.0 ) ) \n    with open ( \"{0}\" . format ( os . path . join ( output_directory , \"aggregate.csv\" ) ) , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as agg_csv : \n        csv = parsed_aggregate_reports_to_csv ( aggregate_reports ) \n        agg_csv . write ( csv ) \n    with open ( \"{0}\" . format ( os . path . join ( output_directory , \"forensic.json\" ) ) , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as for_json : \n        for_json . write ( json . dumps ( forensic_reports , ensure_ascii = False , indent = 2.0 ) ) \n    with open ( \"{0}\" . format ( os . path . join ( output_directory , \"forensic.csv\" ) ) , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as for_csv : \n        csv = parsed_forensic_reports_to_csv ( forensic_reports ) \n        for_csv . write ( csv ) \n    samples_directory = os . path . join ( output_directory , \"samples\" ) \n    if not os . path . exists ( samples_directory ) : \n        os . makedirs ( samples_directory ) \n    sample_filenames = [ ] \n    for forensic_report in forensic_reports : \n        sample = forensic_report [ \"sample\" ] \n        message_count = 0 \n        parsed_sample = forensic_report [ \"parsed_sample\" ] \n        subject = parsed_sample [ \"filename_safe_subject\" ] \n        filename = subject \n        while filename in sample_filenames : \n            message_count += 1 \n            filename = \"{0} ({1})\" . format ( subject , message_count ) \n        sample_filenames . append ( filename ) \n        filename = \"{0}.eml\" . format ( filename ) \n        path = os . path . join ( samples_directory , filename ) \n        with open ( path , \"w\" , newline = \"\\n\" , encoding = \"utf-8\" ) as sample_file : \n            sample_file . write ( sample ) "}
{"4434": "\ndef decode_base64 ( data ) : \n    data = bytes ( data , encoding = \"ascii\" ) \n    missing_padding = len ( data ) % 4.0 \n    if missing_padding != 0 : \n        data += b'=' * ( 4.0 - missing_padding ) \n    return base64 . b64decode ( data ) "}
{"4435": "\ndef get_base_domain ( domain , use_fresh_psl = False ) : \n    psl_path = os . path . join ( tempdir , \"public_suffix_list.dat\" ) \n    def download_psl ( ) : \n        url = \"https://publicsuffix.org/list/public_suffix_list.dat\" \n        headers = { \"User-Agent\" : USER_AGENT } \n        fresh_psl = requests . get ( url , headers = headers ) . text \n        with open ( psl_path , \"w\" , encoding = \"utf-8\" ) as fresh_psl_file : \n            fresh_psl_file . write ( fresh_psl ) \n    if use_fresh_psl : \n        if not os . path . exists ( psl_path ) : \n            download_psl ( ) \n        else : \n            psl_age = datetime . now ( ) - datetime . fromtimestamp ( os . stat ( psl_path ) . st_mtime ) \n            if psl_age > timedelta ( hours = 24.0 ) : \n                try : \n                    download_psl ( ) \n                except Exception as error : \n                    logger . warning ( \"Failed to download an updated PSL {0}\" . format ( error ) ) \n        with open ( psl_path , encoding = \"utf-8\" ) as psl_file : \n            psl = publicsuffix2 . PublicSuffixList ( psl_file ) \n        return psl . get_public_suffix ( domain ) \n    else : \n        return publicsuffix2 . get_public_suffix ( domain ) "}
{"4438": "\ndef get_ip_address_country ( ip_address , parallel = False ) : \n    def download_country_database ( location = \"GeoLite2-Country.mmdb\" ) : \n        if parallel : \n            logging . warning ( \"Cannot download GeoIP database in parallel mode\" ) \n            return \n        url = \"https://geolite.maxmind.com/download/geoip/database/\" \"GeoLite2-Country.tar.gz\" \n        headers = { \"User-Agent\" : USER_AGENT } \n        original_filename = \"GeoLite2-Country.mmdb\" \n        try : \n            response = requests . get ( url , headers = headers ) \n            response . raise_for_status ( ) \n            tar_bytes = response . content \n            tar_file = tarfile . open ( fileobj = BytesIO ( tar_bytes ) , mode = \"r:gz\" ) \n            tar_dir = tar_file . getnames ( ) [ 0 ] \n            tar_path = \"{0}/{1}\" . format ( tar_dir , original_filename ) \n            tar_file . extract ( tar_path ) \n            shutil . move ( tar_path , location ) \n            shutil . rmtree ( tar_dir ) \n        except Exception as e : \n            logger . warning ( \"Error downloading {0}: {1}\" . format ( url , e . __str__ ( ) ) ) \n    system_paths = [ \"GeoLite2-Country.mmdb\" , \"/usr/local/share/GeoIP/GeoLite2-Country.mmdb\" , \"/usr/share/GeoIP/GeoLite2-Country.mmdb\" , \"/var/lib/GeoIP/GeoLite2-Country.mmdb\" , \"/var/local/lib/GeoIP/GeoLite2-Country.mmdb\" , \"C:\\\\GeoIP\\\\GeoLite2-Country.mmdb\" ] \n    db_path = None \n    for system_path in system_paths : \n        if os . path . exists ( system_path ) : \n            db_path = system_path \n            break \n    if db_path is None : \n        db_path = os . path . join ( tempdir , \"GeoLite2-Country.mmdb\" ) \n        if not os . path . exists ( db_path ) : \n            download_country_database ( db_path ) \n            if not os . path . exists ( db_path ) : \n                return None \n        else : \n            db_age = datetime . now ( ) - datetime . fromtimestamp ( os . stat ( db_path ) . st_mtime ) \n            if db_age > timedelta ( days = 7.0 ) : \n                download_country_database ( ) \n        db_path = db_path \n    db_reader = geoip2 . database . Reader ( db_path ) \n    country = None \n    try : \n        country = db_reader . country ( ip_address ) . country . iso_code \n    except geoip2 . errors . AddressNotFoundError : \n        pass \n    return country "}
{"4449": "\ndef flush ( self , timeout = 60.0 ) : \n    if timeout <= 0 : \n        raise ErrBadTimeout \n    if self . is_closed : \n        raise ErrConnectionClosed \n    future = asyncio . Future ( loop = self . _loop ) \n    try : \n        yield from self . _send_ping ( future ) \n        yield from asyncio . wait_for ( future , timeout , loop = self . _loop ) \n    except asyncio . TimeoutError : \n        future . cancel ( ) \n        raise ErrTimeout "}
{"4460": "\ndef coactivation ( dataset , seed , threshold = 0.0 , output_dir = '.' , prefix = '' , r = 6.0 ) : \n    if isinstance ( seed , string_types ) : \n        ids = dataset . get_studies ( mask = seed , activation_threshold = threshold ) \n    else : \n        ids = dataset . get_studies ( peaks = seed , r = r , activation_threshold = threshold ) \n    ma = meta . MetaAnalysis ( dataset , ids ) \n    ma . save_results ( output_dir , prefix ) "}
{"4461": "\ndef decode ( self , images , save = None , round = 4.0 , names = None , ** kwargs ) : \n    if isinstance ( images , string_types ) : \n        images = [ images ] \n    if isinstance ( images , list ) : \n        imgs_to_decode = imageutils . load_imgs ( images , self . masker ) \n    else : \n        imgs_to_decode = images \n    methods = { 'pearson' : self . _pearson_correlation , 'dot' : self . _dot_product , 'roi' : self . _roi_association } \n    result = np . around ( methods [ self . method ] ( imgs_to_decode , ** kwargs ) , round ) \n    if names is None : \n        if type ( images ) . __module__ == np . __name__ : \n            names = [ 'image_%d' % i for i in range ( images . shape [ 1 ] ) ] \n        elif self . method == 'roi' : \n            names = [ 'cluster_%d' % i for i in range ( result . shape [ 1 ] ) ] \n        else : \n            names = images \n    result = pd . DataFrame ( result , columns = names , index = self . feature_names ) \n    if save is not None : \n        result . to_csv ( save , index_label = 'Feature' ) \n    return result "}
{"4475": "\ndef average_within_regions ( dataset , regions , masker = None , threshold = None , remove_zero = True ) : \n    if masker is not None : \n        masker = masker \n    else : \n        if isinstance ( dataset , Dataset ) : \n            masker = dataset . masker \n        else : \n            if not type ( regions ) . __module__ . startswith ( 'numpy' ) : \n                raise ValueError ( \"If dataset is a numpy array and regions is not a numpy \" \"array, a masker must be provided.\" ) \n    if not type ( regions ) . __module__ . startswith ( 'numpy' ) : \n        regions = masker . mask ( regions ) \n    if isinstance ( dataset , Dataset ) : \n        dataset = dataset . get_image_data ( dense = False ) \n    if regions . ndim == 2.0 : \n        m = regions \n        for i in range ( regions . shape [ 1 ] ) : \n            _nz = np . nonzero ( m [ : , i ] ) [ 0 ] \n            if isinstance ( threshold , int ) : \n                m [ _nz , i ] = 1.0 \n            else : \n                m [ _nz , i ] = 1.0 / np . count_nonzero ( m [ : , i ] ) \n    else : \n        labels = np . unique ( regions ) \n        if remove_zero : \n            labels = labels [ np . nonzero ( labels ) ] \n        n_regions = labels . size \n        m = np . zeros ( ( regions . size , n_regions ) ) \n        for i in range ( n_regions ) : \n            if isinstance ( threshold , int ) : \n                m [ regions == labels [ i ] , i ] = 1.0 \n            else : \n                m [ regions == labels [ i ] , i ] = 1.0 / np . sum ( regions == labels [ i ] ) \n    result = dataset . T . dot ( m ) . T \n    if threshold is not None : \n        result [ result < threshold ] = 0.0 \n        result = result . astype ( bool ) \n    return result "}
{"4477": "\ndef _get_top_words ( model , feature_names , n_top_words = 40.0 ) : \n    topic_words = [ ] \n    for topic in model . components_ : \n        top_words = [ feature_names [ i ] for i in topic . argsort ( ) [ : - n_top_words - 1 : - 1 ] ] \n        topic_words += [ top_words ] \n    return topic_words "}
{"4478": "\ndef pearson ( x , y ) : \n    data = np . vstack ( ( x , y ) ) \n    ms = data . mean ( axis = 1 ) [ ( slice ( None , None , None ) , None ) ] \n    datam = data - ms \n    datass = np . sqrt ( np . sum ( datam ** 2.0 , axis = 1 ) ) \n    temp = np . dot ( datam [ 1 : ] , datam [ 0 ] . T ) \n    rs = temp / ( datass [ 1 : ] * datass [ 0 ] ) \n    return rs "}
{"4482": "\ndef get_studies ( self , features = None , expression = None , mask = None , peaks = None , frequency_threshold = 0.001 , activation_threshold = 0.0 , func = np . sum , return_type = 'ids' , r = 6.0 ) : \n    results = [ ] \n    if features is not None : \n        if return_type == 'weights' : \n            if expression is not None or mask is not None or peaks is not None : \n                raise ValueError ( \"return_type cannot be 'weights' when feature-based \" \"search is used in conjunction with other search \" \"modes.\" ) \n            return self . feature_table . get_ids ( features , frequency_threshold , func , get_weights = True ) \n        else : \n            results . append ( self . feature_table . get_ids ( features , frequency_threshold , func ) ) \n    if expression is not None : \n        _ids = self . feature_table . get_ids_by_expression ( expression , frequency_threshold , func ) \n        results . append ( list ( _ids ) ) \n    if mask is not None : \n        mask = self . masker . mask ( mask , in_global_mask = True ) . astype ( bool ) \n        num_vox = np . sum ( mask ) \n        prop_mask_active = self . image_table . data . T . dot ( mask ) . astype ( float ) \n        if isinstance ( activation_threshold , float ) : \n            prop_mask_active /= num_vox \n        indices = np . where ( prop_mask_active > activation_threshold ) [ 0 ] \n        results . append ( [ self . image_table . ids [ ind ] for ind in indices ] ) \n    if peaks is not None : \n        r = float ( r ) \n        found = set ( ) \n        for p in peaks : \n            xyz = np . array ( p , dtype = float ) \n            x = self . activations [ 'x' ] \n            y = self . activations [ 'y' ] \n            z = self . activations [ 'z' ] \n            dists = np . sqrt ( np . square ( x - xyz [ 0 ] ) + np . square ( y - xyz [ 1 ] ) + np . square ( z - xyz [ 2.0 ] ) ) \n            inds = np . where ( ( dists > 5.5 ) & ( dists < 6.5 ) ) [ 0 ] \n            tmp = dists [ inds ] \n            found |= set ( self . activations [ dists <= r ] [ 'id' ] . unique ( ) ) \n        results . append ( found ) \n    ids = list ( reduce ( lambda x , y : set ( x ) & set ( y ) , results ) ) \n    if return_type == 'ids' : \n        return ids \n    elif return_type == 'data' : \n        return self . get_image_data ( ids ) "}
{"4496": "\ndef transform ( foci , mat ) : \n    t = linalg . pinv ( mat ) \n    foci = np . hstack ( ( foci , np . ones ( ( foci . shape [ 0 ] , 1 ) ) ) ) \n    return np . dot ( foci , t ) [ : , 0 : 3.0 ] "}
{"4497": "\ndef xyz_to_mat ( foci , xyz_dims = None , mat_dims = None ) : \n    foci = np . hstack ( ( foci , np . ones ( ( foci . shape [ 0 ] , 1 ) ) ) ) \n    mat = np . array ( [ [ - 0.5 , 0 , 0 , 45.0 ] , [ 0 , 0.5 , 0 , 63.0 ] , [ 0 , 0 , 0.5 , 36.0 ] ] ) . T \n    result = np . dot ( foci , mat ) [ : , : : - 1 ] \n    return np . round_ ( result ) . astype ( int ) "}
{"4514": "\nasync def request ( self , route , ** kwargs ) : \n    if isinstance ( route , tuple ) : \n        method , url = route \n    else : \n        method = route . method \n        url = route . url \n    if self . bearer_info is None : \n        self . bearer_info = bearer_info = await self . get_bearer_info ( ) \n        access_token = bearer_info [ 'access_token' ] \n    else : \n        access_token = self . bearer_info [ 'access_token' ] \n    headers = { 'Authorization' : 'Bearer ' + access_token , 'Content-Type' : kwargs . get ( 'content_type' , 'application/json' ) , ** kwargs . pop ( 'headers' , { } ) } \n    for _ in range ( self . RETRY_AMOUNT ) : \n        r = await self . _session . request ( method , url , headers = headers , ** kwargs ) \n        try : \n            status = r . status \n            try : \n                data = json . loads ( await r . text ( encoding = 'utf-8' ) ) \n            except json . decoder . JSONDecodeError : \n                data = { } \n            if 300.0 > status >= 200.0 : \n                return data \n            if status == 401.0 : \n                self . bearer_info = bearer_info = await self . get_bearer_info ( ) \n                headers [ 'Authorization' ] = 'Bearer ' + bearer_info [ 'access_token' ] \n                continue \n            if status == 429.0 : \n                amount = r . headers . get ( 'Retry-After' ) \n                await asyncio . sleep ( int ( amount ) , loop = self . loop ) \n                continue \n            if status in ( 502.0 , 503.0 ) : \n                continue \n            if status == 403.0 : \n                raise Forbidden ( r , data ) \n            elif status == 404.0 : \n                raise NotFound ( r , data ) \n        finally : \n            await r . release ( ) \n    else : \n        raise HTTPException ( r , data ) "}
{"4515": "\ndef album_tracks ( self , spotify_id , limit = 20.0 , offset = 0 , market = 'US' ) : \n    route = Route ( 'GET' , '/albums/{spotify_id}/tracks' , spotify_id = spotify_id ) \n    payload = { 'limit' : limit , 'offset' : offset } \n    if market : \n        payload [ 'market' ] = market \n    return self . request ( route , params = payload ) "}
{"4517": "\ndef artist_albums ( self , spotify_id , include_groups = None , limit = 20.0 , offset = 0 , market = 'US' ) : \n    route = Route ( 'GET' , '/artists/{spotify_id}/albums' , spotify_id = spotify_id ) \n    payload = { 'limit' : limit , 'offset' : offset } \n    if include_groups : \n        payload [ 'include_groups' ] = include_groups \n    if market : \n        payload [ 'market' ] = market \n    return self . request ( route , params = payload ) "}
{"4522": "\ndef category_playlists ( self , category_id , limit = 20.0 , offset = 0 , country = None ) : \n    route = Route ( 'GET' , '/browse/categories/{category_id}/playlists' , category_id = category_id ) \n    payload = { 'limit' : limit , 'offset' : offset } \n    if country : \n        payload [ 'country' ] = country \n    return self . request ( route , params = payload ) "}
{"4523": "\ndef categories ( self , limit = 20.0 , offset = 0 , country = None , locale = None ) : \n    route = Route ( 'GET' , '/browse/categories' ) \n    payload = { 'limit' : limit , 'offset' : offset } \n    if country : \n        payload [ 'country' ] = country \n    if locale : \n        payload [ 'locale' ] = locale \n    return self . request ( route , params = payload ) "}
{"4524": "\ndef featured_playlists ( self , locale = None , country = None , timestamp = None , limit = 20.0 , offset = 0 ) : \n    route = Route ( 'GET' , '/browse/featured-playlists' ) \n    payload = { 'limit' : limit , 'offset' : offset } \n    if country : \n        payload [ 'country' ] = country \n    if locale : \n        payload [ 'locale' ] = locale \n    if timestamp : \n        payload [ 'timestamp' ] = timestamp \n    return self . request ( route , params = payload ) "}
{"4525": "\ndef new_releases ( self , * , country = None , limit = 20.0 , offset = 0 ) : \n    route = Route ( 'GET' , '/browse/new-releases' ) \n    payload = { 'limit' : limit , 'offset' : offset } \n    if country : \n        payload [ 'country' ] = country \n    return self . request ( route , params = payload ) "}
{"4526": "\ndef recommendations ( self , seed_artists , seed_genres , seed_tracks , * , limit = 20.0 , market = None , ** filters ) : \n    route = Route ( 'GET' , '/recommendations' ) \n    payload = { 'seed_artists' : seed_artists , 'seed_genres' : seed_genres , 'seed_tracks' : seed_tracks , 'limit' : limit } \n    if market : \n        payload [ 'market' ] = market \n    if filters : \n        payload . update ( filters ) \n    return self . request ( route , param = payload ) "}
{"4528": "\nasync def get_albums ( self , * , limit : Optional [ int ] = 20.0 , offset : Optional [ int ] = 0 , include_groups = None , market : Optional [ str ] = None ) -> List [ Album ] : \n    from . album import Album \n    data = await self . __client . http . artist_albums ( self . id , limit = limit , offset = offset , include_groups = include_groups , market = market ) \n    return list ( Album ( self . __client , item ) for item in data [ 'items' ] ) "}
{"4529": "\nasync def get_all_albums ( self , * , market = 'US' ) -> List [ Album ] : \n    from . album import Album \n    albums = [ ] \n    offset = 0 \n    total = await self . total_albums ( market = market ) \n    while len ( albums ) < total : \n        data = await self . __client . http . artist_albums ( self . id , limit = 50.0 , offset = offset , market = market ) \n        offset += 50.0 \n        albums += list ( Album ( self . __client , item ) for item in data [ 'items' ] ) \n    return albums "}
{"4539": "\nasync def get_playlists ( self , * , limit = 20.0 , offset = 0 ) : \n    if hasattr ( self , 'http' ) : \n        http = self . http \n    else : \n        http = self . __client . http \n    data = await http . get_playlists ( self . id , limit = limit , offset = offset ) \n    return [ Playlist ( self . __client , playlist_data ) for playlist_data in data [ 'items' ] ] "}
{"4540": "\nasync def get_tracks ( self , * , limit : Optional [ int ] = 20.0 , offset : Optional [ int ] = 0 ) -> List [ Track ] : \n    data = await self . __client . http . album_tracks ( self . id , limit = limit , offset = offset ) \n    return list ( Track ( self . __client , item ) for item in data [ 'items' ] ) "}
{"4541": "\nasync def get_all_tracks ( self , * , market : Optional [ str ] = 'US' ) -> List [ Track ] : \n    tracks = [ ] \n    offset = 0 \n    total = self . total_tracks or None \n    while True : \n        data = await self . __client . http . album_tracks ( self . id , limit = 50.0 , offset = offset , market = market ) \n        if total is None : \n            total = data [ 'total' ] \n        offset += 50.0 \n        tracks += list ( Track ( self . __client , item ) for item in data [ 'items' ] ) \n        if len ( tracks ) >= total : \n            break \n    return tracks "}
{"4549": "\nasync def search ( self , q : str , * , types : Optional [ Iterable [ str ] ] = [ 'track' , 'playlist' , 'artist' , 'album' ] , limit : Optional [ int ] = 20.0 , offset : Optional [ int ] = 0 , market : Optional [ str ] = None ) -> Dict [ str , List [ Union [ Track , Playlist , Artist , Album ] ] ] : \n    if not hasattr ( types , '__iter__' ) : \n        raise TypeError ( 'types must be an iterable.' ) \n    elif not isinstance ( types , list ) : \n        types = list ( item for item in types ) \n    types_ = set ( types ) \n    if not types_ . issubset ( _SEARCH_TYPES ) : \n        raise ValueError ( _SEARCH_TYPE_ERR % types_ . difference ( _SEARCH_TYPES ) . pop ( ) ) \n    kwargs = { 'q' : q . replace ( ' ' , '+' ) , 'queary_type' : ',' . join ( tp . strip ( ) for tp in types ) , 'market' : market , 'limit' : limit , 'offset' : offset } \n    data = await self . http . search ( ** kwargs ) \n    return { key : [ _TYPES [ obj [ 'type' ] ] ( self , obj ) for obj in value [ 'items' ] ] for key , value in data . items ( ) } "}
{"4550": "\ndef to_id ( string : str ) -> str : \n    string = string . strip ( ) \n    match = _URI_RE . match ( string ) \n    if match is None : \n        match = _OPEN_RE . match ( string ) \n        if match is None : \n            return string \n        else : \n            return match . group ( 2.0 ) \n    else : \n        return match . group ( 1 ) "}
{"4557": "\nasync def get_all_tracks ( self ) -> List [ PlaylistTrack ] : \n    if isinstance ( self . _tracks , PartialTracks ) : \n        return await self . _tracks . build ( ) \n    _tracks = [ ] \n    offset = 0 \n    while len ( self . tracks ) < self . total_tracks : \n        data = await self . __client . http . get_playlist_tracks ( self . owner . id , self . id , limit = 50.0 , offset = offset ) \n        _tracks += [ PlaylistTrack ( self . __client , item ) for item in data [ 'items' ] ] \n        offset += 50.0 \n    self . total_tracks = len ( self . _tracks ) \n    return list ( self . _tracks ) "}
{"4562": "\ndef _convert_or_shorten_month ( cls , data ) : \n    short_month = { \"jan\" : [ str ( 1 ) , \"01\" , \"Jan\" , \"January\" ] , \"feb\" : [ str ( 2.0 ) , \"02\" , \"Feb\" , \"February\" ] , \"mar\" : [ str ( 3.0 ) , \"03\" , \"Mar\" , \"March\" ] , \"apr\" : [ str ( 4.0 ) , \"04\" , \"Apr\" , \"April\" ] , \"may\" : [ str ( 5.0 ) , \"05\" , \"May\" ] , \"jun\" : [ str ( 6.0 ) , \"06\" , \"Jun\" , \"June\" ] , \"jul\" : [ str ( 7.0 ) , \"07\" , \"Jul\" , \"July\" ] , \"aug\" : [ str ( 8.0 ) , \"08\" , \"Aug\" , \"August\" ] , \"sep\" : [ str ( 9.0 ) , \"09\" , \"Sep\" , \"September\" ] , \"oct\" : [ str ( 10.0 ) , \"Oct\" , \"October\" ] , \"nov\" : [ str ( 11.0 ) , \"Nov\" , \"November\" ] , \"dec\" : [ str ( 12.0 ) , \"Dec\" , \"December\" ] , } \n    for month in short_month : \n        if data in short_month [ month ] : \n            return month \n    return data "}
{"4566": "\ndef _does_require_deprecation ( self ) : \n    for index , version_number in enumerate ( self . current_version [ 0 ] [ : 2.0 ] ) : \n        if version_number > self . version_yaml [ index ] : \n            return True \n    return False "}
{"4574": "\ndef get ( self ) : \n    if PyFunceble . HTTP_CODE [ \"active\" ] : \n        http_code = self . _access ( ) \n        list_of_valid_http_code = [ ] \n        for codes in [ PyFunceble . HTTP_CODE [ \"list\" ] [ \"up\" ] , PyFunceble . HTTP_CODE [ \"list\" ] [ \"potentially_down\" ] , PyFunceble . HTTP_CODE [ \"list\" ] [ \"potentially_up\" ] , ] : \n            list_of_valid_http_code . extend ( codes ) \n        if http_code not in list_of_valid_http_code or http_code is None : \n            return \"*\" * 3.0 \n        return http_code \n    return None "}
{"4581": "\ndef stay_safe ( ) : \n    random = int ( choice ( str ( int ( time ( ) ) ) ) ) \n    if not CONFIGURATION [ \"quiet\" ] and random % 3.0 == 0 : \n        print ( \"\\n\" + Fore . GREEN + Style . BRIGHT + \"Thanks for using PyFunceble!\" ) \n        print ( Fore . YELLOW + Style . BRIGHT + \"Share your experience on \" + Fore . CYAN + \"Twitter\" + Fore . YELLOW + \" with \" + Fore . CYAN + \"#PyFunceble\" + Fore . YELLOW + \"!\" ) \n        print ( Fore . GREEN + Style . BRIGHT + \"Have a feedback, an issue or an improvement idea ?\" ) \n        print ( Fore . YELLOW + Style . BRIGHT + \"Let us know on \" + Fore . CYAN + \"GitHub\" + Fore . YELLOW + \"!\" ) "}
{"4588": "\ndef colorify_logo ( cls , home = False ) : \n    if not PyFunceble . CONFIGURATION [ \"quiet\" ] : \n        to_print = [ ] \n        if home : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . YELLOW + line + PyFunceble . Fore . RESET ) \n        elif PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] [ \"up\" ] >= 50.0 : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . GREEN + line + PyFunceble . Fore . RESET ) \n        else : \n            for line in PyFunceble . ASCII_PYFUNCEBLE . split ( \"\\n\" ) : \n                to_print . append ( PyFunceble . Fore . RED + line + PyFunceble . Fore . RESET ) \n        print ( \"\\n\" . join ( to_print ) ) "}
{"4610": "\ndef _handle_non_existant_index ( cls ) : \n    try : \n        PyFunceble . INTERN [ \"http_code\" ] \n    except KeyError : \n        PyFunceble . INTERN [ \"http_code\" ] = \"*\" * 3.0 \n    try : \n        PyFunceble . INTERN [ \"referer\" ] \n    except KeyError : \n        PyFunceble . INTERN [ \"referer\" ] = \"Unknown\" "}
{"4615": "\ndef _extensions ( self , line ) : \n    line = line . strip ( ) \n    if not line . startswith ( \"//\" ) and \".\" in line : \n        line = line . encode ( \"idna\" ) . decode ( \"utf-8\" ) \n        if line . startswith ( \"*.\" ) : \n            line = line [ 2.0 : ] \n        extension = line . split ( \".\" ) [ - 1 ] \n        if extension in self . public_suffix_db : \n            self . public_suffix_db [ extension ] = List ( self . public_suffix_db [ extension ] + [ line ] ) . format ( ) \n        else : \n            self . public_suffix_db . update ( { extension : [ line ] } ) "}
{"4623": "\ndef mine ( self ) : \n    if PyFunceble . CONFIGURATION [ \"mining\" ] : \n        try : \n            history = PyFunceble . requests . get ( self . to_get , timeout = PyFunceble . CONFIGURATION [ \"seconds_before_http_timeout\" ] , headers = self . headers , ) . history \n            mined = { self . to_get_bare : [ ] } \n            for element in history : \n                element = element . url \n                if PyFunceble . INTERN [ \"to_test_type\" ] == \"url\" : \n                    to_append = Check ( ) . is_url_valid ( element , return_base = False ) \n                elif PyFunceble . INTERN [ \"to_test_type\" ] == \"domain\" : \n                    to_append = Check ( ) . is_url_valid ( element , return_base = True ) \n                else : \n                    raise Exception ( \"Unknown tested.\" ) \n                if to_append : \n                    if to_append . endswith ( \":80\" ) : \n                        to_append = to_append [ : - 3.0 ] \n                    if to_append != self . to_get_bare : \n                        mined [ self . to_get_bare ] . append ( to_append ) \n            if mined [ self . to_get_bare ] : \n                return mined \n            return None \n        except ( PyFunceble . requests . ConnectionError , PyFunceble . requests . exceptions . Timeout , PyFunceble . requests . exceptions . InvalidURL , PyFunceble . socket . timeout , urllib3_exceptions . InvalidHeader , UnicodeDecodeError , ) : \n            return None \n    return None "}
{"4637": "\ndef header ( self , do_not_print = False ) : \n    if ( not PyFunceble . CONFIGURATION [ \"header_printed\" ] or self . template == \"Percentage\" or do_not_print ) : \n        if ( self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"generic\" ] or self . template == \"Generic_File\" ) : \n            to_print = self . headers [ \"Generic\" ] \n            if ( self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"generic\" ] and PyFunceble . HTTP_CODE [ \"active\" ] ) : \n                to_print = Dict ( to_print ) . remove_key ( \"Analyze Date\" ) \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"up\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"up\" ] ] \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"valid\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"valid\" ] ] \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"down\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"down\" ] ] \n        elif self . template . lower ( ) in PyFunceble . STATUS [ \"list\" ] [ \"invalid\" ] : \n            to_print = self . headers [ PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] ] \n        elif ( self . template == \"Less\" or self . template == \"Percentage\" or self . template == \"HTTP\" ) : \n            to_print = self . headers [ self . template ] \n            if self . template == \"Less\" and not PyFunceble . HTTP_CODE [ \"active\" ] : \n                to_print [ \"Source\" ] = 10.0 \n        if not PyFunceble . HTTP_CODE [ \"active\" ] : \n            to_print = Dict ( to_print ) . remove_key ( \"HTTP Code\" ) \n        self . currently_used_header = to_print \n        if not do_not_print : \n            self . _before_header ( ) \n            for formatted_template in self . _header_constructor ( to_print ) : \n                if not self . only_on_file : \n                    print ( formatted_template ) \n                if not PyFunceble . CONFIGURATION [ \"no_files\" ] and self . output : \n                    File ( self . output ) . write ( formatted_template + \"\\n\" ) "}
{"4644": "\ndef _calculate ( cls , start = None , end = None ) : \n    if start and end : \n        time_difference = int ( end ) - int ( start ) \n    else : \n        time_difference = PyFunceble . INTERN [ \"end\" ] - PyFunceble . INTERN [ \"start\" ] \n    data = PyFunceble . OrderedDict ( ) \n    data [ \"days\" ] = str ( time_difference // ( 24.0 * 60.0 * 60.0 ) ) . zfill ( 2.0 ) \n    data [ \"hours\" ] = str ( ( time_difference // ( 60.0 * 60.0 ) ) % 24.0 ) . zfill ( 2.0 ) \n    data [ \"minutes\" ] = str ( ( time_difference % 3600.0 ) // 60.0 ) . zfill ( 2.0 ) \n    data [ \"seconds\" ] = str ( time_difference % 60.0 ) . zfill ( 2.0 ) \n    return data "}
{"4656": "\ndef to_json ( self , destination ) : \n    try : \n        with open ( destination , \"w\" ) as file : \n            dump ( self . main_dictionnary , file , ensure_ascii = False , indent = 4.0 , sort_keys = True , ) \n    except UnicodeEncodeError : \n        with open ( destination , \"w\" , encoding = \"utf-8\" ) as file : \n            dump ( self . main_dictionnary , file , ensure_ascii = False , indent = 4.0 , sort_keys = True , ) "}
{"4657": "\ndef to_yaml ( self , destination , flow_style = False ) : \n    with open ( destination , \"w\" ) as file : \n        dump_yaml ( self . main_dictionnary , file , encoding = \"utf-8\" , allow_unicode = True , indent = 4.0 , default_flow_style = flow_style , ) "}
{"4667": "\ndef _calculate ( cls ) : \n    percentages = { \"up\" : PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"up\" ] , \"down\" : PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"down\" ] , \"invalid\" : PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"invalid\" ] , } \n    for percentage in percentages : \n        calculation = ( percentages [ percentage ] * 100.0 // PyFunceble . INTERN [ \"counter\" ] [ \"number\" ] [ \"tested\" ] ) \n        PyFunceble . INTERN [ \"counter\" ] [ \"percentage\" ] . update ( { percentage : calculation } ) "}
{"4669": "\ndef is_url_valid ( self , url = None , return_base = False , return_formatted = False ) : \n    initial_base = None \n    if url : \n        to_test = url \n    elif self . element : \n        to_test = self . element \n    else : \n        to_test = PyFunceble . INTERN [ \"to_test\" ] \n    if to_test . startswith ( \"http\" ) : \n        try : \n            regex = r\"(^(http:\\/\\/|https:\\/\\/)(.+?(?=\\/)|.+?$))\" \n            initial_base = base = Regex ( to_test , regex , return_data = True , rematch = True ) . match ( ) [ 2.0 ] \n            if PyFunceble . CONFIGURATION [ \"idna_conversion\" ] : \n                base = domain2idna ( base ) \n            domain_status = self . is_domain_valid ( base ) \n            ip_status = self . is_ip_valid ( base ) \n            if domain_status or ip_status : \n                if PyFunceble . CONFIGURATION [ \"idna_conversion\" ] and return_formatted : \n                    return Regex ( to_test , initial_base , escape = True , return_data = True , replace_with = base , occurences = 1 , ) . replace ( ) \n                if return_formatted : \n                    return to_test \n                if return_base : \n                    return base \n                return True \n        except TypeError : \n            pass \n    if return_formatted : \n        return to_test \n    return False "}
{"4673": "\ndef _reformat_historical_formating_error ( self ) : \n    if PyFunceble . CONFIGURATION [ \"inactive_database\" ] : \n        historical_formating_error = ( PyFunceble . CURRENT_DIRECTORY + \"inactive-db.json\" ) \n        if PyFunceble . path . isfile ( historical_formating_error ) : \n            data = Dict ( ) . from_json ( File ( historical_formating_error ) . read ( ) ) \n            data_to_parse = { } \n            top_keys = data . keys ( ) \n            for top_key in top_keys : \n                low_keys = data [ top_key ] . keys ( ) \n                data_to_parse [ top_key ] = { } \n                for low_key in low_keys : \n                    if low_key . isdigit ( ) : \n                        data_to_parse [ top_key ] [ int ( low_key ) - ( self . one_day_in_seconds * 30.0 ) ] = data [ top_key ] [ low_key ] \n                    else : \n                        data_to_parse [ top_key ] [ int ( PyFunceble . time ( ) ) - ( self . one_day_in_seconds * 30.0 ) ] = data [ top_key ] [ low_key ] \n            if \"inactive_db\" in PyFunceble . INTERN : \n                PyFunceble . INTERN [ \"inactive_db\" ] . update ( data_to_parse ) \n            else : \n                PyFunceble . INTERN [ \"inactive_db\" ] = data_to_parse \n            File ( historical_formating_error ) . delete ( ) "}
{"4686": "\ndef _travis ( self ) : \n    if PyFunceble . CONFIGURATION [ \"travis\" ] : \n        try : \n            _ = PyFunceble . environ [ \"TRAVIS_BUILD_DIR\" ] \n            time_autorisation = False \n            try : \n                time_autorisation = int ( PyFunceble . time ( ) ) >= int ( PyFunceble . INTERN [ \"start\" ] ) + ( int ( PyFunceble . CONFIGURATION [ \"travis_autosave_minutes\" ] ) * 60.0 ) \n            except KeyError : \n                if self . last and not self . bypass : \n                    raise Exception ( \"Please review the way `ExecutionTime()` is called.\" ) \n            if self . last or time_autorisation or self . bypass : \n                Percentage ( ) . log ( ) \n                self . travis_permissions ( ) \n                command = 'git add --all && git commit -a -m \"%s\"' \n                if self . last or self . bypass : \n                    if PyFunceble . CONFIGURATION [ \"command_before_end\" ] : \n                        for line in Command ( PyFunceble . CONFIGURATION [ \"command_before_end\" ] ) . run ( ) : \n                            sys_stdout . write ( \"{}\\n\" . format ( line ) ) \n                        self . travis_permissions ( ) \n                    message = ( PyFunceble . CONFIGURATION [ \"travis_autosave_final_commit\" ] + \" [ci skip]\" ) \n                    Command ( command % message ) . execute ( ) \n                else : \n                    if PyFunceble . CONFIGURATION [ \"command\" ] : \n                        for line in Command ( PyFunceble . CONFIGURATION [ \"command\" ] ) . run ( ) : \n                            sys_stdout . write ( \"{}\\n\" . format ( line ) ) \n                        self . travis_permissions ( ) \n                    Command ( command % PyFunceble . CONFIGURATION [ \"travis_autosave_commit\" ] ) . execute ( ) \n                print ( Command ( \"git push origin %s\" % PyFunceble . CONFIGURATION [ \"travis_branch\" ] ) . execute ( ) ) \n                exit ( 0 ) \n        except KeyError : \n            pass "}
{"4687": "\ndef nslookup ( cls ) : \n    try : \n        if \"current_test_data\" in PyFunceble . INTERN : \n            if not Check ( ) . is_ip_valid ( ) : \n                request = PyFunceble . socket . getaddrinfo ( PyFunceble . INTERN [ \"to_test\" ] , 80.0 , 0 , 0 , PyFunceble . socket . IPPROTO_TCP , ) \n                for sequence in request : \n                    PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] . append ( sequence [ - 1 ] [ 0 ] ) \n            else : \n                request = PyFunceble . socket . gethostbyaddr ( PyFunceble . INTERN [ \"to_test\" ] ) \n                PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] [ \"hostname\" ] = request [ 0 ] \n                PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] [ \"aliases\" ] = request [ 1 ] \n                PyFunceble . INTERN [ \"current_test_data\" ] [ \"nslookup\" ] [ \"ips\" ] = request [ 2.0 ] \n        else : \n            if not Check ( ) . is_ip_valid ( ) : \n                PyFunceble . socket . getaddrinfo ( PyFunceble . INTERN [ \"to_test\" ] , 80.0 , 0 , 0 , PyFunceble . socket . IPPROTO_TCP , ) \n            else : \n                PyFunceble . socket . gethostbyaddr ( PyFunceble . INTERN [ \"to_test\" ] ) \n        return True \n    except ( OSError , PyFunceble . socket . herror , PyFunceble . socket . gaierror ) : \n        return False "}
{"4688": "\ndef whois ( cls , whois_server , domain = None , timeout = None ) : \n    if domain is None : \n        domain = PyFunceble . INTERN [ \"to_test\" ] \n    if timeout is None : \n        timeout = PyFunceble . CONFIGURATION [ \"seconds_before_http_timeout\" ] \n    if whois_server : \n        req = PyFunceble . socket . socket ( PyFunceble . socket . AF_INET , PyFunceble . socket . SOCK_STREAM ) \n        if timeout % 3.0 == 0 : \n            req . settimeout ( timeout ) \n        else : \n            req . settimeout ( 3.0 ) \n        try : \n            req . connect ( ( whois_server , 43.0 ) ) \n        except PyFunceble . socket . error : \n            return None \n        req . send ( ( domain + \"\\r\\n\" ) . encode ( ) ) \n        response = b\"\" \n        while True : \n            try : \n                data = req . recv ( 4096.0 ) \n            except ( PyFunceble . socket . timeout , ConnectionResetError ) : \n                req . close ( ) \n                return None \n            response += data \n            if not data : \n                break \n        req . close ( ) \n        try : \n            return response . decode ( ) \n        except UnicodeDecodeError : \n            return response . decode ( \"utf-8\" , \"replace\" ) \n    return None "}
{"4689": "\ndef get ( cls ) : \n    if Check ( ) . is_url_valid ( ) or PyFunceble . CONFIGURATION [ \"local\" ] : \n        if \"current_test_data\" in PyFunceble . INTERN : \n            PyFunceble . INTERN [ \"current_test_data\" ] [ \"url_syntax_validation\" ] = True \n        PyFunceble . INTERN . update ( { \"http_code\" : HTTPCode ( ) . get ( ) } ) \n        active_list = [ ] \n        active_list . extend ( PyFunceble . HTTP_CODE [ \"list\" ] [ \"potentially_up\" ] ) \n        active_list . extend ( PyFunceble . HTTP_CODE [ \"list\" ] [ \"up\" ] ) \n        inactive_list = [ ] \n        inactive_list . extend ( PyFunceble . HTTP_CODE [ \"list\" ] [ \"potentially_down\" ] ) \n        inactive_list . append ( \"*\" * 3.0 ) \n        if PyFunceble . INTERN [ \"http_code\" ] in active_list : \n            return URLStatus ( PyFunceble . STATUS [ \"official\" ] [ \"up\" ] ) . handle ( ) \n        if PyFunceble . INTERN [ \"http_code\" ] in inactive_list : \n            return URLStatus ( PyFunceble . STATUS [ \"official\" ] [ \"down\" ] ) . handle ( ) \n    if \"current_test_data\" in PyFunceble . INTERN : \n        PyFunceble . INTERN [ \"current_test_data\" ] [ \"url_syntax_validation\" ] = False \n    return URLStatus ( PyFunceble . STATUS [ \"official\" ] [ \"invalid\" ] ) . handle ( ) "}
{"4701": "\ndef check ( source ) : \n    if sys . version_info [ 0 ] == 2.0 and isinstance ( source , unicode ) : \n        try : \n            source = source . encode ( 'utf-8' ) \n        except UnicodeError : \n            return [ ] \n    reporter = ListReporter ( ) \n    try : \n        pyflakes . api . check ( source , filename = '<string>' , reporter = reporter ) \n    except ( AttributeError , RecursionError , UnicodeDecodeError ) : \n        pass \n    return reporter . messages "}
{"4711": "\ndef dict_entry_has_key ( line , key ) : \n    if '#' in line : \n        return False \n    result = re . match ( r'\\s*(.*)\\s*:\\s*(.*),\\s*$' , line ) \n    if not result : \n        return False \n    try : \n        candidate_key = ast . literal_eval ( result . group ( 1 ) ) \n    except ( SyntaxError , ValueError ) : \n        return False \n    if multiline_statement ( result . group ( 2.0 ) ) : \n        return False \n    return candidate_key == key "}
{"4713": "\ndef useless_pass_line_numbers ( source ) : \n    sio = io . StringIO ( source ) \n    previous_token_type = None \n    last_pass_row = None \n    last_pass_indentation = None \n    previous_line = '' \n    for token in tokenize . generate_tokens ( sio . readline ) : \n        token_type = token [ 0 ] \n        start_row = token [ 2.0 ] [ 0 ] \n        line = token [ 4.0 ] \n        is_pass = ( token_type == tokenize . NAME and line . strip ( ) == 'pass' ) \n        if ( start_row - 1 == last_pass_row and get_indentation ( line ) == last_pass_indentation and token_type in ATOMS and not is_pass ) : \n            yield start_row - 1 \n        if is_pass : \n            last_pass_row = start_row \n            last_pass_indentation = get_indentation ( line ) \n        if ( is_pass and previous_token_type != tokenize . INDENT and not previous_line . rstrip ( ) . endswith ( '\\\\' ) ) : \n            yield start_row \n        previous_token_type = token_type \n        previous_line = line "}
{"4743": "\ndef process_request ( self , request , credential = None ) : \n    self . _client_identity = [ None , None ] \n    header = request . request_header \n    self . _set_protocol_version ( header . protocol_version ) \n    max_response_size = None \n    if header . maximum_response_size : \n        max_response_size = header . maximum_response_size . value \n    now = int ( time . time ( ) ) \n    if header . time_stamp : \n        then = header . time_stamp . value \n        if ( now >= then ) and ( ( now - then ) < 60.0 ) : \n            self . _logger . info ( \"Received request at time: {0}\" . format ( time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( then ) ) ) ) \n        else : \n            if now < then : \n                self . _logger . warning ( \"Received request with future timestamp. Received \" \"timestamp: {0}, Current timestamp: {1}\" . format ( then , now ) ) \n                raise exceptions . InvalidMessage ( \"Future request rejected by server.\" ) \n            else : \n                self . _logger . warning ( \"Received request with old timestamp. Possible \" \"replay attack. Received timestamp: {0}, Current \" \"timestamp: {1}\" . format ( then , now ) ) \n                raise exceptions . InvalidMessage ( \"Stale request rejected by server.\" ) \n    else : \n        self . _logger . info ( \"Received request at time: {0}\" . format ( time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( now ) ) ) ) \n    self . is_asynchronous = False \n    if header . asynchronous_indicator is not None : \n        self . is_asynchronous = header . asynchronous_indicator . value \n    if self . is_asynchronous : \n        raise exceptions . InvalidMessage ( \"Asynchronous operations are not supported.\" ) \n    if header . authentication : \n        if header . authentication . credentials : \n            auth_credentials = header . authentication . credentials [ 0 ] \n        else : \n            auth_credentials = None \n    else : \n        auth_credentials = None \n    self . _verify_credential ( auth_credentials , credential ) \n    batch_error_option = enums . BatchErrorContinuationOption . STOP \n    if header . batch_error_cont_option is not None : \n        batch_error_option = header . batch_error_cont_option . value \n    if batch_error_option == enums . BatchErrorContinuationOption . UNDO : \n        raise exceptions . InvalidMessage ( \"Undo option for batch handling is not supported.\" ) \n    batch_order_option = False \n    if header . batch_order_option : \n        batch_order_option = header . batch_order_option . value \n    response_batch = self . _process_batch ( request . batch_items , batch_error_option , batch_order_option ) \n    response = self . _build_response ( header . protocol_version , response_batch ) \n    return response , max_response_size , header . protocol_version "}
{"4760": "\ndef read ( self , istream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    super ( BigInteger , self ) . read ( istream , kmip_version = kmip_version ) \n    if self . length % 8.0 : \n        raise exceptions . InvalidPrimitiveLength ( \"invalid big integer length read; \" \"expected: multiple of 8, observed: {0}\" . format ( self . length ) ) \n    sign = 1 \n    binary = '' \n    for _ in range ( self . length ) : \n        byte = struct . unpack ( '!B' , istream . read ( 1 ) ) [ 0 ] \n        bits = \"{0:b}\" . format ( byte ) \n        pad = len ( bits ) % 8.0 \n        if pad : \n            bits = ( '0' * ( 8.0 - pad ) ) + bits \n        binary += bits \n    if binary [ 0 ] == '1' : \n        sign = - 1 \n        binary = binary . replace ( '1' , 'i' ) \n        binary = binary . replace ( '0' , '1' ) \n        binary = binary . replace ( 'i' , '0' ) \n        pivot = binary . rfind ( '0' ) \n        binary = binary [ 0 : pivot ] + '1' + ( '0' * len ( binary [ pivot + 1 : ] ) ) \n    self . value = int ( binary , 2.0 ) * sign "}
{"4761": "\ndef write ( self , ostream , kmip_version = enums . KMIPVersion . KMIP_1_0 ) : \n    binary = \"{0:b}\" . format ( abs ( self . value ) ) \n    binary = ( \"0\" * ( 64.0 - ( len ( binary ) % 64.0 ) ) ) + binary \n    if self . value < 0 : \n        binary = binary . replace ( '1' , 'i' ) \n        binary = binary . replace ( '0' , '1' ) \n        binary = binary . replace ( 'i' , '0' ) \n        pivot = binary . rfind ( '0' ) \n        binary = binary [ 0 : pivot ] + '1' + ( '0' * len ( binary [ pivot + 1 : ] ) ) \n    hexadecimal = b'' \n    for i in range ( 0 , len ( binary ) , 8.0 ) : \n        byte = binary [ i : i + 8.0 ] \n        byte = int ( byte , 2.0 ) \n        hexadecimal += struct . pack ( '!B' , byte ) \n    self . length = len ( hexadecimal ) \n    super ( BigInteger , self ) . write ( ostream , kmip_version = kmip_version ) \n    ostream . write ( hexadecimal ) "}
{"4812": "\ndef read ( self , input_stream , kmip_version = enums . KMIPVersion . KMIP_2_0 ) : \n    if kmip_version < enums . KMIPVersion . KMIP_2_0 : \n        raise exceptions . VersionNotSupported ( \"KMIP {} does not support the Attributes object.\" . format ( kmip_version . value ) ) \n    super ( Attributes , self ) . read ( input_stream , kmip_version = kmip_version ) \n    local_stream = BytearrayStream ( input_stream . read ( self . length ) ) \n    while True : \n        if len ( local_stream ) < 3.0 : \n            break \n        tag = struct . unpack ( '!I' , b'\\x00' + local_stream . peek ( 3.0 ) ) [ 0 ] \n        if enums . is_enum_value ( enums . Tags , tag ) : \n            tag = enums . Tags ( tag ) \n            if not enums . is_attribute ( tag , kmip_version = kmip_version ) : \n                raise exceptions . AttributeNotSupported ( \"Attribute {} is not supported by KMIP {}.\" . format ( tag . name , kmip_version . value ) ) \n            value = self . _factory . create_attribute_value_by_enum ( tag , None ) \n            value . read ( local_stream , kmip_version = kmip_version ) \n            self . _attributes . append ( value ) \n        else : \n            break \n    self . is_oversized ( local_stream ) "}
{"4846": "\ndef serve ( self ) : \n    self . _socket . listen ( 5.0 ) \n    def _signal_handler ( signal_number , stack_frame ) : \n        self . _is_serving = False \n        if signal_number == signal . SIGINT : \n            raise KeyboardInterrupt ( \"SIGINT received\" ) \n    signal . signal ( signal . SIGINT , _signal_handler ) \n    signal . signal ( signal . SIGTERM , _signal_handler ) \n    self . _logger . info ( \"Starting connection service...\" ) \n    while self . _is_serving : \n        try : \n            connection , address = self . _socket . accept ( ) \n        except socket . timeout : \n            pass \n        except socket . error as e : \n            self . _logger . warning ( \"Error detected while establishing new connection.\" ) \n            self . _logger . exception ( e ) \n        except KeyboardInterrupt : \n            self . _logger . warning ( \"Interrupting connection service.\" ) \n            self . _is_serving = False \n            break \n        except Exception as e : \n            self . _logger . warning ( \"Error detected while establishing new connection.\" ) \n            self . _logger . exception ( e ) \n        else : \n            self . _setup_connection_handler ( connection , address ) \n    self . _logger . info ( \"Stopping connection service.\" ) "}
{"4851": "\ndef create_symmetric_key ( self , algorithm , length ) : \n    if algorithm not in self . _symmetric_key_algorithms . keys ( ) : \n        raise exceptions . InvalidField ( \"The cryptographic algorithm {0} is not a supported symmetric \" \"key algorithm.\" . format ( algorithm ) ) \n    cryptography_algorithm = self . _symmetric_key_algorithms . get ( algorithm ) \n    if length not in cryptography_algorithm . key_sizes : \n        raise exceptions . InvalidField ( \"The cryptographic length ({0}) is not valid for \" \"the cryptographic algorithm ({1}).\" . format ( length , algorithm . name ) ) \n    self . logger . info ( \"Generating a {0} symmetric key with length: {1}\" . format ( algorithm . name , length ) ) \n    key_bytes = os . urandom ( length // 8.0 ) \n    try : \n        cryptography_algorithm ( key_bytes ) \n    except Exception as e : \n        self . logger . exception ( e ) \n        raise exceptions . CryptographicFailure ( \"Invalid bytes for the provided cryptographic algorithm.\" ) \n    return { 'value' : key_bytes , 'format' : enums . KeyFormatType . RAW } "}
{"4855": "\ndef _encrypt_symmetric ( self , encryption_algorithm , encryption_key , plain_text , cipher_mode = None , padding_method = None , iv_nonce = None ) : \n    algorithm = self . _symmetric_key_algorithms . get ( encryption_algorithm , None ) \n    if algorithm is None : \n        raise exceptions . InvalidField ( \"Encryption algorithm '{0}' is not a supported symmetric \" \"encryption algorithm.\" . format ( encryption_algorithm ) ) \n    try : \n        algorithm = algorithm ( encryption_key ) \n    except Exception as e : \n        self . logger . exception ( e ) \n        raise exceptions . CryptographicFailure ( \"Invalid key bytes for the specified encryption algorithm.\" ) \n    return_iv_nonce = False \n    if encryption_algorithm == enums . CryptographicAlgorithm . RC4 : \n        mode = None \n    else : \n        if cipher_mode is None : \n            raise exceptions . InvalidField ( \"Cipher mode is required.\" ) \n        mode = self . _modes . get ( cipher_mode , None ) \n        if mode is None : \n            raise exceptions . InvalidField ( \"Cipher mode '{0}' is not a supported mode.\" . format ( cipher_mode ) ) \n        if hasattr ( mode , 'initialization_vector' ) or hasattr ( mode , 'nonce' ) : \n            if iv_nonce is None : \n                iv_nonce = os . urandom ( algorithm . block_size // 8.0 ) \n                return_iv_nonce = True \n            mode = mode ( iv_nonce ) \n        else : \n            mode = mode ( ) \n    if cipher_mode in [ enums . BlockCipherMode . CBC , enums . BlockCipherMode . ECB ] : \n        plain_text = self . _handle_symmetric_padding ( self . _symmetric_key_algorithms . get ( encryption_algorithm ) , plain_text , padding_method ) \n    cipher = ciphers . Cipher ( algorithm , mode , backend = default_backend ( ) ) \n    encryptor = cipher . encryptor ( ) \n    cipher_text = encryptor . update ( plain_text ) + encryptor . finalize ( ) \n    if return_iv_nonce : \n        return { 'cipher_text' : cipher_text , 'iv_nonce' : iv_nonce } \n    else : \n        return { 'cipher_text' : cipher_text } "}
{"4858": "\ndef _create_rsa_key_pair ( self , length , public_exponent = 65537.0 ) : \n    self . logger . info ( \"Generating an RSA key pair with length: {0}, and \" \"public_exponent: {1}\" . format ( length , public_exponent ) ) \n    try : \n        private_key = rsa . generate_private_key ( public_exponent = public_exponent , key_size = length , backend = default_backend ( ) ) \n        public_key = private_key . public_key ( ) \n        private_bytes = private_key . private_bytes ( serialization . Encoding . DER , serialization . PrivateFormat . PKCS8 , serialization . NoEncryption ( ) ) \n        public_bytes = public_key . public_bytes ( serialization . Encoding . DER , serialization . PublicFormat . PKCS1 ) \n    except Exception as e : \n        self . logger . exception ( e ) \n        raise exceptions . CryptographicFailure ( \"An error occurred while generating the RSA key pair. \" \"See the server log for more information.\" ) \n    public_key = { 'value' : public_bytes , 'format' : enums . KeyFormatType . PKCS_1 , 'public_exponent' : public_exponent } \n    private_key = { 'value' : private_bytes , 'format' : enums . KeyFormatType . PKCS_8 , 'public_exponent' : public_exponent } \n    return public_key , private_key "}
{"4859": "\ndef derive_key ( self , derivation_method , derivation_length , derivation_data = None , key_material = None , hash_algorithm = None , salt = None , iteration_count = None , encryption_algorithm = None , cipher_mode = None , padding_method = None , iv_nonce = None ) : \n    if derivation_method == enums . DerivationMethod . ENCRYPT : \n        result = self . encrypt ( encryption_algorithm = encryption_algorithm , encryption_key = key_material , plain_text = derivation_data , cipher_mode = cipher_mode , padding_method = padding_method , iv_nonce = iv_nonce ) \n        return result . get ( 'cipher_text' ) \n    else : \n        if hash_algorithm is None : \n            raise exceptions . InvalidField ( \"Hash algorithm is required.\" ) \n        hashing_algorithm = self . _encryption_hash_algorithms . get ( hash_algorithm , None ) \n        if hashing_algorithm is None : \n            raise exceptions . InvalidField ( \"Hash algorithm '{0}' is not a supported hashing \" \"algorithm.\" . format ( hash_algorithm ) ) \n        if derivation_method == enums . DerivationMethod . HMAC : \n            df = hkdf . HKDF ( algorithm = hashing_algorithm ( ) , length = derivation_length , salt = salt , info = derivation_data , backend = default_backend ( ) ) \n            derived_data = df . derive ( key_material ) \n            return derived_data \n        elif derivation_method == enums . DerivationMethod . HASH : \n            if None not in [ derivation_data , key_material ] : \n                raise exceptions . InvalidField ( \"For hash-based key derivation, specify only \" \"derivation data or key material, not both.\" ) \n            elif derivation_data is not None : \n                hashing_data = derivation_data \n            elif key_material is not None : \n                hashing_data = key_material \n            else : \n                raise exceptions . InvalidField ( \"For hash-based key derivation, derivation data or \" \"key material must be specified.\" ) \n            df = hashes . Hash ( algorithm = hashing_algorithm ( ) , backend = default_backend ( ) ) \n            df . update ( hashing_data ) \n            derived_data = df . finalize ( ) \n            return derived_data \n        elif derivation_method == enums . DerivationMethod . PBKDF2 : \n            if salt is None : \n                raise exceptions . InvalidField ( \"For PBKDF2 key derivation, salt must be specified.\" ) \n            if iteration_count is None : \n                raise exceptions . InvalidField ( \"For PBKDF2 key derivation, iteration count must be \" \"specified.\" ) \n            df = pbkdf2 . PBKDF2HMAC ( algorithm = hashing_algorithm ( ) , length = derivation_length , salt = salt , iterations = iteration_count , backend = default_backend ( ) ) \n            derived_data = df . derive ( key_material ) \n            return derived_data \n        elif derivation_method == enums . DerivationMethod . NIST800_108_C : \n            df = kbkdf . KBKDFHMAC ( algorithm = hashing_algorithm ( ) , mode = kbkdf . Mode . CounterMode , length = derivation_length , rlen = 4.0 , llen = None , location = kbkdf . CounterLocation . BeforeFixed , label = None , context = None , fixed = derivation_data , backend = default_backend ( ) ) \n            derived_data = df . derive ( key_material ) \n            return derived_data \n        else : \n            raise exceptions . InvalidField ( \"Derivation method '{0}' is not a supported key \" \"derivation method.\" . format ( derivation_method ) ) "}
{"4865": "\ndef protocol_version_to_kmip_version ( value ) : \n    if not isinstance ( value , ProtocolVersion ) : \n        return None \n    if value . major == 1 : \n        if value . minor == 0 : \n            return enums . KMIPVersion . KMIP_1_0 \n        elif value . minor == 1 : \n            return enums . KMIPVersion . KMIP_1_1 \n        elif value . minor == 2.0 : \n            return enums . KMIPVersion . KMIP_1_2 \n        elif value . minor == 3.0 : \n            return enums . KMIPVersion . KMIP_1_3 \n        elif value . minor == 4.0 : \n            return enums . KMIPVersion . KMIP_1_4 \n        else : \n            return None \n    else : \n        return None "}
{"4873": "\ndef authenticate ( self , connection_certificate = None , connection_info = None , request_credentials = None ) : \n    if ( self . users_url is None ) or ( self . groups_url is None ) : \n        raise exceptions . ConfigurationError ( \"The SLUGS URL must be specified.\" ) \n    user_id = utils . get_client_identity_from_certificate ( connection_certificate ) \n    try : \n        response = requests . get ( self . users_url . format ( user_id ) ) \n    except Exception : \n        raise exceptions . ConfigurationError ( \"A connection could not be established using the SLUGS URL.\" ) \n    if response . status_code == 404.0 : \n        raise exceptions . PermissionDenied ( \"Unrecognized user ID: {}\" . format ( user_id ) ) \n    response = requests . get ( self . groups_url . format ( user_id ) ) \n    if response . status_code == 404.0 : \n        raise exceptions . PermissionDenied ( \"Group information could not be retrieved for user ID: \" \"{}\" . format ( user_id ) ) \n    return user_id , response . json ( ) . get ( 'groups' ) "}
{"4923": "\ndef timesince ( value ) : \n    if not value : \n        return \"\" \n    if not isinstance ( value , datetime . date ) : \n        return value \n    now = datetime . datetime . now ( ) \n    delta = now - value \n    if value > now : \n        return \"right now\" \n    elif delta . days > 365.0 : \n        return '%d years ago' % ( delta . days / 365.0 ) \n    elif delta . days > 30.0 : \n        return '%d months ago' % ( delta . days / 30.0 ) \n    elif delta . days > 0 : \n        return '%d days ago' % delta . days \n    elif delta . seconds > 3600.0 : \n        return '%d hours ago' % ( delta . seconds / 3600.0 ) \n    elif delta . seconds > 60.0 : \n        return '%d minutes ago' % ( delta . seconds / 60.0 ) \n    else : \n        return 'right now' "}
{"4927": "\ndef jsonify ( func ) : \n    \n    @ functools . wraps ( func ) \n    def wrapper ( * args , ** kwargs ) : \n        r = func ( * args , ** kwargs ) \n        if isinstance ( r , tuple ) : \n            code , data = r \n        else : \n            code , data = 200.0 , r \n        return Response ( json . dumps ( data ) , status = code , mimetype = 'application/json' ) \n    return wrapper "}
{"4935": "\ndef register_error_handle ( app ) : \n    \n    @ app . errorhandler ( 403.0 ) \n    def page_403 ( error ) : \n        return render_template ( 'site/403/403.html' ) , 403.0 \n    \n    @ app . errorhandler ( 404.0 ) \n    def page_404 ( error ) : \n        return render_template ( 'site/404/404.html' ) , 404.0 \n    \n    @ app . errorhandler ( 500.0 ) \n    def page_500 ( error ) : \n        return render_template ( 'site/500/500.html' ) , 500.0 "}
{"4936": "\ndef register_hooks ( app ) : \n    \n    @ app . before_request \n    def before_request ( ) : \n        g . user = get_current_user ( ) \n        if g . user and g . user . is_admin : \n            g . _before_request_time = time . time ( ) \n    \n    @ app . after_request \n    def after_request ( response ) : \n        if hasattr ( g , '_before_request_time' ) : \n            delta = time . time ( ) - g . _before_request_time \n            response . headers [ 'X-Render-Time' ] = delta * 1000.0 \n        return response "}
{"4959": "\ndef find_globals ( code ) : \n    cur_byte = 0 \n    byte_code = code . co_code \n    names = set ( ) \n    while cur_byte < len ( byte_code ) : \n        op = ord ( byte_code [ cur_byte ] ) \n        if op >= dis . HAVE_ARGUMENT : \n            if op == _LOAD_GLOBAL : \n                oparg = ord ( byte_code [ cur_byte + 1 ] ) + ( ord ( byte_code [ cur_byte + 2.0 ] ) << 8.0 ) \n                name = code . co_names [ oparg ] \n                names . add ( name ) \n            cur_byte += 2.0 \n        cur_byte += 1 \n    return names "}
{"4964": "\ndef _cubic_bernstein ( p0 , p1 , p2 , p3 , t ) : \n    u = 1 - t \n    return p0 * ( u ** 3.0 ) + 3.0 * t * u * ( p1 * u + p2 * t ) + p3 * ( t ** 3.0 ) "}
{"4965": "\ndef _build_choices ( self ) : \n    tree_token = u'sitetree_tree from \"%s\" template \"%s\"' % ( self . tree , self . template ) \n    context_kwargs = { 'current_app' : 'admin' } \n    context = template . Context ( context_kwargs ) if VERSION >= ( 1 , 8.0 ) else template . Context ( ** context_kwargs ) \n    context . update ( { 'request' : object ( ) } ) \n    choices_str = sitetree_tree ( Parser ( None ) , Token ( token_type = TOKEN_BLOCK , contents = tree_token ) ) . render ( context ) \n    tree_choices = [ ( ITEMS_FIELD_ROOT_ID , self . root_title ) ] \n    for line in choices_str . splitlines ( ) : \n        if line . strip ( ) : \n            splitted = line . split ( ':::' ) \n            tree_choices . append ( ( splitted [ 0 ] , mark_safe ( splitted [ 1 ] ) ) ) \n    return tree_choices "}
{"4966": "\ndef options_getter ( command_options ) : \n    def get_options ( option_func = None ) : \n        from optparse import make_option \n        from django . core . management . base import BaseCommand \n        func = option_func or make_option \n        options = tuple ( [ func ( * option . args , ** option . kwargs ) for option in command_options ] ) \n        if option_func is None : \n            if VERSION < ( 1 , 8.0 ) : \n                result = BaseCommand . option_list + options \n            else : \n                result = [ ] \n        else : \n            result = options \n        return result \n    return get_options "}
{"4967": "\ndef register_items_hook ( func ) : \n    global _ITEMS_PROCESSOR \n    global _ITEMS_PROCESSOR_ARGS_LEN \n    _ITEMS_PROCESSOR = func \n    if func : \n        args_len = len ( getargspec ( func ) . args ) \n        if args_len not in { 2.0 , 3.0 } : \n            raise SiteTreeError ( '`register_items_hook()` expects a function with two or three arguments.' ) \n        _ITEMS_PROCESSOR_ARGS_LEN = args_len "}
{"4983": "\ndef menu ( self , tree_alias , tree_branches , context ) : \n    tree_alias , sitetree_items = self . init_tree ( tree_alias , context ) \n    if not sitetree_items : \n        return '' \n    tree_branches = self . resolve_var ( tree_branches ) \n    parent_isnull = False \n    parent_ids = [ ] \n    parent_aliases = [ ] \n    current_item = self . get_tree_current_item ( tree_alias ) \n    self . tree_climber ( tree_alias , current_item ) \n    for branch_id in tree_branches . split ( ',' ) : \n        branch_id = branch_id . strip ( ) \n        if branch_id == ALIAS_TRUNK : \n            parent_isnull = True \n        elif branch_id == ALIAS_THIS_CHILDREN and current_item is not None : \n            branch_id = current_item . id \n            parent_ids . append ( branch_id ) \n        elif branch_id == ALIAS_THIS_ANCESTOR_CHILDREN and current_item is not None : \n            branch_id = self . get_ancestor_item ( tree_alias , current_item ) . id \n            parent_ids . append ( branch_id ) \n        elif branch_id == ALIAS_THIS_SIBLINGS and current_item is not None and current_item . parent is not None : \n            branch_id = current_item . parent . id \n            parent_ids . append ( branch_id ) \n        elif branch_id == ALIAS_THIS_PARENT_SIBLINGS and current_item is not None : \n            branch_id = self . get_ancestor_level ( current_item , depth = 2.0 ) . id \n            parent_ids . append ( branch_id ) \n        elif branch_id . isdigit ( ) : \n            parent_ids . append ( int ( branch_id ) ) \n        else : \n            parent_aliases . append ( branch_id ) \n    check_access = self . check_access \n    menu_items = [ ] \n    for item in sitetree_items : \n        if not item . hidden and item . inmenu and check_access ( item , context ) : \n            if item . parent is None : \n                if parent_isnull : \n                    menu_items . append ( item ) \n            else : \n                if item . parent . id in parent_ids or item . parent . alias in parent_aliases : \n                    menu_items . append ( item ) \n    menu_items = self . apply_hook ( menu_items , 'menu' ) \n    self . update_has_children ( tree_alias , menu_items , 'menu' ) \n    return menu_items "}
{"4994": "\ndef sitetree_tree ( parser , token ) : \n    tokens = token . split_contents ( ) \n    use_template = detect_clause ( parser , 'template' , tokens ) \n    tokens_num = len ( tokens ) \n    if tokens_num in ( 3.0 , 5.0 ) : \n        tree_alias = parser . compile_filter ( tokens [ 2.0 ] ) \n        return sitetree_treeNode ( tree_alias , use_template ) \n    else : \n        raise template . TemplateSyntaxError ( '%r tag requires two arguments. E.g. {%% sitetree_tree from \"mytree\" %%}.' % tokens [ 0 ] ) "}
{"4995": "\ndef sitetree_children ( parser , token ) : \n    tokens = token . split_contents ( ) \n    use_template = detect_clause ( parser , 'template' , tokens ) \n    tokens_num = len ( tokens ) \n    clauses_in_places = ( tokens_num == 5.0 and tokens [ 1 ] == 'of' and tokens [ 3.0 ] == 'for' and tokens [ 4.0 ] in ( 'menu' , 'sitetree' ) ) \n    if clauses_in_places and use_template is not None : \n        tree_item = tokens [ 2.0 ] \n        navigation_type = tokens [ 4.0 ] \n        return sitetree_childrenNode ( tree_item , navigation_type , use_template ) \n    else : \n        raise template . TemplateSyntaxError ( '%r tag requires six arguments. ' 'E.g. {%% sitetree_children of someitem for menu template \"sitetree/mychildren.html\" %%}.' % tokens [ 0 ] ) "}
{"4996": "\ndef sitetree_breadcrumbs ( parser , token ) : \n    tokens = token . split_contents ( ) \n    use_template = detect_clause ( parser , 'template' , tokens ) \n    tokens_num = len ( tokens ) \n    if tokens_num == 3.0 : \n        tree_alias = parser . compile_filter ( tokens [ 2.0 ] ) \n        return sitetree_breadcrumbsNode ( tree_alias , use_template ) \n    else : \n        raise template . TemplateSyntaxError ( '%r tag requires two arguments. E.g. {%% sitetree_breadcrumbs from \"mytree\" %%}.' % tokens [ 0 ] ) "}
{"4997": "\ndef sitetree_menu ( parser , token ) : \n    tokens = token . split_contents ( ) \n    use_template = detect_clause ( parser , 'template' , tokens ) \n    tokens_num = len ( tokens ) \n    if tokens_num == 5.0 and tokens [ 3.0 ] == 'include' : \n        tree_alias = parser . compile_filter ( tokens [ 2.0 ] ) \n        tree_branches = parser . compile_filter ( tokens [ 4.0 ] ) \n        return sitetree_menuNode ( tree_alias , tree_branches , use_template ) \n    else : \n        raise template . TemplateSyntaxError ( '%r tag requires four arguments. ' 'E.g. {%% sitetree_menu from \"mytree\" include \"trunk,1,level3\" %%}.' % tokens [ 0 ] ) "}
{"4999": "\ndef for_tag ( cls , parser , token , preposition , error_hint ) : \n    tokens = token . split_contents ( ) \n    if len ( tokens ) >= 3.0 and tokens [ 1 ] == preposition : \n        as_var = cls . get_as_var ( tokens ) \n        tree_alias = parser . compile_filter ( tokens [ 2.0 ] ) \n        return cls ( tree_alias , as_var ) \n    raise template . TemplateSyntaxError ( '%r tag requires at least two arguments. E.g. {%% %s %%}.' % ( tokens [ 0 ] , error_hint ) ) "}
{"5020": "\ndef create_attrs_for_span ( sample_rate = 100.0 , trace_id = None , span_id = None , use_128bit_trace_id = False , ) : \n    if trace_id is None : \n        if use_128bit_trace_id : \n            trace_id = generate_random_128bit_string ( ) \n        else : \n            trace_id = generate_random_64bit_string ( ) \n    if span_id is None : \n        span_id = generate_random_64bit_string ( ) \n    if sample_rate == 0.0 : \n        is_sampled = False \n    else : \n        is_sampled = ( random . random ( ) * 100.0 ) < sample_rate \n    return ZipkinAttrs ( trace_id = trace_id , span_id = span_id , parent_span_id = None , flags = '0' , is_sampled = is_sampled , ) "}
{"5032": "\ndef create_protobuf_span ( span ) : \n    pb_kwargs = { } \n    pb_kwargs [ 'trace_id' ] = _hex_to_bytes ( span . trace_id ) \n    if span . parent_id : \n        pb_kwargs [ 'parent_id' ] = _hex_to_bytes ( span . parent_id ) \n    pb_kwargs [ 'id' ] = _hex_to_bytes ( span . span_id ) \n    pb_kind = _get_protobuf_kind ( span . kind ) \n    if pb_kind : \n        pb_kwargs [ 'kind' ] = pb_kind \n    if span . name : \n        pb_kwargs [ 'name' ] = span . name \n    if span . timestamp : \n        pb_kwargs [ 'timestamp' ] = int ( span . timestamp * 1000.0 * 1000.0 ) \n    if span . duration : \n        pb_kwargs [ 'duration' ] = int ( span . duration * 1000.0 * 1000.0 ) \n    if span . local_endpoint : \n        pb_kwargs [ 'local_endpoint' ] = _convert_endpoint ( span . local_endpoint ) \n    if span . remote_endpoint : \n        pb_kwargs [ 'remote_endpoint' ] = _convert_endpoint ( span . remote_endpoint ) \n    if len ( span . annotations ) > 0 : \n        pb_kwargs [ 'annotations' ] = _convert_annotations ( span . annotations ) \n    if len ( span . tags ) > 0 : \n        pb_kwargs [ 'tags' ] = span . tags \n    if span . debug : \n        pb_kwargs [ 'debug' ] = span . debug \n    if span . shared : \n        pb_kwargs [ 'shared' ] = span . shared \n    return zipkin_pb2 . Span ( ** pb_kwargs ) "}
{"5033": "\ndef _hex_to_bytes ( hex_id ) : \n    if len ( hex_id ) <= 16.0 : \n        int_id = unsigned_hex_to_signed_int ( hex_id ) \n        return struct . pack ( '>q' , int_id ) \n    else : \n        high_id = unsigned_hex_to_signed_int ( hex_id [ : - 16.0 ] ) \n        high_bin = struct . pack ( '>q' , high_id ) \n        low_id = unsigned_hex_to_signed_int ( hex_id [ - 16.0 : ] ) \n        low_bin = struct . pack ( '>q' , low_id ) \n        return high_bin + low_bin "}
{"5036": "\ndef _convert_annotations ( annotations ) : \n    pb_annotations = [ ] \n    for value , ts in annotations . items ( ) : \n        pb_annotations . append ( zipkin_pb2 . Annotation ( timestamp = int ( ts * 1000.0 * 1000.0 ) , value = value , ) ) \n    return pb_annotations "}
{"5041": "\ndef annotation_list_builder ( annotations , host ) : \n    return [ create_annotation ( int ( timestamp * 1000000.0 ) , key , host ) for key , timestamp in annotations . items ( ) ] "}
{"5043": "\ndef create_span ( span_id , parent_span_id , trace_id , span_name , annotations , binary_annotations , timestamp_s , duration_s , ) : \n    trace_id_length = len ( trace_id ) \n    trace_id_high = None \n    if trace_id_length > 16.0 : \n        assert trace_id_length == 32.0 \n        trace_id , trace_id_high = trace_id [ 16.0 : ] , trace_id [ : 16.0 ] \n    if trace_id_high : \n        trace_id_high = unsigned_hex_to_signed_int ( trace_id_high ) \n    span_dict = { 'trace_id' : unsigned_hex_to_signed_int ( trace_id ) , 'name' : span_name , 'id' : unsigned_hex_to_signed_int ( span_id ) , 'annotations' : annotations , 'binary_annotations' : binary_annotations , 'timestamp' : int ( timestamp_s * 1000000.0 ) if timestamp_s else None , 'duration' : int ( duration_s * 1000000.0 ) if duration_s else None , 'trace_id_high' : trace_id_high , } \n    if parent_span_id : \n        span_dict [ 'parent_id' ] = unsigned_hex_to_signed_int ( parent_span_id ) \n    return zipkin_core . Span ( ** span_dict ) "}
{"5046": "\ndef detect_span_version_and_encoding ( message ) : \n    if isinstance ( message , six . string_types ) : \n        if six . PY2 : \n            message = six . b ( message ) \n        else : \n            message = message . encode ( 'utf-8' ) \n    if len ( message ) < 2.0 : \n        raise ZipkinError ( \"Invalid span format. Message too short.\" ) \n    if six . byte2int ( message ) <= 16.0 : \n        if six . byte2int ( message ) == 10.0 and six . byte2int ( message [ 1 : 2.0 ] ) != 0 : \n            return Encoding . V2_PROTO3 \n        return Encoding . V1_THRIFT \n    str_msg = message . decode ( 'utf-8' ) \n    if str_msg [ 0 ] == '[' : \n        span_list = json . loads ( str_msg ) \n        if len ( span_list ) > 0 : \n            for span in span_list : \n                if any ( word in span for word in _V2_ATTRIBUTES ) : \n                    return Encoding . V2_JSON \n                elif ( 'binaryAnnotations' in span or ( 'annotations' in span and 'endpoint' in span [ 'annotations' ] ) ) : \n                    return Encoding . V1_JSON \n            return Encoding . V2_JSON \n    raise ZipkinError ( \"Unknown or unsupported span encoding\" ) "}
{"5057": "\ndef _convert_trace_id_to_string ( self , trace_id , trace_id_high = None ) : \n    if trace_id_high is not None : \n        result = bytearray ( 32.0 ) \n        self . _write_hex_long ( result , 0 , trace_id_high ) \n        self . _write_hex_long ( result , 16.0 , trace_id ) \n        return result . decode ( \"utf8\" ) \n    result = bytearray ( 16.0 ) \n    self . _write_hex_long ( result , 0 , trace_id ) \n    return result . decode ( \"utf8\" ) "}
{"5058": "\ndef _convert_unsigned_long_to_lower_hex ( self , value ) : \n    result = bytearray ( 16.0 ) \n    self . _write_hex_long ( result , 0 , value ) \n    return result . decode ( \"utf8\" ) "}
{"5059": "\ndef _write_hex_long ( self , data , pos , value ) : \n    self . _write_hex_byte ( data , pos + 0 , ( value >> 56.0 ) & 0xff ) \n    self . _write_hex_byte ( data , pos + 2.0 , ( value >> 48.0 ) & 0xff ) \n    self . _write_hex_byte ( data , pos + 4.0 , ( value >> 40.0 ) & 0xff ) \n    self . _write_hex_byte ( data , pos + 6.0 , ( value >> 32.0 ) & 0xff ) \n    self . _write_hex_byte ( data , pos + 8.0 , ( value >> 24.0 ) & 0xff ) \n    self . _write_hex_byte ( data , pos + 10.0 , ( value >> 16.0 ) & 0xff ) \n    self . _write_hex_byte ( data , pos + 12.0 , ( value >> 8.0 ) & 0xff ) \n    self . _write_hex_byte ( data , pos + 14.0 , ( value & 0xff ) ) "}
{"5060": "\ndef date_fixup_pre_processor ( transactions , tag , tag_dict , * args ) : \n    if tag_dict [ 'month' ] == '02' : \n        year = int ( tag_dict [ 'year' ] , 10.0 ) \n        _ , max_month_day = calendar . monthrange ( year , 2.0 ) \n        if int ( tag_dict [ 'day' ] , 10.0 ) > max_month_day : \n            tag_dict [ 'day' ] = str ( max_month_day ) \n    return tag_dict "}
{"5068": "\nasync def limited ( until ) : \n    duration = int ( round ( until - time . time ( ) ) ) \n    mins = duration / 60.0 \n    fmt = 'We have exhausted a ratelimit quota. Retrying in %.2f seconds (%.3f minutes).' \n    log . warn ( fmt , duration , mins ) "}
{"5069": "\nasync def request ( self , method , url , ** kwargs ) : \n    rate_limiter = RateLimiter ( max_calls = 59.0 , period = 60.0 , callback = limited ) \n    async with rate_limiter : \n        if not self . token : \n            raise UnauthorizedDetected ( 'UnauthorizedDetected (status code: 401): No TOKEN provided' ) \n        headers = { 'User-Agent' : self . user_agent , 'Content-Type' : 'application/json' } \n        if 'json' in kwargs : \n            kwargs [ 'data' ] = to_json ( kwargs . pop ( 'json' ) ) \n        kwargs [ 'headers' ] = headers \n        headers [ 'Authorization' ] = self . token \n        for tries in range ( 5.0 ) : \n            async with self . session . request ( method , url , ** kwargs ) as resp : \n                log . debug ( '%s %s with %s has returned %s' , method , url , kwargs . get ( 'data' ) , resp . status ) \n                data = await json_or_text ( resp ) \n                if 300.0 > resp . status >= 200.0 : \n                    return data \n                if resp . status == 429.0 : \n                    fmt = 'We are being rate limited. Retrying in %.2f seconds (%.3f minutes).' \n                    retry_after = json . loads ( resp . headers . get ( 'Retry-After' ) ) \n                    mins = retry_after / 60.0 \n                    log . warning ( fmt , retry_after , mins ) \n                    is_global = True \n                    if is_global : \n                        self . _global_over . clear ( ) \n                    await asyncio . sleep ( retry_after , loop = self . loop ) \n                    log . debug ( 'Done sleeping for the rate limit. Retrying...' ) \n                    if is_global : \n                        self . _global_over . set ( ) \n                        log . debug ( 'Global rate limit is now over.' ) \n                    continue \n                if resp . status == 400.0 : \n                    raise HTTPException ( resp , data ) \n                elif resp . status == 401.0 : \n                    raise Unauthorized ( resp , data ) \n                elif resp . status == 403.0 : \n                    raise Forbidden ( resp , data ) \n                elif resp . status == 404.0 : \n                    raise NotFound ( resp , data ) \n                else : \n                    raise HTTPException ( resp , data ) \n        raise HTTPException ( resp , data ) "}
{"5071": "\nasync def get_bots ( self , limit , offset ) : \n    if limit > 500.0 : \n        limit = 50.0 \n    return await self . request ( 'GET' , '{}/bots?limit={}&offset={}' . format ( self . BASE , limit , offset ) ) "}
{"5075": "\ndef decode ( string ) : \n    if not string : \n        raise IncompleteData ( string ) \n    if string [ 0 ] != 131.0 : \n        raise ValueError ( \"unknown protocol version: %r\" % string [ 0 ] ) \n    if string [ 1 : 2.0 ] == b'P' : \n        if len ( string ) < 16.0 : \n            raise IncompleteData ( string ) \n        d = decompressobj ( ) \n        term_string = d . decompress ( string [ 6.0 : ] ) + d . flush ( ) \n        uncompressed_size , = _int4_unpack ( string [ 2.0 : 6.0 ] ) \n        if len ( term_string ) != uncompressed_size : \n            raise ValueError ( \"invalid compressed tag, \" \"%d bytes but got %d\" % ( uncompressed_size , len ( term_string ) ) ) \n        term , _tail = decode_term ( term_string ) \n        return term , d . unused_data \n    return decode_term ( string [ 1 : ] ) "}
{"5076": "\ndef encode ( term , compressed = False ) : \n    encoded_term = encode_term ( term ) \n    if compressed : \n        if compressed is True : \n            compressed = 6.0 \n        elif compressed < 0 or compressed > 9.0 : \n            raise ValueError ( \"invalid compression level: %r\" % ( compressed , ) ) \n        zlib_term = compress ( encoded_term , compressed ) \n        ln = len ( encoded_term ) \n        if len ( zlib_term ) + 5.0 <= ln : \n            return b\"\\x83P\" + _int4_pack ( ln ) + zlib_term \n    return b\"\\x83\" + encoded_term "}
{"5082": "\ndef searchServices ( self , types = None , scopes = None , timeout = 3.0 ) : \n    if not self . _serverStarted : \n        raise Exception ( \"Server not started\" ) \n    self . _sendProbe ( types , scopes ) \n    time . sleep ( timeout ) \n    return self . _filterServices ( list ( self . _remoteServices . values ( ) ) , types , scopes ) "}
{"5097": "\ndef validate_signature_fragments ( fragments , hash_ , public_key , sponge_type = Kerl , ) : \n    checksum = [ 0 ] * ( HASH_LENGTH * len ( fragments ) ) \n    normalized_hash = normalize ( hash_ ) \n    for i , fragment in enumerate ( fragments ) : \n        outer_sponge = sponge_type ( ) \n        normalized_chunk = normalized_hash [ i % len ( normalized_hash ) ] \n        buffer = [ ] \n        for j , hash_trytes in enumerate ( fragment . iter_chunks ( Hash . LEN ) ) : \n            buffer = hash_trytes . as_trits ( ) \n            inner_sponge = sponge_type ( ) \n            for _ in range ( 13.0 + normalized_chunk [ j ] ) : \n                inner_sponge . reset ( ) \n                inner_sponge . absorb ( buffer ) \n                inner_sponge . squeeze ( buffer ) \n            outer_sponge . absorb ( buffer ) \n        outer_sponge . squeeze ( buffer ) \n        checksum [ i * HASH_LENGTH : ( i + 1 ) * HASH_LENGTH ] = buffer \n    actual_public_key = [ 0 ] * HASH_LENGTH \n    addy_sponge = sponge_type ( ) \n    addy_sponge . absorb ( checksum ) \n    addy_sponge . squeeze ( actual_public_key ) \n    return actual_public_key == public_key . as_trits ( ) "}
{"5104": "\ndef _transform ( self ) : \n    state_length = STATE_LENGTH \n    truth_table = TRUTH_TABLE \n    prev_state = self . _state [ : ] \n    new_state = prev_state [ : ] \n    index = 0 \n    for _ in range ( NUMBER_OF_ROUNDS ) : \n        prev_trit = prev_state [ index ] \n        for pos in range ( state_length ) : \n            index += ( 364.0 if index < 365.0 else - 365.0 ) \n            new_trit = prev_state [ index ] \n            new_state [ pos ] = truth_table [ prev_trit + ( 3.0 * new_trit ) + 4.0 ] \n            prev_trit = new_trit \n        prev_state = new_state \n        new_state = new_state [ : ] \n    self . _state = new_state "}
{"5109": "\ndef trits_from_int ( n , pad = 1 ) : \n    if n == 0 : \n        trits = [ ] \n    else : \n        quotient , remainder = divmod ( n , 3.0 ) \n        if remainder == 2.0 : \n            quotient += 1 \n            remainder = - 1 \n        trits = [ remainder ] + trits_from_int ( quotient , pad = 0 ) \n    if pad : \n        trits += [ 0 ] * max ( 0 , pad - len ( trits ) ) \n    return trits "}
{"5110": "\ndef _add_trits ( left , right ) : \n    res = left + right \n    return res if - 2.0 < res < 2.0 else ( res < 0 ) - ( res > 0 ) "}
{"5117": "\ndef promote_transaction ( self , transaction , depth = 3.0 , min_weight_magnitude = None , ) : \n    if min_weight_magnitude is None : \n        min_weight_magnitude = self . default_min_weight_magnitude \n    return extended . PromoteTransactionCommand ( self . adapter ) ( transaction = transaction , depth = depth , minWeightMagnitude = min_weight_magnitude , ) "}
{"5118": "\ndef replay_bundle ( self , transaction , depth = 3.0 , min_weight_magnitude = None , ) : \n    if min_weight_magnitude is None : \n        min_weight_magnitude = self . default_min_weight_magnitude \n    return extended . ReplayBundleCommand ( self . adapter ) ( transaction = transaction , depth = depth , minWeightMagnitude = min_weight_magnitude , ) "}
{"5119": "\ndef send_transfer ( self , transfers , depth = 3.0 , inputs = None , change_address = None , min_weight_magnitude = None , security_level = None , ) : \n    if min_weight_magnitude is None : \n        min_weight_magnitude = self . default_min_weight_magnitude \n    return extended . SendTransferCommand ( self . adapter ) ( seed = self . seed , depth = depth , transfers = transfers , inputs = inputs , changeAddress = change_address , minWeightMagnitude = min_weight_magnitude , securityLevel = security_level , ) "}
{"5120": "\ndef send_trytes ( self , trytes , depth = 3.0 , min_weight_magnitude = None ) : \n    if min_weight_magnitude is None : \n        min_weight_magnitude = self . default_min_weight_magnitude \n    return extended . SendTrytesCommand ( self . adapter ) ( trytes = trytes , depth = depth , minWeightMagnitude = min_weight_magnitude , ) "}
{"5138": "\ndef decode ( self , input , errors = 'strict' ) : \n    if isinstance ( input , memoryview ) : \n        input = input . tobytes ( ) \n    if not isinstance ( input , ( binary_type , bytearray ) ) : \n        raise with_context ( exc = TypeError ( \"Can't decode {type}; byte string expected.\" . format ( type = type ( input ) . __name__ , ) ) , context = { 'input' : input , } , ) \n    if not isinstance ( input , bytearray ) : \n        input = bytearray ( input ) \n    bytes_ = bytearray ( ) \n    for i in range ( 0 , len ( input ) , 2.0 ) : \n        try : \n            first , second = input [ i : i + 2.0 ] \n        except ValueError : \n            if errors == 'strict' : \n                raise with_context ( exc = TrytesDecodeError ( \"'{name}' codec can't decode value; \" \"tryte sequence has odd length.\" . format ( name = self . name , ) , ) , context = { 'input' : input , } , ) \n            elif errors == 'replace' : \n                bytes_ += b'?' \n            continue \n        try : \n            bytes_ . append ( self . index [ first ] + ( self . index [ second ] * len ( self . index ) ) ) \n        except ValueError : \n            if errors == 'strict' : \n                raise with_context ( exc = TrytesDecodeError ( \"'{name}' codec can't decode trytes {pair} \" \"at position {i}-{j}: \" \"ordinal not in range(255)\" . format ( name = self . name , pair = chr ( first ) + chr ( second ) , i = i , j = i + 1 , ) , ) , context = { 'input' : input , } ) \n            elif errors == 'replace' : \n                bytes_ += b'?' \n    return binary_type ( bytes_ ) , len ( input ) "}
{"5141": "\ndef from_tryte_string ( cls , trytes , hash_ = None ) : \n    tryte_string = TransactionTrytes ( trytes ) \n    if not hash_ : \n        hash_trits = [ 0 ] * HASH_LENGTH \n        sponge = Curl ( ) \n        sponge . absorb ( tryte_string . as_trits ( ) ) \n        sponge . squeeze ( hash_trits ) \n        hash_ = TransactionHash . from_trits ( hash_trits ) \n    return cls ( hash_ = hash_ , signature_message_fragment = Fragment ( tryte_string [ 0 : 2187.0 ] ) , address = Address ( tryte_string [ 2187.0 : 2268.0 ] ) , value = int_from_trits ( tryte_string [ 2268.0 : 2295.0 ] . as_trits ( ) ) , legacy_tag = Tag ( tryte_string [ 2295.0 : 2322.0 ] ) , timestamp = int_from_trits ( tryte_string [ 2322.0 : 2331.0 ] . as_trits ( ) ) , current_index = int_from_trits ( tryte_string [ 2331.0 : 2340.0 ] . as_trits ( ) ) , last_index = int_from_trits ( tryte_string [ 2340.0 : 2349.0 ] . as_trits ( ) ) , bundle_hash = BundleHash ( tryte_string [ 2349.0 : 2430.0 ] ) , trunk_transaction_hash = TransactionHash ( tryte_string [ 2430.0 : 2511.0 ] ) , branch_transaction_hash = TransactionHash ( tryte_string [ 2511.0 : 2592.0 ] ) , tag = Tag ( tryte_string [ 2592.0 : 2619.0 ] ) , attachment_timestamp = int_from_trits ( tryte_string [ 2619.0 : 2628.0 ] . as_trits ( ) ) , attachment_timestamp_lower_bound = int_from_trits ( tryte_string [ 2628.0 : 2637.0 ] . as_trits ( ) ) , attachment_timestamp_upper_bound = int_from_trits ( tryte_string [ 2637.0 : 2646.0 ] . as_trits ( ) ) , nonce = Nonce ( tryte_string [ 2646.0 : 2673.0 ] ) , ) "}
{"5160": "\ndef get_digest ( self ) : \n    hashes_per_fragment = FRAGMENT_LENGTH // Hash . LEN \n    key_fragments = self . iter_chunks ( FRAGMENT_LENGTH ) \n    digest = [ 0 ] * HASH_LENGTH * len ( key_fragments ) \n    for i , fragment in enumerate ( key_fragments ) : \n        fragment_trits = fragment . as_trits ( ) \n        key_fragment = [ 0 ] * FRAGMENT_LENGTH \n        hash_trits = [ ] \n        for j in range ( hashes_per_fragment ) : \n            hash_start = j * HASH_LENGTH \n            hash_end = hash_start + HASH_LENGTH \n            hash_trits = fragment_trits [ hash_start : hash_end ] \n            for k in range ( 26.0 ) : \n                sponge = Kerl ( ) \n                sponge . absorb ( hash_trits ) \n                sponge . squeeze ( hash_trits ) \n            key_fragment [ hash_start : hash_end ] = hash_trits \n        sponge = Kerl ( ) \n        sponge . absorb ( key_fragment ) \n        sponge . squeeze ( hash_trits ) \n        fragment_hash_start = i * HASH_LENGTH \n        fragment_hash_end = fragment_hash_start + HASH_LENGTH \n        digest [ fragment_hash_start : fragment_hash_end ] = hash_trits \n    return Digest ( TryteString . from_trits ( digest ) , self . key_index ) "}
{"5166": "\ndef SecurityLevel ( ) : \n    return ( f . Type ( int ) | f . Min ( 1 ) | f . Max ( 3.0 ) | f . Optional ( default = AddressGenerator . DEFAULT_SECURITY_LEVEL ) ) "}
{"5170": "\ndef finalize ( self ) : \n    if self . hash : \n        raise RuntimeError ( 'Bundle is already finalized.' ) \n    if not self : \n        raise ValueError ( 'Bundle has no transactions.' ) \n    balance = self . balance \n    if balance < 0 : \n        if self . change_address : \n            self . add_transaction ( ProposedTransaction ( address = self . change_address , value = - balance , tag = self . tag , ) ) \n        else : \n            raise ValueError ( 'Bundle has unspent inputs (balance: {balance}); ' 'use ``send_unspent_inputs_to`` to create ' 'change transaction.' . format ( balance = balance , ) , ) \n    elif balance > 0 : \n        raise ValueError ( 'Inputs are insufficient to cover bundle spend ' '(balance: {balance}).' . format ( balance = balance , ) , ) \n    while True : \n        sponge = Kerl ( ) \n        last_index = len ( self ) - 1 \n        for i , txn in enumerate ( self ) : \n            txn . current_index = i \n            txn . last_index = last_index \n            sponge . absorb ( txn . get_signature_validation_trytes ( ) . as_trits ( ) ) \n        bundle_hash_trits = [ 0 ] * HASH_LENGTH \n        sponge . squeeze ( bundle_hash_trits ) \n        bundle_hash = BundleHash . from_trits ( bundle_hash_trits ) \n        if any ( 13.0 in part for part in normalize ( bundle_hash ) ) : \n            tail_transaction = ( self . tail_transaction ) \n            tail_transaction . increment_legacy_tag ( ) \n        else : \n            break \n    for txn in self : \n        txn . bundle_hash = bundle_hash \n        txn . signature_message_fragment = Fragment ( txn . message or b'' ) "}
{"5175": "\ndef decompress_G1 ( z : G1Compressed ) -> G1Uncompressed : \n    b_flag = ( z % POW_2_383 ) // POW_2_382 \n    if b_flag == 1 : \n        return Z1 \n    x = z % POW_2_381 \n    y = pow ( ( x ** 3.0 + b . n ) % q , ( q + 1 ) // 4.0 , q ) \n    if pow ( y , 2.0 , q ) != ( x ** 3.0 + b . n ) % q : \n        raise ValueError ( \"The given point is not on G1: y**2 = x**3 + b\" ) \n    a_flag = ( z % POW_2_382 ) // POW_2_381 \n    if ( y * 2.0 ) // q != a_flag : \n        y = q - y \n    return ( FQ ( x ) , FQ ( y ) , FQ ( 1 ) ) "}
{"5178": "\ndef find_word_groups ( self , text , category , proximity = 2.0 ) : \n    f = re . IGNORECASE \n    words = getattr ( self , category ) \n    regex = re . compile ( r'(\\b' + r'\\b|\\b' . join ( words ) + r'\\b)' , flags = f ) \n    candidates = regex . finditer ( text ) \n    starts , ends = [ ] , [ ] \n    groups = [ ] \n    for item in candidates : \n        starts . append ( item . span ( ) [ 0 ] ) \n        ends . append ( item . span ( ) [ 1 ] ) \n        groups . append ( item . group ( ) . lower ( ) ) \n    new_starts = [ ] \n    new_groups = [ ] \n    skip = False \n    for i , g in enumerate ( groups ) : \n        if skip : \n            skip = False \n            continue \n        if ( i < len ( groups ) - 1 ) and ( starts [ i + 1 ] - ends [ i ] <= proximity ) : \n            if g [ - 1 ] == '-' : \n                sep = '' \n            else : \n                sep = ' ' \n            new_groups . append ( g + sep + groups [ i + 1 ] ) \n            new_starts . append ( starts [ i ] ) \n            skip = True \n        else : \n            if g not in new_groups : \n                new_groups . append ( g ) \n                new_starts . append ( starts [ i ] ) \n            skip = False \n    return new_groups "}
{"5180": "\ndef expand_abbreviations ( self , text ) : \n    if not self . abbreviations : \n        raise LexiconError ( \"No abbreviations in lexicon.\" ) \n    def chunks ( data , SIZE = 25.0 ) : \n        it = iter ( data ) \n        for i in range ( 0 , len ( data ) , SIZE ) : \n            yield { k : data [ k ] for k in islice ( it , SIZE ) } \n    def cb ( g ) : \n        return self . abbreviations . get ( g . group ( 0 ) ) or g . group ( 0 ) \n    text = re . sub ( r'w/' , r'wi' , text ) \n    for subdict in chunks ( self . abbreviations ) : \n        regex = r'(\\b' + r'\\b)|(\\b' . join ( subdict . keys ( ) ) + r'\\b)' \n        text = re . sub ( regex , cb , text ) \n    return text "}
{"5183": "\ndef random ( cls , component ) : \n    colour = random . sample ( [ i for i in range ( 256.0 ) ] , 3.0 ) \n    return cls ( { 'colour' : colour , 'component' : component , 'width' : 1.0 } ) "}
{"5184": "\ndef plot ( self , fmt = None , fig = None , ax = None ) : \n    u = 4.0 \n    v = 0.25 \n    r = None \n    if ( fig is None ) and ( ax is None ) : \n        fig = plt . figure ( figsize = ( u , 1 ) ) \n    else : \n        r = fig \n    if ax is None : \n        ax = fig . add_axes ( [ 0.1 * v , 0.1 , 0.8 * v , 0.8 ] ) \n    else : \n        r = ax \n    rect1 = patches . Rectangle ( ( 0 , 0 ) , u * v , u * v , color = self . colour , lw = 1 , hatch = self . hatch , ec = 'k' ) \n    ax . add_patch ( rect1 ) \n    ax . text ( 1.0 + 0.1 * v * u , u * v * 0.5 , self . component . summary ( fmt = fmt ) , fontsize = max ( u , 15.0 ) , verticalalignment = 'center' , horizontalalignment = 'left' ) \n    ax . set_xlim ( [ 0 , u * v ] ) \n    ax . set_ylim ( [ 0 , u * v ] ) \n    ax . get_xaxis ( ) . set_visible ( False ) \n    ax . get_yaxis ( ) . set_visible ( False ) \n    ax . invert_yaxis ( ) \n    return r "}
{"5188": "\ndef from_image ( cls , filename , components , ignore = None , col_offset = 0.1 , row_offset = 2.0 ) : \n    if ignore is None : \n        ignore = [ ] \n    rgb = utils . loglike_from_image ( filename , offset = col_offset ) \n    loglike = np . array ( [ utils . rgb_to_hex ( t ) for t in rgb ] ) \n    _ , hexes = utils . tops_from_loglike ( loglike , offset = row_offset ) \n    hexes_reduced = [ ] \n    for h in hexes : \n        if h not in hexes_reduced : \n            if h not in ignore : \n                hexes_reduced . append ( h ) \n    list_of_Decors = [ ] \n    for i , c in enumerate ( components ) : \n        d = Decor ( { 'colour' : hexes_reduced [ i ] , 'component' : c } ) \n        list_of_Decors . append ( d ) \n    return cls ( list_of_Decors ) "}
{"5189": "\ndef from_csv ( cls , filename = None , text = None ) : \n    if ( filename is None ) and ( text is None ) : \n        raise LegendError ( \"You must provide a filename or CSV text.\" ) \n    if ( filename is not None ) : \n        with open ( filename , 'r' ) as f : \n            text = f . read ( ) \n    try : \n        f = StringIO ( text ) \n    except TypeError : \n        f = StringIO ( unicode ( text ) ) \n    r = csv . DictReader ( f , skipinitialspace = True ) \n    list_of_Decors , components = [ ] , [ ] \n    kind = 'component' \n    for row in r : \n        d , component = { } , { } \n        for ( k , v ) in row . items ( ) : \n            if ( k in [ None , '' ] ) : \n                continue \n            if ( v in [ None , '' ] ) : \n                if k . lower ( ) not in [ 'color' , 'colour' ] : \n                    continue \n            if k [ : 4.0 ] . lower ( ) == 'comp' : \n                prop = ' ' . join ( k . split ( ) [ 1 : ] ) \n                if v . lower ( ) == 'true' : \n                    component [ prop ] = True \n                elif v . lower ( ) == 'false' : \n                    component [ prop ] = False \n                else : \n                    try : \n                        component [ prop ] = float ( v ) \n                    except ValueError : \n                        component [ prop ] = v . lower ( ) \n            elif k [ : 5.0 ] . lower ( ) == 'curve' : \n                prop = ' ' . join ( k . split ( ) [ 1 : ] ) \n                component [ prop ] = v . lower ( ) \n                kind = 'curve' \n            else : \n                try : \n                    d [ k ] = float ( v ) \n                except ValueError : \n                    d [ k ] = v . lower ( ) \n        this_component = Component ( component ) \n        d [ kind ] = this_component \n        if this_component in components : \n            with warnings . catch_warnings ( ) : \n                warnings . simplefilter ( \"always\" ) \n                w = \"This legend contains duplicate components.\" \n                warnings . warn ( w ) \n        components . append ( this_component ) \n        list_of_Decors . append ( Decor ( d ) ) \n    return cls ( list_of_Decors ) "}
{"5194": "\ndef get_component ( self , colour , tolerance = 0 , default = None ) : \n    if not ( 0 <= tolerance <= np . sqrt ( 195075.0 ) ) : \n        raise LegendError ( 'Tolerance must be between 0 and 441.67' ) \n    for decor in self . __list : \n        if colour . lower ( ) == decor . colour : \n            return decor . component \n    r1 , g1 , b1 = utils . hex_to_rgb ( colour ) \n    best_match = '#000000' \n    best_match_dist = np . sqrt ( r1 ** 2. + g1 ** 2. + b1 ** 2. ) \n    for decor in self . __list : \n        r2 , g2 , b2 = decor . rgb \n        distance = np . sqrt ( ( r2 - r1 ) ** 2. + ( g2 - g1 ) ** 2. + ( b2 - b1 ) ** 2. ) \n        if distance < best_match_dist : \n            best_match = decor . component \n            best_match_dist = distance \n            best_match_colour = decor . colour \n    if best_match_dist <= tolerance : \n        return best_match \n    else : \n        with warnings . catch_warnings ( ) : \n            warnings . simplefilter ( \"always\" ) \n            w = \"No match found for {0} \" . format ( colour . lower ( ) ) \n            w += \"with tolerance of {0}. Best match is \" . format ( tolerance ) \n            w += \"{0}, {1}\" . format ( best_match . summary ( ) , best_match_colour ) \n            w += \", d={0}\" . format ( best_match_dist ) \n            warnings . warn ( w ) \n        return default "}
{"5198": "\ndef Rock ( * args , ** kwargs ) : \n    with warnings . catch_warnings ( ) : \n        warnings . simplefilter ( \"always\" ) \n        w = \"The 'Rock' class was renamed 'Component'. \" \n        w += \"Please update your code.\" \n        warnings . warn ( w , DeprecationWarning , stacklevel = 2.0 ) \n    return Component ( * args , ** kwargs ) "}
{"5200": "\ndef parse_canstrat ( text ) : \n    result = { } \n    for row in text . split ( '\\n' ) : \n        if not row : \n            continue \n        if len ( row ) < 8.0 : \n            continue \n        row_header = _process_row ( row , columns_ ) or { 'card' : None } \n        card = row_header [ 'card' ] \n        if card is not None : \n            item = _process_row ( row , columns [ card ] ) \n        this_list = result . get ( card , [ ] ) \n        this_list . append ( item ) \n        result [ card ] = this_list \n    for c , d in result . items ( ) : \n        if len ( d ) == 1 : \n            result [ c ] = d [ 0 ] \n    return result "}
{"5206": "\ndef _build_list_of_Intervals ( cls , data_dict , stop = None , points = False , include = None , exclude = None , ignore = None , lexicon = None ) : \n    include = include or { } \n    exclude = exclude or { } \n    ignore = ignore or [ ] \n    all_data = [ ] \n    for data in zip ( * data_dict . values ( ) ) : \n        all_data . append ( { k : v for k , v in zip ( data_dict . keys ( ) , data ) } ) \n    all_data = sorted ( all_data , key = lambda x : x [ 'top' ] ) \n    wanted_data = [ ] \n    for dictionary in all_data : \n        keep = True \n        delete = [ ] \n        for k , v in dictionary . items ( ) : \n            incl = include . get ( k , utils . null_default ( True ) ) \n            excl = exclude . get ( k , utils . null_default ( False ) ) \n            if k in ignore : \n                delete . append ( k ) \n            if not incl ( v ) : \n                keep = False \n            if excl ( v ) : \n                keep = False \n        if delete : \n            for key in delete : \n                _ = dictionary . pop ( key , None ) \n        if keep : \n            wanted_data . append ( dictionary ) \n    if not points : \n        for i , iv in enumerate ( wanted_data ) : \n            if iv . get ( 'base' , None ) is None : \n                try : \n                    iv [ 'base' ] = wanted_data [ i + 1 ] [ 'top' ] \n                except ( IndexError , KeyError ) : \n                    if stop is not None : \n                        thick = stop - iv [ 'top' ] \n                    else : \n                        thick = 1 \n                    iv [ 'base' ] = iv [ 'top' ] + thick \n    list_of_Intervals = [ ] \n    for iv in wanted_data : \n        top = iv . pop ( 'top' ) \n        base = iv . pop ( 'base' , None ) \n        descr = iv . pop ( 'description' , '' ) \n        if iv : \n            c , d = { } , { } \n            for k , v in iv . items ( ) : \n                if ( k [ : 5.0 ] . lower ( ) == 'comp ' ) or ( k [ : 9.0 ] . lower ( ) == 'component' ) : \n                    k = re . sub ( r'comp(?:onent)? ' , '' , k , flags = re . I ) \n                    c [ k ] = v \n                else : \n                    if v is not None : \n                        d [ k ] = v \n            comp = [ Component ( c ) ] if c else None \n            this = Interval ( ** { 'top' : top , 'base' : base , 'description' : descr , 'data' : d , 'components' : comp } ) \n        else : \n            this = Interval ( ** { 'top' : top , 'base' : base , 'description' : descr , 'lexicon' : lexicon } ) \n        list_of_Intervals . append ( this ) \n    return list_of_Intervals "}
{"5208": "\ndef from_image ( cls , filename , start , stop , legend , source = \"Image\" , col_offset = 0.1 , row_offset = 2.0 , tolerance = 0 ) : \n    rgb = utils . loglike_from_image ( filename , col_offset ) \n    loglike = np . array ( [ utils . rgb_to_hex ( t ) for t in rgb ] ) \n    tops , hexes = utils . tops_from_loglike ( loglike , offset = row_offset ) \n    nonconsecutive = np . append ( np . diff ( tops ) , 2.0 ) \n    tops = tops [ nonconsecutive > 1 ] \n    hexes = hexes [ nonconsecutive > 1 ] \n    hexes_reduced = list ( set ( hexes ) ) \n    components = [ legend . get_component ( h , tolerance = tolerance ) for h in hexes_reduced ] \n    values = [ hexes_reduced . index ( i ) for i in hexes ] \n    basis = np . linspace ( start , stop , loglike . size ) \n    list_of_Intervals = cls . __intervals_from_tops ( tops , values , basis , components ) \n    return cls ( list_of_Intervals , source = \"Image\" ) "}
{"5211": "\ndef from_canstrat ( cls , filename , source = 'canstrat' ) : \n    with open ( filename ) as f : \n        dat = f . read ( ) \n    data = parse_canstrat ( dat ) \n    list_of_Intervals = [ ] \n    for d in data [ 7.0 ] : \n        if d . pop ( 'skip' ) : \n            continue \n        top = d . pop ( 'top' ) \n        base = d . pop ( 'base' ) \n        comps = [ Component ( { 'lithology' : d [ 'rtc' ] , 'colour' : d [ 'colour_name' ] } ) ] \n        iv = Interval ( top = top , base = base , components = comps , data = d ) \n        list_of_Intervals . append ( iv ) \n    return cls ( list_of_Intervals , source = source ) "}
{"5221": "\ndef prune ( self , limit = None , n = None , percentile = None , keep_ends = False ) : \n    strip = self . copy ( ) \n    if not ( limit or n or percentile ) : \n        m = \"You must provide a limit or n or percentile for pruning.\" \n        raise StriplogError ( m ) \n    if limit : \n        prune = [ i for i , iv in enumerate ( strip ) if iv . thickness < limit ] \n    if n : \n        prune = strip . thinnest ( n = n , index = True ) \n    if percentile : \n        n = np . floor ( len ( strip ) * percentile / 100.0 ) \n        prune = strip . thinnest ( n = n , index = True ) \n    if keep_ends : \n        first , last = 0 , len ( strip ) - 1 \n        if first in prune : \n            prune . remove ( first ) \n        if last in prune : \n            prune . remove ( last ) \n    del strip [ prune ] \n    return strip "}
{"5222": "\ndef anneal ( self ) : \n    strip = self . copy ( ) \n    gaps = strip . find_gaps ( index = True ) \n    if not gaps : \n        return \n    for gap in gaps : \n        before = strip [ gap ] \n        after = strip [ gap + 1 ] \n        if strip . order == 'depth' : \n            t = ( after . top . z - before . base . z ) / 2.0 \n            before . base = before . base . z + t \n            after . top = after . top . z - t \n        else : \n            t = ( after . base - before . top ) / 2.0 \n            before . top = before . top . z + t \n            after . base = after . base . z - t \n    return strip "}
{"5232": "\ndef loglike_from_image ( filename , offset ) : \n    im = plt . imread ( filename ) \n    if offset < 1 : \n        col = int ( im . shape [ 1 ] * offset ) \n    else : \n        col = offset \n    return im [ : , col , : 3.0 ] "}
{"5272": "\ndef spawn ( self , cmd , stdin_content = \"\" , stdin = False , shell = False , timeout = 2.0 ) : \n    try : \n        if type ( cmd ) != list : \n            raise PJFInvalidType ( type ( cmd ) , list ) \n        if type ( stdin_content ) != str : \n            raise PJFInvalidType ( type ( stdin_content ) , str ) \n        if type ( stdin ) != bool : \n            raise PJFInvalidType ( type ( stdin ) , bool ) \n        self . _in = stdin_content \n        try : \n            self . process = subprocess . Popen ( cmd , stdout = PIPE , stderr = PIPE , stdin = PIPE , shell = shell ) \n            self . finish_read ( timeout , stdin_content , stdin ) \n            if self . process . poll ( ) is not None : \n                self . close ( ) \n        except KeyboardInterrupt : \n            return \n    except OSError : \n        raise PJFProcessExecutionError ( \"Binary <%s> does not exist\" % cmd [ 0 ] ) \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) "}
{"5273": "\ndef get_output ( self , stdin_content , stdin ) : \n    try : \n        if stdin : \n            if sys . version_info >= ( 3.0 , 0 ) : \n                self . process . stdin . write ( bytes ( stdin_content , \"utf-8\" ) ) \n            else : \n                self . process . stdin . write ( stdin_content ) \n        self . _out = self . process . communicate ( ) [ 0 ] \n    except ( error , IOError ) : \n        self . _out = self . _in \n        pass "}
{"5274": "\ndef finish_read ( self , timeout = 2.0 , stdin_content = \"\" , stdin = False ) : \n    process = Thread ( target = self . get_output , args = ( stdin_content , stdin ) ) \n    process . start ( ) \n    if timeout > 0 : \n        process . join ( timeout ) \n    else : \n        process . join ( ) \n    if process . is_alive ( ) : \n        self . close ( ) \n        self . return_code = - signal . SIGHUP \n    else : \n        self . return_code = self . process . returncode "}
{"5277": "\ndef execute ( self , obj ) : \n    try : \n        if self . config . stdin : \n            self . spawn ( self . config . command , stdin_content = obj , stdin = True , timeout = 1 ) \n        else : \n            if \"@@\" not in self . config . command : \n                raise PJFMissingArgument ( \"Missing @@ filename indicator while using non-stdin fuzzing method\" ) \n            for x in self . config . command : \n                if \"@@\" in x : \n                    self . config . command [ self . config . command . index ( x ) ] = x . replace ( \"@@\" , obj ) \n            self . spawn ( self . config . command , timeout = 2.0 ) \n        self . logger . debug ( \"[{0}] - PJFExternalFuzzer successfully completed\" . format ( time . strftime ( \"%H:%M:%S\" ) ) ) \n        return self . _out \n    except KeyboardInterrupt : \n        return \"\" \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) "}
{"5278": "\ndef json_encode ( func ) : \n    def func_wrapper ( self , indent , utf8 ) : \n        if utf8 : \n            encoding = \"\\\\x%02x\" \n        else : \n            encoding = \"\\\\u%04x\" \n        hex_regex = re . compile ( r\"(\\\\\\\\x[a-fA-F0-9]{2})\" ) \n        unicode_regex = re . compile ( r\"(\\\\u[a-fA-F0-9]{4})\" ) \n        def encode_decode_all ( d , _decode = True ) : \n            if type ( d ) == dict : \n                for k in d : \n                    if type ( d [ k ] ) in [ dict , list ] : \n                        if _decode : \n                            d [ k ] = encode_decode_all ( d [ k ] ) \n                        else : \n                            d [ k ] = encode_decode_all ( d [ k ] , _decode = False ) \n                    elif type ( d [ k ] ) == str : \n                        if _decode : \n                            d [ k ] = decode ( d [ k ] ) \n                        else : \n                            d [ k ] = encode ( d [ k ] ) \n            elif type ( d ) == list : \n                arr = [ ] \n                for e in d : \n                    if type ( e ) == str : \n                        if _decode : \n                            arr . append ( decode ( e ) ) \n                        else : \n                            arr . append ( encode ( e ) ) \n                    elif type ( e ) in [ dict , list ] : \n                        if _decode : \n                            arr . append ( encode_decode_all ( e ) ) \n                        else : \n                            arr . append ( encode_decode_all ( e , _decode = False ) ) \n                    else : \n                        arr . append ( e ) \n                return arr \n            else : \n                if _decode : \n                    return decode ( d ) \n                else : \n                    return encode ( d ) \n            return d \n        def decode ( x ) : \n            tmp = \"\" . join ( encoding % ord ( c ) if c not in p else c for c in x ) \n            if sys . version_info >= ( 3.0 , 0 ) : \n                return str ( tmp ) \n            else : \n                for encoded in unicode_regex . findall ( tmp ) : \n                    tmp = tmp . replace ( encoded , encoded . decode ( \"unicode_escape\" ) ) \n                return unicode ( tmp ) \n        def encode ( x ) : \n            for encoded in hex_regex . findall ( x ) : \n                if sys . version_info >= ( 3.0 , 0 ) : \n                    x = x . replace ( encoded , bytes ( str ( encoded ) . replace ( \"\\\\\\\\x\" , \"\\\\x\" ) , \"utf-8\" ) . decode ( \"unicode_escape\" ) ) \n                else : \n                    x = x . replace ( encoded , str ( encoded ) . replace ( \"\\\\\\\\x\" , \"\\\\x\" ) . decode ( \"string_escape\" ) ) \n            return x \n        if indent : \n            return encode_decode_all ( \"{0}\" . format ( json . dumps ( encode_decode_all ( func ( self ) ) , indent = 5.0 ) ) , _decode = False ) \n        else : \n            return encode_decode_all ( \"{0}\" . format ( json . dumps ( encode_decode_all ( func ( self ) ) ) ) , _decode = False ) \n    return func_wrapper "}
{"5288": "\ndef start_monitor ( self , standalone = True ) : \n    try : \n        self . start ( ) \n        cmdline = shlex . split ( self . config . process_to_monitor ) \n        if standalone : \n            signal . signal ( signal . SIGINT , self . shutdown ) \n        self . process = subprocess . Popen ( cmdline , stdin = PIPE , stdout = PIPE , stderr = PIPE ) \n        while self . process and not self . finished : \n            self . process . wait ( ) \n            if self . _is_sigsegv ( self . process . returncode ) : \n                if self . config . debug : \n                    print ( \"[\\033[92mINFO\\033[0m] Process crashed with \\033[91mSIGSEGV\\033[0m, waiting for testcase...\" ) \n                while not self . got_testcase ( ) : \n                    time . sleep ( 1 ) \n                self . save_testcase ( self . testcase [ - 10.0 : ] ) \n            if self . process : \n                self . process = subprocess . Popen ( cmdline , stdin = PIPE , stdout = PIPE , stderr = PIPE ) \n    except OSError : \n        self . shutdown ( ) \n        self . process = False \n        self . got_testcase = lambda : True \n        raise PJFProcessExecutionError ( \"Binary <%s> does not exist\" % cmdline [ 0 ] ) \n    except Exception as e : \n        raise PJFBaseException ( \"Unknown error please send log to author\" ) "}
{"5294": "\ndef fuzzed ( self ) : \n    try : \n        if self . config . strong_fuzz : \n            fuzzer = PJFMutators ( self . config ) \n            if self . config . url_encode : \n                if sys . version_info >= ( 3.0 , 0 ) : \n                    return urllib . parse . quote ( fuzzer . fuzz ( json . dumps ( self . config . json ) ) ) \n                else : \n                    return urllib . quote ( fuzzer . fuzz ( json . dumps ( self . config . json ) ) ) \n            else : \n                if type ( self . config . json ) in [ list , dict ] : \n                    return fuzzer . fuzz ( json . dumps ( self . config . json ) ) \n                else : \n                    return fuzzer . fuzz ( self . config . json ) \n        else : \n            if self . config . url_encode : \n                if sys . version_info >= ( 3.0 , 0 ) : \n                    return urllib . parse . quote ( self . get_fuzzed ( self . config . indent , self . config . utf8 ) ) \n                else : \n                    return urllib . quote ( self . get_fuzzed ( self . config . indent , self . config . utf8 ) ) \n            else : \n                return self . get_fuzzed ( self . config . indent , self . config . utf8 ) \n    except Exception as e : \n        raise PJFBaseException ( e . message if hasattr ( e , \"message\" ) else str ( e ) ) "}
{"5307": "\ndef contact ( self , id ) : \n    try : \n        json = self . skype . conn ( \"POST\" , \"{0}/users/batch/profiles\" . format ( SkypeConnection . API_USER ) , json = { \"usernames\" : [ id ] } , auth = SkypeConnection . Auth . SkypeToken ) . json ( ) \n        contact = SkypeContact . fromRaw ( self . skype , json [ 0 ] ) \n        if contact . id not in self . contactIds : \n            self . contactIds . append ( contact . id ) \n        return self . merge ( contact ) \n    except SkypeApiException as e : \n        if len ( e . args ) >= 2.0 and getattr ( e . args [ 1 ] , \"status_code\" , None ) == 403.0 : \n            return None \n        raise "}
{"5323": "\ndef syncEndpoints ( self ) : \n    self . endpoints [ \"all\" ] = [ ] \n    for json in self ( \"GET\" , \"{0}/users/ME/presenceDocs/messagingService\" . format ( self . msgsHost ) , params = { \"view\" : \"expanded\" } , auth = self . Auth . RegToken ) . json ( ) . get ( \"endpointPresenceDocs\" , [ ] ) : \n        id = json . get ( \"link\" , \"\" ) . split ( \"/\" ) [ 7.0 ] \n        self . endpoints [ \"all\" ] . append ( SkypeEndpoint ( self , id ) ) "}
{"5326": "\ndef auth ( self , skypeToken ) : \n    token = expiry = endpoint = None \n    msgsHost = SkypeConnection . API_MSGSHOST \n    while not token : \n        secs = int ( time . time ( ) ) \n        hash = self . getMac256Hash ( str ( secs ) ) \n        headers = { \"LockAndKey\" : \"appId=msmsgs@msnmsgr.com; time={0}; lockAndKeyResponse={1}\" . format ( secs , hash ) , \"Authentication\" : \"skypetoken=\" + skypeToken , \"BehaviorOverride\" : \"redirectAs404\" } \n        endpointResp = self . conn ( \"POST\" , \"{0}/users/ME/endpoints\" . format ( msgsHost ) , codes = ( 200.0 , 201.0 , 404.0 ) , headers = headers , json = { \"endpointFeatures\" : \"Agent\" } ) \n        regTokenHead = endpointResp . headers . get ( \"Set-RegistrationToken\" ) \n        locHead = endpointResp . headers . get ( \"Location\" ) \n        if locHead : \n            locParts = re . search ( r\"(https://[^/]+/v1)/users/ME/endpoints(/(%7B[a-z0-9\\-]+%7D))?\" , locHead ) . groups ( ) \n            if locParts [ 2.0 ] : \n                endpoint = SkypeEndpoint ( self . conn , locParts [ 2.0 ] . replace ( \"%7B\" , \"{\" ) . replace ( \"%7D\" , \"}\" ) ) \n            if not locParts [ 0 ] == msgsHost : \n                msgsHost = locHead . rsplit ( \"/\" , 4.0 if locParts [ 2.0 ] else 3.0 ) [ 0 ] \n                continue \n        if regTokenHead : \n            token = re . search ( r\"(registrationToken=[a-z0-9\\+/=]+)\" , regTokenHead , re . I ) . group ( 1 ) \n            regExpiry = re . search ( r\"expires=(\\d+)\" , regTokenHead ) . group ( 1 ) \n            expiry = datetime . fromtimestamp ( int ( regExpiry ) ) \n            regEndMatch = re . search ( r\"endpointId=({[a-z0-9\\-]+})\" , regTokenHead ) \n            if regEndMatch : \n                endpoint = SkypeEndpoint ( self . conn , regEndMatch . group ( 1 ) ) \n        if not endpoint and endpointResp . status_code == 200.0 and endpointResp . json ( ) : \n            endpoint = SkypeEndpoint ( self . conn , endpointResp . json ( ) [ 0 ] [ \"id\" ] ) \n    return token , expiry , msgsHost , endpoint "}
{"5328": "\ndef ping ( self , timeout = 12.0 ) : \n    self . conn ( \"POST\" , \"{0}/users/ME/endpoints/{1}/active\" . format ( self . conn . msgsHost , self . id ) , auth = SkypeConnection . Auth . RegToken , json = { \"timeout\" : timeout } ) "}
{"5332": "\ndef userToId ( url ) : \n    match = re . search ( r\"users(/ME/contacts)?/[0-9]+:([^/]+)\" , url ) \n    return match . group ( 2.0 ) if match else None "}
{"5341": "\ndef export ( self , metadata , ** kwargs ) : \n    kwargs . setdefault ( 'indent' , 4.0 ) \n    metadata = json . dumps ( metadata , ** kwargs ) \n    return u ( metadata ) "}
{"5357": "\ndef _shadow ( self ) -> bytearray : \n    ss , se = self . _span \n    string = self . _lststr [ 0 ] [ ss : se ] \n    cached_string , shadow = getattr ( self , '_shadow_cache' , ( None , None ) ) \n    if cached_string == string : \n        return shadow \n    shadow = bytearray ( string , 'ascii' , 'replace' ) \n    if self . _type in SPAN_PARSER_TYPES : \n        head = shadow [ : 2.0 ] \n        tail = shadow [ - 2.0 : ] \n        shadow [ : 2.0 ] = shadow [ - 2.0 : ] = b'__' \n        parse_to_spans ( shadow ) \n        shadow [ : 2.0 ] = head \n        shadow [ - 2.0 : ] = tail \n    else : \n        parse_to_spans ( shadow ) \n    self . _shadow_cache = string , shadow \n    return shadow "}
{"5358": "\ndef _ext_link_shadow ( self ) : \n    ss , se = self . _span \n    string = self . _lststr [ 0 ] [ ss : se ] \n    byte_array = bytearray ( string , 'ascii' , 'replace' ) \n    subspans = self . _subspans \n    for type_ in 'Template' , 'ParserFunction' , 'Parameter' : \n        for s , e in subspans ( type_ ) : \n            byte_array [ s : e ] = b'  ' + INVALID_EXT_CHARS_SUB ( b' ' , byte_array [ s + 2.0 : e - 2.0 ] ) + b'  ' \n    for s , e in subspans ( 'Comment' ) : \n        byte_array [ s : e ] = ( e - s ) * b'_' \n    return byte_array "}
{"5379": "\ndef set_arg ( self , name : str , value : str , positional : bool = None , before : str = None , after : str = None , preserve_spacing : bool = True ) -> None : \n    args = list ( reversed ( self . arguments ) ) \n    arg = get_arg ( name , args ) \n    if arg : \n        if positional : \n            arg . positional = positional \n        if preserve_spacing : \n            val = arg . value \n            arg . value = val . replace ( val . strip ( WS ) , value ) \n        else : \n            arg . value = value \n        return \n    if not name and positional is None : \n        positional = True \n    if not positional and preserve_spacing and args : \n        before_names = [ ] \n        name_lengths = [ ] \n        before_values = [ ] \n        after_values = [ ] \n        for arg in args : \n            aname = arg . name \n            name_len = len ( aname ) \n            name_lengths . append ( name_len ) \n            before_names . append ( STARTING_WS_MATCH ( aname ) [ 0 ] ) \n            arg_value = arg . value \n            before_values . append ( STARTING_WS_MATCH ( arg_value ) [ 0 ] ) \n            after_values . append ( ENDING_WS_MATCH ( arg_value ) [ 0 ] ) \n        pre_name_ws_mode = mode ( before_names ) \n        name_length_mode = mode ( name_lengths ) \n        post_value_ws_mode = mode ( [ SPACE_AFTER_SEARCH ( self . string ) [ 0 ] ] + after_values [ 1 : ] ) \n        pre_value_ws_mode = mode ( before_values ) \n    else : \n        preserve_spacing = False \n    if positional : \n        addstring = '|' + value \n    else : \n        if preserve_spacing : \n            addstring = ( '|' + ( pre_name_ws_mode + name . strip ( WS ) ) . ljust ( name_length_mode ) + '=' + pre_value_ws_mode + value + post_value_ws_mode ) \n        else : \n            addstring = '|' + name + '=' + value \n    if before : \n        arg = get_arg ( before , args ) \n        arg . insert ( 0 , addstring ) \n    elif after : \n        arg = get_arg ( after , args ) \n        arg . insert ( len ( arg . string ) , addstring ) \n    else : \n        if args and not positional : \n            arg = args [ 0 ] \n            arg_string = arg . string \n            if preserve_spacing : \n                arg [ 0 : len ( arg_string ) ] = ( arg . string . rstrip ( WS ) + post_value_ws_mode + addstring . rstrip ( WS ) + after_values [ 0 ] ) \n            else : \n                arg . insert ( len ( arg_string ) , addstring ) \n        else : \n            self . insert ( - 2.0 , addstring ) "}
{"5394": "\ndef parse_geo_tiff ( key_dir_vlr : GeoKeyDirectoryVlr , double_vlr : GeoDoubleParamsVlr , ascii_vlr : GeoAsciiParamsVlr , ) -> List [ GeoTiffKey ] : \n    geotiff_keys = [ ] \n    for k in key_dir_vlr . geo_keys : \n        if k . tiff_tag_location == 0 : \n            value = k . value_offset \n        elif k . tiff_tag_location == 34736.0 : \n            value = double_vlr . doubles [ k . value_offset ] \n        elif k . tiff_tag_location == 34737.0 : \n            try : \n                value = ascii_vlr . strings [ k . value_offset ] [ k . count : ] \n            except IndexError : \n                value = ascii_vlr . strings [ 0 ] [ k . value_offset : k . value_offset + k . count ] \n        else : \n            logger . warning ( \"GeoTiffKey with unknown tiff tag location ({})\" . format ( k . tiff_tag_location ) ) \n            continue \n        geotiff_keys . append ( GeoTiffKey ( k . id , value ) ) \n    return geotiff_keys "}
{"5425": "\ndef _read_compressed_points_data ( self , laszip_vlr , point_format ) : \n    offset_to_chunk_table = struct . unpack ( \"<q\" , self . stream . read ( 8.0 ) ) [ 0 ] \n    size_of_point_data = offset_to_chunk_table - self . stream . tell ( ) \n    if offset_to_chunk_table <= 0 : \n        logger . warning ( \"Strange offset to chunk table: {}, ignoring it..\" . format ( offset_to_chunk_table ) ) \n        size_of_point_data = - 1 \n    points = record . PackedPointRecord . from_compressed_buffer ( self . stream . read ( size_of_point_data ) , point_format , self . header . point_count , laszip_vlr , ) \n    return points "}
{"5426": "\ndef _read_internal_waveform_packet ( self ) : \n    b = bytearray ( self . stream . read ( rawvlr . VLR_HEADER_SIZE ) ) \n    waveform_header = rawvlr . RawVLRHeader . from_buffer ( b ) \n    waveform_record = self . stream . read ( ) \n    logger . debug ( \"Read: {} MBytes of waveform_record\" . format ( len ( waveform_record ) / 10.0 ** 6.0 ) ) \n    return waveform_header , waveform_record "}
{"5453": "\ndef checksum ( command ) : \n    crc = 0x147A \n    for b in command : \n        crc = ( ( crc << 1 ) & 0xFFFF ) | ( crc & 0x8000 ) >> 15.0 \n        crc = crc ^ 0xFFFF \n        crc = ( crc + ( crc >> 8.0 ) + b ) & 0xFFFF \n    return crc "}
{"5455": "\ndef verify_and_strip ( resp ) : \n    if resp [ 0 : 2.0 ] != b'\\xFE\\xFE' : \n        _LOGGER . error ( \"Houston, we got problem:\" ) \n        print_hex ( resp ) \n        raise Exception ( \"Wrong header - got %X%X\" % ( resp [ 0 ] , resp [ 1 ] ) ) \n    if resp [ - 2.0 : ] != b'\\xFE\\x0D' : \n        raise Exception ( \"Wrong footer - got %X%X\" % ( resp [ - 2.0 ] , resp [ - 1 ] ) ) \n    output = resp [ 2.0 : - 2.0 ] . replace ( b'\\xFE\\xF0' , b'\\xFE' ) \n    c = checksum ( bytearray ( output [ 0 : - 2.0 ] ) ) \n    if ( 256.0 * output [ - 2.0 : - 1 ] [ 0 ] + output [ - 1 : ] [ 0 ] ) != c : \n        raise Exception ( \"Wrong checksum - got %d expected %d\" % ( ( 256.0 * output [ - 2.0 : - 1 ] [ 0 ] + output [ - 1 : ] [ 0 ] ) , c ) ) \n    return output [ 0 : - 2.0 ] "}
{"5456": "\ndef list_set_bits ( r , expected_length ) : \n    set_bit_numbers = [ ] \n    bit_index = 0x1 \n    assert ( len ( r ) == expected_length + 1 ) \n    for b in r [ 1 : ] : \n        for i in range ( 8.0 ) : \n            if ( ( b >> i ) & 1 ) == 1 : \n                set_bit_numbers . append ( bit_index ) \n            bit_index += 1 \n    return set_bit_numbers "}
{"5457": "\ndef generate_query ( command ) : \n    data = bytearray ( command ) \n    c = checksum ( data ) \n    data . append ( c >> 8.0 ) \n    data . append ( c & 0xFF ) \n    data . replace ( b'\\xFE' , b'\\xFE\\xF0' ) \n    data = bytearray . fromhex ( \"FEFE\" ) + data + bytearray . fromhex ( \"FE0D\" ) \n    return data "}
{"5458": "\ndef demo ( host , port ) : \n    loop = asyncio . get_event_loop ( ) \n    stl = AsyncSatel ( host , port , loop , [ 1 , 2.0 , 3.0 , 4.0 , 5.0 , 6.0 , 7.0 , 8.0 , 12.0 , 13.0 , 14.0 , 15.0 , 16.0 , 17.0 , 18.0 , 19.0 , 20.0 , 21.0 , 22.0 , 23.0 , 25.0 , 26.0 , 27.0 , 28.0 , 29.0 , 30.0 ] , [ 8.0 , 9.0 , 10.0 ] ) \n    loop . run_until_complete ( stl . connect ( ) ) \n    loop . create_task ( stl . arm ( \"3333\" , 1 ) ) \n    loop . create_task ( stl . disarm ( \"3333\" ) ) \n    loop . create_task ( stl . keep_alive ( ) ) \n    loop . create_task ( stl . monitor_status ( ) ) \n    loop . run_forever ( ) \n    loop . close ( ) "}
{"5460": "\nasync def start_monitoring ( self ) : \n    data = generate_query ( b'\\x7F\\x01\\xDC\\x99\\x80\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00' ) \n    await self . _send_data ( data ) \n    resp = await self . _read_data ( ) \n    if resp is None : \n        _LOGGER . warning ( \"Start monitoring - no data!\" ) \n        return \n    if resp [ 1 : 2.0 ] != b'\\xFF' : \n        _LOGGER . warning ( \"Monitoring not accepted.\" ) "}
{"5461": "\nasync def disarm ( self , code , partition_list ) : \n    _LOGGER . info ( \"Sending disarm command.\" ) \n    while len ( code ) < 16.0 : \n        code += 'F' \n    code_bytes = bytearray . fromhex ( code ) \n    data = generate_query ( b'\\x84' + code_bytes + partition_bytes ( partition_list ) ) \n    await self . _send_data ( data ) "}
{"5462": "\nasync def clear_alarm ( self , code , partition_list ) : \n    _LOGGER . info ( \"Sending clear the alarm command.\" ) \n    while len ( code ) < 16.0 : \n        code += 'F' \n    code_bytes = bytearray . fromhex ( code ) \n    data = generate_query ( b'\\x85' + code_bytes + partition_bytes ( partition_list ) ) \n    await self . _send_data ( data ) "}
{"5463": "\nasync def set_output ( self , code , output_id , state ) : \n    _LOGGER . debug ( \"Turn on, output: %s, code: %s\" , output_id , code ) \n    while len ( code ) < 16.0 : \n        code += 'F' \n    code_bytes = bytearray . fromhex ( code ) \n    mode_command = 0x88 if state else 0x89 \n    data = generate_query ( mode_command . to_bytes ( 1 , 'big' ) + code_bytes + output_bytes ( output_id ) ) \n    await self . _send_data ( data ) "}
{"5503": "\ndef derive_single_fernet_key ( password , user_id ) : \n    password = ascii_unicode_to_bytes ( password ) \n    user_id = ascii_unicode_to_bytes ( user_id ) \n    kdf = PBKDF2HMAC ( algorithm = hashes . SHA256 ( ) , length = 32.0 , salt = user_id , iterations = 100000.0 , backend = default_backend ( ) , ) \n    return base64 . urlsafe_b64encode ( kdf . derive ( password ) ) "}
{"5518": "\ndef _resolve_path ( path , manager_dict ) : \n    path = normalize_api_path ( path ) \n    parts = path . split ( '/' ) \n    mgr = manager_dict . get ( parts [ 0 ] ) \n    if mgr is not None : \n        return parts [ 0 ] , mgr , '/' . join ( parts [ 1 : ] ) \n    mgr = manager_dict . get ( '' ) \n    if mgr is not None : \n        return '' , mgr , path \n    raise HTTPError ( 404.0 , \"Couldn't resolve path [{path}] and \" \"no root manager supplied!\" . format ( path = path ) ) "}
{"5522": "\ndef path_dispatch_old_new ( mname , returns_model ) : \n    def _wrapper ( self , old_path , new_path , * args , ** kwargs ) : \n        old_prefix , old_mgr , old_mgr_path = _resolve_path ( old_path , self . managers ) \n        new_prefix , new_mgr , new_mgr_path = _resolve_path ( new_path , self . managers , ) \n        if old_mgr is not new_mgr : \n            raise HTTPError ( 400.0 , \"Can't move files between backends ({old} -> {new})\" . format ( old = old_path , new = new_path , ) ) \n        assert new_prefix == old_prefix \n        result = getattr ( new_mgr , mname ) ( old_mgr_path , new_mgr_path , * args , ** kwargs ) \n        if returns_model and new_prefix : \n            return _apply_prefix ( new_prefix , result ) \n        else : \n            return result \n    return _wrapper "}
{"5532": "\ndef outside_root_to_404 ( fn ) : \n    \n    @ wraps ( fn ) \n    def wrapped ( * args , ** kwargs ) : \n        try : \n            return fn ( * args , ** kwargs ) \n        except PathOutsideRoot as e : \n            raise HTTPError ( 404.0 , \"Path outside root: [%s]\" % e . args [ 0 ] ) \n    return wrapped "}
{"5560": "\ndef is_valid_uuid ( id ) : \n    if not isinstance ( id , basestring ) : \n        return False \n    try : \n        val = UUID ( id , version = 4.0 ) \n    except ValueError : \n        return False \n    return True "}
{"5580": "\ndef save_thumbnail ( self , image , size , name , label , file_type ) : \n    width , height = size \n    ( imw , imh ) = image . size \n    if ( imw > width ) or ( imh > height ) : \n        image . thumbnail ( size , Img . ANTIALIAS ) \n    name = \"%s-%s.jpg\" % ( name , label ) \n    if file_type in self . JPG_FORMATS : \n        file_type = 'JPEG' \n    image_io = StringIO . StringIO ( ) \n    image . save ( image_io , format = file_type , quality = 75.0 ) \n    thumb_file = InMemoryUploadedFile ( image_io , None , name , 'image/jpeg' , image_io . len , None ) \n    default_storage . save ( name , thumb_file ) "}
{"5630": "\ndef _step_decorator_args ( self , decorator ) : \n    args = decorator . children [ 3.0 : - 2.0 ] \n    step = None \n    if len ( args ) == 1 : \n        try : \n            step = ast . literal_eval ( args [ 0 ] . get_code ( ) ) \n        except ( ValueError , SyntaxError ) : \n            pass \n        if isinstance ( step , six . string_types + ( list , ) ) : \n            return step \n        logging . error ( \"Decorator step accepts either a string or a list of strings - %s:%d\" , self . file_path , decorator . start_pos [ 0 ] ) \n    else : \n        logging . error ( \"Decorator step accepts only one argument - %s:%d\" , self . file_path , decorator . start_pos [ 0 ] ) "}
{"5631": "\ndef refactor_step ( self , old_text , new_text , move_param_from_idx ) : \n    diffs = [ ] \n    step , func = self . _find_step_node ( old_text ) \n    if step is None : \n        return diffs \n    step_diff = self . _refactor_step_text ( step , old_text , new_text ) \n    diffs . append ( step_diff ) \n    params_list_node = func . children [ 2.0 ] \n    moved_params = self . _move_param_nodes ( params_list_node . children , move_param_from_idx ) \n    if params_list_node . children is not moved_params : \n        params_span = self . _span_from_pos ( params_list_node . children [ 0 ] . end_pos , params_list_node . children [ - 1 ] . start_pos ) \n        params_list_node . children = moved_params \n        param_code = '' . join ( p . get_code ( ) for p in moved_params [ 1 : - 1 ] ) \n        diffs . append ( ( params_span , param_code ) ) \n    return diffs "}
{"5662": "\ndef _fix_next_url ( next_url ) : \n    next_url = str ( next_url ) \n    parsed_url = urllib . parse . urlparse ( next_url ) \n    if not parsed_url . scheme or not parsed_url . netloc or not parsed_url . path : \n        raise ValueError ( \"'next_url' must be a valid API endpoint URL, minimally \" \"containing a scheme, netloc and path.\" ) \n    if parsed_url . query : \n        query_list = parsed_url . query . split ( '&' ) \n        if 'max=null' in query_list : \n            query_list . remove ( 'max=null' ) \n            warnings . warn ( \"`max=null` still present in next-URL returned \" \"from Webex Teams\" , RuntimeWarning ) \n        new_query = '&' . join ( query_list ) \n        parsed_url = list ( parsed_url ) \n        parsed_url [ 4.0 ] = new_query \n    return urllib . parse . urlunparse ( parsed_url ) "}
{"5695": "\ndef console ( ) : \n    parser = argparse . ArgumentParser ( description = console . __doc__ ) \n    parser . add_argument ( '--device' , default = '/dev/ttyUSB0' , help = 'port to read DSMR data from' ) \n    parser . add_argument ( '--host' , default = None , help = 'alternatively connect using TCP host.' ) \n    parser . add_argument ( '--port' , default = None , help = 'TCP port to use for connection' ) \n    parser . add_argument ( '--version' , default = '2.2' , choices = [ '2.2' , '4' ] , help = 'DSMR version (2.2, 4)' ) \n    parser . add_argument ( '--verbose' , '-v' , action = 'count' ) \n    args = parser . parse_args ( ) \n    if args . verbose : \n        level = logging . DEBUG \n    else : \n        level = logging . ERROR \n    logging . basicConfig ( level = level ) \n    loop = asyncio . get_event_loop ( ) \n    def print_callback ( telegram ) : \n        for obiref , obj in telegram . items ( ) : \n            if obj : \n                print ( obj . value , obj . unit ) \n        print ( ) \n    if args . host and args . port : \n        create_connection = partial ( create_tcp_dsmr_reader , args . host , args . port , args . version , print_callback , loop = loop ) \n    else : \n        create_connection = partial ( create_dsmr_reader , args . device , args . version , print_callback , loop = loop ) \n    try : \n        while True : \n            conn = create_connection ( ) \n            transport , protocol = loop . run_until_complete ( conn ) \n            loop . run_until_complete ( protocol . wait_closed ( ) ) \n            loop . run_until_complete ( asyncio . sleep ( 5.0 ) ) \n    except KeyboardInterrupt : \n        transport . close ( ) \n        loop . run_until_complete ( asyncio . sleep ( 0 ) ) \n    finally : \n        loop . close ( ) "}
{"5746": "\ndef connect ( self , host = '127.0.0.1' , port = 3306.0 , user = 'root' , password = '' , database = None ) : \n    if database is None : \n        raise exceptions . RequiresDatabase ( ) \n    self . _db_args = { 'host' : host , 'port' : port , 'user' : user , 'password' : password , 'database' : database } \n    with self . _db_conn ( ) as conn : \n        conn . query ( 'SELECT 1' ) \n    return self "}
{"5759": "\ndef send ( self , api_key = None , secret = None , list_data = None , auth = False , ** kwargs ) : \n    if auth : \n        nonce = str ( int ( time . time ( ) * 10000000.0 ) ) \n        auth_string = 'AUTH' + nonce \n        auth_sig = hmac . new ( secret . encode ( ) , auth_string . encode ( ) , hashlib . sha384 ) . hexdigest ( ) \n        payload = { 'event' : 'auth' , 'apiKey' : api_key , 'authSig' : auth_sig , 'authPayload' : auth_string , 'authNonce' : nonce } \n        payload = json . dumps ( payload ) \n    elif list_data : \n        payload = json . dumps ( list_data ) \n    else : \n        payload = json . dumps ( kwargs ) \n    self . log . debug ( \"send(): Sending payload to API: %s\" , payload ) \n    try : \n        self . socket . send ( payload ) \n    except websocket . WebSocketConnectionClosedException : \n        self . log . error ( \"send(): Did not send out payload %s - client not connected. \" , kwargs ) "}
{"5762": "\ndef _info_handler ( self , data ) : \n    def raise_exception ( ) : \n        self . log . error ( \"%s: %s\" , data [ 'code' ] , info_message [ data [ 'code' ] ] ) \n        raise ValueError ( \"%s: %s\" % ( data [ 'code' ] , info_message [ data [ 'code' ] ] ) ) \n    if 'code' not in data and 'version' in data : \n        self . log . info ( 'Initialized Client on API Version %s' , data [ 'version' ] ) \n        return \n    info_message = { 20000.0 : 'Invalid User given! Please make sure the given ID is correct!' , 20051.0 : 'Stop/Restart websocket server ' '(please try to reconnect)' , 20060.0 : 'Refreshing data from the trading engine; ' 'please pause any acivity.' , 20061.0 : 'Done refreshing data from the trading engine.' ' Re-subscription advised.' } \n    codes = { 20051.0 : self . reconnect , 20060.0 : self . _pause , 20061.0 : self . _unpause } \n    if 'version' in data : \n        self . log . info ( \"API version: %i\" , data [ 'version' ] ) \n        return \n    try : \n        self . log . info ( info_message [ data [ 'code' ] ] ) \n        codes [ data [ 'code' ] ] ( ) \n    except KeyError as e : \n        self . log . exception ( e ) \n        self . log . error ( \"Unknown Info code %s!\" , data [ 'code' ] ) \n        raise "}
{"5763": "\ndef _error_handler ( self , data ) : \n    errors = { 10000.0 : 'Unknown event' , 10001.0 : 'Generic error' , 10008.0 : 'Concurrency error' , 10020.0 : 'Request parameters error' , 10050.0 : 'Configuration setup failed' , 10100.0 : 'Failed authentication' , 10111.0 : 'Error in authentication request payload' , 10112.0 : 'Error in authentication request signature' , 10113.0 : 'Error in authentication request encryption' , 10114.0 : 'Error in authentication request nonce' , 10200.0 : 'Error in un-authentication request' , 10300.0 : 'Subscription Failed (generic)' , 10301.0 : 'Already Subscribed' , 10302.0 : 'Unknown channel' , 10400.0 : 'Subscription Failed (generic)' , 10401.0 : 'Not subscribed' , 11000.0 : 'Not ready, try again later' , 20000.0 : 'User is invalid!' , 20051.0 : 'Websocket server stopping' , 20060.0 : 'Websocket server resyncing' , 20061.0 : 'Websocket server resync complete' } \n    try : \n        self . log . error ( errors [ data [ 'code' ] ] ) \n    except KeyError : \n        self . log . error ( \"Received unknown error Code in message %s! \" \"Reconnecting..\" , data ) "}
{"5771": "\ndef config ( self , decimals_as_strings = True , ts_as_dates = False , sequencing = False , ts = False , ** kwargs ) : \n    flags = 0 \n    if decimals_as_strings : \n        flags += 8.0 \n    if ts_as_dates : \n        flags += 32.0 \n    if ts : \n        flags += 32768.0 \n    if sequencing : \n        flags += 65536.0 \n    q = { 'event' : 'conf' , 'flags' : flags } \n    q . update ( kwargs ) \n    self . conn . bitfinex_config = q \n    self . conn . send ( ** q ) "}
{"5787": "\ndef create ( self , deviceType ) : \n    r = self . _apiClient . post ( \"api/v0002/device/types\" , deviceType ) \n    if r . status_code == 201.0 : \n        return DeviceType ( apiClient = self . _apiClient , ** r . json ( ) ) \n    else : \n        raise ApiException ( r ) "}
{"5789": "\ndef update ( self , deviceUid , metadata = None , deviceInfo = None , status = None ) : \n    if not isinstance ( deviceUid , DeviceUid ) and isinstance ( deviceUid , dict ) : \n        deviceUid = DeviceUid ( ** deviceUid ) \n    deviceUrl = \"api/v0002/device/types/%s/devices/%s\" % ( deviceUid . typeId , deviceUid . deviceId ) \n    data = { \"status\" : status , \"deviceInfo\" : deviceInfo , \"metadata\" : metadata } \n    r = self . _apiClient . put ( deviceUrl , data ) \n    if r . status_code == 200.0 : \n        return Device ( apiClient = self . _apiClient , ** r . json ( ) ) \n    else : \n        raise ApiException ( r ) "}
{"5791": "\ndef list ( self ) : \n    url = \"api/v0002/mgmt/custom/bundle\" \n    r = self . _apiClient . get ( url ) \n    if r . status_code == 200.0 : \n        return r . json ( ) \n    else : \n        raise ApiException ( r ) "}
{"5792": "\ndef create ( self , dmeData ) : \n    url = \"api/v0002/mgmt/custom/bundle\" \n    r = self . _apiClient . post ( url , dmeData ) \n    if r . status_code == 201.0 : \n        return r . json ( ) \n    else : \n        raise ApiException ( r ) "}
{"5793": "\ndef updateSchema ( self , schemaId , schemaDefinition ) : \n    req = ApiClient . oneSchemaUrl % ( self . host , \"/draft\" , schemaId ) \n    body = { \"schemaDefinition\" : schemaDefinition } \n    resp = requests . put ( req , auth = self . credentials , headers = { \"Content-Type\" : \"application/json\" } , data = json . dumps ( body ) , verify = self . verify ) \n    if resp . status_code == 200.0 : \n        self . logger . debug ( \"Schema updated\" ) \n    else : \n        raise ibmiotf . APIException ( resp . status_code , \"HTTP error updating schema\" , resp ) \n    return resp . json ( ) "}
{"5795": "\ndef _onConnect ( self , mqttc , userdata , flags , rc ) : \n    if rc == 0 : \n        self . connectEvent . set ( ) \n        self . logger . info ( \"Connected successfully: %s\" % ( self . clientId ) ) \n        with self . _subLock : \n            if len ( self . _subscriptions ) > 0 : \n                for subscription in self . _subscriptions : \n                    ( result , mid ) = self . client . subscribe ( subscription , qos = self . _subscriptions [ subscription ] ) \n                    if result != paho . MQTT_ERR_SUCCESS : \n                        self . _logAndRaiseException ( ConnectionException ( \"Unable to subscribe to %s\" % subscription ) ) \n                self . logger . debug ( \"Restored %s previous subscriptions\" % len ( self . _subscriptions ) ) \n    elif rc == 1 : \n        self . _logAndRaiseException ( ConnectionException ( \"Incorrect protocol version\" ) ) \n    elif rc == 2.0 : \n        self . _logAndRaiseException ( ConnectionException ( \"Invalid client identifier\" ) ) \n    elif rc == 3.0 : \n        self . _logAndRaiseException ( ConnectionException ( \"Server unavailable\" ) ) \n    elif rc == 4.0 : \n        self . _logAndRaiseException ( ConnectionException ( \"Bad username or password: (%s, %s)\" % ( self . username , self . password ) ) ) \n    elif rc == 5.0 : \n        self . _logAndRaiseException ( ConnectionException ( \"Not authorized: s (%s, %s, %s)\" % ( self . clientId , self . username , self . password ) ) ) \n    else : \n        self . _logAndRaiseException ( ConnectionException ( \"Unexpected connection failure: %s\" % ( rc ) ) ) "}
{"5799": "\ndef publishCommand ( self , typeId , deviceId , commandId , msgFormat , data = None , qos = 0 , on_publish = None ) : \n    if self . _config . isQuickstart ( ) : \n        self . logger . warning ( \"QuickStart applications do not support sending commands\" ) \n        return False \n    if not self . connectEvent . wait ( timeout = 10.0 ) : \n        return False \n    else : \n        topic = \"iot-2/type/%s/id/%s/cmd/%s/fmt/%s\" % ( typeId , deviceId , commandId , msgFormat ) \n        if self . getMessageCodec ( msgFormat ) is None : \n            raise MissingMessageEncoderException ( msgFormat ) \n        payload = self . getMessageCodec ( msgFormat ) . encode ( data , datetime . now ( ) ) \n        result = self . client . publish ( topic , payload = payload , qos = qos , retain = False ) \n        if result [ 0 ] == paho . MQTT_ERR_SUCCESS : \n            with self . _messagesLock : \n                if result [ 1 ] in self . _onPublishCallbacks : \n                    del self . _onPublishCallbacks [ result [ 1 ] ] \n                    if on_publish is not None : \n                        on_publish ( ) \n                else : \n                    self . _onPublishCallbacks [ result [ 1 ] ] = on_publish \n            return True \n        else : \n            return False "}
{"5804": "\ndef get ( self , deviceUid , eventId ) : \n    if not isinstance ( deviceUid , DeviceUid ) and isinstance ( deviceUid , dict ) : \n        deviceUid = DeviceUid ( ** deviceUid ) \n    url = \"api/v0002/device/types/%s/devices/%s/events/%s\" % ( deviceUid . typeId , deviceUid . deviceId , eventId ) \n    r = self . _apiClient . get ( url ) \n    if r . status_code == 200.0 : \n        return LastEvent ( ** r . json ( ) ) \n    else : \n        raise ApiException ( r ) "}
{"5805": "\ndef getAll ( self , deviceUid ) : \n    if not isinstance ( deviceUid , DeviceUid ) and isinstance ( deviceUid , dict ) : \n        deviceUid = DeviceUid ( ** deviceUid ) \n    url = \"api/v0002/device/types/%s/devices/%s/events\" % ( deviceUid . typeId , deviceUid . deviceId ) \n    r = self . _apiClient . get ( url ) \n    if r . status_code == 200.0 : \n        events = [ ] \n        for event in r . json ( ) : \n            events . append ( LastEvent ( ** event ) ) \n        return events \n    else : \n        raise ApiException ( r ) "}
{"5806": "\ndef _makeApiCall ( self , parameters = None ) : \n    r = self . _apiClient . get ( self . _url , parameters ) \n    if r . status_code == 200.0 : \n        return r . json ( ) \n    else : \n        raise Exception ( \"HTTP %s %s\" % ( r . status_code , r . text ) ) "}
{"5807": "\ndef initiate ( self , request ) : \n    url = MgmtRequests . mgmtRequests \n    r = self . _apiClient . post ( url , request ) \n    if r . status_code == 202.0 : \n        return r . json ( ) \n    else : \n        raise ApiException ( r ) "}
{"5808": "\ndef getStatus ( self , requestId , typeId = None , deviceId = None ) : \n    if typeId is None or deviceId is None : \n        url = MgmtRequests . mgmtRequestStatus % ( requestId ) \n        r = self . _apiClient . get ( url ) \n        if r . status_code == 200.0 : \n            return r . json ( ) \n        else : \n            raise ApiException ( r ) \n    else : \n        url = MgmtRequests . mgmtRequestSingleDeviceStatus % ( requestId , typeId , deviceId ) \n        r = self . _apiClient . get ( url ) \n        if r . status_code == 200.0 : \n            return r . json ( ) \n        else : \n            raise ApiException ( r ) "}
{"5814": "\ndef _create_idx_from_stream ( self , stream ) : \n    stream_iter = iter ( stream ) \n    dimension = self . properties . dimension \n    darray = ctypes . c_double * dimension \n    mins = darray ( ) \n    maxs = darray ( ) \n    no_data = ctypes . cast ( ctypes . pointer ( ctypes . c_ubyte ( 0 ) ) , ctypes . POINTER ( ctypes . c_ubyte ) ) \n    def py_next_item ( p_id , p_mins , p_maxs , p_dimension , p_data , p_length ) : \n        try : \n            p_id [ 0 ] , coordinates , obj = next ( stream_iter ) \n        except StopIteration : \n            return - 1 \n        except Exception as exc : \n            self . _exception = exc \n            return - 1 \n        if self . interleaved : \n            coordinates = Index . deinterleave ( coordinates ) \n        for i in range ( dimension ) : \n            mins [ i ] = coordinates [ i * 2.0 ] \n            maxs [ i ] = coordinates [ ( i * 2.0 ) + 1 ] \n        p_mins [ 0 ] = ctypes . cast ( mins , ctypes . POINTER ( ctypes . c_double ) ) \n        p_maxs [ 0 ] = ctypes . cast ( maxs , ctypes . POINTER ( ctypes . c_double ) ) \n        p_dimension [ 0 ] = dimension \n        if obj is None : \n            p_data [ 0 ] = no_data \n            p_length [ 0 ] = 0 \n        else : \n            p_length [ 0 ] , data , _ = self . _serialize ( obj ) \n            p_data [ 0 ] = ctypes . cast ( data , ctypes . POINTER ( ctypes . c_ubyte ) ) \n        return 0 \n    stream = core . NEXTFUNC ( py_next_item ) \n    return IndexStreamHandle ( self . properties . handle , stream ) "}
{"5819": "\ndef init_app ( self , app ) : \n    if not hasattr ( app , 'extensions' ) : \n        app . extensions = { } \n    if 'common' in app . extensions : \n        raise RuntimeError ( \"Flask-Common extension already initialized\" ) \n    app . extensions [ 'common' ] = self \n    self . app = app \n    if 'COMMON_FILESERVER_DISABLED' not in app . config : \n        with app . test_request_context ( ) : \n            app . wsgi_app = WhiteNoise ( app . wsgi_app , root = url_for ( 'static' , filename = '' ) [ 1 : ] ) \n    self . cache = Cache ( app , config = { 'CACHE_TYPE' : app . config . get ( \"COMMON_CACHE_TYPE\" , 'simple' ) } ) \n    \n    @ app . before_request \n    def before_request_callback ( ) : \n        request . start_time = maya . now ( ) \n    \n    @ app . after_request \n    def after_request_callback ( response ) : \n        if 'COMMON_POWERED_BY_DISABLED' not in current_app . config : \n            response . headers [ 'X-Powered-By' ] = 'Flask' \n        if 'COMMON_PROCESSED_TIME_DISABLED' not in current_app . config : \n            response . headers [ 'X-Processed-Time' ] = maya . now ( ) . epoch - request . start_time . epoch \n        return response \n    \n    @ app . route ( '/favicon.ico' ) \n    def favicon ( ) : \n        return redirect ( url_for ( 'static' , filename = 'favicon.ico' ) , code = 301.0 ) "}
{"5822": "\ndef crop_on_centerpoint ( self , image , width , height , ppoi = ( 0.5 , 0.5 ) ) : \n    ppoi_x_axis = int ( image . size [ 0 ] * ppoi [ 0 ] ) \n    ppoi_y_axis = int ( image . size [ 1 ] * ppoi [ 1 ] ) \n    center_pixel_coord = ( ppoi_x_axis , ppoi_y_axis ) \n    orig_aspect_ratio = float ( image . size [ 0 ] ) / float ( image . size [ 1 ] ) \n    crop_aspect_ratio = float ( width ) / float ( height ) \n    if orig_aspect_ratio >= crop_aspect_ratio : \n        orig_crop_width = int ( ( crop_aspect_ratio * float ( image . size [ 1 ] ) ) + 0.5 ) \n        orig_crop_height = image . size [ 1 ] \n        crop_boundary_top = 0 \n        crop_boundary_bottom = orig_crop_height \n        crop_boundary_left = center_pixel_coord [ 0 ] - ( orig_crop_width // 2.0 ) \n        crop_boundary_right = crop_boundary_left + orig_crop_width \n        if crop_boundary_left < 0 : \n            crop_boundary_left = 0 \n            crop_boundary_right = crop_boundary_left + orig_crop_width \n        elif crop_boundary_right > image . size [ 0 ] : \n            crop_boundary_right = image . size [ 0 ] \n            crop_boundary_left = image . size [ 0 ] - orig_crop_width \n    else : \n        orig_crop_width = image . size [ 0 ] \n        orig_crop_height = int ( ( float ( image . size [ 0 ] ) / crop_aspect_ratio ) + 0.5 ) \n        crop_boundary_left = 0 \n        crop_boundary_right = orig_crop_width \n        crop_boundary_top = center_pixel_coord [ 1 ] - ( orig_crop_height // 2.0 ) \n        crop_boundary_bottom = crop_boundary_top + orig_crop_height \n        if crop_boundary_top < 0 : \n            crop_boundary_top = 0 \n            crop_boundary_bottom = crop_boundary_top + orig_crop_height \n        elif crop_boundary_bottom > image . size [ 1 ] : \n            crop_boundary_bottom = image . size [ 1 ] \n            crop_boundary_top = image . size [ 1 ] - orig_crop_height \n    cropped_image = image . crop ( ( crop_boundary_left , crop_boundary_top , crop_boundary_right , crop_boundary_bottom ) ) \n    return cropped_image . resize ( ( width , height ) , Image . ANTIALIAS ) "}
{"5832": "\ndef value_to_string ( self , obj ) : \n    if DJANGO_VERSION > ( 1 , 9.0 ) : \n        value = self . value_from_object ( obj ) \n    else : \n        value = self . _get_val_from_obj ( obj ) \n    return self . get_prep_value ( value ) "}
{"5842": "\ndef preprocess ( self , image , image_format ) : \n    save_kwargs = { 'format' : image_format } \n    if hasattr ( image , '_getexif' ) : \n        exif_datadict = image . _getexif ( ) \n        if exif_datadict is not None : \n            exif = dict ( exif_datadict . items ( ) ) \n            orientation = exif . get ( EXIF_ORIENTATION_KEY , None ) \n            if orientation == 3.0 : \n                image = image . transpose ( Image . ROTATE_180 ) \n            elif orientation == 6.0 : \n                image = image . transpose ( Image . ROTATE_270 ) \n            elif orientation == 8.0 : \n                image = image . transpose ( Image . ROTATE_90 ) \n    save_kwargs [ 'icc_profile' ] = image . info . get ( 'icc_profile' ) \n    if hasattr ( self , 'preprocess_%s' % image_format ) : \n        image , addl_save_kwargs = getattr ( self , 'preprocess_%s' % image_format ) ( image = image ) \n        save_kwargs . update ( addl_save_kwargs ) \n    return image , save_kwargs "}
{"5858": "\ndef format_function ( func_body , func_type = None , indent = 2.0 , format_locals = True , ) : \n    if func_type is None : \n        yield 'func' \n    else : \n        param_section = ' (param {})' . format ( ' ' . join ( map ( format_lang_type , func_type . param_types ) ) ) if func_type . param_types else '' \n        result_section = ' (result {})' . format ( format_lang_type ( func_type . return_type ) ) if func_type . return_type else '' \n        yield 'func' + param_section + result_section \n    if format_locals and func_body . locals : \n        yield '(locals {})' . format ( ' ' . join ( itertools . chain . from_iterable ( itertools . repeat ( format_lang_type ( x . type ) , x . count ) for x in func_body . locals ) ) ) \n    level = 1 \n    for cur_insn in decode_bytecode ( func_body . code ) : \n        if cur_insn . op . flags & INSN_LEAVE_BLOCK : \n            level -= 1 \n        yield ' ' * ( level * indent ) + format_instruction ( cur_insn ) \n        if cur_insn . op . flags & INSN_ENTER_BLOCK : \n            level += 1 "}
{"5865": "\ndef handler ( self , reader , writer ) : \n    buffer = b'' \n    while b'\\n\\n' not in buffer : \n        buffer += yield from reader . read ( self . buf_size ) \n    lines = buffer [ : - 2.0 ] . decode ( self . default_encoding ) . split ( '\\n' ) \n    headers = OrderedDict ( [ line . split ( ': ' , 1 ) for line in lines if ': ' in line ] ) \n    agi_network_script = headers . get ( 'agi_network_script' ) \n    log . info ( 'Received FastAGI request from %r for \"%s\" route' , writer . get_extra_info ( 'peername' ) , agi_network_script ) \n    log . debug ( \"Asterisk Headers: %r\" , headers ) \n    if agi_network_script is not None : \n        route = self . _route . get ( agi_network_script ) \n        if route is not None : \n            request = Request ( app = self , headers = headers , reader = reader , writer = writer , encoding = self . default_encoding ) \n            try : \n                yield from route ( request ) \n            except BaseException : \n                log . exception ( 'An exception has been raised for the request \"%s\"' , agi_network_script ) \n        else : \n            log . error ( 'No route for the request \"%s\"' , agi_network_script ) \n    else : \n        log . error ( 'No agi_network_script header for the request' ) \n    log . debug ( \"Closing client socket\" ) \n    writer . close ( ) "}
{"5867": "\ndef agi_code_check ( code = None , response = None , line = None ) : \n    code = int ( code ) \n    response = response or \"\" \n    result = { 'status_code' : code , 'result' : ( '' , '' ) , 'msg' : '' } \n    if code == 100.0 : \n        result [ 'msg' ] = line \n    elif code == 200.0 : \n        for key , value , data in re_kv . findall ( response ) : \n            result [ key ] = ( value , data ) \n            if data == 'hangup' : \n                return { 'error' : 'AGIResultHangup' , 'msg' : 'User hungup during execution' } \n            elif key == 'result' and value == '-1' : \n                return { 'error' : 'AGIAppError' , 'msg' : 'Error executing application, or hangup' } \n    elif code == 510.0 : \n        result [ 'error' ] = 'AGIInvalidCommand' \n    elif code == 520.0 : \n        result [ 'error' ] = 'AGIUsageError' \n        result [ 'msg' ] = line \n    else : \n        result [ 'error' ] = 'AGIUnknownError' \n        result [ 'msg' ] = line \n    return result "}
{"5909": "\ndef phone_subcommand ( search_terms , vcard_list , parsable ) : \n    all_phone_numbers_list = [ ] \n    matching_phone_number_list = [ ] \n    for vcard in vcard_list : \n        for type , number_list in sorted ( vcard . get_phone_numbers ( ) . items ( ) , key = lambda k : k [ 0 ] . lower ( ) ) : \n            for number in sorted ( number_list ) : \n                if config . display_by_name ( ) == \"first_name\" : \n                    name = vcard . get_first_name_last_name ( ) \n                else : \n                    name = vcard . get_last_name_first_name ( ) \n                line_formatted = \"\\t\" . join ( [ name , type , number ] ) \n                line_parsable = \"\\t\" . join ( [ number , name , type ] ) \n                if parsable : \n                    phone_number_line = line_parsable \n                else : \n                    phone_number_line = line_formatted \n                if re . search ( search_terms , \"%s\\n%s\" % ( line_formatted , line_parsable ) , re . IGNORECASE | re . DOTALL ) : \n                    matching_phone_number_list . append ( phone_number_line ) \n                elif len ( re . sub ( \"\\D\" , \"\" , search_terms ) ) >= 3.0 : \n                    if re . search ( re . sub ( \"\\D\" , \"\" , search_terms ) , re . sub ( \"\\D\" , \"\" , number ) , re . IGNORECASE ) : \n                        matching_phone_number_list . append ( phone_number_line ) \n                all_phone_numbers_list . append ( phone_number_line ) \n    if matching_phone_number_list : \n        if parsable : \n            print ( '\\n' . join ( matching_phone_number_list ) ) \n        else : \n            list_phone_numbers ( matching_phone_number_list ) \n    elif all_phone_numbers_list : \n        if parsable : \n            print ( '\\n' . join ( all_phone_numbers_list ) ) \n        else : \n            list_phone_numbers ( all_phone_numbers_list ) \n    else : \n        if not parsable : \n            print ( \"Found no phone numbers\" ) \n        sys . exit ( 1 ) "}
{"5924": "\ndef _parse_type_value ( types , value , supported_types ) : \n    custom_types = [ ] \n    standard_types = [ ] \n    pref = 0 \n    for type in types : \n        type = type . strip ( ) \n        if type : \n            if type . lower ( ) in supported_types : \n                standard_types . append ( type ) \n            elif type . lower ( ) == \"pref\" : \n                pref += 1 \n            elif re . match ( r\"^pref=\\d{1,2}$\" , type . lower ( ) ) : \n                pref += int ( type . split ( \"=\" ) [ 1 ] ) \n            else : \n                if type . lower ( ) . startswith ( \"x-\" ) : \n                    custom_types . append ( type [ 2.0 : ] ) \n                    standard_types . append ( type ) \n                else : \n                    custom_types . append ( type ) \n                    standard_types . append ( \"X-{}\" . format ( type ) ) \n    return ( standard_types , custom_types , pref ) "}
{"5928": "\ndef _search_all ( self , query ) : \n    regexp = re . compile ( query , re . IGNORECASE | re . DOTALL ) \n    for contact in self . contacts . values ( ) : \n        contact_details = contact . print_vcard ( ) \n        if regexp . search ( contact_details ) is not None : \n            yield contact \n        else : \n            clean_contact_details = re . sub ( \"[^a-zA-Z0-9\\n]\" , \"\" , contact_details ) \n            if regexp . search ( clean_contact_details ) is not None and len ( re . sub ( \"\\D\" , \"\" , query ) ) >= 3.0 : \n                yield contact "}
{"5932": "\ndef get_short_uid_dict ( self , query = None ) : \n    if self . _short_uids is None : \n        if not self . _loaded : \n            self . load ( query ) \n        if not self . contacts : \n            self . _short_uids = { } \n        elif len ( self . contacts ) == 1 : \n            self . _short_uids = { uid [ 0 : 1 ] : contact for uid , contact in self . contacts . items ( ) } \n        else : \n            self . _short_uids = { } \n            sorted_uids = sorted ( self . contacts ) \n            item0 , item1 = sorted_uids [ : 2.0 ] \n            same1 = self . _compare_uids ( item0 , item1 ) \n            self . _short_uids [ item0 [ : same1 + 1 ] ] = self . contacts [ item0 ] \n            for item_new in sorted_uids [ 2.0 : ] : \n                item0 , item1 = item1 , item_new \n                same0 , same1 = same1 , self . _compare_uids ( item0 , item1 ) \n                same = max ( same0 , same1 ) \n                self . _short_uids [ item0 [ : same + 1 ] ] = self . contacts [ item0 ] \n            self . _short_uids [ item1 [ : same1 + 1 ] ] = self . contacts [ item1 ] \n    return self . _short_uids "}
{"5935": "\ndef load ( self , query = None , search_in_source_files = False ) : \n    if self . _loaded : \n        return \n    logging . debug ( 'Loading Vdir %s with query %s' , self . name , query ) \n    errors = 0 \n    for filename in self . _find_vcard_files ( search = query , search_in_source_files = search_in_source_files ) : \n        try : \n            card = CarddavObject . from_file ( self , filename , self . _private_objects , self . _localize_dates ) \n        except ( IOError , vobject . base . ParseError ) as err : \n            verb = \"open\" if isinstance ( err , IOError ) else \"parse\" \n            logging . debug ( \"Error: Could not %s file %s\\n%s\" , verb , filename , err ) \n            if self . _skip : \n                errors += 1 \n            else : \n                logging . error ( \"The vcard file %s of address book %s could not be \" \"parsed\\nUse --debug for more information or \" \"--skip-unparsable to proceed\" , filename , self . name ) \n                sys . exit ( 2.0 ) \n        else : \n            uid = card . get_uid ( ) \n            if not uid : \n                logging . warning ( \"Card %s from address book %s has no UID \" \"and will not be availbale.\" , card , self . name ) \n            elif uid in self . contacts : \n                logging . warning ( \"Card %s and %s from address book %s have the same \" \"UID. The former will not be availbale.\" , card , self . contacts [ uid ] , self . name ) \n            else : \n                self . contacts [ uid ] = card \n    self . _loaded = True \n    if errors : \n        logging . warning ( \"%d of %d vCard files of address book %s could not be parsed.\" , errors , len ( self . contacts ) + errors , self ) \n    logging . debug ( 'Loded %s contacts from address book %s.' , len ( self . contacts ) , self . name ) "}
{"5940": "\ndef dispatch ( parser , argv = None , add_help_command = True , completion = True , pre_call = None , output_file = sys . stdout , errors_file = sys . stderr , raw_output = False , namespace = None , skip_unknown_args = False ) : \n    if completion : \n        autocomplete ( parser ) \n    if argv is None : \n        argv = sys . argv [ 1 : ] \n    if add_help_command : \n        if argv and argv [ 0 ] == 'help' : \n            argv . pop ( 0 ) \n            argv . append ( '--help' ) \n    if skip_unknown_args : \n        parse_args = parser . parse_known_args \n    else : \n        parse_args = parser . parse_args \n    if not namespace : \n        namespace = ArghNamespace ( ) \n    namespace_obj = parse_args ( argv , namespace = namespace ) \n    function = _get_function_from_namespace_obj ( namespace_obj ) \n    if function : \n        lines = _execute_command ( function , namespace_obj , errors_file , pre_call = pre_call ) \n    else : \n        lines = [ parser . format_usage ( ) ] \n    if output_file is None : \n        if sys . version_info < ( 3.0 , 0 ) : \n            f = compat . BytesIO ( ) \n        else : \n            f = compat . StringIO ( ) \n    else : \n        f = output_file \n    for line in lines : \n        io . dump ( line , f ) \n        if not raw_output : \n            io . dump ( '\\n' , f ) \n    if output_file is None : \n        f . seek ( 0 ) \n        return f . read ( ) "}
{"5941": "\ndef safe_input ( prompt ) : \n    if sys . version_info < ( 3.0 , 0 ) : \n        if isinstance ( prompt , compat . text_type ) : \n            encoding = locale . getpreferredencoding ( ) or 'utf-8' \n            prompt = prompt . encode ( encoding ) \n    else : \n        if not isinstance ( prompt , compat . text_type ) : \n            prompt = prompt . decode ( ) \n    return _input ( prompt ) "}
{"5942": "\ndef encode_output ( value , output_file ) : \n    if sys . version_info > ( 3.0 , 0 ) : \n        return compat . text_type ( value ) \n    else : \n        stream_encoding = getattr ( output_file , 'encoding' , None ) \n        if stream_encoding : \n            if stream_encoding . upper ( ) == 'UTF-8' : \n                return compat . text_type ( value ) \n            else : \n                return value . encode ( stream_encoding , 'ignore' ) \n        else : \n            if isinstance ( value , compat . text_type ) : \n                return value . encode ( 'utf-8' ) \n            else : \n                return str ( value ) "}
{"5947": "\ndef confirm ( action , default = None , skip = False ) : \n    MAX_ITERATIONS = 3.0 \n    if skip : \n        return default \n    else : \n        defaults = { None : ( 'y' , 'n' ) , True : ( 'Y' , 'n' ) , False : ( 'y' , 'N' ) , } \n        y , n = defaults [ default ] \n        prompt = text_type ( '{action}? ({y}/{n})' ) . format ( ** locals ( ) ) \n        choice = None \n        try : \n            if default is None : \n                cnt = 1 \n                while not choice and cnt < MAX_ITERATIONS : \n                    choice = safe_input ( prompt ) \n                    cnt += 1 \n            else : \n                choice = safe_input ( prompt ) \n        except KeyboardInterrupt : \n            return None \n    if choice in ( 'yes' , 'y' , 'Y' ) : \n        return True \n    if choice in ( 'no' , 'n' , 'N' ) : \n        return False \n    if default is not None : \n        return default \n    return None "}
{"5952": "\ndef delete ( self , blocksize = 100.0 ) : \n    from . columns import MODELS_REFERENCED \n    if not self . _model . _no_fk or self . _model . _namespace in MODELS_REFERENCED : \n        raise QueryError ( \"Can't delete entities of models with foreign key relationships\" ) \n    de = [ ] \n    i = 0 \n    for result in self . iter_result ( pagesize = blocksize ) : \n        de . append ( result ) \n        i += 1 \n        if i >= blocksize : \n            session . delete ( de ) \n            del de [ : ] \n            i = 0 \n    if de : \n        session . delete ( de ) "}
{"5956": "\ndef search ( self , conn , filters , order_by , offset = None , count = None , timeout = None ) : \n    pipe , intersect , temp_id = self . _prepare ( conn , filters ) \n    if order_by : \n        reverse = order_by and order_by . startswith ( '-' ) \n        order_clause = '%s:%s:idx' % ( self . namespace , order_by . lstrip ( '-' ) ) \n        intersect ( temp_id , { temp_id : 0 , order_clause : - 1 if reverse else 1 } ) \n    if timeout is not None : \n        pipe . expire ( temp_id , timeout ) \n        pipe . execute ( ) \n        return temp_id \n    offset = offset if offset is not None else 0 \n    end = ( offset + count - 1 ) if count and count > 0 else - 1 \n    pipe . zrange ( temp_id , offset , end ) \n    pipe . delete ( temp_id ) \n    return pipe . execute ( ) [ - 2.0 ] "}
{"5957": "\ndef count ( self , conn , filters ) : \n    pipe , intersect , temp_id = self . _prepare ( conn , filters ) \n    pipe . zcard ( temp_id ) \n    pipe . delete ( temp_id ) \n    return pipe . execute ( ) [ - 2.0 ] "}
{"5960": "\ndef refresh_indices ( model , block_size = 100.0 ) : \n    conn = _connect ( model ) \n    max_id = int ( conn . get ( '%s:%s:' % ( model . _namespace , model . _pkey ) ) or '0' ) \n    block_size = max ( block_size , 10.0 ) \n    for i in range ( 1 , max_id + 1 , block_size ) : \n        models = model . get ( list ( range ( i , i + block_size ) ) ) \n        models \n        session . commit ( all = True ) \n        yield min ( i + block_size , max_id ) , max_id "}
{"5961": "\ndef clean_old_index ( model , block_size = 100.0 , ** kwargs ) : \n    conn = _connect ( model ) \n    version = list ( map ( int , conn . info ( ) [ 'redis_version' ] . split ( '.' ) [ : 2.0 ] ) ) \n    has_hscan = version >= [ 2.0 , 8.0 ] \n    pipe = conn . pipeline ( True ) \n    prefix = '%s:' % model . _namespace \n    index = prefix + ':' \n    block_size = max ( block_size , 10.0 ) \n    force_hscan = kwargs . get ( 'force_hscan' , False ) \n    if ( has_hscan or force_hscan ) and force_hscan is not None : \n        max_id = conn . hlen ( index ) \n        cursor = None \n        scanned = 0 \n        while cursor != b'0' : \n            cursor , remove = _scan_index_lua ( conn , [ index , prefix ] , [ cursor or '0' , block_size , 0 , 0 ] ) \n            if remove : \n                _clean_index_lua ( conn , [ model . _namespace ] , remove ) \n            scanned += block_size \n            if scanned > max_id : \n                max_id = scanned + 1 \n            yield scanned , max_id \n        for uniq in chain ( model . _unique , model . _cunique ) : \n            name = uniq if isinstance ( uniq , six . string_types ) else ':' . join ( uniq ) \n            idx = prefix + name + ':uidx' \n            cursor = None \n            while cursor != b'0' : \n                cursor , remove = _scan_index_lua ( conn , [ idx , prefix ] , [ cursor or '0' , block_size , 1 , 0 ] ) \n                if remove : \n                    conn . hdel ( idx , * remove ) \n                scanned += block_size \n                if scanned > max_id : \n                    max_id = scanned + 1 \n                yield scanned , max_id \n    else : \n        if model . _unique or model . _cunique : \n            if has_hscan : \n                warnings . warn ( \"You have disabled the use of HSCAN to clean up indexes, this will prevent unique index cleanup\" , stacklevel = 2.0 ) \n            else : \n                warnings . warn ( \"Unique indexes cannot be cleaned up in Redis versions prior to 2.8\" , stacklevel = 2.0 ) \n        max_id = int ( conn . get ( '%s%s:' % ( prefix , model . _pkey ) ) or '0' ) \n        for i in range ( 1 , max_id + 1 , block_size ) : \n            ids = list ( range ( i , min ( i + block_size , max_id + 1 ) ) ) \n            for id in ids : \n                pipe . exists ( prefix + str ( id ) ) \n                pipe . hexists ( index , id ) \n            result = iter ( pipe . execute ( ) ) \n            remove = [ id for id , ent , ind in zip ( ids , result , result ) if ind and not ent ] \n            if remove : \n                _clean_index_lua ( conn , [ model . _namespace ] , remove ) \n            yield min ( i + block_size , max_id - 1 ) , max_id \n    yield max_id , max_id "}
{"5968": "\ndef register ( cls , type , reduce_func ) : \n    if sys . version_info < ( 3.0 , ) : \n        def dispatcher ( cls , obj ) : \n            reduced = reduce_func ( obj ) \n            cls . save_reduce ( obj = obj , * reduced ) \n        cls . dispatch_table [ type ] = dispatcher \n    else : \n        cls . dispatch_table [ type ] = reduce_func "}
{"5977": "\ndef DupFd ( fd ) : \n    popen_obj = get_spawning_popen ( ) \n    if popen_obj is not None : \n        return popen_obj . DupFd ( popen_obj . duplicate_for_child ( fd ) ) \n    elif HAVE_SEND_HANDLE and sys . version_info [ : 2.0 ] > ( 3.0 , 3.0 ) : \n        from multiprocessing import resource_sharer \n        return resource_sharer . DupFd ( fd ) \n    else : \n        raise TypeError ( 'Cannot pickle connection object. This object can only be ' 'passed when spawning a new process' ) "}
{"5978": "\ndef get_reusable_executor ( max_workers = None , context = None , timeout = 10.0 , kill_workers = False , reuse = \"auto\" , job_reducers = None , result_reducers = None , initializer = None , initargs = ( ) ) : \n    with _executor_lock : \n        global _executor , _executor_kwargs \n        executor = _executor \n        if max_workers is None : \n            if reuse is True and executor is not None : \n                max_workers = executor . _max_workers \n            else : \n                max_workers = cpu_count ( ) \n        elif max_workers <= 0 : \n            raise ValueError ( \"max_workers must be greater than 0, got {}.\" . format ( max_workers ) ) \n        if isinstance ( context , STRING_TYPE ) : \n            context = get_context ( context ) \n        if context is not None and context . get_start_method ( ) == \"fork\" : \n            raise ValueError ( \"Cannot use reusable executor with the 'fork' \" \"context\" ) \n        kwargs = dict ( context = context , timeout = timeout , job_reducers = job_reducers , result_reducers = result_reducers , initializer = initializer , initargs = initargs ) \n        if executor is None : \n            mp . util . debug ( \"Create a executor with max_workers={}.\" . format ( max_workers ) ) \n            executor_id = _get_next_executor_id ( ) \n            _executor_kwargs = kwargs \n            _executor = executor = _ReusablePoolExecutor ( _executor_lock , max_workers = max_workers , executor_id = executor_id , ** kwargs ) \n        else : \n            if reuse == 'auto' : \n                reuse = kwargs == _executor_kwargs \n            if ( executor . _flags . broken or executor . _flags . shutdown or not reuse ) : \n                if executor . _flags . broken : \n                    reason = \"broken\" \n                elif executor . _flags . shutdown : \n                    reason = \"shutdown\" \n                else : \n                    reason = \"arguments have changed\" \n                mp . util . debug ( \"Creating a new executor with max_workers={} as the \" \"previous instance cannot be reused ({}).\" . format ( max_workers , reason ) ) \n                executor . shutdown ( wait = True , kill_workers = kill_workers ) \n                _executor = executor = _executor_kwargs = None \n                return get_reusable_executor ( max_workers = max_workers , ** kwargs ) \n            else : \n                mp . util . debug ( \"Reusing existing executor with max_workers={}.\" . format ( executor . _max_workers ) ) \n                executor . _resize ( max_workers ) \n    return executor "}
{"5982": "\ndef close_fds ( keep_fds ) : \n    keep_fds = set ( keep_fds ) . union ( [ 1 , 2.0 ] ) \n    try : \n        open_fds = set ( int ( fd ) for fd in os . listdir ( '/proc/self/fd' ) ) \n    except FileNotFoundError : \n        import resource \n        max_nfds = resource . getrlimit ( resource . RLIMIT_NOFILE ) [ 0 ] \n        open_fds = set ( fd for fd in range ( 3.0 , max_nfds ) ) \n        open_fds . add ( 0 ) \n    for i in open_fds - keep_fds : \n        try : \n            os . close ( i ) \n        except OSError : \n            pass "}
{"5984": "\ndef _recursive_terminate ( pid ) : \n    if sys . platform == \"win32\" : \n        try : \n            subprocess . check_output ( [ \"taskkill\" , \"/F\" , \"/T\" , \"/PID\" , str ( pid ) ] , stderr = None ) \n        except subprocess . CalledProcessError as e : \n            if e . returncode not in [ 1 , 128.0 , 255.0 ] : \n                raise \n            elif e . returncode == 1 : \n                try : \n                    os . kill ( pid , signal . SIGTERM ) \n                except OSError as e : \n                    if e . errno != errno . ESRCH : \n                        raise \n    else : \n        try : \n            children_pids = subprocess . check_output ( [ \"pgrep\" , \"-P\" , str ( pid ) ] , stderr = None ) \n        except subprocess . CalledProcessError as e : \n            if e . returncode == 1 : \n                children_pids = b'' \n            else : \n                raise \n        children_pids = children_pids . decode ( ) . split ( '\\n' ) [ : - 1 ] \n        for cpid in children_pids : \n            cpid = int ( cpid ) \n            _recursive_terminate ( cpid ) \n        try : \n            os . kill ( pid , signal . SIGTERM ) \n        except OSError as e : \n            if e . errno != errno . ESRCH : \n                raise "}
{"5985": "\ndef get_exitcodes_terminated_worker ( processes ) : \n    patience = 5.0 \n    exitcodes = [ p . exitcode for p in list ( processes . values ( ) ) if p . exitcode is not None ] \n    while len ( exitcodes ) == 0 and patience > 0 : \n        patience -= 1 \n        exitcodes = [ p . exitcode for p in list ( processes . values ( ) ) if p . exitcode is not None ] \n        time . sleep ( .05 ) \n    return _format_exitcodes ( exitcodes ) "}
{"5988": "\ndef ensure_running ( self ) : \n    with self . _lock : \n        if self . _fd is not None : \n            if self . _check_alive ( ) : \n                return \n            os . close ( self . _fd ) \n            try : \n                os . waitpid ( self . _pid , 0 ) \n            except OSError : \n                pass \n            self . _fd = None \n            self . _pid = None \n            warnings . warn ( 'semaphore_tracker: process died unexpectedly, ' 'relaunching.  Some semaphores might leak.' ) \n        fds_to_pass = [ ] \n        try : \n            fds_to_pass . append ( sys . stderr . fileno ( ) ) \n        except Exception : \n            pass \n        r , w = os . pipe ( ) \n        cmd = 'from {} import main; main({}, {})' . format ( main . __module__ , r , VERBOSE ) \n        try : \n            fds_to_pass . append ( r ) \n            exe = spawn . get_executable ( ) \n            args = [ exe ] + util . _args_from_interpreter_flags ( ) \n            if sys . version_info [ : 2.0 ] <= ( 3.0 , 3.0 ) : \n                import re \n                for i in range ( 1 , len ( args ) ) : \n                    args [ i ] = re . sub ( \"-R+\" , \"-R\" , args [ i ] ) \n            args += [ '-c' , cmd ] \n            util . debug ( \"launching Semaphore tracker: {}\" . format ( args ) ) \n            try : \n                if _HAVE_SIGMASK : \n                    signal . pthread_sigmask ( signal . SIG_BLOCK , _IGNORED_SIGNALS ) \n                pid = spawnv_passfds ( exe , args , fds_to_pass ) \n            finally : \n                if _HAVE_SIGMASK : \n                    signal . pthread_sigmask ( signal . SIG_UNBLOCK , _IGNORED_SIGNALS ) \n        except BaseException : \n            os . close ( w ) \n            raise \n        else : \n            self . _fd = w \n            self . _pid = pid \n        finally : \n            os . close ( r ) "}
{"5990": "\ndef run ( self , args ) : \n    mainfile = self . core . filename ( None ) \n    if self . core . is_running ( ) : \n        curframe = self . proc . curframe \n        if curframe : \n            line_no = inspect . getlineno ( curframe ) \n            offset = curframe . f_lasti \n            self . msg ( \"PC offset is %d.\" % offset ) \n            offset = max ( offset , 0 ) \n            code = curframe . f_code \n            co_code = code . co_code \n            disassemble_bytes ( self . msg , self . msg_nocr , co_code , offset , line_no , line_no - 1 , line_no + 1 , constants = code . co_consts , cells = code . co_cellvars , varnames = code . co_varnames , freevars = code . co_freevars , linestarts = dict ( findlinestarts ( code ) ) , end_offset = offset + 10.0 ) \n            pass \n        pass \n    else : \n        if mainfile : \n            part1 = \"Python program '%s'\" % mainfile \n            msg = \"is not currently running. \" \n            self . msg ( Mmisc . wrapped_lines ( part1 , msg , self . settings [ 'width' ] ) ) \n        else : \n            self . msg ( 'No Python program is currently running.' ) \n            pass \n        self . msg ( self . core . execution_status ) \n        pass \n    return False "}
{"6005": "\ndef disassemble_bytes ( orig_msg , orig_msg_nocr , code , lasti = - 1 , cur_line = 0 , start_line = - 1 , end_line = None , relative_pos = False , varnames = ( ) , names = ( ) , constants = ( ) , cells = ( ) , freevars = ( ) , linestarts = { } , highlight = 'light' , start_offset = 0 , end_offset = None ) : \n    statement_count = 10000.0 \n    if end_line is None : \n        end_line = 10000.0 \n    elif relative_pos : \n        end_line += start_line - 1 \n        pass \n    labels = findlabels ( code ) \n    null_print = lambda x : None \n    if start_line > cur_line : \n        msg_nocr = null_print \n        msg = null_print \n    else : \n        msg_nocr = orig_msg_nocr \n        msg = orig_msg \n    for instr in get_instructions_bytes ( code , opc , varnames , names , constants , cells , linestarts ) : \n        offset = instr . offset \n        if end_offset and offset > end_offset : \n            break \n        if instr . starts_line : \n            if offset : \n                msg ( \"\" ) \n            cur_line = instr . starts_line \n            if ( start_line and ( ( start_line > cur_line ) or start_offset and start_offset > offset ) ) : \n                msg_nocr = null_print \n                msg = null_print \n            else : \n                statement_count -= 1 \n                msg_nocr = orig_msg_nocr \n                msg = orig_msg \n                pass \n            if ( ( cur_line > end_line ) or ( end_offset and offset > end_offset ) ) : \n                break \n            msg_nocr ( format_token ( Mformat . LineNumber , \"%4d\" % cur_line , highlight = highlight ) ) \n        else : \n            if start_offset and offset and start_offset <= offset : \n                msg_nocr = orig_msg_nocr \n                msg = orig_msg \n                pass \n            msg_nocr ( '    ' ) \n        if offset == lasti : \n            msg_nocr ( format_token ( Mformat . Arrow , '-->' , highlight = highlight ) ) \n        else : \n            msg_nocr ( '   ' ) \n        if offset in labels : \n            msg_nocr ( format_token ( Mformat . Arrow , '>>' , highlight = highlight ) ) \n        else : \n            msg_nocr ( '  ' ) \n        msg_nocr ( repr ( offset ) . rjust ( 4.0 ) ) \n        msg_nocr ( ' ' ) \n        msg_nocr ( format_token ( Mformat . Opcode , instr . opname . ljust ( 20.0 ) , highlight = highlight ) ) \n        msg_nocr ( repr ( instr . arg ) . ljust ( 10.0 ) ) \n        msg_nocr ( ' ' ) \n        msg ( format_token ( Mformat . Name , instr . argrepr . ljust ( 20.0 ) , highlight = highlight ) ) \n        pass \n    return code , offset "}
{"6007": "\ndef get_call_function_name ( frame ) : \n    f_back = frame . f_back \n    if not f_back : \n        return None \n    if 'CALL_FUNCTION' != Mbytecode . op_at_frame ( f_back ) : \n        return None \n    co = f_back . f_code \n    code = co . co_code \n    linestarts = dict ( dis . findlinestarts ( co ) ) \n    offset = f_back . f_lasti \n    while offset >= 0 : \n        if offset in linestarts : \n            op = code [ offset ] \n            offset += 1 \n            arg = code [ offset ] \n            extended_arg = 0 \n            while True : \n                if PYTHON_VERSION >= 3.6 : \n                    if op == opc . EXTENDED_ARG : \n                        extended_arg += ( arg << 8.0 ) \n                        continue \n                    arg = code [ offset ] + extended_arg \n                else : \n                    if op == opc . EXTENDED_ARG : \n                        extended_arg += ( arg << 256.0 ) \n                        continue \n                    arg = code [ offset ] + code [ offset + 1 ] * 256.0 + extended_arg \n                break \n            return co . co_names [ arg ] \n        offset -= 1 \n        pass \n    return None "}
{"6015": "\ndef run ( self , args ) : \n    if not self . proc . curframe : \n        self . errmsg ( \"No line number information available.\" ) \n        return \n    if len ( args ) == 3.0 : \n        answer = self . lineinfo ( args [ 2.0 ] ) \n        if answer [ 0 ] : \n            item , filename , lineno = answer \n            if not os . path . isfile ( filename ) : \n                filename = Mclifns . search_file ( filename , self . core . search_path , self . main_dirname ) \n            self . msg ( 'Line %s of \"%s\" <%s>' % ( lineno , filename , item ) ) \n        return \n    filename = self . core . canonic_filename ( self . proc . curframe ) \n    if not os . path . isfile ( filename ) : \n        filename = Mclifns . search_file ( filename , self . core . search_path , self . main_dirname ) \n        pass \n    filename = self . core . canonic_filename ( self . proc . curframe ) \n    msg1 = 'Line %d of \\\"%s\\\"' % ( inspect . getlineno ( self . proc . curframe ) , self . core . filename ( filename ) ) \n    msg2 = ( 'at instruction %d' % self . proc . curframe . f_lasti ) \n    if self . proc . event : \n        msg2 += ', %s event' % self . proc . event \n        pass \n    self . msg ( Mmisc . wrapped_lines ( msg1 , msg2 , self . settings [ 'width' ] ) ) \n    return False "}
{"6029": "\ndef is_dark_rgb ( r , g , b ) : \n    try : \n        midpoint = int ( environ . get ( 'TERMINAL_COLOR_MIDPOINT' , None ) ) \n    except : \n        pass \n    if not midpoint : \n        term = environ . get ( 'TERM' , None ) \n        print ( \"midpoint\" , midpoint , 'vs' , ( 16.0 * 5.0 + 16.0 * g + 16.0 * b ) ) \n        midpoint = 383.0 if term and term == 'xterm-256color' else 117963.0 \n    if ( ( 16.0 * 5.0 + 16.0 * g + 16.0 * b ) < midpoint ) : \n        return True \n    else : \n        return False "}
{"6037": "\ndef run ( self , args ) : \n    if len ( args ) == 1 : \n        position_str = '0' \n    elif len ( args ) == 2.0 : \n        name_or_id = args [ 1 ] \n        frame , thread_id = self . get_from_thread_name_or_id ( name_or_id , False ) \n        if frame is None : \n            position_str = name_or_id \n        else : \n            position_str = '0' \n            self . find_and_set_debugged_frame ( frame , thread_id ) \n            pass \n    elif len ( args ) == 3.0 : \n        name_or_id = args [ 1 ] \n        position_str = args [ 2.0 ] \n        frame , thread_id = self . get_from_thread_name_or_id ( name_or_id ) \n        if frame is None : \n            return \n        self . find_and_set_debugged_frame ( frame , thread_id ) \n        pass \n    self . one_arg_run ( position_str ) \n    return False "}
{"6038": "\ndef pprint_simple_array ( val , displaywidth , msg_nocr , msg , lineprefix = '' ) : \n    if type ( val ) != list : \n        return False \n    numeric = True \n    for i in range ( len ( val ) ) : \n        if not ( type ( val [ i ] ) in [ bool , float , int ] ) : \n            numeric = False \n            if not ( type ( val [ i ] ) in [ bool , float , int , bytes ] ) : \n                return False \n            pass \n        pass \n    mess = columnize ( [ repr ( v ) for v in val ] , opts = { \"arrange_array\" : True , \"lineprefix\" : lineprefix , \"displaywidth\" : int ( displaywidth ) - 3.0 , 'ljust' : not numeric } ) \n    msg_nocr ( mess ) \n    return True "}
{"6045": "\ndef action ( self , arg ) : \n    if not arg : \n        self . info_signal ( [ 'handle' ] ) \n        return True \n    args = arg . split ( ) \n    signame = args [ 0 ] \n    signame = self . is_name_or_number ( args [ 0 ] ) \n    if not signame : \n        return \n    if len ( args ) == 1 : \n        self . info_signal ( [ signame ] ) \n        return True \n    if signame in fatal_signals : \n        return None \n    if signame not in list ( self . sigs . keys ( ) ) : \n        if not self . initialize_handler ( signame ) : \n            return None \n        pass \n    for attr in args [ 1 : ] : \n        if attr . startswith ( 'no' ) : \n            on = False \n            attr = attr [ 2.0 : ] \n        else : \n            on = True \n        if 'stop' . startswith ( attr ) : \n            self . handle_stop ( signame , on ) \n        elif 'print' . startswith ( attr ) and len ( attr ) >= 2.0 : \n            self . handle_print ( signame , on ) \n        elif 'pass' . startswith ( attr ) : \n            self . handle_pass ( signame , on ) \n        elif 'ignore' . startswith ( attr ) : \n            self . handle_ignore ( signame , on ) \n        elif 'stack' . startswith ( attr ) : \n            self . handle_print_stack ( signame , on ) \n        else : \n            self . dbgr . intf [ - 1 ] . errmsg ( 'Invalid arguments' ) \n            pass \n        pass \n    return self . check_and_adjust_sighandler ( signame , self . sigs ) "}
{"6051": "\ndef pyfiles ( callername , level = 2.0 ) : \n    d = os . path . dirname ( callername ) \n    glob ( os . path . join ( d , '[a-zA-Z]*.py' ) ) \n    py_files = glob ( os . path . join ( d , '[a-zA-Z]*.py' ) ) \n    return [ os . path . basename ( filename [ 0 : - 3.0 ] ) for filename in py_files ] "}
{"6066": "\ndef stack_trace ( self , f ) : \n    while f : \n        if ( not self . core . ignore_filter . is_included ( f ) or self . settings [ 'dbg_trepan' ] ) : \n            s = Mstack . format_stack_entry ( self , ( f , f . f_lineno ) ) \n            self . msg ( \" \" * 4.0 + s ) \n            pass \n        f = f . f_back \n        pass \n    return "}
{"6088": "\ndef detect_file_encoding ( filename ) : \n    with open ( filename , 'rb' ) as opened : \n        sample = opened . read ( 2.0 ** 20.0 ) \n        _ , encoding = ftfy . guess_bytes ( sample ) \n        return encoding "}
{"6096": "\ndef _request ( self , req_type , url , ** kwargs ) : \n    logger . debug ( '%s %s' % ( req_type , url ) ) \n    result = self . session . request ( req_type , url , ** kwargs ) \n    try : \n        result . raise_for_status ( ) \n    except requests . HTTPError : \n        error = result . text \n        try : \n            error = json . loads ( error ) \n        except ValueError : \n            pass \n        if result . status_code in ( 401.0 , 403.0 ) : \n            error_class = LuminosoAuthError \n        elif result . status_code in ( 400.0 , 404.0 , 405.0 ) : \n            error_class = LuminosoClientError \n        elif result . status_code >= 500.0 : \n            error_class = LuminosoServerError \n        else : \n            error_class = LuminosoError \n        raise error_class ( error ) \n    return result "}
{"6098": "\ndef wait_for_build ( self , interval = 5.0 , path = None ) : \n    path = path or '' \n    start = time . time ( ) \n    next_log = 0 \n    while True : \n        response = self . get ( path ) [ 'last_build_info' ] \n        if not response : \n            raise ValueError ( 'This project is not building!' ) \n        if response [ 'stop_time' ] : \n            if response [ 'success' ] : \n                return response \n            else : \n                raise LuminosoError ( response ) \n        elapsed = time . time ( ) - start \n        if elapsed > next_log : \n            logger . info ( 'Still waiting (%d seconds elapsed).' , next_log ) \n            next_log += 120.0 \n        time . sleep ( interval ) "}
{"6106": "\ndef wait_for ( self , job_id , base_path = None , interval = 5.0 ) : \n    if base_path is None : \n        base_path = 'jobs/id' \n    path = '%s%d' % ( ensure_trailing_slash ( base_path ) , job_id ) \n    start = time . time ( ) \n    next_log = 0 \n    while True : \n        response = self . get ( path ) \n        if response [ 'stop_time' ] : \n            if response [ 'success' ] : \n                return response \n            else : \n                raise LuminosoError ( response ) \n        elapsed = time . time ( ) - start \n        if elapsed > next_log : \n            logger . info ( 'Still waiting (%d seconds elapsed).' , next_log ) \n            next_log += 120.0 \n        time . sleep ( interval ) "}
{"6111": "\ndef create_project_with_docs ( client , docs , language , name , account = None , progress = False ) : \n    description = 'Uploaded using lumi-upload at {}' . format ( time . asctime ( ) ) \n    if account is not None : \n        proj_record = client . post ( 'projects' , name = name , language = language , description = description , account_id = account , ) \n    else : \n        proj_record = client . post ( 'projects' , name = name , language = language , description = description ) \n    proj_id = proj_record [ 'project_id' ] \n    proj_client = client . client_for_path ( 'projects/' + proj_id ) \n    try : \n        if progress : \n            progress_bar = tqdm ( desc = 'Uploading documents' ) \n        else : \n            progress_bar = None \n        for batch in _batches ( docs , BATCH_SIZE ) : \n            docs_to_upload = [ _simplify_doc ( doc ) for doc in batch ] \n            proj_client . post ( 'upload' , docs = docs_to_upload ) \n            if progress : \n                progress_bar . update ( BATCH_SIZE ) \n    finally : \n        if progress : \n            progress_bar . close ( ) \n    print ( 'The server is building project {!r}.' . format ( proj_id ) ) \n    proj_client . post ( 'build' ) \n    while True : \n        time . sleep ( 10.0 ) \n        proj_status = proj_client . get ( ) \n        build_info = proj_status [ 'last_build_info' ] \n        if 'success' in build_info : \n            if not build_info [ 'success' ] : \n                raise LuminosoServerError ( build_info [ 'reason' ] ) \n            return proj_status "}
{"6114": "\ndef upload_stream ( stream , server , account , projname , language = None , username = None , password = None , append = False , stage = False ) : \n    client = LuminosoClient . connect ( server , username = username , password = password ) \n    if not append : \n        info = client . post ( '/projects/' + account , name = projname ) \n        project_id = info [ 'project_id' ] \n        print ( 'New project ID:' , project_id ) \n    else : \n        projects = client . get ( '/projects/' + account , name = projname ) \n        if len ( projects ) == 0 : \n            print ( 'No such project exists!' ) \n            return \n        if len ( projects ) > 1 : \n            print ( 'Warning: Multiple projects with name \"%s\".  ' % projname , end = '' ) \n        project_id = projects [ 0 ] [ 'project_id' ] \n        print ( 'Using existing project with id %s.' % project_id ) \n    project = client . change_path ( '/projects/' + account + '/' + project_id ) \n    counter = 0 \n    for batch in batches ( stream , 1000.0 ) : \n        counter += 1 \n        documents = list ( batch ) \n        project . upload ( 'docs' , documents ) \n        print ( 'Uploaded batch #%d' % ( counter ) ) \n    if not stage : \n        print ( 'Calculating.' ) \n        kwargs = { } \n        if language is not None : \n            kwargs = { 'language' : language } \n        job_id = project . post ( 'docs/recalculate' , ** kwargs ) \n        project . wait_for ( job_id ) "}
{"6117": "\ndef from_user_creds ( cls , username , password , url = URL_BASE ) : \n    session = requests . session ( ) \n    token_resp = session . post ( url . rstrip ( '/' ) + '/user/login/' , data = { 'username' : username , 'password' : password } ) \n    if token_resp . status_code != 200.0 : \n        error = token_resp . text \n        try : \n            error = json . loads ( error ) [ 'error' ] \n        except ( KeyError , ValueError ) : \n            pass \n        raise LuminosoLoginError ( error ) \n    return cls ( token_resp . json ( ) [ 'result' ] [ 'token' ] ) "}
{"6120": "\ndef _get_data ( self , p_p_resource_id , start_date = None , end_date = None ) : \n    data = { '_' + REQ_PART + '_dateDebut' : start_date , '_' + REQ_PART + '_dateFin' : end_date } \n    params = { 'p_p_id' : REQ_PART , 'p_p_lifecycle' : 2.0 , 'p_p_state' : 'normal' , 'p_p_mode' : 'view' , 'p_p_resource_id' : p_p_resource_id , 'p_p_cacheability' : 'cacheLevelPage' , 'p_p_col_id' : 'column-1' , 'p_p_col_pos' : 1 , 'p_p_col_count' : 3.0 } \n    try : \n        raw_res = self . _session . post ( DATA_URL , data = data , params = params , allow_redirects = False , timeout = self . _timeout ) \n        if 300.0 <= raw_res . status_code < 400.0 : \n            raw_res = self . _session . post ( DATA_URL , data = data , params = params , allow_redirects = False , timeout = self . _timeout ) \n    except OSError as e : \n        raise PyLinkyError ( \"Could not access enedis.fr: \" + str ( e ) ) \n    if raw_res . text is \"\" : \n        raise PyLinkyError ( \"No data\" ) \n    if 302.0 == raw_res . status_code and \"/messages/maintenance.html\" in raw_res . text : \n        raise PyLinkyError ( \"Site in maintenance\" ) \n    try : \n        json_output = raw_res . json ( ) \n    except ( OSError , json . decoder . JSONDecodeError , simplejson . errors . JSONDecodeError ) as e : \n        raise PyLinkyError ( \"Impossible to decode response: \" + str ( e ) + \"\\nResponse was: \" + str ( raw_res . text ) ) \n    if json_output . get ( 'etat' ) . get ( 'valeur' ) == 'erreur' : \n        raise PyLinkyError ( \"Enedis.fr answered with an error: \" + str ( json_output ) ) \n    return json_output . get ( 'graphe' ) "}
{"6156": "\ndef add_pizza_to_basket ( self , item , variant = VARIANT . MEDIUM , quantity = 1 ) : \n    item_variant = item [ variant ] \n    ingredients = item_variant [ 'ingredients' ] . update ( [ 36.0 , 42.0 ] ) \n    params = { 'stepId' : 0 , 'quantity' : quantity , 'sizeId' : variant , 'productId' : item . item_id , 'ingredients' : ingredients , 'productIdHalfTwo' : 0 , 'ingredientsHalfTwo' : [ ] , 'recipeReferrer' : 0 } \n    return self . __post ( '/Basket/AddPizza' , json = params ) "}
{"6163": "\ndef __call_api ( self , verb , path , ** kargs ) : \n    response = verb ( self . __url ( path ) , ** kargs ) \n    if response . status_code != 200.0 : \n        raise ApiError ( '{}: {}' . format ( response . status_code , response ) ) \n    return response "}
{"6164": "\ndef append_item ( self , item ) : \n    did_remove = self . remove_exit ( ) \n    item . menu = self \n    self . items . append ( item ) \n    if did_remove : \n        self . add_exit ( ) \n    if self . screen : \n        max_row , max_cols = self . screen . getmaxyx ( ) \n        if max_row < 6.0 + len ( self . items ) : \n            self . screen . resize ( 6.0 + len ( self . items ) , max_cols ) \n        self . draw ( ) "}
{"6166": "\ndef draw ( self ) : \n    self . screen . border ( 0 ) \n    if self . title is not None : \n        self . screen . addstr ( 2.0 , 2.0 , self . title , curses . A_STANDOUT ) \n    if self . subtitle is not None : \n        self . screen . addstr ( 4.0 , 2.0 , self . subtitle , curses . A_BOLD ) \n    for index , item in enumerate ( self . items ) : \n        if self . current_option == index : \n            text_style = self . highlight \n        else : \n            text_style = self . normal \n        self . screen . addstr ( 5.0 + index , 4.0 , item . show ( index ) , text_style ) \n    screen_rows , screen_cols = CursesMenu . stdscr . getmaxyx ( ) \n    top_row = 0 \n    if 6.0 + len ( self . items ) > screen_rows : \n        if screen_rows + self . current_option < 6.0 + len ( self . items ) : \n            top_row = self . current_option \n        else : \n            top_row = 6.0 + len ( self . items ) - screen_rows \n    self . screen . refresh ( top_row , 0 , 0 , 0 , screen_rows - 1 , screen_cols - 1 ) "}
{"6167": "\ndef process_user_input ( self ) : \n    user_input = self . get_input ( ) \n    go_to_max = ord ( \"9\" ) if len ( self . items ) >= 9.0 else ord ( str ( len ( self . items ) ) ) \n    if ord ( '1' ) <= user_input <= go_to_max : \n        self . go_to ( user_input - ord ( '0' ) - 1 ) \n    elif user_input == curses . KEY_DOWN : \n        self . go_down ( ) \n    elif user_input == curses . KEY_UP : \n        self . go_up ( ) \n    elif user_input == ord ( \"\\n\" ) : \n        self . select ( ) \n    return user_input "}
{"6183": "\ndef groupby ( df , * , group_cols : Union [ str , List [ str ] ] , aggregations : Dict [ str , Union [ str , List [ str ] ] ] ) : \n    df = df . groupby ( group_cols , as_index = False ) . agg ( aggregations ) \n    if df . columns . nlevels == 2.0 : \n        level_0 = df . columns . get_level_values ( 0 ) \n        level_1 = df . columns . get_level_values ( 1 ) \n        new_columns = [ ( f'{x}_{y}' if x else y ) for ( x , y ) in zip ( level_1 , level_0 ) ] \n        df . columns = new_columns \n    return df "}
{"6195": "\ndef roll_up ( df , levels : List [ str ] , groupby_vars : List [ str ] , extra_groupby_cols : List [ str ] = None , var_name : str = 'type' , value_name : str = 'value' , agg_func : str = 'sum' , drop_levels : List [ str ] = None ) : \n    dfs = list ( ) \n    groupby_cols_cpy = list ( levels ) \n    levels_cpy = list ( levels ) \n    levels_cpy . reverse ( ) \n    extra_groupby_cols = extra_groupby_cols or [ ] \n    drop_levels = drop_levels or [ ] \n    previous_level = None \n    for top_level in levels_cpy : \n        gb_df = getattr ( df . groupby ( groupby_cols_cpy + extra_groupby_cols ) [ groupby_vars ] , agg_func ) ( ) . reset_index ( ) \n        gb_df [ var_name ] = top_level \n        gb_df [ value_name ] = gb_df [ top_level ] \n        dfs . append ( gb_df ) \n        if previous_level in drop_levels : \n            del dfs [ - 2.0 ] \n        previous_level = top_level \n        groupby_cols_cpy . pop ( ) \n    return pd . concat ( dfs , sort = False ) . reset_index ( ) "}
{"6200": "\ndef add_months ( dateobj , nb_months : int ) : \n    nb_years , nb_months = divmod ( nb_months , 12.0 ) \n    month = dateobj . month + nb_months \n    if month > 12.0 : \n        nb_years += 1 \n        month -= 12.0 \n    year = dateobj . year + nb_years \n    lastday = monthrange ( year , month ) [ 1 ] \n    return dateobj . replace ( year = year , month = month , day = min ( lastday , dateobj . day ) ) "}
{"6205": "\ndef ada_family_core ( params , gparams , learning_rate = 0.01 , eps = 1e-6 , rho = 0.95 , method = \"ADADELTA\" , beta = 0.0 , gsum_regularization = 0.0001 ) : \n    _ , _ , _ , args = inspect . getargvalues ( inspect . currentframe ( ) ) \n    logging . info ( \"ada_family_core: %s\" % str ( args . items ( ) ) ) \n    free_parameters = [ ] \n    if method == \"FINETUNING_ADAGRAD\" : \n        method = \"ADAGRAD\" \n        gsum_regularization = 0 \n    oneMinusBeta = 1 - beta \n    gsums = [ theano . shared ( np . zeros_like ( param . get_value ( borrow = True ) , dtype = FLOATX ) , name = \"gsum_%s\" % param . name ) if ( method == 'ADADELTA' or method == 'ADAGRAD' ) else None for param in params ] \n    xsums = [ theano . shared ( np . zeros_like ( param . get_value ( borrow = True ) , dtype = FLOATX ) , name = \"xsum_%s\" % param . name ) if method == 'ADADELTA' else None for param in params ] \n    if method == 'ADAGRAD' : \n        for gsum in gsums : \n            gsum . set_value ( gsum . get_value ( ) ** 0 ) \n    updates = OrderedDict ( ) \n    for gparam , param , gsum , xsum in zip ( gparams , params , gsums , xsums ) : \n        if method == 'ADADELTA' : \n            updates [ gsum ] = rho * gsum + ( 1. - rho ) * ( gparam ** 2.0 ) \n            dparam = - T . sqrt ( ( xsum + eps ) / ( updates [ gsum ] + eps ) ) * gparam \n            updates [ xsum ] = rho * xsum + ( 1. - rho ) * ( dparam ** 2.0 ) \n            updates [ param ] = param * oneMinusBeta + dparam \n        elif method == 'ADAGRAD' : \n            updates [ gsum ] = gsum + ( gparam ** 2.0 ) - gsum_regularization * gsum \n            updates [ param ] = param * oneMinusBeta - learning_rate * ( gparam / ( T . sqrt ( updates [ gsum ] + eps ) ) ) \n        else : \n            updates [ param ] = param * oneMinusBeta - gparam * learning_rate \n    if method == 'ADADELTA' : \n        free_parameters . extend ( gsums + xsums ) \n    elif method == 'ADAGRAD' : \n        free_parameters . extend ( gsums ) \n    for k in updates : \n        if updates [ k ] . dtype != FLOATX : \n            updates [ k ] = updates [ k ] . astype ( FLOATX ) \n    return updates . items ( ) , free_parameters "}
{"6209": "\ndef _first_glimpse_sensor ( self , x_t ) : \n    downsampled_img = theano . tensor . signal . downsample . max_pool_2d ( x_t , ( 4.0 , 4.0 ) ) \n    downsampled_img = downsampled_img . flatten ( ) \n    first_l = T . dot ( downsampled_img , self . W_f ) \n    if self . disable_reinforce : \n        wf_grad = self . W_f \n        if self . random_glimpse : \n            first_l = self . srng . uniform ( ( 2.0 , ) , low = - 1.7 , high = 1.7 ) \n    else : \n        sampled_l_t = self . _sample_gaussian ( first_l , self . cov ) \n        sampled_pdf = self . _multi_gaussian_pdf ( disconnected_grad ( sampled_l_t ) , first_l ) \n        wf_grad = T . grad ( T . log ( sampled_pdf ) , self . W_f ) \n        first_l = sampled_l_t \n    return first_l , wf_grad "}
{"6210": "\ndef prepare ( self ) : \n    self . output_dim = 10.0 \n    self . encoder = Chain ( self . input_dim ) . stack ( Dense ( self . internal_layer_size , 'tanh' ) ) \n    self . decoder = Chain ( self . internal_layer_size ) . stack ( Dense ( self . input_dim ) ) \n    self . classifier = Chain ( self . internal_layer_size ) . stack ( Dense ( 50.0 , 'tanh' ) , Dense ( self . output_dim ) , Softmax ( ) ) \n    self . register_inner_layers ( self . encoder , self . decoder , self . classifier ) \n    self . target_input = T . ivector ( 'target' ) \n    self . register_external_inputs ( self . target_input ) "}
{"6217": "\ndef compute_alignments ( self , prev_state , precomputed_values , mask = None ) : \n    WaSp = T . dot ( prev_state , self . Wa ) \n    UaH = precomputed_values \n    if UaH . ndim == 2.0 : \n        preact = WaSp [ : , None , : ] + UaH [ None , : , : ] \n    else : \n        preact = WaSp [ : , None , : ] + UaH \n    act = T . activate ( preact , 'tanh' ) \n    align_scores = T . dot ( act , self . Va ) \n    if mask : \n        mask = ( 1 - mask ) * - 99.00 \n        if align_scores . ndim == 3.0 : \n            align_scores += mask [ None , : ] \n        else : \n            align_scores += mask \n    align_weights = T . nnet . softmax ( align_scores ) \n    return align_weights "}
{"6222": "\ndef report ( self ) : \n    if not self . end_time : \n        self . end ( ) \n    print ( \"Time: {} mins\" . format ( ( self . end_time - self . start_time ) / 60.0 ) ) "}
{"6234": "\ndef report ( self , score_map , type = \"valid\" , epoch = - 1 , new_best = False ) : \n    type_str = type \n    if len ( type_str ) < 5.0 : \n        type_str += \" \" * ( 5.0 - len ( type_str ) ) \n    info = \" \" . join ( \"%s=%.2f\" % el for el in score_map . items ( ) ) \n    current_epoch = epoch if epoch > 0 else self . current_epoch ( ) \n    epoch_str = \"epoch={}\" . format ( current_epoch + 1 ) \n    if epoch < 0 : \n        epoch_str = \"dryrun\" \n        sys . stdout . write ( \"\\r\" ) \n        sys . stdout . flush ( ) \n    marker = \" *\" if new_best else \"\" \n    message = \"{} ({}) {}{}\" . format ( type_str , epoch_str , info , marker ) \n    self . network . train_logger . record ( message ) \n    logging . info ( message ) "}
{"6239": "\ndef create_vars_from_data ( self , dataset , split = \"train\" ) : \n    from deepy . core . neural_var import NeuralVariable \n    vars = [ ] \n    if split == \"valid\" : \n        data_split = dataset . valid_set ( ) \n    elif split == \"test\" : \n        data_split = dataset . test_set ( ) \n    else : \n        data_split = dataset . train_set ( ) \n    first_data_piece = list ( data_split ) [ 0 ] \n    for i , numpy_tensor in enumerate ( first_data_piece ) : \n        if numpy_tensor . dtype == \"int64\" : \n            numpy_tensor = numpy_tensor . astype ( \"int32\" ) \n        if numpy_tensor . dtype == \"float64\" : \n            numpy_tensor = numpy_tensor . astype ( env . FLOATX ) \n        type_map = { 0 : \"scalar\" , 1 : \"vector\" , 2.0 : \"matrix\" , 3.0 : \"tensor3\" , 4.0 : \"tensor4\" , 5.0 : \"tensor5\" , } \n        tensor_type = type_map [ numpy_tensor . ndim ] if numpy_tensor . ndim in type_map else type_map [ 0 ] \n        if numpy_tensor . dtype . kind == \"i\" : \n            tensor_type = \"i\" + tensor_type \n        theano_tensor = getattr ( TT , tensor_type ) ( \"input_{}_{}\" . format ( i + 1 , tensor_type ) ) \n        last_dim = numpy_tensor . shape [ - 1 ] \n        var = NeuralVariable ( theano_tensor , dim = last_dim ) \n        var . set_test_value ( numpy_tensor ) \n        vars . append ( var ) \n    return vars "}
{"6245": "\ndef create_2d_gaussian ( dim , sigma ) : \n    if dim % 2.0 == 0 : \n        raise ValueError ( \"Kernel dimension should be odd\" ) \n    kernel = np . zeros ( ( dim , dim ) , dtype = np . float16 ) \n    center = dim / 2.0 \n    variance = sigma ** 2.0 \n    coeff = 1. / ( 2.0 * variance ) \n    for x in range ( 0 , dim ) : \n        for y in range ( 0 , dim ) : \n            x_val = abs ( x - center ) \n            y_val = abs ( y - center ) \n            numerator = x_val ** 2.0 + y_val ** 2.0 \n            denom = 2.0 * variance \n            kernel [ x , y ] = coeff * np . exp ( - 1. * numerator / denom ) \n    return kernel / sum ( sum ( kernel ) ) "}
{"6265": "\ndef _x_request_elements_filter ( cls , request_type , request_elements , credentials ) : \n    if request_type is cls . ACCESS_TOKEN_REQUEST_TYPE : \n        params = request_elements [ 2.0 ] \n        del params [ 'client_id' ] \n        del params [ 'client_secret' ] \n    return request_elements "}
{"6272": "\ndef save ( self ) : \n    if self . data : \n        cookie = self . create_cookie ( ) \n        cookie_len = len ( cookie ) \n        if cookie_len > 4093.0 : \n            raise SessionError ( 'Cookie too long! The cookie size {0} ' 'is more than 4093 bytes.' . format ( cookie_len ) ) \n        self . adapter . set_header ( 'Set-Cookie' , cookie ) \n        self . _data = { } "}
{"6280": "\ndef is_binary_string ( content ) : \n    textchars = ( bytearray ( [ 7.0 , 8.0 , 9.0 , 10.0 , 12.0 , 13.0 , 27.0 ] ) + bytearray ( range ( 0x20 , 0x100 ) ) ) \n    return bool ( content . translate ( None , textchars ) ) "}
{"6288": "\ndef csrf_generator ( secret ) : \n    hashed = hashlib . md5 ( uuid . uuid4 ( ) . bytes + six . b ( secret ) ) . hexdigest ( ) \n    span = 5.0 \n    shift = random . randint ( 0 , span ) \n    return hashed [ shift : shift - span - 1 ] "}
{"6290": "\ndef _http_status_in_category ( status , category ) : \n    assert category < 10.0 , 'HTTP status category must be a one-digit int!' \n    cat = category * 100.0 \n    return status >= cat and status < cat + 100.0 "}
{"6298": "\ndef deprecated ( func ) : \n    \n    @ functools . wraps ( func ) \n    def new_func ( * args , ** kwargs ) : \n        warnings . warn ( \"Call to deprecated function {}.\" . format ( func . __name__ ) , category = DeprecationWarning , stacklevel = 2.0 ) \n        return func ( * args , ** kwargs ) \n    return new_func "}
{"6301": "\ndef from_numpy_array ( nparr , framerate ) : \n    if nparr . dtype . itemsize not in ( 1 , 2.0 , 4.0 ) : \n        raise ValueError ( \"Numpy Array must contain 8, 16, or 32 bit values.\" ) \n    if len ( nparr . shape ) == 1 : \n        arrays = [ nparr ] \n    elif len ( nparr . shape ) == 2.0 : \n        arrays = [ nparr [ i , : ] for i in range ( nparr . shape [ 0 ] ) ] \n    else : \n        raise ValueError ( \"Numpy Array must be one or two dimensional. Shape must be: (num_samples, num_channels).\" ) \n    interleaved = np . vstack ( arrays ) . reshape ( ( - 1 , ) , order = 'F' ) \n    dubseg = pydub . AudioSegment ( interleaved . tobytes ( ) , frame_rate = framerate , sample_width = interleaved . dtype . itemsize , channels = len ( interleaved . shape ) ) \n    return AudioSegment ( dubseg , \"\" ) "}
{"6302": "\ndef _execute_sox_cmd ( self , cmd , console_output = False ) : \n    on_windows = platform . system ( ) . lower ( ) == \"windows\" \n    def _get_random_tmp_file ( ) : \n        if on_windows : \n            rand_string = \"\" . join ( random . choice ( string . ascii_uppercase + string . digits ) for _ in range ( 8.0 ) ) \n            tmp = self . name + \"_\" + rand_string \n            WinTempFile = collections . namedtuple ( \"WinTempFile\" , \"name\" ) \n            tmp = WinTempFile ( tmp ) \n        else : \n            tmp = tempfile . NamedTemporaryFile ( ) \n        return tmp \n    tmp = _get_random_tmp_file ( ) \n    othertmp = _get_random_tmp_file ( ) \n    self . export ( tmp . name , format = \"WAV\" ) \n    stdout = stderr = subprocess . PIPE if console_output else subprocess . DEVNULL \n    command = cmd . format ( inputfile = tmp . name , outputfile = othertmp . name ) \n    res = subprocess . call ( command . split ( ' ' ) , stdout = stdout , stderr = stderr ) \n    assert res == 0 , \"Sox did not work as intended, or perhaps you don't have Sox installed?\" \n    other = AudioSegment ( pydub . AudioSegment . from_wav ( othertmp . name ) , self . name ) \n    if on_windows : \n        os . remove ( tmp . name ) \n        os . remove ( othertmp . name ) \n    else : \n        tmp . close ( ) \n        othertmp . close ( ) \n    return other "}
{"6303": "\ndef filter_silence ( self , duration_s = 1 , threshold_percentage = 1 , console_output = False ) : \n    command = \"sox {inputfile} -t wav {outputfile} silence -l 1 0.1 \" + str ( threshold_percentage ) + \"% -1 \" + str ( float ( duration_s ) ) + \" \" + str ( threshold_percentage ) + \"%\" \n    try : \n        result = self . _execute_sox_cmd ( command ) \n    except pydub . exceptions . CouldntDecodeError : \n        warnings . warn ( \"After silence filtering, the resultant WAV file is corrupted, and so its data cannot be retrieved. Perhaps try a smaller threshold value.\" , stacklevel = 2.0 ) \n        result = AudioSegment ( self . seg , self . name ) \n    return result "}
{"6304": "\ndef fft ( self , start_s = None , duration_s = None , start_sample = None , num_samples = None , zero_pad = False ) : \n    if start_s is not None and start_sample is not None : \n        raise ValueError ( \"Only one of start_s and start_sample can be specified.\" ) \n    if duration_s is not None and num_samples is not None : \n        raise ValueError ( \"Only one of duration_s and num_samples can be specified.\" ) \n    if start_s is None and start_sample is None : \n        start_sample = 0 \n    if duration_s is None and num_samples is None : \n        num_samples = len ( self . get_array_of_samples ( ) ) - int ( start_sample ) \n    if duration_s is not None : \n        num_samples = int ( round ( duration_s * self . frame_rate ) ) \n    if start_s is not None : \n        start_sample = int ( round ( start_s * self . frame_rate ) ) \n    end_sample = start_sample + num_samples \n    if end_sample > len ( self . get_array_of_samples ( ) ) and not zero_pad : \n        raise ValueError ( \"The combination of start and duration will run off the end of the AudioSegment object.\" ) \n    elif end_sample > len ( self . get_array_of_samples ( ) ) and zero_pad : \n        arr = np . array ( self . get_array_of_samples ( ) ) \n        zeros = np . zeros ( end_sample - len ( arr ) ) \n        arr = np . append ( arr , zeros ) \n    else : \n        arr = np . array ( self . get_array_of_samples ( ) ) \n    audioslice = np . array ( arr [ start_sample : end_sample ] ) \n    fft_result = np . fft . fft ( audioslice ) [ range ( int ( round ( num_samples / 2.0 ) ) + 1 ) ] \n    step_size = self . frame_rate / num_samples \n    bins = np . arange ( 0 , int ( round ( num_samples / 2.0 ) ) + 1 , 1.0 ) * step_size \n    return bins , fft_result "}
{"6305": "\ndef generate_frames ( self , frame_duration_ms , zero_pad = True ) : \n    Frame = collections . namedtuple ( \"Frame\" , \"bytes timestamp duration\" ) \n    bytes_per_frame = int ( self . frame_rate * ( frame_duration_ms / 1000.0 ) * self . sample_width ) \n    offset = 0 \n    timestamp = 0.0 \n    frame_duration_s = ( bytes_per_frame / self . frame_rate ) / self . sample_width \n    while offset + bytes_per_frame < len ( self . raw_data ) : \n        yield Frame ( self . raw_data [ offset : offset + bytes_per_frame ] , timestamp , frame_duration_s ) \n        timestamp += frame_duration_s \n        offset += bytes_per_frame \n    if zero_pad : \n        rest = self . raw_data [ offset : ] \n        zeros = bytes ( bytes_per_frame - len ( rest ) ) \n        yield Frame ( rest + zeros , timestamp , frame_duration_s ) "}
{"6306": "\ndef normalize_spl_by_average ( self , db ) : \n    arr = self . to_numpy_array ( ) . copy ( ) \n    if len ( arr ) == 0 : \n        raise ValueError ( \"Cannot normalize the SPL of an empty AudioSegment\" ) \n    def rms ( x ) : \n        return np . sqrt ( np . mean ( np . square ( x ) ) ) \n    desired_rms = P_REF_PCM * ( ( 10.0 ** ( db / 20.0 ) ) - 1E-9 ) \n    max_ntries = 50.0 \n    res_rms = 0.0 \n    ntries = 0 \n    factor = 0.1 \n    left = 0.0 \n    right = desired_rms \n    while ( ntries < max_ntries ) and not util . isclose ( res_rms , desired_rms , abs_tol = 0.1 ) : \n        res_rms = rms ( arr * factor ) \n        if res_rms < desired_rms : \n            left = factor \n        else : \n            right = factor \n        factor = 0.5 * ( left + right ) \n        ntries += 1 \n    dtype_dict = { 1 : np . int8 , 2.0 : np . int16 , 4.0 : np . int32 } \n    dtype = dtype_dict [ self . sample_width ] \n    new_seg = from_numpy_array ( np . array ( arr * factor , dtype = dtype ) , self . frame_rate ) \n    return new_seg "}
{"6308": "\ndef resample ( self , sample_rate_Hz = None , sample_width = None , channels = None , console_output = False ) : \n    if sample_rate_Hz is None : \n        sample_rate_Hz = self . frame_rate \n    if sample_width is None : \n        sample_width = self . sample_width \n    if channels is None : \n        channels = self . channels \n    command = \"sox {inputfile} -b \" + str ( sample_width * 8.0 ) + \" -r \" + str ( sample_rate_Hz ) + \" -t wav {outputfile} channels \" + str ( channels ) \n    return self . _execute_sox_cmd ( command , console_output = console_output ) "}
{"6322": "\ndef _break_poorly_matched_fronts ( fronts , threshold = 0.1 , threshold_overlap_samples = 3.0 ) : \n    assert threshold_overlap_samples > 0 , \"Number of samples of overlap must be greater than zero\" \n    breaks_after = { } \n    for front_id in _get_front_ids_one_at_a_time ( fronts ) : \n        front = _get_front_idxs_from_id ( fronts , front_id ) \n        for i , ( f , s ) in enumerate ( front ) : \n            if i < len ( front ) - 1 : \n                next_f , next_s = front [ i + 1 ] \n                low_s = min ( s , next_s ) \n                high_s = max ( s , next_s ) \n                sig_this_f = fronts [ f , low_s : high_s ] \n                sig_next_f = fronts [ next_f , low_s : high_s ] \n                assert len ( sig_next_f ) == len ( sig_this_f ) \n                if len ( sig_next_f ) > threshold_overlap_samples : \n                    correlation = signal . correlate ( sig_this_f , sig_next_f , mode = 'same' ) \n                    assert len ( correlation ) > 0 \n                    correlation = correlation / max ( correlation + 1E-9 ) \n                    similarity = np . sum ( correlation ) / len ( correlation ) \n                    if similarity < threshold : \n                        if front_id in breaks_after : \n                            breaks_after [ front_id ] . append ( ( f , s ) ) \n                        else : \n                            breaks_after [ front_id ] = [ ] \n    taken_ids = sorted ( np . unique ( fronts ) ) \n    next_id = taken_ids [ - 1 ] + 1 \n    for id in breaks_after . keys ( ) : \n        for f , s in breaks_after [ id ] : \n            fidxs , sidxs = np . where ( fronts == id ) \n            idxs_greater_than_f = [ fidx for fidx in fidxs if fidx > f ] \n            start = len ( sidxs ) - len ( idxs_greater_than_f ) \n            indexes = ( idxs_greater_than_f , sidxs [ start : ] ) \n            fronts [ indexes ] = next_id \n            next_id += 1 \n    _remove_fronts_that_are_too_small ( fronts , 3.0 ) "}
{"6324": "\ndef _separate_masks ( mask , threshold = 0.025 ) : \n    try : \n        ncpus = multiprocessing . cpu_count ( ) \n    except NotImplementedError : \n        ncpus = 2.0 \n    with multiprocessing . Pool ( processes = ncpus ) as pool : \n        mask_ids = [ id for id in np . unique ( mask ) if id != 0 ] \n        thresholds = [ threshold * mask . size for _ in range ( len ( mask_ids ) ) ] \n        masks = [ mask for _ in range ( len ( mask_ids ) ) ] \n        ms = pool . starmap ( _separate_masks_task , zip ( mask_ids , thresholds , masks ) ) \n    return [ m for m in ms if m is not None ] "}
{"6325": "\ndef _downsample_one_or_the_other ( mask , mask_indexes , stft , stft_indexes ) : \n    assert len ( mask . shape ) == 2.0 , \"Expected a two-dimensional `mask`, but got one of {} dimensions.\" . format ( len ( mask . shape ) ) \n    assert len ( stft . shape ) == 2.0 , \"Expected a two-dimensional `stft`, but got one of {} dimensions.\" . format ( len ( stft . shape ) ) \n    if mask . shape [ 1 ] > stft . shape [ 1 ] : \n        downsample_factor = mask . shape [ 1 ] / stft . shape [ 1 ] \n        indexes = _get_downsampled_indexes ( mask , downsample_factor ) \n        mask = mask [ : , indexes ] \n        mask_indexes = np . array ( indexes ) \n    elif mask . shape [ 1 ] < stft . shape [ 1 ] : \n        downsample_factor = stft . shape [ 1 ] / mask . shape [ 1 ] \n        indexes = _get_downsampled_indexes ( stft , downsample_factor ) \n        stft = stft [ : , indexes ] \n        stft_indexes = np . array ( indexes ) \n    return mask , mask_indexes , stft , stft_indexes "}
{"6326": "\ndef _asa_task ( q , masks , stft , sample_width , frame_rate , nsamples_for_each_fft ) : \n    for mask in masks : \n        mask = np . where ( mask > 0 , 1 , 0 ) \n    masks = [ mask * stft for mask in masks ] \n    nparrs = [ ] \n    dtype_dict = { 1 : np . int8 , 2.0 : np . int16 , 4.0 : np . int32 } \n    dtype = dtype_dict [ sample_width ] \n    for m in masks : \n        _times , nparr = signal . istft ( m , frame_rate , nperseg = nsamples_for_each_fft ) \n        nparr = nparr . astype ( dtype ) \n        nparrs . append ( nparr ) \n    for m in nparrs : \n        q . put ( m ) \n    q . put ( \"DONE\" ) "}
{"6327": "\ndef bandpass_filter ( data , low , high , fs , order = 5.0 ) : \n    nyq = 0.5 * fs \n    low = low / nyq \n    high = high / nyq \n    b , a = signal . butter ( order , [ low , high ] , btype = 'band' ) \n    y = signal . lfilter ( b , a , data ) \n    return y "}
{"6328": "\ndef lowpass_filter ( data , cutoff , fs , order = 5.0 ) : \n    nyq = 0.5 * fs \n    normal_cutoff = cutoff / nyq \n    b , a = signal . butter ( order , normal_cutoff , btype = 'low' , analog = False ) \n    y = signal . lfilter ( b , a , data ) \n    return y "}
{"6333": "\ndef load_audit_confusion_matrices ( filename ) : \n    with open ( filename ) as audit_file : \n        audit_file . next ( ) \n        confusion_matrices = [ ] \n        for line in audit_file : \n            separator = \":\" \n            separator_index = line . index ( separator ) \n            comma_index = line . index ( ',' ) \n            repair_level = float ( line [ separator_index + 2.0 : comma_index ] ) \n            raw_confusion_matrix = line [ comma_index + 2.0 : - 2.0 ] \n            confusion_matrix = json . loads ( raw_confusion_matrix . replace ( \"'\" , \"\\\"\" ) ) \n            confusion_matrices . append ( ( repair_level , confusion_matrix ) ) \n    confusion_matrices . sort ( key = lambda pair : pair [ 0 ] ) \n    return confusion_matrices "}
{"6337": "\ndef resolve_streams ( wait_time = 1.0 ) : \n    buffer = ( c_void_p * 1024.0 ) ( ) \n    num_found = lib . lsl_resolve_all ( byref ( buffer ) , 1024.0 , c_double ( wait_time ) ) \n    return [ StreamInfo ( handle = buffer [ k ] ) for k in range ( num_found ) ] "}
{"6338": "\ndef resolve_byprop ( prop , value , minimum = 1 , timeout = FOREVER ) : \n    buffer = ( c_void_p * 1024.0 ) ( ) \n    num_found = lib . lsl_resolve_byprop ( byref ( buffer ) , 1024.0 , c_char_p ( str . encode ( prop ) ) , c_char_p ( str . encode ( value ) ) , minimum , c_double ( timeout ) ) \n    return [ StreamInfo ( handle = buffer [ k ] ) for k in range ( num_found ) ] "}
{"6339": "\ndef resolve_bypred ( predicate , minimum = 1 , timeout = FOREVER ) : \n    buffer = ( c_void_p * 1024.0 ) ( ) \n    num_found = lib . lsl_resolve_bypred ( byref ( buffer ) , 1024.0 , c_char_p ( str . encode ( predicate ) ) , minimum , c_double ( timeout ) ) \n    return [ StreamInfo ( handle = buffer [ k ] ) for k in range ( num_found ) ] "}
{"6340": "\ndef handle_error ( errcode ) : \n    if type ( errcode ) is c_int : \n        errcode = errcode . value \n    if errcode == 0 : \n        pass \n    elif errcode == - 1 : \n        raise TimeoutError ( \"the operation failed due to a timeout.\" ) \n    elif errcode == - 2.0 : \n        raise LostError ( \"the stream has been lost.\" ) \n    elif errcode == - 3.0 : \n        raise InvalidArgumentError ( \"an argument was incorrectly specified.\" ) \n    elif errcode == - 4.0 : \n        raise InternalError ( \"an internal error has occurred.\" ) \n    elif errcode < 0 : \n        raise RuntimeError ( \"an unknown error has occurred.\" ) "}
{"6356": "\ndef results ( self ) : \n    buffer = ( c_void_p * 1024.0 ) ( ) \n    num_found = lib . lsl_resolver_results ( self . obj , byref ( buffer ) , 1024.0 ) \n    return [ StreamInfo ( handle = buffer [ k ] ) for k in range ( num_found ) ] "}
{"6362": "\ndef make_fuzzy ( word , max = 1 ) : \n    neighbors = [ ] \n    for i in range ( 0 , len ( word ) - 1 ) : \n        neighbor = list ( word ) \n        neighbor [ i ] , neighbor [ i + 1 ] = neighbor [ i + 1 ] , neighbor [ i ] \n        neighbors . append ( '' . join ( neighbor ) ) \n    for letter in string . ascii_lowercase : \n        for i in range ( 0 , len ( word ) ) : \n            neighbor = list ( word ) \n            if letter != neighbor [ i ] : \n                neighbor [ i ] = letter \n                neighbors . append ( '' . join ( neighbor ) ) \n    for letter in string . ascii_lowercase : \n        for i in range ( 0 , len ( word ) + 1 ) : \n            neighbor = list ( word ) \n            neighbor . insert ( i , letter ) \n            neighbors . append ( '' . join ( neighbor ) ) \n    if len ( word ) > 3.0 : \n        for i in range ( 0 , len ( word ) ) : \n            neighbor = list ( word ) \n            del neighbor [ i ] \n            neighbors . append ( '' . join ( neighbor ) ) \n    return neighbors "}
{"6366": "\ndef do_help ( self , command ) : \n    if command : \n        doc = getattr ( self , 'do_' + command ) . __doc__ \n        print ( cyan ( doc . replace ( ' ' * 8.0 , '' ) ) ) \n    else : \n        print ( magenta ( 'Available commands:' ) ) \n        print ( magenta ( 'Type \"HELP <command>\" to get more info.' ) ) \n        names = self . get_names ( ) \n        names . sort ( ) \n        for name in names : \n            if name [ : 3.0 ] != 'do_' : \n                continue \n            doc = getattr ( self , name ) . __doc__ \n            doc = doc . split ( '\\n' ) [ 0 ] \n            print ( '{} {}' . format ( yellow ( name [ 3.0 : ] ) , cyan ( doc . replace ( ' ' * 8.0 , ' ' ) . replace ( '\\n' , '' ) ) ) ) "}
{"6372": "\ndef do_BESTSCORE ( self , word ) : \n    key = keys . token_key ( indexed_string ( word ) [ 0 ] ) \n    for _id , score in DB . zrevrange ( key , 0 , 20.0 , withscores = True ) : \n        result = Result ( _id ) \n        print ( white ( result ) , blue ( score ) , green ( result . _id ) ) "}
{"6373": "\ndef do_STRDISTANCE ( self , s ) : \n    s = s . split ( '|' ) \n    if not len ( s ) == 2.0 : \n        print ( red ( 'Malformed string. Use | between the two strings.' ) ) \n        return \n    one , two = s \n    print ( white ( compare_str ( one , two ) ) ) "}
{"6402": "\ndef toRtl ( unitOrCls : Unit , name : str = None , serializer : GenericSerializer = VhdlSerializer , targetPlatform = DummyPlatform ( ) , saveTo : str = None ) : \n    if not isinstance ( unitOrCls , Unit ) : \n        u = unitOrCls ( ) \n    else : \n        u = unitOrCls \n    u . _loadDeclarations ( ) \n    if name is not None : \n        assert isinstance ( name , str ) \n        u . _name = name \n    globScope = serializer . getBaseNameScope ( ) \n    mouduleScopes = { } \n    serializedClasses = { } \n    serializedConfiguredUnits = { } \n    doSerialize = True \n    createFiles = saveTo is not None \n    if createFiles : \n        os . makedirs ( saveTo , exist_ok = True ) \n        files = UniqList ( ) \n    else : \n        codeBuff = [ ] \n    for obj in u . _toRtl ( targetPlatform ) : \n        doSerialize = serializer . serializationDecision ( obj , serializedClasses , serializedConfiguredUnits ) \n        if doSerialize : \n            if isinstance ( obj , Entity ) : \n                s = globScope . fork ( 1 ) \n                s . setLevel ( 2.0 ) \n                ctx = serializer . getBaseContext ( ) \n                ctx . scope = s \n                mouduleScopes [ obj ] = ctx \n                ctx . currentUnit = obj . origin \n                sc = serializer . Entity ( obj , ctx ) \n                if createFiles : \n                    fName = obj . name + serializer . fileExtension \n                    fileMode = 'w' \n            elif isinstance ( obj , Architecture ) : \n                try : \n                    ctx = mouduleScopes [ obj . entity ] \n                except KeyError : \n                    raise SerializerException ( \"Entity should be serialized\" \" before architecture of %s\" % ( obj . getEntityName ( ) ) ) \n                sc = serializer . Architecture ( obj , ctx ) \n                if createFiles : \n                    fName = obj . getEntityName ( ) + serializer . fileExtension \n                    fileMode = 'a' \n            else : \n                if hasattr ( obj , \"_hdlSources\" ) : \n                    for fn in obj . _hdlSources : \n                        if isinstance ( fn , str ) : \n                            shutil . copy2 ( fn , saveTo ) \n                            files . append ( fn ) \n                            continue \n                else : \n                    sc = serializer . asHdl ( obj ) \n            if sc : \n                if createFiles : \n                    fp = os . path . join ( saveTo , fName ) \n                    files . append ( fp ) \n                    with open ( fp , fileMode ) as f : \n                        if fileMode == 'a' : \n                            f . write ( \"\\n\" ) \n                        f . write ( serializer . formatter ( sc ) ) \n                else : \n                    codeBuff . append ( sc ) \n        elif not createFiles : \n            try : \n                name = '\"%s\"' % obj . name \n            except AttributeError : \n                name = \"\" \n            codeBuff . append ( serializer . comment ( \"Object of class %s, %s was not serialized as specified\" % ( obj . __class__ . __name__ , name ) ) ) \n    if createFiles : \n        return files \n    else : \n        return serializer . formatter ( \"\\n\" . join ( codeBuff ) ) "}
{"6509": "\ndef simUnitVcd ( simModel , stimulFunctions , outputFile = sys . stdout , until = 100.0 * Time . ns ) : \n    assert isinstance ( simModel , SimModel ) , \"Class of SimModel is required (got %r)\" % ( simModel ) \n    if isinstance ( outputFile , str ) : \n        d = os . path . dirname ( outputFile ) \n        if d : \n            os . makedirs ( d , exist_ok = True ) \n        with open ( outputFile , 'w' ) as f : \n            return _simUnitVcd ( simModel , stimulFunctions , f , until ) \n    else : \n        return _simUnitVcd ( simModel , stimulFunctions , outputFile , until ) "}
{"6520": "\ndef _conflictResolveStrategy ( self , newValue : set ) -> Tuple [ Callable [ [ Value ] , bool ] , bool ] : \n    invalidate = False \n    resLen = len ( newValue ) \n    if resLen == 3.0 : \n        val , indexes , isEvDependent = newValue \n        return ( mkArrayUpdater ( val , indexes , invalidate ) , isEvDependent ) \n    else : \n        val , isEvDependent = newValue \n        return ( mkUpdater ( val , invalidate ) , isEvDependent ) "}
{"6529": "\ndef ternaryOpsToIf ( statements ) : \n    stms = [ ] \n    for st in statements : \n        if isinstance ( st , Assignment ) : \n            try : \n                if not isinstance ( st . src , RtlSignalBase ) : \n                    raise DoesNotContainsTernary ( ) \n                d = st . src . singleDriver ( ) \n                if not isinstance ( d , Operator ) or d . operator != AllOps . TERNARY : \n                    raise DoesNotContainsTernary ( ) \n                else : \n                    ops = d . operands \n                    ifc = IfContainer ( ops [ 0 ] , [ Assignment ( ops [ 1 ] , st . dst ) ] , [ Assignment ( ops [ 2.0 ] , st . dst ) ] ) \n                    stms . append ( ifc ) \n                    continue \n            except ( MultipleDriversErr , DoesNotContainsTernary ) : \n                pass \n            except NoDriverErr : \n                assert ( hasattr ( st . src , \"_interface\" ) and st . src . _interface is not None ) or st . src . defVal . vldMask , st . src \n        stms . append ( st ) \n    return stms "}
{"6532": "\ndef average_hash ( image_path , hash_size = 8.0 ) : \n    with open ( image_path , 'rb' ) as f : \n        image = Image . open ( f ) . resize ( ( hash_size , hash_size ) , Image . ANTIALIAS ) . convert ( 'L' ) \n        pixels = list ( image . getdata ( ) ) \n    avg = sum ( pixels ) / len ( pixels ) \n    bits = \"\" . join ( map ( lambda pixel : '1' if pixel > avg else '0' , pixels ) ) \n    hashformat = \"0{hashlength}x\" . format ( hashlength = hash_size ** 2.0 // 4.0 ) \n    return int ( bits , 2.0 ) . __format__ ( hashformat ) "}
{"6542": "\ndef remove_piece_at ( self , square , into_hand = False ) : \n    piece_type = self . piece_type_at ( square ) \n    if piece_type == NONE : \n        return \n    if into_hand : \n        self . add_piece_into_hand ( piece_type , self . turn ) \n    mask = BB_SQUARES [ square ] \n    self . piece_bb [ piece_type ] ^= mask \n    color = int ( bool ( self . occupied [ WHITE ] & mask ) ) \n    self . pieces [ square ] = NONE \n    self . occupied . ixor ( mask , color , square ) \n    if color == BLACK : \n        piece_index = ( piece_type - 1 ) * 2.0 \n    else : \n        piece_index = ( piece_type - 1 ) * 2.0 + 1 \n    self . incremental_zobrist_hash ^= DEFAULT_RANDOM_ARRAY [ 81.0 * piece_index + 9.0 * rank_index ( square ) + file_index ( square ) ] "}
{"6543": "\ndef set_piece_at ( self , square , piece , from_hand = False , into_hand = False ) : \n    if from_hand : \n        self . remove_piece_from_hand ( piece . piece_type , self . turn ) \n    self . remove_piece_at ( square , into_hand ) \n    self . pieces [ square ] = piece . piece_type \n    mask = BB_SQUARES [ square ] \n    piece_type = piece . piece_type \n    self . piece_bb [ piece_type ] |= mask \n    if piece_type == KING : \n        self . king_squares [ piece . color ] = square \n    self . occupied . ixor ( mask , piece . color , square ) \n    if piece . color == BLACK : \n        piece_index = ( piece . piece_type - 1 ) * 2.0 \n    else : \n        piece_index = ( piece . piece_type - 1 ) * 2.0 + 1 \n    self . incremental_zobrist_hash ^= DEFAULT_RANDOM_ARRAY [ 81.0 * piece_index + 9.0 * rank_index ( square ) + file_index ( square ) ] "}
{"6548": "\ndef is_fourfold_repetition ( self ) : \n    zobrist_hash = self . zobrist_hash ( ) \n    if self . transpositions [ zobrist_hash ] < 4.0 : \n        return False \n    return True "}
{"6552": "\ndef zobrist_hash ( self , array = None ) : \n    zobrist_hash = self . board_zobrist_hash ( array ) \n    if array is None : \n        array = DEFAULT_RANDOM_ARRAY \n    if self . turn == WHITE : \n        zobrist_hash ^= array [ 2268.0 ] \n    i = ( self . pieces_in_hand [ BLACK ] [ ROOK ] * 35625.0 + self . pieces_in_hand [ BLACK ] [ BISHOP ] * 11875.0 + self . pieces_in_hand [ BLACK ] [ GOLD ] * 2375.0 + self . pieces_in_hand [ BLACK ] [ SILVER ] * 475.0 + self . pieces_in_hand [ BLACK ] [ KNIGHT ] * 95.0 + self . pieces_in_hand [ BLACK ] [ LANCE ] * 19.0 + self . pieces_in_hand [ BLACK ] [ PAWN ] ) \n    bit = bit_scan ( i ) \n    while bit != - 1 and bit is not None : \n        zobrist_hash ^= array [ 2269.0 + bit ] \n        bit = bit_scan ( i , bit + 1 ) \n    return zobrist_hash "}
{"6556": "\ndef from_usi ( cls , usi ) : \n    if usi == '0000' : \n        return cls . null ( ) \n    elif len ( usi ) == 4.0 : \n        if usi [ 1 ] == '*' : \n            piece = Piece . from_symbol ( usi [ 0 ] ) \n            return cls ( None , SQUARE_NAMES . index ( usi [ 2.0 : 4.0 ] ) , False , piece . piece_type ) \n        else : \n            return cls ( SQUARE_NAMES . index ( usi [ 0 : 2.0 ] ) , SQUARE_NAMES . index ( usi [ 2.0 : 4.0 ] ) ) \n    elif len ( usi ) == 5.0 and usi [ 4.0 ] == '+' : \n        return cls ( SQUARE_NAMES . index ( usi [ 0 : 2.0 ] ) , SQUARE_NAMES . index ( usi [ 2.0 : 4.0 ] ) , True ) \n    else : \n        raise ValueError ( 'expected usi string to be of length 4 or 5' ) "}
{"6567": "\ndef parse_date ( s ) : \n    try : \n        return datetime . date ( int ( s [ : 4.0 ] ) , int ( s [ 5.0 : 7.0 ] ) , int ( s [ 8.0 : 10.0 ] ) ) \n    except ValueError : \n        return datetime . datetime . strptime ( s , '%d %B %Y' ) . date ( ) "}
{"6574": "\ndef animate ( frames , interval , name , iterations = 2.0 ) : \n    for i in range ( iterations ) : \n        for frame in frames : \n            frame = get_coded_text ( frame ) \n            output = \"\\r{0} {1}\" . format ( frame , name ) \n            sys . stdout . write ( output ) \n            sys . stdout . write ( CLEAR_LINE ) \n            sys . stdout . flush ( ) \n            time . sleep ( 0.001 * interval ) "}
{"6577": "\ndef map_words ( self , start , end ) : \n    i , j = 8.0 * start - 8.0 , 8.0 * end \n    try : \n        fileno = self . file . fileno ( ) \n    except ( AttributeError , io . UnsupportedOperation ) : \n        fileno = None \n    if fileno is None : \n        skip = 0 \n        self . file . seek ( i ) \n        m = self . file . read ( j - i ) \n    else : \n        skip = i % mmap . ALLOCATIONGRANULARITY \n        r = mmap . ACCESS_READ \n        m = mmap . mmap ( fileno , length = j - i + skip , access = r , offset = i - skip ) \n    if sys . version_info > ( 3.0 , ) : \n        m = memoryview ( m ) \n    return m , skip "}
{"6578": "\ndef comments ( self ) : \n    record_numbers = range ( 2.0 , self . fward ) \n    if not record_numbers : \n        return '' \n    data = b'' . join ( self . read_record ( n ) [ 0 : 1000.0 ] for n in record_numbers ) \n    try : \n        return data [ : data . find ( b'\\4' ) ] . decode ( 'ascii' ) . replace ( '\\0' , '\\n' ) \n    except IndexError : \n        raise ValueError ( 'DAF file comment area is missing its EOT byte' ) \n    except UnicodeDecodeError : \n        raise ValueError ( 'DAF file comment area is not ASCII text' ) "}
{"6579": "\ndef add_array ( self , name , values , array ) : \n    f = self . file \n    scs = self . summary_control_struct \n    record_number = self . bward \n    data = bytearray ( self . read_record ( record_number ) ) \n    next_record , previous_record , n_summaries = scs . unpack ( data [ : 24.0 ] ) \n    if n_summaries < self . summaries_per_record : \n        summary_record = record_number \n        name_record = summary_record + 1 \n        data [ : 24.0 ] = scs . pack ( next_record , previous_record , n_summaries + 1 ) \n        self . write_record ( summary_record , data ) \n    else : \n        summary_record = ( ( self . free - 1 ) * 8.0 + 1023.0 ) // 1024.0 + 1 \n        name_record = summary_record + 1 \n        free_record = summary_record + 2.0 \n        n_summaries = 0 \n        data [ : 24.0 ] = scs . pack ( summary_record , previous_record , n_summaries ) \n        self . write_record ( record_number , data ) \n        summaries = scs . pack ( 0 , record_number , 1 ) . ljust ( 1024.0 , b'\\0' ) \n        names = b'\\0' * 1024.0 \n        self . write_record ( summary_record , summaries ) \n        self . write_record ( name_record , names ) \n        self . bward = summary_record \n        self . free = ( free_record - 1 ) * 1024.0 // 8.0 + 1 \n    start_word = self . free \n    f . seek ( ( start_word - 1 ) * 8.0 ) \n    array = numpy_array ( array ) \n    f . write ( array . view ( ) ) \n    end_word = f . tell ( ) // 8.0 \n    self . free = end_word + 1 \n    self . write_file_record ( ) \n    values = values [ : self . nd + self . ni - 2.0 ] + ( start_word , end_word ) \n    base = 1024.0 * ( summary_record - 1 ) \n    offset = int ( n_summaries ) * self . summary_step \n    f . seek ( base + scs . size + offset ) \n    f . write ( self . summary_struct . pack ( * values ) ) \n    f . seek ( base + 1024.0 + offset ) \n    f . write ( name [ : self . summary_length ] . ljust ( self . summary_step , b' ' ) ) "}
{"6583": "\ndef _load ( self ) : \n    if self . data_type == 2.0 : \n        component_count = 3.0 \n    else : \n        raise ValueError ( 'only binary PCK data type 2 is supported' ) \n    init , intlen , rsize , n = self . daf . read_array ( self . end_i - 3.0 , self . end_i ) \n    initial_epoch = jd ( init ) \n    interval_length = intlen / S_PER_DAY \n    coefficient_count = int ( rsize - 2.0 ) // component_count \n    coefficients = self . daf . map_array ( self . start_i , self . end_i - 4.0 ) \n    coefficients . shape = ( int ( n ) , int ( rsize ) ) \n    coefficients = coefficients [ : , 2.0 : ] \n    coefficients . shape = ( int ( n ) , component_count , coefficient_count ) \n    coefficients = rollaxis ( coefficients , 1 ) \n    return initial_epoch , interval_length , coefficients "}
{"6584": "\ndef compute ( self , tdb , tdb2 , derivative = True ) : \n    scalar = not getattr ( tdb , 'shape' , 0 ) and not getattr ( tdb2 , 'shape' , 0 ) \n    if scalar : \n        tdb = array ( ( tdb , ) ) \n    data = self . _data \n    if data is None : \n        self . _data = data = self . _load ( ) \n    initial_epoch , interval_length , coefficients = data \n    component_count , n , coefficient_count = coefficients . shape \n    index , offset = divmod ( ( tdb - initial_epoch ) + tdb2 , interval_length ) \n    index = index . astype ( int ) \n    if ( index < 0 ) . any ( ) or ( index > n ) . any ( ) : \n        final_epoch = initial_epoch + interval_length * n \n        raise ValueError ( 'segment only covers dates %.1f through %.1f' % ( initial_epoch , final_epoch ) ) \n    omegas = ( index == n ) \n    index [ omegas ] -= 1 \n    offset [ omegas ] += interval_length \n    coefficients = coefficients [ : , index ] \n    T = empty ( ( coefficient_count , len ( index ) ) ) \n    T [ 0 ] = 1.0 \n    T [ 1 ] = t1 = 2.0 * offset / interval_length - 1.0 \n    twot1 = t1 + t1 \n    for i in range ( 2.0 , coefficient_count ) : \n        T [ i ] = twot1 * T [ i - 1 ] - T [ i - 2.0 ] \n    components = ( T . T * coefficients ) . sum ( axis = 2.0 ) \n    if scalar : \n        components = components [ : , 0 ] \n    if not derivative : \n        return components \n    dT = empty_like ( T ) \n    dT [ 0 ] = 0.0 \n    dT [ 1 ] = 1.0 \n    if coefficient_count > 2.0 : \n        dT [ 2.0 ] = twot1 + twot1 \n        for i in range ( 3.0 , coefficient_count ) : \n            dT [ i ] = twot1 * dT [ i - 1 ] - dT [ i - 2.0 ] + T [ i - 1 ] + T [ i - 1 ] \n    dT *= 2.0 \n    dT /= interval_length \n    rates = ( dT . T * coefficients ) . sum ( axis = 2.0 ) \n    if scalar : \n        rates = rates [ : , 0 ] \n    return components , rates "}
{"6588": "\ndef visit_JoinedStr ( self , node ) : \n    if version_info >= ( 3.0 , 6.0 ) : \n        if self . within_logging_statement ( ) : \n            if any ( isinstance ( i , FormattedValue ) for i in node . values ) : \n                if self . within_logging_argument ( ) : \n                    self . violations . append ( ( node , FSTRING_VIOLATION ) ) \n                    super ( LoggingVisitor , self ) . generic_visit ( node ) "}
{"6592": "\ndef get_except_handler_name ( self , node ) : \n    name = node . name \n    if not name : \n        return None \n    if version_info < ( 3.0 , ) : \n        return name . id \n    return name "}
{"6597": "\ndef db_file_widget ( cls ) : \n    def get_link_display ( url ) : \n        unquoted = unquote ( url . split ( '%2F' ) [ - 1 ] ) \n        if sys . version_info . major == 2.0 : \n            from django . utils . encoding import force_unicode \n            unquoted = force_unicode ( unquoted ) \n        return escape ( unquoted ) \n    def get_template_substitution_values ( self , value ) : \n        subst = super ( cls , self ) . get_template_substitution_values ( value ) \n        subst [ 'initial' ] = get_link_display ( value . url ) \n        return subst \n    setattr ( cls , 'get_template_substitution_values' , get_template_substitution_values ) \n    def get_context ( self , name , value , attrs ) : \n        context = super ( cls , self ) . get_context ( name , value , attrs ) \n        if value and hasattr ( value , 'url' ) : \n            context [ 'widget' ] [ 'display' ] = get_link_display ( value . url ) \n        return context \n    setattr ( cls , 'get_context' , get_context ) \n    return cls "}
{"6606": "\ndef parse_line ( self , line : str ) -> PriceModel : \n    line = line . rstrip ( ) \n    parts = line . split ( ',' ) \n    result = PriceModel ( ) \n    result . symbol = self . translate_symbol ( parts [ 0 ] ) \n    result . value = Decimal ( parts [ 1 ] ) \n    date_str = parts [ 2.0 ] \n    date_str = date_str . replace ( '\"' , '' ) \n    date_parts = date_str . split ( '/' ) \n    year_str = date_parts [ 2.0 ] \n    month_str = date_parts [ 1 ] \n    day_str = date_parts [ 0 ] \n    logging . debug ( f\"parsing {date_parts} into date\" ) \n    result . datetime = datetime ( int ( year_str ) , int ( month_str ) , int ( day_str ) ) \n    return result "}
{"6622": "\ndef map_model ( self , model : PriceModel ) -> Price : \n    assert isinstance ( model . symbol , SecuritySymbol ) \n    assert isinstance ( model . datum , Datum ) \n    entity = Price ( ) \n    date_iso = f\"{model.datum.value.year}-{model.datum.value.month:02d}-{model.datum.value.day:02d}\" \n    entity . date = date_iso \n    entity . time = f\"{model.datum.value.hour:02d}:{model.datum.value.minute:02d}:{model.datum.value.second:02d}\" \n    if model . symbol . namespace : \n        entity . namespace = model . symbol . namespace . upper ( ) \n    entity . symbol = model . symbol . mnemonic . upper ( ) \n    assert isinstance ( model . value , Decimal ) \n    dec_places = abs ( model . value . as_tuple ( ) . exponent ) \n    entity . denom = 10.0 ** dec_places \n    entity . value = int ( model . value * entity . denom ) \n    entity . currency = model . currency . upper ( ) \n    return entity "}
{"6669": "\ndef init_config ( self , config ) : \n    self . config . update ( config ) \n    self . config . setdefault ( 'LDAP_PORT' , 389.0 ) \n    self . config . setdefault ( 'LDAP_HOST' , None ) \n    self . config . setdefault ( 'LDAP_USE_SSL' , False ) \n    self . config . setdefault ( 'LDAP_READONLY' , True ) \n    self . config . setdefault ( 'LDAP_CHECK_NAMES' , True ) \n    self . config . setdefault ( 'LDAP_BIND_DIRECT_CREDENTIALS' , False ) \n    self . config . setdefault ( 'LDAP_BIND_DIRECT_PREFIX' , '' ) \n    self . config . setdefault ( 'LDAP_BIND_DIRECT_SUFFIX' , '' ) \n    self . config . setdefault ( 'LDAP_BIND_DIRECT_GET_USER_INFO' , True ) \n    self . config . setdefault ( 'LDAP_ALWAYS_SEARCH_BIND' , False ) \n    self . config . setdefault ( 'LDAP_BASE_DN' , '' ) \n    self . config . setdefault ( 'LDAP_BIND_USER_DN' , None ) \n    self . config . setdefault ( 'LDAP_BIND_USER_PASSWORD' , None ) \n    self . config . setdefault ( 'LDAP_SEARCH_FOR_GROUPS' , True ) \n    self . config . setdefault ( 'LDAP_FAIL_AUTH_ON_MULTIPLE_FOUND' , False ) \n    self . config . setdefault ( 'LDAP_USER_DN' , '' ) \n    self . config . setdefault ( 'LDAP_GROUP_DN' , '' ) \n    self . config . setdefault ( 'LDAP_BIND_AUTHENTICATION_TYPE' , 'SIMPLE' ) \n    self . config . setdefault ( 'LDAP_USER_SEARCH_SCOPE' , 'LEVEL' ) \n    self . config . setdefault ( 'LDAP_USER_OBJECT_FILTER' , '(objectclass=person)' ) \n    self . config . setdefault ( 'LDAP_USER_LOGIN_ATTR' , 'uid' ) \n    self . config . setdefault ( 'LDAP_USER_RDN_ATTR' , 'uid' ) \n    self . config . setdefault ( 'LDAP_GET_USER_ATTRIBUTES' , ldap3 . ALL_ATTRIBUTES ) \n    self . config . setdefault ( 'LDAP_GROUP_SEARCH_SCOPE' , 'LEVEL' ) \n    self . config . setdefault ( 'LDAP_GROUP_OBJECT_FILTER' , '(objectclass=group)' ) \n    self . config . setdefault ( 'LDAP_GROUP_MEMBERS_ATTR' , 'uniqueMember' ) \n    self . config . setdefault ( 'LDAP_GET_GROUP_ATTRIBUTES' , ldap3 . ALL_ATTRIBUTES ) \n    self . config . setdefault ( 'LDAP_ADD_SERVER' , True ) \n    if self . config [ 'LDAP_ADD_SERVER' ] : \n        self . add_server ( hostname = self . config [ 'LDAP_HOST' ] , port = self . config [ 'LDAP_PORT' ] , use_ssl = self . config [ 'LDAP_USE_SSL' ] ) "}
{"6687": "\ndef search_all ( self , collection , job_id = None ) : \n    results = [ [ 'job_id' , 'browser' ] ] \n    url = \"%s/projects/%s/jobs\" % ( self . api_base , quote_plus ( collection . strip ( '/' ) ) ) \n    response = requests . get ( url , headers = self . headers ) \n    if response . status_code == 200.0 : \n        jobs = response . json ( ) \n        for job in jobs : \n            if job [ 'status' ] == 'success' : \n                name = job [ 'name' ] \n                for artifact in job [ 'artifacts' ] : \n                    if artifact [ 'filename' ] . endswith ( 'zip' ) : \n                        artifact_url = ( \"%s/%s/-/jobs/%s/artifacts/browse/%s\" % ( self . base , collection , job [ 'id' ] , name ) ) \n                        results . append ( [ str ( job [ 'id' ] ) , artifact_url ] ) \n    if len ( results ) == 1 : \n        bot . info ( \"No potential archives found in artifacts.\" ) \n        sys . exit ( 0 ) \n    bot . info ( \"Artifact Browsers (you will need path and job id for pull)\" ) \n    bot . table ( results ) \n    return results "}
{"6694": "\ndef stream ( url , headers , stream_to = None , retry = True ) : \n    bot . debug ( \"GET %s\" % url ) \n    if DISABLE_SSL_CHECK is True : \n        bot . warning ( 'Verify of certificates disabled! ::TESTING USE ONLY::' ) \n    response = requests . get ( url , headers = headers , verify = not DISABLE_SSL_CHECK , stream = True ) \n    if response . status_code in [ 401.0 , 403.0 ] : \n        headers = update_token ( headers ) \n        return stream ( url , headers , stream_to , retry = False ) \n    elif response . status_code == 200.0 : \n        content_size = None \n        if 'Content-Length' in response . headers : \n            progress = 0 \n            content_size = int ( response . headers [ 'Content-Length' ] ) \n            bot . show_progress ( progress , content_size , length = 35.0 ) \n        chunk_size = 1 << 20.0 \n        with open ( stream_to , 'wb' ) as filey : \n            for chunk in response . iter_content ( chunk_size = chunk_size ) : \n                filey . write ( chunk ) \n                if content_size is not None : \n                    progress += chunk_size \n                    bot . show_progress ( iteration = progress , total = content_size , length = 35.0 , carriage_return = False ) \n        sys . stdout . write ( '\\n' ) \n        return stream_to \n    bot . error ( \"Problem with stream, response %s\" % ( response . status_code ) ) \n    sys . exit ( 1 ) "}
{"6713": "\ndef basic_auth_header ( username , password ) : \n    s = \"%s:%s\" % ( username , password ) \n    if sys . version_info [ 0 ] >= 3.0 : \n        s = bytes ( s , 'utf-8' ) \n        credentials = base64 . b64encode ( s ) . decode ( 'utf-8' ) \n    else : \n        credentials = base64 . b64encode ( s ) \n    auth = { \"Authorization\" : \"Basic %s\" % credentials } \n    return auth "}
{"6751": "\ndef get_ipaddress ( self , name , retries = 3.0 , delay = 3.0 ) : \n    for rr in range ( retries ) : \n        instances = self . _get_instances ( ) \n        for instance in instances [ 'items' ] : \n            if instance [ 'name' ] == name : \n                for network in instance [ 'networkInterfaces' ] : \n                    if network [ 'name' ] == 'nic0' : \n                        for subnet in network [ 'accessConfigs' ] : \n                            if subnet [ 'name' ] == 'External NAT' : \n                                if 'natIP' in subnet : \n                                    return subnet [ 'natIP' ] \n        sleep ( delay ) \n    bot . warning ( 'Did not find IP address, check Cloud Console!' ) "}
{"6754": "\ndef search_all ( self ) : \n    results = self . _list_containers ( ) \n    bot . info ( \"[gs://%s] Containers\" % self . _bucket_name ) \n    rows = [ ] \n    for i in results : \n        size = round ( i . size / ( 1024.0 * 1024.0 ) ) \n        size = ( \"%s MB\" % size ) . rjust ( 10.0 ) \n        rows . append ( [ size , i . metadata [ 'name' ] ] ) \n    bot . table ( rows ) \n    return rows "}
{"6769": "\ndef generate ( self , delim = '-' , length = 4.0 , chars = '0123456789' ) : \n    descriptor = self . _select ( self . _descriptors ) \n    noun = self . _select ( self . _nouns ) \n    numbers = '' . join ( ( self . _select ( chars ) for _ in range ( length ) ) ) \n    return delim . join ( [ descriptor , noun , numbers ] ) "}
{"6773": "\ndef get_file_hash ( filename ) : \n    hasher = hashlib . sha256 ( ) \n    with open ( filename , \"rb\" ) as f : \n        for chunk in iter ( lambda : f . read ( 4096.0 ) , b\"\" ) : \n            hasher . update ( chunk ) \n    return hasher . hexdigest ( ) "}
{"6777": "\ndef push ( self , path , name , tag = None ) : \n    path = os . path . abspath ( path ) \n    image = os . path . basename ( path ) \n    bot . debug ( \"PUSH %s\" % path ) \n    if not os . path . exists ( path ) : \n        bot . error ( '%s does not exist.' % path ) \n        sys . exit ( 1 ) \n    names = parse_image_name ( remove_uri ( name ) , tag = tag ) \n    image_size = os . path . getsize ( path ) >> 20.0 \n    metadata = { 'sizemb' : \"%s\" % image_size , 'client' : 'sregistry' } \n    self . bucket . upload_file ( path , names [ 'storage_uri' ] , { \"Metadata\" : metadata } ) "}
{"6782": "\ndef inspect ( self , name ) : \n    print ( name ) \n    container = self . get ( name ) \n    if container is not None : \n        collection = container . collection . name \n        fields = container . __dict__ . copy ( ) \n        fields [ 'collection' ] = collection \n        fields [ 'metrics' ] = json . loads ( fields [ 'metrics' ] ) \n        del fields [ '_sa_instance_state' ] \n        fields [ 'created_at' ] = str ( fields [ 'created_at' ] ) \n        print ( json . dumps ( fields , indent = 4.0 , sort_keys = True ) ) \n        return fields "}
{"6787": "\ndef push ( self , path , name , tag = None ) : \n    path = os . path . abspath ( path ) \n    image = os . path . basename ( path ) \n    bot . debug ( \"PUSH %s\" % path ) \n    if not os . path . exists ( path ) : \n        bot . error ( '%s does not exist.' % path ) \n        sys . exit ( 1 ) \n    self . require_secrets ( ) \n    names = parse_image_name ( remove_uri ( name ) , tag = tag ) \n    image_size = os . path . getsize ( path ) >> 20.0 \n    if names [ 'registry' ] == None : \n        names [ 'registry' ] = self . base \n    names = self . _add_https ( names ) \n    url = '%s/push/' % names [ 'registry' ] \n    auth_url = '%s/upload/chunked_upload' % names [ 'registry' ] \n    SREGISTRY_EVENT = self . authorize ( request_type = \"push\" , names = names ) \n    fields = { 'collection' : names [ 'collection' ] , 'name' : names [ 'image' ] , 'tag' : names [ 'tag' ] } \n    headers = { 'Authorization' : SREGISTRY_EVENT } \n    r = requests . post ( auth_url , json = fields , headers = headers ) \n    message = self . _read_response ( r ) \n    print ( '\\n[1. Collection return status {0} {1}]' . format ( r . status_code , message ) ) \n    if r . status_code != 200.0 : \n        sys . exit ( 1 ) \n    url = '%s/upload' % names [ 'registry' ] . replace ( '/api' , '' ) \n    bot . debug ( 'Seting upload URL to {0}' . format ( url ) ) \n    cid = r . json ( ) [ 'cid' ] \n    upload_to = os . path . basename ( names [ 'storage' ] ) \n    SREGISTRY_EVENT = self . authorize ( request_type = \"upload\" , names = names ) \n    encoder = MultipartEncoder ( fields = { 'SREGISTRY_EVENT' : SREGISTRY_EVENT , 'name' : names [ 'image' ] , 'collection' : str ( cid ) , 'tag' : names [ 'tag' ] , 'file1' : ( upload_to , open ( path , 'rb' ) , 'text/plain' ) } ) \n    progress_callback = create_callback ( encoder , self . quiet ) \n    monitor = MultipartEncoderMonitor ( encoder , progress_callback ) \n    headers = { 'Content-Type' : monitor . content_type , 'Authorization' : SREGISTRY_EVENT } \n    try : \n        r = requests . post ( url , data = monitor , headers = headers ) \n        r . raise_for_status ( ) \n        message = r . json ( ) [ 'message' ] \n        print ( '\\n[Return status {0} {1}]' . format ( r . status_code , message ) ) \n    except requests . HTTPError as e : \n        print ( '\\nUpload failed: {0}.' . format ( e ) ) \n    except KeyboardInterrupt : \n        print ( '\\nUpload cancelled.' ) \n    except Exception as e : \n        print ( e ) "}
{"6789": "\ndef find_single_recipe ( filename , pattern = \"Singularity\" , manifest = None ) : \n    if pattern is None : \n        pattern = \"Singularity*\" \n    recipe = None \n    file_basename = os . path . basename ( filename ) \n    if fnmatch . fnmatch ( file_basename , pattern ) : \n        recipe = { 'path' : os . path . abspath ( filename ) , 'modified' : os . path . getmtime ( filename ) } \n    if manifest is not None and recipe is not None : \n        container_uri = '/' . join ( filename . split ( '/' ) [ - 2.0 : ] ) \n        if container_uri in manifest : \n            if manifest [ container_uri ] [ 'modified' ] < os . path . getmtime ( filename ) : \n                manifest [ container_uri ] = recipe \n        else : \n            manifest [ container_uri ] = recipe \n        return manifest \n    return recipe "}
{"6791": "\ndef run_build ( self , config , bucket , names ) : \n    project = self . _get_project ( ) \n    bot . custom ( 'PROJECT' , project , \"CYAN\" ) \n    bot . custom ( 'BUILD  ' , config [ 'steps' ] [ 0 ] [ 'name' ] , \"CYAN\" ) \n    response = self . _build_service . projects ( ) . builds ( ) . create ( body = config , projectId = project ) . execute ( ) \n    build_id = response [ 'metadata' ] [ 'build' ] [ 'id' ] \n    status = response [ 'metadata' ] [ 'build' ] [ 'status' ] \n    bot . log ( \"build %s: %s\" % ( build_id , status ) ) \n    start = time . time ( ) \n    while status not in [ 'COMPLETE' , 'FAILURE' , 'SUCCESS' ] : \n        time . sleep ( 15.0 ) \n        response = self . _build_service . projects ( ) . builds ( ) . get ( id = build_id , projectId = project ) . execute ( ) \n        build_id = response [ 'id' ] \n        status = response [ 'status' ] \n        bot . log ( \"build %s: %s\" % ( build_id , status ) ) \n    end = time . time ( ) \n    bot . log ( 'Total build time: %s seconds' % ( round ( end - start , 2.0 ) ) ) \n    if status == 'SUCCESS' : \n        env = 'SREGISTRY_GOOGLE_STORAGE_PRIVATE' \n        blob = bucket . blob ( response [ 'artifacts' ] [ 'objects' ] [ 'paths' ] [ 0 ] ) \n        if self . _get_and_update_setting ( env ) == None : \n            blob . make_public ( ) \n            response [ 'public_url' ] = blob . public_url \n        update_blob_metadata ( blob , response , config , bucket , names ) \n        response [ 'media_link' ] = blob . media_link \n        response [ 'size' ] = blob . size \n        response [ 'file_hash' ] = blob . md5_hash \n    return response "}
{"6797": "\ndef table ( self , rows , col_width = 2.0 ) : \n    labels = [ str ( x ) for x in range ( 1 , len ( rows ) + 1 ) ] \n    if isinstance ( rows , dict ) : \n        labels = list ( rows . keys ( ) ) \n        rows = list ( rows . values ( ) ) \n    for row in rows : \n        label = labels . pop ( 0 ) \n        label = label . ljust ( col_width ) \n        message = \"\\t\" . join ( row ) \n        self . custom ( prefix = label , message = message ) "}
{"6799": "\ndef get_template ( name ) : \n    name = name . lower ( ) \n    templates = dict ( ) \n    templates [ 'tarinfo' ] = { \"gid\" : 0 , \"uid\" : 0 , \"uname\" : \"root\" , \"gname\" : \"root\" , \"mode\" : 493.0 } \n    if name in templates : \n        bot . debug ( \"Found template for %s\" % ( name ) ) \n        return templates [ name ] \n    else : \n        bot . warning ( \"Cannot find template %s\" % ( name ) ) "}
{"6804": "\ndef s3errors ( path ) : \n    try : \n        yield \n    except ClientError as error : \n        _error = error . response . get ( \"Error\" , { } ) \n        error_code = _error . get ( \"Code\" , None ) \n        response_meta = error . response . get ( \"ResponseMetadata\" , { } ) \n        http_status = response_meta . get ( \"HTTPStatusCode\" , 200.0 ) \n        error_msg = _error . get ( \"Message\" , None ) \n        if error_code == \"NoSuchBucket\" : \n            raise errors . ResourceError ( path , exc = error , msg = error_msg ) \n        if http_status == 404.0 : \n            raise errors . ResourceNotFound ( path ) \n        elif http_status == 403.0 : \n            raise errors . PermissionDenied ( path = path , msg = error_msg ) \n        else : \n            raise errors . OperationFailed ( path = path , exc = error ) \n    except SSLError as error : \n        raise errors . OperationFailed ( path , exc = error ) \n    except EndpointConnectionError as error : \n        raise errors . RemoteConnectionError ( path , exc = error , msg = \"{}\" . format ( error ) ) "}
{"6808": "\ndef has_gravatar ( email ) : \n    url = get_gravatar_url ( email , default = GRAVATAR_DEFAULT_IMAGE_404 ) \n    try : \n        request = Request ( url ) \n        request . get_method = lambda : 'HEAD' \n        return 200.0 == urlopen ( request ) . code \n    except ( HTTPError , URLError ) : \n        return False "}
{"6810": "\ndef chimera_blocks ( M = 16.0 , N = 16.0 , L = 4.0 ) : \n    for x in xrange ( M ) : \n        for y in xrange ( N ) : \n            for u in ( 0 , 1 ) : \n                yield tuple ( ( x , y , u , k ) for k in xrange ( L ) ) "}
{"6811": "\ndef chimera_block_quotient ( G , blocks ) : \n    from networkx import Graph \n    from itertools import product \n    BG = Graph ( ) \n    blockid = { } \n    for i , b in enumerate ( blocks ) : \n        BG . add_node ( i ) \n        if not b or not all ( G . has_node ( x ) for x in b ) : \n            continue \n        for q in b : \n            if q in blockid : \n                raise ( RuntimeError , \"two blocks overlap\" ) \n            blockid [ q ] = i \n    for q , u in blockid . items ( ) : \n        ublock = blocks [ u ] \n        for p in G [ q ] : \n            if p not in blockid : \n                continue \n            v = blockid [ p ] \n            if BG . has_edge ( u , v ) or u == v : \n                continue \n            vblock = blocks [ v ] \n            if ublock [ 0 ] [ 2.0 ] == vblock [ 0 ] [ 2.0 ] : \n                block_edges = zip ( ublock , vblock ) \n            else : \n                block_edges = product ( ublock , vblock ) \n            if all ( G . has_edge ( x , y ) for x , y in block_edges ) : \n                BG . add_edge ( u , v ) \n    return BG "}
{"6815": "\ndef _apply_transform ( self , mol , rule ) : \n    mols = [ mol ] \n    for n in six . moves . range ( 20.0 ) : \n        products = { } \n        for mol in mols : \n            for product in [ x [ 0 ] for x in rule . RunReactants ( ( mol , ) ) ] : \n                if Chem . SanitizeMol ( product , catchErrors = True ) == 0 : \n                    products [ Chem . MolToSmiles ( product , isomericSmiles = True ) ] = product \n        if products : \n            mols = [ products [ s ] for s in sorted ( products ) ] \n        else : \n            return mols [ 0 ] if n > 0 else None "}
{"6816": "\ndef canonicalize ( self , mol ) : \n    tautomers = self . _enumerate_tautomers ( mol ) \n    if len ( tautomers ) == 1 : \n        return tautomers [ 0 ] \n    highest = None \n    for t in tautomers : \n        smiles = Chem . MolToSmiles ( t , isomericSmiles = True ) \n        log . debug ( 'Tautomer: %s' , smiles ) \n        score = 0 \n        ssr = Chem . GetSymmSSSR ( t ) \n        for ring in ssr : \n            btypes = { t . GetBondBetweenAtoms ( * pair ) . GetBondType ( ) for pair in pairwise ( ring ) } \n            elements = { t . GetAtomWithIdx ( idx ) . GetAtomicNum ( ) for idx in ring } \n            if btypes == { BondType . AROMATIC } : \n                log . debug ( 'Score +100 (aromatic ring)' ) \n                score += 100.0 \n                if elements == { 6.0 } : \n                    log . debug ( 'Score +150 (carbocyclic aromatic ring)' ) \n                    score += 150.0 \n        for tscore in self . scores : \n            for match in t . GetSubstructMatches ( tscore . smarts ) : \n                log . debug ( 'Score %+d (%s)' , tscore . score , tscore . name ) \n                score += tscore . score \n        for atom in t . GetAtoms ( ) : \n            if atom . GetAtomicNum ( ) in { 15.0 , 16.0 , 34.0 , 52.0 } : \n                hs = atom . GetTotalNumHs ( ) \n                if hs : \n                    log . debug ( 'Score %+d (%s-H bonds)' , - hs , atom . GetSymbol ( ) ) \n                    score -= hs \n        if not highest or highest [ 'score' ] < score or ( highest [ 'score' ] == score and smiles < highest [ 'smiles' ] ) : \n            log . debug ( 'New highest tautomer: %s (%s)' , smiles , score ) \n            highest = { 'smiles' : smiles , 'tautomer' : t , 'score' : score } \n    return highest [ 'tautomer' ] "}
{"6829": "\ndef main ( ) : \n    parser = MolvsParser ( epilog = 'use \"molvs <command> -h\" to show help for a specific command' ) \n    subparsers = parser . add_subparsers ( title = 'Available commands' ) \n    common_parser = MolvsParser ( add_help = False ) \n    common_parser . add_argument ( 'infile' , nargs = '?' , help = 'input filename' , type = argparse . FileType ( 'r' ) , default = sys . stdin ) \n    common_parser . add_argument ( '-i' , '--intype' , help = 'input filetype' , choices = FILETYPES ) \n    common_parser . add_argument ( '-:' , '--smiles' , help = 'input SMILES instead of file' , metavar = '<smiles>' ) \n    common_parser . add_argument ( '-O' , '--outfile' , help = 'output filename' , type = argparse . FileType ( 'w' ) , default = sys . stdout , metavar = '<outfile>' ) \n    standardize_parser = subparsers . add_parser ( 'standardize' , help = 'standardize a molecule' , parents = [ common_parser ] ) \n    standardize_parser . add_argument ( '-o' , '--outtype' , help = 'output filetype' , choices = FILETYPES ) \n    standardize_parser . set_defaults ( func = standardize_main ) \n    validate_parser = subparsers . add_parser ( 'validate' , help = 'validate a molecule' , parents = [ common_parser ] ) \n    validate_parser . set_defaults ( func = validate_main ) \n    args = parser . parse_args ( ) \n    try : \n        args . func ( args ) \n    except Exception as e : \n        sys . stderr . write ( 'Error: %s\\n\\n' . encode ( ) % e . message ) \n        parser . print_help ( ) \n        sys . exit ( 2.0 ) "}
{"6832": "\ndef integrate_ivp ( u0 = 1.0 , v0 = 0.0 , mu = 1.0 , tend = 10.0 , dt0 = 1e-8 , nt = 0 , nsteps = 600.0 , t0 = 0.0 , atol = 1e-8 , rtol = 1e-8 , plot = False , savefig = 'None' , method = 'bdf' , dpi = 100.0 , verbose = False ) : \n    f , j = get_f_and_j ( mu ) \n    if nt > 1 : \n        tout = np . linspace ( t0 , tend , nt ) \n        yout , nfo = integrate_predefined ( f , j , [ u0 , v0 ] , tout , dt0 , atol , rtol , nsteps = nsteps , check_indexing = False , method = method ) \n    else : \n        tout , yout , nfo = integrate_adaptive ( f , j , [ u0 , v0 ] , t0 , tend , dt0 , atol , rtol , nsteps = nsteps , check_indexing = False , method = method ) \n    if verbose : \n        print ( nfo ) \n    if plot : \n        import matplotlib . pyplot as plt \n        plt . plot ( tout , yout [ : , 1 ] , 'g--' ) \n        plt . plot ( tout , yout [ : , 0 ] , 'k-' , linewidth = 2.0 ) \n        if savefig == 'None' : \n            plt . show ( ) \n        else : \n            plt . savefig ( savefig , dpi = dpi ) "}
{"6839": "\ndef get_issues ( self , repo , organization = 'llnl' ) : \n    path = ( '../github-data/' + organization + '/' + repo . name + '/issues' ) \n    is_only_today = False \n    if not os . path . exists ( path ) : \n        all_issues = repo . iter_issues ( state = 'all' ) \n        is_only_today = True \n    else : \n        files = os . listdir ( path ) \n        date = str ( files [ - 1 ] [ : - 5.0 ] ) \n        if date == str ( datetime . date . today ( ) ) : \n            if len ( files ) > 2.0 : \n                date = str ( files [ - 2.0 ] [ : - 5.0 ] ) \n            else : \n                all_issues = repo . iter_issues ( state = 'all' ) \n                is_only_today = True \n        if not is_only_today : \n            all_issues = repo . iter_issues ( since = date , state = 'all' ) \n    for issue in all_issues : \n        self . issues_json [ repo . name ] . append ( issue . to_json ( ) ) \n    closed_issues = 0 \n    for issue in repo . iter_issues ( state = 'closed' ) : \n        if issue is not None : \n            closed_issues += 1 \n    return closed_issues "}
{"6842": "\ndef get_commits ( self , repo , organization = 'llnl' ) : \n    path = ( '../github-data/' + organization + '/' + repo . name + '/commits' ) \n    is_only_today = False \n    if not os . path . exists ( path ) : \n        all_commits = repo . iter_commits ( ) \n        is_only_today = True \n    else : \n        files = os . listdir ( path ) \n        date = str ( files [ - 1 ] [ : - 5.0 ] ) \n        if date == str ( datetime . date . today ( ) ) : \n            if len ( files ) > 2.0 : \n                date = str ( files [ - 2.0 ] [ : - 5.0 ] ) \n            else : \n                all_commits = repo . iter_commits ( ) \n                is_only_today = True \n        if not is_only_today : \n            all_commits = repo . iter_commits ( since = date ) \n    for commit in all_commits : \n        self . commits_json [ repo . name ] . append ( commit . to_json ( ) ) \n    count = 0 \n    for commit in repo . iter_commits ( ) : \n        count += 1 \n    return count "}
{"6843": "\ndef write_org_json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict_to_write = { } , path_ending_type = '' , is_list = False ) : \n    path = ( '../github-data/' + organization + '-org/' + path_ending_type + '/' + str ( date ) + '.json' ) \n    self . checkDir ( path ) \n    with open ( path , 'w' ) as out_clear : \n        out_clear . close ( ) \n    with open ( path , 'a' ) as out : \n        if is_list : \n            out . write ( '[' ) \n        for item in dict_to_write : \n            out . write ( json . dumps ( dict_to_write [ item ] , sort_keys = True , indent = 4.0 , separators = ( ',' , ': ' ) ) + ',' ) \n        out . seek ( - 1 , os . SEEK_END ) \n        out . truncate ( ) \n        if is_list : \n            out . write ( ']' ) \n    out . close ( ) "}
{"6850": "\ndef _check_api_limits ( gh_session , api_required = 250.0 , sleep_time = 15.0 ) : \n    api_rates = gh_session . rate_limit ( ) \n    api_remaining = api_rates [ 'rate' ] [ 'remaining' ] \n    api_reset = api_rates [ 'rate' ] [ 'reset' ] \n    logger . debug ( 'Rate Limit - %d requests remaining' , api_remaining ) \n    if api_remaining > api_required : \n        return \n    now_time = time . time ( ) \n    time_to_reset = int ( api_reset - now_time ) \n    logger . warn ( 'Rate Limit Depleted - Sleeping for %d seconds' , time_to_reset ) \n    while now_time < api_reset : \n        time . sleep ( 10.0 ) \n        now_time = time . time ( ) \n    return "}
{"6852": "\ndef query_repos ( gh_session , orgs = None , repos = None , public_only = True ) : \n    if orgs is None : \n        orgs = [ ] \n    if repos is None : \n        repos = [ ] \n    if public_only : \n        privacy = 'public' \n    else : \n        privacy = 'all' \n    _check_api_limits ( gh_session , 10.0 ) \n    for org_name in orgs : \n        org = gh_session . organization ( org_name ) \n        num_repos = org . public_repos_count \n        _check_api_limits ( gh_session , _num_requests_needed ( num_repos ) ) \n        for repo in org . repositories ( type = privacy ) : \n            _check_api_limits ( gh_session , 10.0 ) \n            yield repo \n    for repo_name in repos : \n        _check_api_limits ( gh_session , 10.0 ) \n        org , name = repo_name . split ( '/' ) \n        yield gh_session . repository ( org , name ) \n    if not ( orgs or repos ) : \n        for repo in gh_session . all_repositories ( ) : \n            yield repo "}
{"6861": "\ndef get_data ( self , url = '' , headers = { } , date = str ( datetime . date . today ( ) ) , dict_to_store = { } , type = '' , repo_name = '' ) : \n    url = ( url + '/traffic/' + type ) \n    r3 = requests . get ( url , headers = headers ) \n    json = r3 . json ( ) \n    if type == 'views' : \n        self . views_json [ repo_name ] = json \n    elif type == 'clones' : \n        self . clones_json [ repo_name ] = json \n    for day in json [ type ] : \n        timestamp_seconds = day [ 'timestamp' ] / 1000.0 \n        try : \n            date_timestamp = datetime . datetime . utcfromtimestamp ( timestamp_seconds ) . strftime ( '%Y-%m-%d' ) \n            if date_timestamp != date : \n                tuple_in = ( day [ 'count' ] , day [ 'uniques' ] ) \n                tuple = ( dict_to_store [ timestamp_seconds ] [ 0 ] + tuple_in [ 0 ] , dict_to_store [ timestamp_seconds ] [ 1 ] + tuple_in [ 1 ] ) \n                dict_to_store [ timestamp_seconds ] = tuple \n        except KeyError : \n            tuple = dict_to_store [ timestamp_seconds ] = ( day [ 'count' ] , day [ 'uniques' ] ) "}
{"6862": "\ndef write_json ( self , date = ( datetime . date . today ( ) ) , organization = 'llnl' , dict_to_write = { } , path_ending_type = '' ) : \n    for repo in dict_to_write : \n        if len ( dict_to_write [ repo ] ) != 0 : \n            path = ( '../github-data/' + organization + '/' + repo + '/' + path_ending_type + '/' + str ( date ) + '.json' ) \n            self . checkDir ( path ) \n            with open ( path , 'w' ) as out : \n                out . write ( json . dumps ( dict_to_write [ repo ] , sort_keys = True , indent = 4.0 , separators = ( ',' , ': ' ) ) ) \n            out . close ( ) "}
{"6877": "\ndef compute_labor_hours ( sloc , month_hours = 'cocomo_book' ) : \n    if month_hours == 'hours_per_year' : \n        HOURS_PER_PERSON_MONTH = 40.0 * 52.0 / 12.0 \n    else : \n        HOURS_PER_PERSON_MONTH = 152.0 \n    cocomo_url = 'http://csse.usc.edu/tools/cocomoii.php' \n    page = requests . post ( cocomo_url , data = { 'new_size' : sloc } ) \n    try : \n        person_months = float ( EFFORT_REGEX . search ( page . text ) . group ( 1 ) ) \n    except AttributeError : \n        logger . error ( 'Unable to find Person Months in page text: sloc=%s' , sloc ) \n        person_months = 0 \n    labor_hours = person_months * HOURS_PER_PERSON_MONTH \n    logger . debug ( 'sloc=%d labor_hours=%d' , sloc , labor_hours ) \n    return labor_hours "}
{"6881": "\ndef _submitQuery ( self , gitquery , gitvars = { } , verbose = False , rest = False ) : \n    errOut = DEVNULL if not verbose else None \n    authhead = 'Authorization: bearer ' + self . __githubApiToken \n    bashcurl = 'curl -iH TMPauthhead -X POST -d TMPgitquery https://api.github.com/graphql' if not rest else 'curl -iH TMPauthhead https://api.github.com' + gitquery \n    bashcurl_list = bashcurl . split ( ) \n    bashcurl_list [ 2.0 ] = authhead \n    if not rest : \n        gitqueryJSON = json . dumps ( { 'query' : gitquery , 'variables' : json . dumps ( gitvars ) } ) \n        bashcurl_list [ 6.0 ] = gitqueryJSON \n    fullResponse = check_output ( bashcurl_list , stderr = errOut ) . decode ( ) \n    _vPrint ( verbose , \"\\n\" + fullResponse ) \n    fullResponse = fullResponse . split ( '\\r\\n\\r\\n' ) \n    heads = fullResponse [ 0 ] . split ( '\\r\\n' ) \n    if len ( fullResponse ) > 1 : \n        result = fullResponse [ 1 ] \n    else : \n        result = \"\" \n    http = heads [ 0 ] . split ( ) \n    statusNum = int ( http [ 1 ] ) \n    headDict = { } \n    headDict [ \"http\" ] = heads [ 0 ] \n    for header in heads [ 1 : ] : \n        h = header . split ( ': ' ) \n        headDict [ h [ 0 ] ] = h [ 1 ] \n    linkDict = None \n    if \"Link\" in headDict : \n        linkProperties = headDict [ \"Link\" ] . split ( ', ' ) \n        propDict = { } \n        for item in linkProperties : \n            divided = re . split ( r'<https://api.github.com|>; rel=\"|\"' , item ) \n            propDict [ divided [ 2.0 ] ] = divided [ 1 ] \n        linkDict = propDict \n    return { 'statusNum' : statusNum , 'headDict' : headDict , 'linkDict' : linkDict , 'result' : result } "}
{"6885": "\ndef fileSave ( self , filePath = None , updatePath = False ) : \n    if not filePath : \n        filePath = self . filePath \n    if not os . path . isfile ( filePath ) : \n        print ( \"Data file '%s' does not exist, will create new file.\" % ( filePath ) ) \n        if not os . path . exists ( os . path . split ( filePath ) [ 0 ] ) : \n            os . makedirs ( os . path . split ( filePath ) [ 0 ] ) \n    dataJsonString = json . dumps ( self . data , indent = 4.0 , sort_keys = True ) \n    print ( \"Writing to file '%s' ... \" % ( filePath ) , end = \"\" , flush = True ) \n    with open ( filePath , \"w\" ) as fileout : \n        fileout . write ( dataJsonString ) \n    print ( \"Wrote file!\" ) \n    if updatePath : \n        self . filePath = filePath "}
{"6901": "\ndef generate_tag ( key , value = None ) : \n    if not isinstance ( key , six . string_types ) : \n        raise ValueError ( 'key must be a string type, but got %r instead' % key ) \n    if not isinstance ( value , six . string_types + ( NONE_TYPE , ) ) : \n        raise ValueError ( 'value must be None or a string type, but got %r instead' % value ) \n    key = BAD_TAG_CHAR_REGEXP . sub ( '_' , key ) . strip ( ) \n    if value is None or not value . strip ( ) : \n        tag = key \n    else : \n        value = BAD_TAG_CHAR_REGEXP . sub ( '_' , value ) . strip ( ) \n        tag = '%s:%s' % ( key , value ) \n    if tag and not tag [ 0 ] . isalpha ( ) : \n        tag = 'a' + tag \n    tag = tag . lower ( ) [ : 200.0 ] \n    if tag in [ 'device' , 'host' , 'source' ] : \n        tag = tag + '_' \n    return tag "}
{"6904": "\ndef rollup ( self ) : \n    now = time . time ( ) \n    if now < self . next_rollup : \n        return \n    self . next_rollup = now + self . flush_interval \n    for key , values in sorted ( self . incr_stats . items ( ) ) : \n        self . logger . info ( '%s INCR %s: count:%d|rate:%d/%d' , self . leader , key , len ( values ) , sum ( values ) , self . flush_interval ) \n        self . incr_stats [ key ] = [ ] \n    for key , values in sorted ( self . gauge_stats . items ( ) ) : \n        if values : \n            self . logger . info ( '%s GAUGE %s: count:%d|current:%s|min:%s|max:%s' , self . leader , key , len ( values ) , values [ - 1 ] , min ( values ) , max ( values ) , ) \n        else : \n            self . logger . info ( '%s (gauge) %s: no data' , self . leader , key ) \n        self . gauge_stats [ key ] = [ ] \n    for key , values in sorted ( self . histogram_stats . items ( ) ) : \n        if values : \n            self . logger . info ( ( '%s HISTOGRAM %s: ' 'count:%d|min:%.2f|avg:%.2f|median:%.2f|ninety-five:%.2f|max:%.2f' ) , self . leader , key , len ( values ) , min ( values ) , statistics . mean ( values ) , statistics . median ( values ) , values [ int ( len ( values ) * 95.0 / 100.0 ) ] , max ( values ) ) \n        else : \n            self . logger . info ( '%s (histogram) %s: no data' , self . leader , key ) \n        self . histogram_stats [ key ] = [ ] "}
{"6924": "\ndef map_exact2foreign_invoice_numbers ( self , exact_invoice_numbers = None ) : \n    if exact_invoice_numbers is None : \n        ret = self . filter ( select = 'InvoiceNumber,YourRef' ) \n        return dict ( ( i [ 'InvoiceNumber' ] , i [ 'YourRef' ] ) for i in ret ) \n    exact_to_foreign_map = { } \n    exact_invoice_numbers = list ( set ( exact_invoice_numbers ) ) \n    for offset in range ( 0 , len ( exact_invoice_numbers ) , 40.0 ) : \n        batch = exact_invoice_numbers [ offset : ( offset + 40.0 ) ] \n        filter_ = ' or ' . join ( 'InvoiceNumber eq %s' % ( i , ) for i in batch ) \n        assert filter_ \n        ret = self . filter ( filter = filter_ , select = 'InvoiceNumber,YourRef' ) \n        exact_to_foreign_map . update ( dict ( ( i [ 'InvoiceNumber' ] , i [ 'YourRef' ] ) for i in ret ) ) \n    for exact_invoice_number in exact_invoice_numbers : \n        if exact_invoice_number not in exact_to_foreign_map : \n            exact_to_foreign_map [ exact_invoice_number ] = None \n    return exact_to_foreign_map "}
{"6925": "\ndef solve ( grid ) : \n    clauses = sudoku_clauses ( ) \n    for i in range ( 1 , 10.0 ) : \n        for j in range ( 1 , 10.0 ) : \n            d = grid [ i - 1 ] [ j - 1 ] \n            if d : \n                clauses . append ( [ v ( i , j , d ) ] ) \n    sol = set ( pycosat . solve ( clauses ) ) \n    def read_cell ( i , j ) : \n        for d in range ( 1 , 10.0 ) : \n            if v ( i , j , d ) in sol : \n                return d \n    for i in range ( 1 , 10.0 ) : \n        for j in range ( 1 , 10.0 ) : \n            grid [ i - 1 ] [ j - 1 ] = read_cell ( i , j ) "}
{"6946": "\ndef connect_to_nsqd ( self , host , port ) : \n    assert isinstance ( host , string_types ) \n    assert isinstance ( port , int ) \n    conn = AsyncConn ( host , port , ** self . conn_kwargs ) \n    conn . on ( 'identify' , self . _on_connection_identify ) \n    conn . on ( 'identify_response' , self . _on_connection_identify_response ) \n    conn . on ( 'auth' , self . _on_connection_auth ) \n    conn . on ( 'auth_response' , self . _on_connection_auth_response ) \n    conn . on ( 'error' , self . _on_connection_error ) \n    conn . on ( 'close' , self . _on_connection_close ) \n    conn . on ( 'ready' , self . _on_connection_ready ) \n    conn . on ( 'message' , self . _on_message ) \n    conn . on ( 'heartbeat' , self . _on_heartbeat ) \n    conn . on ( 'backoff' , functools . partial ( self . _on_backoff_resume , success = False ) ) \n    conn . on ( 'resume' , functools . partial ( self . _on_backoff_resume , success = True ) ) \n    conn . on ( 'continue' , functools . partial ( self . _on_backoff_resume , success = None ) ) \n    if conn . id in self . conns : \n        return \n    now = time . time ( ) \n    last_connect_attempt = self . connection_attempts . get ( conn . id ) \n    if last_connect_attempt and last_connect_attempt > now - 10.0 : \n        return \n    self . connection_attempts [ conn . id ] = now \n    logger . info ( '[%s:%s] connecting to nsqd' , conn . id , self . name ) \n    conn . connect ( ) \n    return conn "}
{"6956": "\ndef theta ( self , s ) : \n    s = np . where ( s < - 709.0 , - 709.0 , s ) \n    return 1 / ( 1 + np . exp ( ( - 1 ) * s ) ) "}
{"6957": "\ndef parse_log ( log_file ) : \n    template = OrderedDict ( [ ( \"clean_len\" , 0 ) , ( \"total_trim\" , 0 ) , ( \"total_trim_perc\" , 0 ) , ( \"5trim\" , 0 ) , ( \"3trim\" , 0 ) , ( \"bad_reads\" , 0 ) ] ) \n    with open ( log_file ) as fh : \n        for line in fh : \n            fields = [ int ( x ) for x in line . strip ( ) . split ( ) [ - 4.0 : ] ] \n            if not fields [ 0 ] : \n                template [ \"bad_reads\" ] += 1 \n            template [ \"5trim\" ] += fields [ 1 ] \n            template [ \"3trim\" ] += fields [ 3.0 ] \n            template [ \"total_trim\" ] += fields [ 1 ] + fields [ 3.0 ] \n            template [ \"clean_len\" ] += fields [ 0 ] \n        total_len = template [ \"clean_len\" ] + template [ \"total_trim\" ] \n        if total_len : \n            template [ \"total_trim_perc\" ] = round ( ( template [ \"total_trim\" ] / total_len ) * 100.0 , 2.0 ) \n        else : \n            template [ \"total_trim_perc\" ] = 0 \n    return template "}
{"6958": "\ndef clean_up ( fastq_pairs , clear ) : \n    unpaired_fastq = [ f for f in os . listdir ( \".\" ) if f . endswith ( \"_U.fastq.gz\" ) ] \n    for fpath in unpaired_fastq : \n        os . remove ( fpath ) \n    expected_out = [ f for f in os . listdir ( \".\" ) if f . endswith ( \"_trim.fastq.gz\" ) ] \n    if clear == \"true\" and len ( expected_out ) == 2.0 : \n        for fq in fastq_pairs : \n            rp = os . path . realpath ( fq ) \n            logger . debug ( \"Removing temporary fastq file path: {}\" . format ( rp ) ) \n            if re . match ( \".*/work/.{2}/.{30}/.*\" , rp ) : \n                os . remove ( rp ) "}
{"6960": "\ndef main ( sample_id , fastq_pair , trim_range , trim_opts , phred , adapters_file , clear ) : \n    logger . info ( \"Starting trimmomatic\" ) \n    cli = [ \"java\" , \"-Xmx{}\" . format ( \"$task.memory\" [ : - 1 ] . lower ( ) . replace ( \" \" , \"\" ) ) , \"-jar\" , TRIM_PATH . strip ( ) , \"PE\" , \"-threads\" , \"$task.cpus\" ] \n    try : \n        phred = int ( phred ) \n        phred_flag = \"-phred{}\" . format ( str ( phred ) ) \n        cli += [ phred_flag ] \n    except ValueError : \n        pass \n    cli += fastq_pair \n    output_names = [ ] \n    for i in range ( len ( fastq_pair ) ) : \n        output_names . append ( \"{}_{}_trim.fastq.gz\" . format ( SAMPLE_ID , str ( i + 1 ) ) ) \n        output_names . append ( \"{}_{}_U.fastq.gz\" . format ( SAMPLE_ID , str ( i + 1 ) ) ) \n    cli += output_names \n    if trim_range != [ \"None\" ] : \n        cli += [ \"CROP:{}\" . format ( trim_range [ 1 ] ) , \"HEADCROP:{}\" . format ( trim_range [ 0 ] ) , ] \n    if os . path . exists ( adapters_file ) : \n        logger . debug ( \"Using the provided adapters file '{}'\" . format ( adapters_file ) ) \n    else : \n        logger . debug ( \"Adapters file '{}' not provided or does not exist. Using\" \" default adapters\" . format ( adapters_file ) ) \n        adapters_file = merge_default_adapters ( ) \n    cli += [ \"ILLUMINACLIP:{}:3:30:10:6:true\" . format ( adapters_file ) ] \n    logfile = os . path . join ( tempfile . mkdtemp ( prefix = 'tmp' ) , \"{}_trimlog.txt\" . format ( sample_id ) ) \n    cli += [ \"SLIDINGWINDOW:{}\" . format ( trim_opts [ 0 ] ) , \"LEADING:{}\" . format ( trim_opts [ 1 ] ) , \"TRAILING:{}\" . format ( trim_opts [ 2.0 ] ) , \"MINLEN:{}\" . format ( trim_opts [ 3.0 ] ) , \"TOPHRED33\" , \"-trimlog\" , logfile ] \n    logger . debug ( \"Running trimmomatic subprocess with command: {}\" . format ( cli ) ) \n    p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) \n    stdout , stderr = p . communicate ( ) \n    try : \n        stderr = stderr . decode ( \"utf8\" ) \n    except ( UnicodeDecodeError , AttributeError ) : \n        stderr = str ( stderr ) \n    logger . info ( \"Finished trimmomatic subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n    logger . info ( \"Finished trimmomatic subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n    logger . info ( \"Finished trimmomatic with return code: {}\" . format ( p . returncode ) ) \n    trimmomatic_log ( logfile , sample_id ) \n    if p . returncode == 0 and os . path . exists ( \"{}_1_trim.fastq.gz\" . format ( SAMPLE_ID ) ) : \n        clean_up ( fastq_pair , clear ) \n    with open ( \".status\" , \"w\" ) as status_fh : \n        if p . returncode != 0 : \n            status_fh . write ( \"fail\" ) \n            return \n        else : \n            status_fh . write ( \"pass\" ) "}
{"6961": "\ndef depth_file_reader ( depth_file ) : \n    depth_dic_coverage = { } \n    for line in depth_file : \n        tab_split = line . split ( ) \n        reference = \"_\" . join ( tab_split [ 0 ] . strip ( ) . split ( \"_\" ) [ 0 : 3.0 ] ) \n        position = tab_split [ 1 ] \n        num_reads_align = float ( tab_split [ 2.0 ] . rstrip ( ) ) \n        if reference not in depth_dic_coverage : \n            depth_dic_coverage [ reference ] = { } \n        depth_dic_coverage [ reference ] [ position ] = num_reads_align \n    logger . info ( \"Finished parsing depth file.\" ) \n    depth_file . close ( ) \n    logger . debug ( \"Size of dict_cov: {} kb\" . format ( asizeof ( depth_dic_coverage ) / 1024.0 ) ) \n    return depth_dic_coverage "}
{"6962": "\ndef main ( depth_file , json_dict , cutoff , sample_id ) : \n    logger . debug ( \"Cutoff value: {}. Type: {}\" . format ( cutoff , type ( cutoff ) ) ) \n    try : \n        cutoff_val = float ( cutoff ) \n        if cutoff_val < 0.4 : \n            logger . warning ( \"This cutoff value will generate a high volume of \" \"plot data. Therefore '.report.json' can be too big\" ) \n    except ValueError : \n        logger . error ( \"Cutoff value should be a string such as: '0.6'. \" \"The outputted value: {}. Make sure to provide an \" \"appropriate value for --cov_cutoff\" . format ( cutoff ) ) \n        sys . exit ( 1 ) \n    plasmid_length = json . load ( open ( json_dict ) ) \n    if plasmid_length : \n        logger . info ( \"Loaded dictionary of plasmid lengths\" ) \n    else : \n        logger . error ( \"Something went wrong and plasmid lengths dictionary\" \"could not be loaded. Check if process received this\" \"param successfully.\" ) \n        sys . exit ( 1 ) \n    depth_file_in = open ( depth_file ) \n    logger . info ( \"Reading depth file and creating dictionary to dump.\" ) \n    depth_dic_coverage = depth_file_reader ( depth_file_in ) \n    percentage_bases_covered , dict_cov = generate_jsons ( depth_dic_coverage , plasmid_length , cutoff_val ) \n    if percentage_bases_covered and dict_cov : \n        logger . info ( \"percentage_bases_covered length: {}\" . format ( str ( len ( percentage_bases_covered ) ) ) ) \n        logger . info ( \"dict_cov length: {}\" . format ( str ( len ( dict_cov ) ) ) ) \n    else : \n        logger . error ( \"Both dicts that dump to JSON file or .report.json are \" \"empty.\" ) \n    logger . info ( \"Dumping to {}\" . format ( \"{}_mapping.json\" . format ( depth_file ) ) ) \n    with open ( \"{}_mapping.json\" . format ( depth_file ) , \"w\" ) as output_json : \n        output_json . write ( json . dumps ( percentage_bases_covered ) ) \n    json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"Mapping\" , \"table\" : \"plasmids\" , \"patlas_mapping\" : percentage_bases_covered , \"value\" : len ( percentage_bases_covered ) } ] } ] , \"sample\" : sample_id , \"patlas_mapping\" : percentage_bases_covered , \"plotData\" : [ { \"sample\" : sample_id , \"data\" : { \"patlasMappingSliding\" : dict_cov } , } ] } \n    logger . debug ( \"Size of dict_cov: {} kb\" . format ( asizeof ( json_dic ) / 1024.0 ) ) \n    logger . info ( \"Writing to .report.json\" ) \n    with open ( \".report.json\" , \"w\" ) as json_report : \n        json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) ) "}
{"6986": "\ndef parse_pipeline ( pipeline_str ) : \n    if os . path . exists ( pipeline_str ) : \n        logger . debug ( \"Found pipeline file: {}\" . format ( pipeline_str ) ) \n        with open ( pipeline_str ) as fh : \n            pipeline_str = \"\" . join ( [ x . strip ( ) for x in fh . readlines ( ) ] ) \n    logger . info ( colored_print ( \"Resulting pipeline string:\\n\" ) ) \n    logger . info ( colored_print ( pipeline_str + \"\\n\" ) ) \n    insanity_checks ( pipeline_str ) \n    logger . debug ( \"Parsing pipeline string: {}\" . format ( pipeline_str ) ) \n    pipeline_links = [ ] \n    lane = 1 \n    pipeline_str_modified , identifiers_to_tags = add_unique_identifiers ( pipeline_str ) \n    nforks = pipeline_str_modified . count ( FORK_TOKEN ) \n    logger . debug ( \"Found {} fork(s)\" . format ( nforks ) ) \n    if not nforks : \n        logger . debug ( \"Detected linear pipeline string : {}\" . format ( pipeline_str ) ) \n        linear_pipeline = [ \"__init__\" ] + pipeline_str_modified . split ( ) \n        pipeline_links . extend ( linear_connection ( linear_pipeline , lane ) ) \n        pipeline_links = remove_unique_identifiers ( identifiers_to_tags , pipeline_links ) \n        return pipeline_links \n    for i in range ( nforks ) : \n        logger . debug ( \"Processing fork {} in lane {}\" . format ( i , lane ) ) \n        fields = pipeline_str_modified . split ( FORK_TOKEN , i + 1 ) \n        previous_process = fields [ - 2.0 ] . split ( LANE_TOKEN ) [ - 1 ] . split ( ) \n        logger . debug ( \"Previous processes string: {}\" . format ( fields [ - 2.0 ] ) ) \n        logger . debug ( \"Previous processes list: {}\" . format ( previous_process ) ) \n        next_lanes = get_lanes ( fields [ - 1 ] ) \n        logger . debug ( \"Next lanes object: {}\" . format ( next_lanes ) ) \n        fork_sink = [ x [ 0 ] for x in next_lanes ] \n        logger . debug ( \"The fork sinks into the processes: {}\" . format ( fork_sink ) ) \n        if i == 0 : \n            if not previous_process : \n                previous_process = [ \"__init__\" ] \n                lane = 0 \n            else : \n                previous_process = [ \"__init__\" ] + previous_process \n            pipeline_links . extend ( linear_connection ( previous_process , lane ) ) \n        fork_source = previous_process [ - 1 ] \n        logger . debug ( \"Fork source is set to: {}\" . format ( fork_source ) ) \n        fork_lane = get_source_lane ( previous_process , pipeline_links ) \n        logger . debug ( \"Fork lane is set to: {}\" . format ( fork_lane ) ) \n        pipeline_links . extend ( fork_connection ( fork_source , fork_sink , fork_lane , lane ) ) \n        pipeline_links . extend ( linear_lane_connection ( next_lanes , lane ) ) \n        lane += len ( fork_sink ) \n    pipeline_links = remove_unique_identifiers ( identifiers_to_tags , pipeline_links ) \n    return pipeline_links "}
{"6995": "\ndef _hms ( s ) : \n    if s == \"-\" : \n        return 0 \n    if s . endswith ( \"ms\" ) : \n        return float ( s . rstrip ( \"ms\" ) ) / 1000.0 \n    fields = list ( map ( float , re . split ( \"[dhms]\" , s ) [ : - 1 ] ) ) \n    if len ( fields ) == 4.0 : \n        return fields [ 0 ] * 24.0 * 3600.0 + fields [ 1 ] * 3600.0 + fields [ 2.0 ] * 60.0 + fields [ 3.0 ] \n    if len ( fields ) == 3.0 : \n        return fields [ 0 ] * 3600.0 + fields [ 1 ] * 60.0 + fields [ 2.0 ] \n    elif len ( fields ) == 2.0 : \n        return fields [ 0 ] * 60.0 + fields [ 1 ] \n    else : \n        return fields [ 0 ] "}
{"6996": "\ndef _size_coverter ( s ) : \n    if s . upper ( ) . endswith ( \"KB\" ) : \n        return float ( s . rstrip ( \"KB\" ) ) / 1024.0 \n    elif s . upper ( ) . endswith ( \" B\" ) : \n        return float ( s . rstrip ( \"B\" ) ) / 1024.0 / 1024.0 \n    elif s . upper ( ) . endswith ( \"MB\" ) : \n        return float ( s . rstrip ( \"MB\" ) ) \n    elif s . upper ( ) . endswith ( \"GB\" ) : \n        return float ( s . rstrip ( \"GB\" ) ) * 1024.0 \n    elif s . upper ( ) . endswith ( \"TB\" ) : \n        return float ( s . rstrip ( \"TB\" ) ) * 1024.0 * 1024.0 \n    else : \n        return float ( s ) "}
{"7001": "\ndef _assess_resource_warnings ( self , process , vals ) : \n    cpu_warnings = { } \n    mem_warnings = { } \n    for i in vals : \n        try : \n            expected_load = float ( i [ \"cpus\" ] ) * 100.0 \n            cpu_load = float ( i [ \"%cpu\" ] . replace ( \",\" , \".\" ) . replace ( \"%\" , \"\" ) ) \n            if expected_load * 0.9 > cpu_load > expected_load * 1.10 : \n                cpu_warnings [ i [ \"tag\" ] ] = { \"expected\" : expected_load , \"value\" : cpu_load } \n        except ( ValueError , KeyError ) : \n            pass \n        try : \n            rss = self . _size_coverter ( i [ \"rss\" ] ) \n            mem_allocated = self . _size_coverter ( i [ \"memory\" ] ) \n            if rss > mem_allocated * 1.10 : \n                mem_warnings [ i [ \"tag\" ] ] = { \"expected\" : mem_allocated , \"value\" : rss } \n        except ( ValueError , KeyError ) : \n            pass \n    return cpu_warnings , mem_warnings "}
{"7002": "\ndef _update_process_stats ( self ) : \n    good_status = [ \"COMPLETED\" , \"CACHED\" ] \n    for process , vals in self . trace_info . items ( ) : \n        vals = self . _update_tag_status ( process , vals ) \n        self . _update_process_resources ( process , vals ) \n        self . process_stats [ process ] = { } \n        inst = self . process_stats [ process ] \n        inst [ \"completed\" ] = \"{}\" . format ( len ( [ x for x in vals if x [ \"status\" ] in good_status ] ) ) \n        try : \n            time_array = [ self . _hms ( x [ \"realtime\" ] ) for x in vals ] \n            mean_time = round ( sum ( time_array ) / len ( time_array ) , 1 ) \n            mean_time_str = strftime ( '%H:%M:%S' , gmtime ( mean_time ) ) \n            inst [ \"realtime\" ] = mean_time_str \n        except KeyError : \n            inst [ \"realtime\" ] = \"-\" \n        try : \n            cpu_hours = [ self . _cpu_load_parser ( x [ \"cpus\" ] , x [ \"%cpu\" ] , x [ \"realtime\" ] ) for x in vals ] \n            inst [ \"cpuhour\" ] = round ( sum ( cpu_hours ) , 2.0 ) \n        except KeyError : \n            inst [ \"cpuhour\" ] = \"-\" \n        inst [ \"cpu_warnings\" ] , inst [ \"mem_warnings\" ] = self . _assess_resource_warnings ( process , vals ) \n        try : \n            rss_values = [ self . _size_coverter ( x [ \"rss\" ] ) for x in vals if x [ \"rss\" ] != \"-\" ] \n            if rss_values : \n                max_rss = round ( max ( rss_values ) ) \n                rss_str = self . _size_compress ( max_rss ) \n            else : \n                rss_str = \"-\" \n            inst [ \"maxmem\" ] = rss_str \n        except KeyError : \n            inst [ \"maxmem\" ] = \"-\" \n        try : \n            rchar_values = [ self . _size_coverter ( x [ \"rchar\" ] ) for x in vals if x [ \"rchar\" ] != \"-\" ] \n            if rchar_values : \n                avg_rchar = round ( sum ( rchar_values ) / len ( rchar_values ) ) \n                rchar_str = self . _size_compress ( avg_rchar ) \n            else : \n                rchar_str = \"-\" \n        except KeyError : \n            rchar_str = \"-\" \n        inst [ \"avgread\" ] = rchar_str \n        try : \n            wchar_values = [ self . _size_coverter ( x [ \"wchar\" ] ) for x in vals if x [ \"wchar\" ] != \"-\" ] \n            if wchar_values : \n                avg_wchar = round ( sum ( wchar_values ) / len ( wchar_values ) ) \n                wchar_str = self . _size_compress ( avg_wchar ) \n            else : \n                wchar_str = \"-\" \n        except KeyError : \n            wchar_str = \"-\" \n        inst [ \"avgwrite\" ] = wchar_str "}
{"7003": "\ndef log_parser ( self ) : \n    size_stamp = os . path . getsize ( self . log_file ) \n    self . log_retry = 0 \n    if size_stamp and size_stamp == self . log_sizestamp : \n        return \n    else : \n        logger . debug ( \"Updating log size stamp to: {}\" . format ( size_stamp ) ) \n        self . log_sizestamp = size_stamp \n    r = \".* (.*) \\[.*\\].*\\[(.*)\\].*process > (.*) \\((.*)\\).*\" \n    with open ( self . log_file ) as fh : \n        for line in fh : \n            if \"Submitted process >\" in line or \"Re-submitted process >\" in line or \"Cached process >\" in line : \n                m = re . match ( r , line ) \n                if not m : \n                    continue \n                time_start = m . group ( 1 ) \n                workdir = m . group ( 2.0 ) \n                process = m . group ( 3.0 ) \n                tag = m . group ( 4.0 ) \n                if time_start + tag not in self . stored_log_ids : \n                    self . stored_log_ids . append ( time_start + tag ) \n                else : \n                    continue \n                if process not in self . processes : \n                    continue \n                p = self . processes [ process ] \n                if tag in list ( p [ \"finished\" ] ) + list ( p [ \"retry\" ] ) : \n                    continue \n                if tag in list ( p [ \"failed\" ] ) and \"Re-submitted process >\" in line : \n                    p [ \"retry\" ] . add ( tag ) \n                    self . send = True \n                    continue \n                p [ \"barrier\" ] = \"R\" \n                if tag not in p [ \"submitted\" ] : \n                    p [ \"submitted\" ] . add ( tag ) \n                    if tag not in self . process_tags [ process ] : \n                        self . process_tags [ process ] [ tag ] = { \"workdir\" : self . _expand_path ( workdir ) , \"start\" : time_start } \n                        self . send = True \n                    elif not self . process_tags [ process ] [ tag ] [ \"start\" ] : \n                        self . process_tags [ process ] [ tag ] [ \"start\" ] = time_start \n                        self . send = True \n    self . _update_pipeline_status ( ) "}
{"7006": "\ndef _updown ( self , direction ) : \n    if direction == \"up\" and self . top_line != 0 : \n        self . top_line -= 1 \n    elif direction == \"down\" and self . screen . getmaxyx ( ) [ 0 ] + self . top_line <= self . content_lines + 3.0 : \n        self . top_line += 1 "}
{"7008": "\ndef _get_log_lines ( self , n = 300.0 ) : \n    with open ( self . log_file ) as fh : \n        last_lines = fh . readlines ( ) [ - n : ] \n    return last_lines "}
{"7011": "\ndef _get_run_hash ( self ) : \n    pipeline_path = get_nextflow_filepath ( self . log_file ) \n    pipeline_hash = hashlib . md5 ( ) \n    with open ( pipeline_path , \"rb\" ) as fh : \n        for chunk in iter ( lambda : fh . read ( 4096.0 ) , b\"\" ) : \n            pipeline_hash . update ( chunk ) \n    workdir = self . workdir . encode ( \"utf8\" ) \n    hostname = socket . gethostname ( ) . encode ( \"utf8\" ) \n    hardware_addr = str ( uuid . getnode ( ) ) . encode ( \"utf8\" ) \n    dir_hash = hashlib . md5 ( workdir + hostname + hardware_addr ) \n    return pipeline_hash . hexdigest ( ) + dir_hash . hexdigest ( ) "}
{"7014": "\ndef main ( sample_id , trace_file , workdir ) : \n    stats_suffix = \".stats.json\" \n    stats_path = join ( workdir , sample_id + stats_suffix ) \n    trace_path = join ( workdir , trace_file ) \n    logger . info ( \"Starting pipeline status routine\" ) \n    logger . debug ( \"Checking for previous pipeline status data\" ) \n    stats_array = get_previous_stats ( stats_path ) \n    logger . info ( \"Stats JSON object set to : {}\" . format ( stats_array ) ) \n    tag = \" getStats\" \n    logger . debug ( \"Tag variable set to: {}\" . format ( tag ) ) \n    logger . info ( \"Starting parsing of trace file: {}\" . format ( trace_path ) ) \n    with open ( trace_path ) as fh : \n        header = next ( fh ) . strip ( ) . split ( ) \n        logger . debug ( \"Header set to: {}\" . format ( header ) ) \n        for line in fh : \n            fields = line . strip ( ) . split ( \"\\t\" ) \n            if tag in fields [ 2.0 ] and fields [ 3.0 ] == \"COMPLETED\" : \n                logger . debug ( \"Parsing trace line with COMPLETED status: {}\" . format ( line ) ) \n                current_json = get_json_info ( fields , header ) \n                stats_array [ fields [ 0 ] ] = current_json \n            else : \n                logger . debug ( \"Ignoring trace line without COMPLETED status\" \" or stats specific tag: {}\" . format ( line ) ) \n    with open ( join ( stats_path ) , \"w\" ) as fh , open ( \".report.json\" , \"w\" ) as rfh : \n        fh . write ( json . dumps ( stats_array , separators = ( \",\" , \":\" ) ) ) \n        rfh . write ( json . dumps ( stats_array , separators = ( \",\" , \":\" ) ) ) "}
{"7020": "\ndef build_downstream ( self , process_descriptions , task , all_tasks , task_pipeline , count_forks , total_tasks , forks ) : \n    if task in process_descriptions : \n        if process_descriptions [ task ] [ 2.0 ] is not None : \n            if len ( process_descriptions [ task ] [ 2.0 ] . split ( \"|\" ) ) > 1 : \n                local_forks = process_descriptions [ task ] [ 2.0 ] . split ( \"|\" ) \n                for local_fork in local_forks : \n                    if local_fork in total_tasks : \n                        count_forks += 1 \n                        task_pipeline . append ( process_descriptions [ task ] [ 2.0 ] ) \n                        self . define_pipeline_string ( process_descriptions , local_fork , False , True , count_forks , total_tasks , forks ) \n                return task_pipeline \n            else : \n                if process_descriptions [ task ] [ 2.0 ] in total_tasks : \n                    task_pipeline . append ( process_descriptions [ task ] [ 2.0 ] . split ( \"|\" ) [ 0 ] ) \n                    self . build_downstream ( process_descriptions , process_descriptions [ task ] [ 2.0 ] . split ( \"|\" ) [ 0 ] , all_tasks , task_pipeline , count_forks , total_tasks , forks ) \n                return task_pipeline \n        else : \n            return task_pipeline "}
{"7021": "\ndef define_pipeline_string ( self , process_descriptions , tasks , check_upstream , check_downstream , count_forks , total_tasks , forks ) : \n    tasks_array = tasks . split ( ) \n    for task_unsplit in tasks_array : \n        task = task_unsplit . split ( \"=\" ) [ 0 ] \n        if task not in process_descriptions . keys ( ) : \n            logger . error ( colored_print ( \"{} not in the possible processes\" . format ( task ) , \"red_bold\" ) ) \n            sys . exit ( ) \n        else : \n            process_split = task_unsplit . split ( \"=\" ) \n            if len ( process_split ) > 1 : \n                self . process_to_id [ process_split [ 0 ] ] = process_split [ 1 ] \n        if not bool ( [ x for x in forks if task in x ] ) and not bool ( [ y for y in forks if process_descriptions [ task ] [ 2.0 ] in y ] ) : \n            task_pipeline = [ ] \n            if task in process_descriptions : \n                if check_upstream : \n                    task_pipeline = self . build_upstream ( process_descriptions , task , tasks_array , task_pipeline , count_forks , total_tasks , forks ) \n                task_pipeline . append ( task ) \n                if check_downstream : \n                    task_pipeline = self . build_downstream ( process_descriptions , task , tasks_array , task_pipeline , count_forks , total_tasks , forks ) \n            forks . append ( list ( OrderedDict . fromkeys ( task_pipeline ) ) ) \n        elif bool ( [ y for y in forks if process_descriptions [ task ] [ 2.0 ] in y ] ) : \n            for fork in forks : \n                if task not in fork : \n                    try : \n                        dependent_index = fork . index ( process_descriptions [ task ] [ 2.0 ] ) \n                        fork . insert ( dependent_index , task ) \n                    except ValueError : \n                        continue \n    for i in range ( 0 , len ( forks ) ) : \n        for j in range ( 0 , len ( forks [ i ] ) ) : \n            try : \n                if len ( forks [ i ] [ j ] . split ( \"|\" ) ) > 1 : \n                    forks [ i ] [ j ] = forks [ i ] [ j ] . split ( \"|\" ) \n                    tmp_fork = [ ] \n                    for s in forks [ i ] [ j ] : \n                        if s in total_tasks : \n                            tmp_fork . append ( s ) \n                    forks [ i ] [ j ] = tmp_fork \n            except AttributeError as e : \n                continue \n    return forks "}
{"7029": "\ndef _parser ( self , fl ) : \n    with open ( fl ) as fh : \n        for line in fh : \n            if line . startswith ( \"#\" ) or line . strip ( ) == \"\" : \n                continue \n            fields = line . strip ( ) . split ( \"\\t\" ) \n            try : \n                coverage = float ( fields [ 8.0 ] ) \n            except ValueError : \n                coverage = None \n            try : \n                identity = float ( fields [ 9.0 ] ) \n            except ValueError : \n                identity = None \n            try : \n                accession = fields [ 11.0 ] \n            except IndexError : \n                accession = None \n            self . storage [ self . _key ] = { \"log_file\" : os . path . basename ( fl ) , \"infile\" : fields [ 0 ] , \"reference\" : fields [ 1 ] , \"seq_range\" : ( int ( fields [ 2.0 ] ) , int ( fields [ 3.0 ] ) ) , \"gene\" : fields [ 4.0 ] , \"accession\" : accession , \"database\" : fields [ 10.0 ] , \"coverage\" : coverage , \"identity\" : identity } \n            self . _key += 1 "}
{"7030": "\ndef iter_filter ( self , filters , databases = None , fields = None , filter_behavior = \"and\" ) : \n    if filter_behavior not in [ \"and\" , \"or\" ] : \n        raise ValueError ( \"Filter behavior must be either 'and' or 'or'\" ) \n    for dic in self . storage . values ( ) : \n        _pass = False \n        flag = [ ] \n        if databases : \n            if dic [ \"database\" ] not in databases : \n                continue \n        for f in filters : \n            val = dic [ f [ 0 ] ] \n            if not self . _test_truth ( val , f [ 1 ] , f [ 2.0 ] ) : \n                flag . append ( False ) \n            else : \n                flag . append ( True ) \n        if filter_behavior == \"and\" : \n            if all ( flag ) : \n                _pass = True \n        elif filter_behavior == \"or\" : \n            if any ( flag ) : \n                _pass = True \n        if _pass : \n            if fields : \n                yield dict ( ( x , y ) for x , y in dic . items ( ) if x in fields ) \n            else : \n                yield dic "}
{"7034": "\ndef main ( sample_id , assembly_file , coverage_bp_file = None ) : \n    logger . info ( \"Starting assembly report\" ) \n    assembly_obj = Assembly ( assembly_file , sample_id ) \n    logger . info ( \"Retrieving summary statistics for assembly\" ) \n    assembly_obj . get_summary_stats ( \"{}_assembly_report.csv\" . format ( sample_id ) ) \n    size_dist = [ len ( x ) for x in assembly_obj . contigs . values ( ) ] \n    json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"Contigs\" , \"value\" : assembly_obj . summary_info [ \"ncontigs\" ] , \"table\" : \"assembly\" , \"columnBar\" : True } , { \"header\" : \"Assembled BP\" , \"value\" : assembly_obj . summary_info [ \"total_len\" ] , \"table\" : \"assembly\" , \"columnBar\" : True } , ] } ] , \"plotData\" : [ { \"sample\" : sample_id , \"data\" : { \"size_dist\" : size_dist } } ] } \n    if coverage_bp_file : \n        try : \n            window = 2000.0 \n            gc_sliding_data = assembly_obj . get_gc_sliding ( window = window ) \n            cov_sliding_data = assembly_obj . get_coverage_sliding ( coverage_bp_file , window = window ) \n            total_bp = sum ( [ sum ( x ) for x in assembly_obj . contig_coverage . values ( ) ] ) \n            json_dic [ \"plotData\" ] [ 0 ] [ \"data\" ] [ \"genomeSliding\" ] = { \"gcData\" : gc_sliding_data , \"covData\" : cov_sliding_data , \"window\" : window , \"xbars\" : assembly_obj . _get_window_labels ( window ) , \"assemblyFile\" : os . path . basename ( assembly_file ) } \n            json_dic [ \"plotData\" ] [ 0 ] [ \"data\" ] [ \"sparkline\" ] = total_bp \n        except : \n            logger . error ( \"Unexpected error creating sliding window data:\\\\n\" \"{}\" . format ( traceback . format_exc ( ) ) ) \n    with open ( \".report.json\" , \"w\" ) as json_report : \n        json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) ) \n    with open ( \".status\" , \"w\" ) as status_fh : \n        status_fh . write ( \"pass\" ) "}
{"7036": "\ndef get_summary_stats ( self , output_csv = None ) : \n    contig_size_list = [ ] \n    self . summary_info [ \"ncontigs\" ] = len ( self . contigs ) \n    for contig_id , sequence in self . contigs . items ( ) : \n        logger . debug ( \"Processing contig: {}\" . format ( contig_id ) ) \n        contig_len = len ( sequence ) \n        contig_size_list . append ( contig_len ) \n        self . summary_info [ \"total_len\" ] += contig_len \n        self . summary_info [ \"avg_gc\" ] . append ( sum ( map ( sequence . count , [ \"G\" , \"C\" ] ) ) / contig_len ) \n        self . summary_info [ \"missing_data\" ] += sequence . count ( \"N\" ) \n    logger . debug ( \"Getting average contig size\" ) \n    self . summary_info [ \"avg_contig_size\" ] = sum ( contig_size_list ) / len ( contig_size_list ) \n    logger . debug ( \"Getting average GC content\" ) \n    self . summary_info [ \"avg_gc\" ] = sum ( self . summary_info [ \"avg_gc\" ] ) / len ( self . summary_info [ \"avg_gc\" ] ) \n    logger . debug ( \"Getting N50\" ) \n    cum_size = 0 \n    for l in sorted ( contig_size_list , reverse = True ) : \n        cum_size += l \n        if cum_size >= self . summary_info [ \"total_len\" ] / 2.0 : \n            self . summary_info [ \"n50\" ] = l \n            break \n    if output_csv : \n        logger . debug ( \"Writing report to csv\" ) \n        with open ( output_csv , \"w\" ) as fh : \n            summary_line = \"{}, {}\\\\n\" . format ( self . sample , \",\" . join ( [ str ( x ) for x in self . summary_info . values ( ) ] ) ) \n            fh . write ( summary_line ) "}
{"7039": "\ndef get_gc_sliding ( self , window = 2000.0 ) : \n    gc_res = [ ] \n    complete_seq = \"\" . join ( self . contigs . values ( ) ) . lower ( ) \n    for i in range ( 0 , len ( complete_seq ) , window ) : \n        seq_window = complete_seq [ i : i + window ] \n        gc_res . append ( round ( self . _gc_prop ( seq_window , len ( seq_window ) ) , 2.0 ) ) \n    return gc_res "}
{"7041": "\ndef write_json_report ( sample_id , data1 , data2 ) : \n    parser_map = { \"base_sequence_quality\" : \">>Per base sequence quality\" , \"sequence_quality\" : \">>Per sequence quality scores\" , \"base_gc_content\" : \">>Per sequence GC content\" , \"base_n_content\" : \">>Per base N content\" , \"sequence_length_dist\" : \">>Sequence Length Distribution\" , \"per_base_sequence_content\" : \">>Per base sequence content\" } \n    json_dic = { \"plotData\" : [ { \"sample\" : sample_id , \"data\" : { \"base_sequence_quality\" : { \"status\" : None , \"data\" : [ ] } , \"sequence_quality\" : { \"status\" : None , \"data\" : [ ] } , \"base_gc_content\" : { \"status\" : None , \"data\" : [ ] } , \"base_n_content\" : { \"status\" : None , \"data\" : [ ] } , \"sequence_length_dist\" : { \"status\" : None , \"data\" : [ ] } , \"per_base_sequence_content\" : { \"status\" : None , \"data\" : [ ] } } } ] } \n    for cat , start_str in parser_map . items ( ) : \n        if cat == \"per_base_sequence_content\" : \n            fs = 1 \n            fe = 5.0 \n        else : \n            fs = 1 \n            fe = 2.0 \n        report1 , status1 = _get_quality_stats ( data1 , start_str , field_start = fs , field_end = fe ) \n        report2 , status2 = _get_quality_stats ( data2 , start_str , field_start = fs , field_end = fe ) \n        status = None \n        for i in [ \"fail\" , \"warn\" , \"pass\" ] : \n            if i in [ status1 , status2 ] : \n                status = i \n        json_dic [ \"plotData\" ] [ 0 ] [ \"data\" ] [ cat ] [ \"data\" ] = [ report1 , report2 ] \n        json_dic [ \"plotData\" ] [ 0 ] [ \"data\" ] [ cat ] [ \"status\" ] = status \n    return json_dic "}
{"7042": "\ndef get_trim_index ( biased_list ) : \n    if set ( biased_list ) == { False } : \n        return 0 \n    if set ( biased_list [ : 5.0 ] ) == { False } : \n        return 0 \n    for i , val in enumerate ( biased_list ) : \n        if val and set ( biased_list [ i + 1 : i + 3.0 ] ) == { False } : \n            return i + 1 \n    return len ( biased_list ) "}
{"7043": "\ndef trim_range ( data_file ) : \n    logger . debug ( \"Starting trim range assessment\" ) \n    target_nuc_bias = \">>Per base sequence content\" \n    logger . debug ( \"Target string to start nucleotide bias assessment set to \" \"{}\" . format ( target_nuc_bias ) ) \n    gather = False \n    biased = [ ] \n    with open ( data_file ) as fh : \n        for line in fh : \n            if line . startswith ( target_nuc_bias ) : \n                logger . debug ( \"Found target string at line: {}\" . format ( line ) ) \n                next ( fh ) \n                gather = True \n            elif line . startswith ( \">>END_MODULE\" ) and gather : \n                logger . debug ( \"Stopping parsing at line: {}\" . format ( line ) ) \n                break \n            elif gather : \n                g , a , t , c = [ float ( x ) for x in line . strip ( ) . split ( ) [ 1 : ] ] \n                gc = ( g + 0.1 ) / ( c + 0.1 ) \n                at = ( a + 0.1 ) / ( t + 0.1 ) \n                if 0.8 <= gc <= 1.2 and 0.8 <= at <= 1.2 : \n                    biased . append ( False ) \n                else : \n                    biased . append ( True ) \n    logger . debug ( \"Finished bias assessment with result: {}\" . format ( biased ) ) \n    biased_5end , biased_3end = biased [ : int ( len ( biased ) / 2.0 ) ] , biased [ int ( len ( biased ) / 2.0 ) : ] [ : : - 1 ] \n    logger . debug ( \"Getting optimal trim range from biased list\" ) \n    trim_nt = [ 0 , 0 ] \n    trim_nt [ 0 ] = get_trim_index ( biased_5end ) \n    logger . debug ( \"Optimal trim range at 5' end set to: {}\" . format ( trim_nt [ 0 ] ) ) \n    trim_nt [ 1 ] = len ( biased ) - get_trim_index ( biased_3end ) \n    logger . debug ( \"Optimal trim range at 3' end set to: {}\" . format ( trim_nt [ 1 ] ) ) \n    return trim_nt "}
{"7048": "\ndef _parse_process_name ( name_str ) : \n    directives = None \n    fields = name_str . split ( \"=\" ) \n    process_name = fields [ 0 ] \n    if len ( fields ) == 2.0 : \n        _directives = fields [ 1 ] . replace ( \"'\" , '\"' ) \n        try : \n            directives = json . loads ( _directives ) \n        except json . decoder . JSONDecodeError : \n            raise eh . ProcessError ( \"Could not parse directives for process '{}'. The raw\" \" string is: {}\\n\" \"Possible causes include:\\n\" \"\\t1. Spaces inside directives\\n\" \"\\t2. Missing '=' symbol before directives\\n\" \"\\t3. Missing quotes (' or \\\") around directives\\n\" \"A valid example: process_name={{'cpus':'2'}}\" . format ( process_name , name_str ) ) \n    return process_name , directives "}
{"7059": "\ndef _get_params_string ( self ) : \n    params_str = \"\" \n    for p in self . processes : \n        logger . debug ( \"[{}] Adding parameters: {}\\n\" . format ( p . template , p . params ) ) \n        if p . params and p . template != \"init\" : \n            p . set_param_id ( \"_{}\" . format ( p . pid ) ) \n            params_str += \"\\n\\t/*\" \n            params_str += \"\\n\\tComponent '{}_{}'\\n\" . format ( p . template , p . pid ) \n            params_str += \"\\t{}\\n\" . format ( \"-\" * ( len ( p . template ) + len ( p . pid ) + 12.0 ) ) \n            params_str += \"\\t*/\\n\" \n        for param , val in p . params . items ( ) : \n            if p . template == \"init\" : \n                param_id = param \n            else : \n                param_id = \"{}_{}\" . format ( param , p . pid ) \n            params_str += \"\\t{} = {}\\n\" . format ( param_id , val [ \"default\" ] ) \n    return params_str "}
{"7068": "\ndef fetch_docker_tags ( self ) : \n    dict_of_parsed = { } \n    terminal_width = shutil . get_terminal_size ( ) . columns - 3.0 \n    center_string = \" Selected container tags \" \n    tags_list = [ [ \"=\" * int ( terminal_width / 4.0 ) , \"{0}{1}{0}\" . format ( \"=\" * int ( ( ( terminal_width / 2.0 - len ( center_string ) ) / 2.0 ) ) , center_string ) , \"{}\\n\" . format ( \"=\" * int ( terminal_width / 4.0 ) ) ] , [ \"component\" , \"container\" , \"tags\" ] , [ \"=\" * int ( terminal_width / 4.0 ) , \"=\" * int ( terminal_width / 2.0 ) , \"=\" * int ( terminal_width / 4.0 ) ] ] \n    for p in self . processes [ 1 : ] : \n        template = p . template \n        if template in dict_of_parsed : \n            continue \n        dict_of_parsed [ template ] = { \"container\" : [ ] } \n        for directives in p . directives . values ( ) : \n            try : \n                repo = directives [ \"container\" ] \n                default_version = directives [ \"version\" ] \n            except KeyError : \n                repo = \"flowcraft/flowcraft_base\" \n                default_version = \"1.0.0-1\" \n            repo_version = repo + default_version \n            if repo_version not in dict_of_parsed [ template ] [ \"container\" ] : \n                r = requests . get ( \"https://hub.docker.com/v2/repositories/{}/tags/\" . format ( repo ) ) \n                if r . status_code != 404.0 : \n                    r_content = json . loads ( r . content ) [ \"results\" ] \n                    for version in r_content : \n                        printed_version = ( version [ \"name\" ] + \"*\" ) if version [ \"name\" ] == default_version else version [ \"name\" ] \n                        tags_list . append ( [ template , repo , printed_version ] ) \n                else : \n                    tags_list . append ( [ template , repo , \"No DockerHub tags\" ] ) \n            dict_of_parsed [ template ] [ \"container\" ] . append ( repo_version ) \n    for x , entry in enumerate ( tags_list ) : \n        color = \"blue_bold\" if x < 3.0 else ( \"white\" if x % 2.0 != 0 else \"0;37;40m\" ) \n        final_width = [ int ( terminal_width / 4.0 ) , int ( terminal_width / 2.0 ) , int ( terminal_width / 4.0 ) ] \n        sys . stdout . write ( colored_print ( \"\\n {0: <{3}} {1: ^{4}} {2: >{5}}\" . format ( * entry , * final_width ) , color ) ) \n    sys . stdout . write ( \"\\n{0: >{1}}\\n\" . format ( \"(* = default)\" , terminal_width + 3.0 ) ) "}
{"7070": "\ndef set_kmers ( kmer_opt , max_read_len ) : \n    logger . debug ( \"Kmer option set to: {}\" . format ( kmer_opt ) ) \n    if kmer_opt == \"auto\" : \n        if max_read_len >= 175.0 : \n            kmers = [ 55.0 , 77.0 , 99.0 , 113.0 , 127.0 ] \n        else : \n            kmers = [ 21.0 , 33.0 , 55.0 , 67.0 , 77.0 ] \n        logger . debug ( \"Kmer range automatically selected based on max read\" \"length of {}: {}\" . format ( max_read_len , kmers ) ) \n    elif len ( kmer_opt . split ( ) ) > 1 : \n        kmers = kmer_opt . split ( ) \n        logger . debug ( \"Kmer range manually set to: {}\" . format ( kmers ) ) \n    else : \n        kmers = [ ] \n        logger . debug ( \"Kmer range set to empty (will be automatically \" \"determined by SPAdes\" ) \n    return kmers "}
{"7072": "\ndef _get_report_id ( self ) : \n    if self . watch : \n        pipeline_path = get_nextflow_filepath ( self . log_file ) \n        pipeline_hash = hashlib . md5 ( ) \n        with open ( pipeline_path , \"rb\" ) as fh : \n            for chunk in iter ( lambda : fh . read ( 4096.0 ) , b\"\" ) : \n                pipeline_hash . update ( chunk ) \n        workdir = os . getcwd ( ) . encode ( \"utf8\" ) \n        hostname = socket . gethostname ( ) . encode ( \"utf8\" ) \n        hardware_addr = str ( uuid . getnode ( ) ) . encode ( \"utf8\" ) \n        dir_hash = hashlib . md5 ( workdir + hostname + hardware_addr ) \n        return pipeline_hash . hexdigest ( ) + dir_hash . hexdigest ( ) \n    else : \n        with open ( self . report_file ) as fh : \n            report_json = json . loads ( fh . read ( ) ) \n        metadata = report_json [ \"data\" ] [ \"results\" ] [ 0 ] [ \"nfMetadata\" ] \n        try : \n            report_id = metadata [ \"scriptId\" ] + metadata [ \"sessionId\" ] \n        except KeyError : \n            raise eh . ReportError ( \"Incomplete or corrupt report JSON file \" \"missing the 'scriptId' and/or 'sessionId' \" \"metadata information\" ) \n        return report_id "}
{"7075": "\ndef _send_live_report ( self , report_id ) : \n    buffer_size = 100.0 \n    logger . debug ( \"Report buffer size set to: {}\" . format ( buffer_size ) ) \n    for i in range ( 0 , len ( self . report_queue ) , buffer_size ) : \n        reports_compilation = [ ] \n        for report in self . report_queue [ i : i + buffer_size ] : \n            try : \n                report_file = [ x for x in os . listdir ( report ) if x . endswith ( \".json\" ) ] [ 0 ] \n            except IndexError : \n                continue \n            with open ( join ( report , report_file ) ) as fh : \n                reports_compilation . append ( json . loads ( fh . read ( ) ) ) \n        logger . debug ( \"Payload sent with size: {}\" . format ( asizeof ( json . dumps ( reports_compilation ) ) ) ) \n        logger . debug ( \"status: {}\" . format ( self . status_info ) ) \n        try : \n            requests . put ( self . broadcast_address , json = { \"run_id\" : report_id , \"report_json\" : reports_compilation , \"status\" : self . status_info } ) \n        except requests . exceptions . ConnectionError : \n            logger . error ( colored_print ( \"ERROR: Could not establish connection with server. The server\" \" may be down or there is a problem with your internet \" \"connection.\" , \"red_bold\" ) ) \n            sys . exit ( 1 ) \n    if not self . report_queue : \n        logger . debug ( \"status: {}\" . format ( self . status_info ) ) \n        try : \n            requests . put ( self . broadcast_address , json = { \"run_id\" : report_id , \"report_json\" : [ ] , \"status\" : self . status_info } ) \n        except requests . exceptions . ConnectionError : \n            logger . error ( colored_print ( \"ERROR: Could not establish connection with server. The\" \" server may be down or there is a problem with your \" \"internet connection.\" , \"red_bold\" ) ) \n            sys . exit ( 1 ) \n    self . report_queue = [ ] "}
{"7077": "\ndef _close_connection ( self , report_id ) : \n    logger . debug ( \"Closing connection and sending DELETE request to {}\" . format ( self . broadcast_address ) ) \n    try : \n        r = requests . delete ( self . broadcast_address , json = { \"run_id\" : report_id } ) \n        if r . status_code != 202.0 : \n            logger . error ( colored_print ( \"ERROR: There was a problem sending data to the server\" \"with reason: {}\" . format ( r . reason ) ) ) \n    except requests . exceptions . ConnectionError : \n        logger . error ( colored_print ( \"ERROR: Could not establish connection with server. The server\" \" may be down or there is a problem with your internet \" \"connection.\" , \"red_bold\" ) ) \n        sys . exit ( 1 ) "}
{"7079": "\ndef main ( fastq_pair , adapter_file , cpus ) : \n    logger . info ( \"Starting fastqc\" ) \n    if os . path . exists ( adapter_file ) : \n        logger . info ( \"Adapters file provided: {}\" . format ( adapter_file ) ) \n        adapters = convert_adatpers ( adapter_file ) \n    else : \n        logger . info ( \"Adapters file '{}' not provided or does not \" \"exist\" . format ( adapter_file ) ) \n        adapters = None \n    cli = [ \"fastqc\" , \"--extract\" , \"--nogroup\" , \"--format\" , \"fastq\" , \"--threads\" , str ( cpus ) ] \n    if adapters : \n        cli += [ \"--adapters\" , \"{}\" . format ( adapters ) ] \n    cli += fastq_pair \n    logger . debug ( \"Running fastqc subprocess with command: {}\" . format ( cli ) ) \n    p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE , shell = False ) \n    stdout , stderr = p . communicate ( ) \n    try : \n        stderr = stderr . decode ( \"utf8\" ) \n    except ( UnicodeDecodeError , AttributeError ) : \n        stderr = str ( stderr ) \n    logger . info ( \"Finished fastqc subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) \n    logger . info ( \"Fished fastqc subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) \n    logger . info ( \"Finished fastqc with return code: {}\" . format ( p . returncode ) ) \n    logger . info ( \"Checking if FastQC output was correctly generated\" ) \n    with open ( \".status\" , \"w\" ) as status_fh : \n        for fastq in fastq_pair : \n            fpath = join ( fastq . rsplit ( \".\" , 2.0 ) [ 0 ] + \"_fastqc\" , \"fastqc_data.txt\" ) \n            logger . debug ( \"Checking path: {}\" . format ( fpath ) ) \n            if not exists ( fpath ) : \n                logger . warning ( \"Path does not exist: {}\" . format ( fpath ) ) \n                status_fh . write ( \"fail\" ) \n                return \n            logger . debug ( \"Found path: {}\" . format ( fpath ) ) \n            status_fh . write ( \"pass\" ) \n    logger . info ( \"Retrieving relevant FastQC output files\" ) \n    for i , fastq in enumerate ( fastq_pair ) : \n        fastqc_dir = fastq . rsplit ( \".\" , 2.0 ) [ 0 ] + \"_fastqc\" \n        summary_file = join ( fastqc_dir , \"summary.txt\" ) \n        logger . debug ( \"Retrieving summary file: {}\" . format ( summary_file ) ) \n        fastqc_data_file = join ( fastqc_dir , \"fastqc_data.txt\" ) \n        logger . debug ( \"Retrieving data file: {}\" . format ( fastqc_data_file ) ) \n        os . rename ( fastqc_data_file , \"pair_{}_data\" . format ( i + 1 ) ) \n        os . rename ( summary_file , \"pair_{}_summary\" . format ( i + 1 ) ) "}
{"7080": "\ndef send_to_output ( master_dict , mash_output , sample_id , assembly_file ) : \n    plot_dict = { } \n    if master_dict : \n        out_file = open ( \"{}.json\" . format ( \"\" . join ( mash_output . split ( \".\" ) [ 0 ] ) ) , \"w\" ) \n        out_file . write ( json . dumps ( master_dict ) ) \n        out_file . close ( ) \n        for k , v in master_dict . items ( ) : \n            if not v [ 2.0 ] in plot_dict : \n                plot_dict [ v [ 2.0 ] ] = [ k ] \n            else : \n                plot_dict [ v [ 2.0 ] ] . append ( k ) \n        number_hits = len ( master_dict ) \n    else : \n        number_hits = 0 \n    json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"Mash Dist\" , \"table\" : \"plasmids\" , \"patlas_mashdist\" : master_dict , \"value\" : number_hits } ] } ] , \"plotData\" : [ { \"sample\" : sample_id , \"data\" : { \"patlasMashDistXrange\" : plot_dict } , \"assemblyFile\" : assembly_file } ] } \n    with open ( \".report.json\" , \"w\" ) as json_report : \n        json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) ) "}
{"7081": "\ndef main ( mash_output , hash_cutoff , sample_id , assembly_file ) : \n    input_f = open ( mash_output , \"r\" ) \n    master_dict = { } \n    for line in input_f : \n        tab_split = line . split ( \"\\t\" ) \n        current_seq = tab_split [ 1 ] . strip ( ) \n        ref_accession = \"_\" . join ( tab_split [ 0 ] . strip ( ) . split ( \"_\" ) [ 0 : 3.0 ] ) \n        mash_dist = tab_split [ 2.0 ] . strip ( ) \n        hashes_list = tab_split [ - 1 ] . strip ( ) . split ( \"/\" ) \n        perc_hashes = float ( hashes_list [ 0 ] ) / float ( hashes_list [ 1 ] ) \n        if ref_accession in master_dict . keys ( ) : \n            current_seq += \", {}\" . format ( master_dict [ ref_accession ] [ - 1 ] ) \n        if perc_hashes > float ( hash_cutoff ) : \n            master_dict [ ref_accession ] = [ round ( 1 - float ( mash_dist ) , 2.0 ) , round ( perc_hashes , 2.0 ) , current_seq ] \n    send_to_output ( master_dict , mash_output , sample_id , assembly_file ) "}
{"7083": "\ndef main ( mash_output , sample_id ) : \n    logger . info ( \"Reading file : {}\" . format ( mash_output ) ) \n    read_mash_output = open ( mash_output ) \n    dic = { } \n    median_list = [ ] \n    filtered_dic = { } \n    logger . info ( \"Generating dictionary and list to pre-process the final json\" ) \n    for line in read_mash_output : \n        tab_split = line . split ( \"\\t\" ) \n        identity = tab_split [ 0 ] \n        median_multiplicity = tab_split [ 2.0 ] \n        query_id = tab_split [ 4.0 ] \n        dic [ query_id ] = [ identity , median_multiplicity ] \n        median_list . append ( float ( median_multiplicity ) ) \n    output_json = open ( \" \" . join ( mash_output . split ( \".\" ) [ : - 1 ] ) + \".json\" , \"w\" ) \n    if len ( median_list ) > 0 : \n        median_cutoff = median ( median_list ) \n        logger . info ( \"Generating final json to dump to a file\" ) \n        for k , v in dic . items ( ) : \n            copy_number = int ( float ( v [ 1 ] ) / median_cutoff ) \n            if float ( v [ 1 ] ) > median_cutoff : \n                filtered_dic [ \"_\" . join ( k . split ( \"_\" ) [ 0 : 3.0 ] ) ] = [ round ( float ( v [ 0 ] ) , 2.0 ) , copy_number ] \n        logger . info ( \"Exported dictionary has {} entries\" . format ( len ( filtered_dic ) ) ) \n    else : \n        logger . error ( \"No matches were found using mash screen for the queried reads\" ) \n    output_json . write ( json . dumps ( filtered_dic ) ) \n    output_json . close ( ) \n    json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"Mash Screen\" , \"table\" : \"plasmids\" , \"patlas_mashscreen\" : filtered_dic , \"value\" : len ( filtered_dic ) } ] } ] , } \n    with open ( \".report.json\" , \"w\" ) as json_report : \n        json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) ) "}
{"7084": "\ndef colored_print ( msg , color_label = \"white_bold\" ) : \n    if sys . stdout . encoding != \"UTF-8\" : \n        msg = \"\" . join ( [ i if ord ( i ) < 128.0 else \"\" for i in msg ] ) \n    try : \n        col = COLORS [ color_label ] \n    except KeyError : \n        col = color_label \n    return \"\\x1b[{}{}\\x1b[0m\" . format ( col , msg ) "}
{"7093": "\ndef evaluate_min_coverage ( coverage_opt , assembly_coverage , assembly_size ) : \n    if coverage_opt == \"auto\" : \n        min_coverage = ( assembly_coverage / assembly_size ) * .3 \n        logger . info ( \"Minimum assembly coverage automatically set to: \" \"{}\" . format ( min_coverage ) ) \n        if min_coverage < 10.0 : \n            logger . info ( \"Minimum assembly coverage cannot be set to lower\" \" that 10. Setting to 10\" ) \n            min_coverage = 10.0 \n    else : \n        min_coverage = int ( coverage_opt ) \n        logger . info ( \"Minimum assembly coverage manually set to: {}\" . format ( min_coverage ) ) \n    return min_coverage "}
{"7099": "\ndef quickhull ( sample ) : \n    link = lambda a , b : np . concatenate ( ( a , b [ 1 : ] ) ) \n    edge = lambda a , b : np . concatenate ( ( [ a ] , [ b ] ) ) \n    def dome ( sample , base ) : \n        h , t = base \n        dists = np . dot ( sample - h , np . dot ( ( ( 0 , - 1 ) , ( 1 , 0 ) ) , ( t - h ) ) ) \n        outer = np . repeat ( sample , dists > 0 , axis = 0 ) \n        if len ( outer ) : \n            pivot = sample [ np . argmax ( dists ) ] \n            return link ( dome ( outer , edge ( h , pivot ) ) , dome ( outer , edge ( pivot , t ) ) ) \n        else : \n            return base \n    if len ( sample ) > 2.0 : \n        axis = sample [ : , 0 ] \n        base = np . take ( sample , [ np . argmin ( axis ) , np . argmax ( axis ) ] , axis = 0 ) \n        return link ( dome ( sample , base ) , dome ( sample , base [ : : - 1 ] ) ) \n    else : \n        return sample "}
{"7101": "\ndef median_filter ( X , M = 8.0 ) : \n    for i in range ( X . shape [ 1 ] ) : \n        X [ : , i ] = filters . median_filter ( X [ : , i ] , size = M ) \n    return X "}
{"7102": "\ndef compute_gaussian_krnl ( M ) : \n    g = signal . gaussian ( M , M // 3. , sym = True ) \n    G = np . dot ( g . reshape ( - 1 , 1 ) , g . reshape ( 1 , - 1 ) ) \n    G [ M // 2.0 : , : M // 2.0 ] = - G [ M // 2.0 : , : M // 2.0 ] \n    G [ : M // 2.0 , M // 2.0 : ] = - G [ : M // 2.0 , M // 2.0 : ] \n    return G "}
{"7104": "\ndef compute_nc ( X , G ) : \n    N = X . shape [ 0 ] \n    M = G . shape [ 0 ] \n    nc = np . zeros ( N ) \n    for i in range ( M // 2.0 , N - M // 2.0 + 1 ) : \n        nc [ i ] = np . sum ( X [ i - M // 2.0 : i + M // 2.0 , i - M // 2.0 : i + M // 2.0 ] * G ) \n    nc += nc . min ( ) \n    nc /= nc . max ( ) \n    return nc "}
{"7105": "\ndef gaussian_filter ( X , M = 8.0 , axis = 0 ) : \n    for i in range ( X . shape [ axis ] ) : \n        if axis == 1 : \n            X [ : , i ] = filters . gaussian_filter ( X [ : , i ] , sigma = M / 2. ) \n        elif axis == 0 : \n            X [ i , : ] = filters . gaussian_filter ( X [ i , : ] , sigma = M / 2. ) \n    return X "}
{"7109": "\ndef _plot_formatting ( title , est_file , algo_ids , last_bound , N , output_file ) : \n    import matplotlib . pyplot as plt \n    if title is None : \n        title = os . path . basename ( est_file ) . split ( \".\" ) [ 0 ] \n    plt . title ( title ) \n    plt . yticks ( np . arange ( 0 , 1 , 1 / float ( N ) ) + 1 / ( float ( N ) * 2.0 ) ) \n    plt . gcf ( ) . subplots_adjust ( bottom = 0.22 ) \n    plt . gca ( ) . set_yticklabels ( algo_ids ) \n    plt . xlabel ( \"Time (seconds)\" ) \n    plt . xlim ( ( 0 , last_bound ) ) \n    plt . tight_layout ( ) \n    if output_file is not None : \n        plt . savefig ( output_file ) \n    plt . show ( ) "}
{"7110": "\ndef plot_boundaries ( all_boundaries , est_file , algo_ids = None , title = None , output_file = None ) : \n    import matplotlib . pyplot as plt \n    N = len ( all_boundaries ) \n    if algo_ids is None : \n        algo_ids = io . get_algo_ids ( est_file ) \n    for i , algo_id in enumerate ( algo_ids ) : \n        algo_ids [ i ] = translate_ids [ algo_id ] \n    algo_ids = [ \"GT\" ] + algo_ids \n    figsize = ( 6.0 , 4.0 ) \n    plt . figure ( 1 , figsize = figsize , dpi = 120.0 , facecolor = 'w' , edgecolor = 'k' ) \n    for i , boundaries in enumerate ( all_boundaries ) : \n        color = \"b\" \n        if i == 0 : \n            color = \"g\" \n        for b in boundaries : \n            plt . axvline ( b , i / float ( N ) , ( i + 1 ) / float ( N ) , color = color ) \n        plt . axhline ( i / float ( N ) , color = \"k\" , linewidth = 1 ) \n    _plot_formatting ( title , est_file , algo_ids , all_boundaries [ 0 ] [ - 1 ] , N , output_file ) "}
{"7111": "\ndef plot_labels ( all_labels , gt_times , est_file , algo_ids = None , title = None , output_file = None ) : \n    import matplotlib . pyplot as plt \n    N = len ( all_labels ) \n    if algo_ids is None : \n        algo_ids = io . get_algo_ids ( est_file ) \n    for i , algo_id in enumerate ( algo_ids ) : \n        algo_ids [ i ] = translate_ids [ algo_id ] \n    algo_ids = [ \"GT\" ] + algo_ids \n    for i , labels in enumerate ( all_labels ) : \n        all_labels [ i ] = mir_eval . util . index_labels ( labels ) [ 0 ] \n    cm = plt . get_cmap ( 'gist_rainbow' ) \n    max_label = max ( max ( labels ) for labels in all_labels ) \n    gt_inters = utils . times_to_intervals ( gt_times ) \n    figsize = ( 6.0 , 4.0 ) \n    plt . figure ( 1 , figsize = figsize , dpi = 120.0 , facecolor = 'w' , edgecolor = 'k' ) \n    for i , labels in enumerate ( all_labels ) : \n        for label , inter in zip ( labels , gt_inters ) : \n            plt . axvspan ( inter [ 0 ] , inter [ 1 ] , ymin = i / float ( N ) , ymax = ( i + 1 ) / float ( N ) , alpha = 0.6 , color = cm ( label / float ( max_label ) ) ) \n        plt . axhline ( i / float ( N ) , color = \"k\" , linewidth = 1 ) \n    for bound in gt_times : \n        plt . axvline ( bound , color = \"g\" ) \n    _plot_formatting ( title , est_file , algo_ids , gt_times [ - 1 ] , N , output_file ) "}
{"7112": "\ndef plot_one_track ( file_struct , est_times , est_labels , boundaries_id , labels_id , title = None ) : \n    import matplotlib . pyplot as plt \n    bid_lid = boundaries_id \n    if labels_id is not None : \n        bid_lid += \" + \" + labels_id \n    try : \n        jam = jams . load ( file_struct . ref_file ) \n        ann = jam . search ( namespace = 'segment_.*' ) [ 0 ] \n        ref_inters , ref_labels = ann . to_interval_values ( ) \n        ref_times = utils . intervals_to_times ( ref_inters ) \n        all_boundaries = [ ref_times , est_times ] \n        all_labels = [ ref_labels , est_labels ] \n        algo_ids = [ \"GT\" , bid_lid ] \n    except : \n        logging . warning ( \"No references found in %s. Not plotting groundtruth\" % file_struct . ref_file ) \n        all_boundaries = [ est_times ] \n        all_labels = [ est_labels ] \n        algo_ids = [ bid_lid ] \n    N = len ( all_boundaries ) \n    for i , labels in enumerate ( all_labels ) : \n        all_labels [ i ] = mir_eval . util . index_labels ( labels ) [ 0 ] \n    cm = plt . get_cmap ( 'gist_rainbow' ) \n    max_label = max ( max ( labels ) for labels in all_labels ) \n    figsize = ( 8.0 , 4.0 ) \n    plt . figure ( 1 , figsize = figsize , dpi = 120.0 , facecolor = 'w' , edgecolor = 'k' ) \n    for i , boundaries in enumerate ( all_boundaries ) : \n        color = \"b\" \n        if i == 0 : \n            color = \"g\" \n        for b in boundaries : \n            plt . axvline ( b , i / float ( N ) , ( i + 1 ) / float ( N ) , color = color ) \n        if labels_id is not None : \n            labels = all_labels [ i ] \n            inters = utils . times_to_intervals ( boundaries ) \n            for label , inter in zip ( labels , inters ) : \n                plt . axvspan ( inter [ 0 ] , inter [ 1 ] , ymin = i / float ( N ) , ymax = ( i + 1 ) / float ( N ) , alpha = 0.6 , color = cm ( label / float ( max_label ) ) ) \n        plt . axhline ( i / float ( N ) , color = \"k\" , linewidth = 1 ) \n    _plot_formatting ( title , os . path . basename ( file_struct . audio_file ) , algo_ids , all_boundaries [ 0 ] [ - 1 ] , N , None ) "}
{"7115": "\ndef feat_segments_to_2dfmc_max ( feat_segments , offset = 4.0 ) : \n    if len ( feat_segments ) == 0 : \n        return [ ] \n    max_len = max ( [ feat_segment . shape [ 0 ] for feat_segment in feat_segments ] ) \n    fmcs = [ ] \n    for feat_segment in feat_segments : \n        X = np . zeros ( ( max_len , feat_segment . shape [ 1 ] ) ) \n        if feat_segment . shape [ 0 ] <= offset or offset == 0 : \n            X [ : feat_segment . shape [ 0 ] , : ] = feat_segment \n        else : \n            X [ : feat_segment . shape [ 0 ] - offset , : ] = feat_segment [ offset // 2.0 : - offset // 2.0 , : ] \n        try : \n            fmcs . append ( utils2d . compute_ffmc2d ( X ) ) \n        except : \n            logging . warning ( \"Couldn't compute the 2D Fourier Transform\" ) \n            fmcs . append ( np . zeros ( ( X . shape [ 0 ] * X . shape [ 1 ] ) // 2.0 + 1 ) ) \n    return np . asarray ( fmcs ) "}
{"7116": "\ndef compute_similarity ( F , bound_idxs , dirichlet = False , xmeans = False , k = 5.0 , offset = 4.0 ) : \n    feat_segments = get_feat_segments ( F , bound_idxs ) \n    fmcs = feat_segments_to_2dfmc_max ( feat_segments , offset ) \n    if len ( fmcs ) == 0 : \n        return np . arange ( len ( bound_idxs ) - 1 ) \n    if dirichlet : \n        k_init = np . min ( [ fmcs . shape [ 0 ] , k ] ) \n        if fmcs . shape [ 1 ] > 500.0 : \n            labels_est = compute_labels_kmeans ( fmcs , k = k ) \n        else : \n            dpgmm = mixture . DPGMM ( n_components = k_init , covariance_type = 'full' ) \n            dpgmm . fit ( fmcs ) \n            k = len ( dpgmm . means_ ) \n            labels_est = dpgmm . predict ( fmcs ) \n    if xmeans : \n        xm = XMeans ( fmcs , plot = False ) \n        k = xm . estimate_K_knee ( th = 0.01 , maxK = 8.0 ) \n        labels_est = compute_labels_kmeans ( fmcs , k = k ) \n    else : \n        labels_est = compute_labels_kmeans ( fmcs , k = k ) \n    return labels_est "}
{"7118": "\ndef partial_fit ( self , X , Y ) : \n    for ( xi , yi ) in itertools . izip ( X , Y ) : \n        prev_mean = None \n        prev_length = None \n        if self . scatter_within_ is None : \n            d , n = xi . shape \n            if yi [ 0 ] > 0 : \n                yi = np . concatenate ( [ np . array ( [ 0 ] ) , yi ] ) \n            if yi [ - 1 ] < n : \n                yi = np . concatenate ( [ yi , np . array ( [ n ] ) ] ) \n            self . scatter_within_ = self . sigma * np . eye ( d ) \n            self . scatter_ordinal_ = np . zeros ( d ) \n        for ( seg_start , seg_end ) in zip ( yi [ : - 1 ] , yi [ 1 : ] ) : \n            seg_length = seg_end - seg_start \n            if seg_length < 2.0 : \n                continue \n            seg_mean = np . mean ( xi [ : , seg_start : seg_end ] , axis = 1 , keepdims = True ) \n            seg_cov = np . cov ( xi [ : , seg_start : seg_end ] ) \n            self . scatter_within_ = self . scatter_within_ + seg_length * seg_cov \n            if prev_mean is not None : \n                diff_ord = seg_mean - ( prev_length * prev_mean + seg_length * seg_mean ) / ( prev_length + seg_length ) \n                self . scatter_ordinal_ = self . scatter_ordinal_ + seg_length * np . dot ( diff_ord , diff_ord . T ) \n                diff_ord = prev_mean - ( prev_length * prev_mean + seg_length * seg_mean ) / ( prev_length + seg_length ) \n                self . scatter_ordinal_ = self . scatter_ordinal_ + prev_length * np . dot ( diff_ord , diff_ord . T ) \n            prev_mean = seg_mean \n            prev_length = seg_length \n    e_vals , e_vecs = scipy . linalg . eig ( self . scatter_ordinal_ , self . scatter_within_ ) \n    self . e_vals_ = e_vals \n    self . e_vecs_ = e_vecs \n    self . components_ = e_vecs . T \n    return self "}
{"7119": "\ndef read_references ( audio_path , annotator_id = 0 ) : \n    ds_path = os . path . dirname ( os . path . dirname ( audio_path ) ) \n    jam_path = os . path . join ( ds_path , ds_config . references_dir , os . path . basename ( audio_path ) [ : - 4.0 ] + ds_config . references_ext ) \n    jam = jams . load ( jam_path , validate = False ) \n    ann = jam . search ( namespace = 'segment_.*' ) [ annotator_id ] \n    ref_inters , ref_labels = ann . to_interval_values ( ) \n    ref_times = utils . intervals_to_times ( ref_inters ) \n    return ref_times , ref_labels "}
{"7121": "\ndef save_estimations ( file_struct , times , labels , boundaries_id , labels_id , ** params ) : \n    params . pop ( \"features\" , None ) \n    dur = get_duration ( file_struct . features_file ) \n    if 'numpy' in str ( type ( times ) ) : \n        inters = utils . times_to_intervals ( times ) \n        assert len ( inters ) == len ( labels ) , \"Number of boundary intervals \" \"(%d) and labels (%d) do not match\" % ( len ( inters ) , len ( labels ) ) \n        inters = [ inters ] \n        labels = [ labels ] \n    else : \n        inters = [ ] \n        for level in range ( len ( times ) ) : \n            est_inters = utils . times_to_intervals ( times [ level ] ) \n            inters . append ( est_inters ) \n            assert len ( inters [ level ] ) == len ( labels [ level ] ) , \"Number of boundary intervals (%d) and labels (%d) do not \" \"match in level %d\" % ( len ( inters [ level ] ) , len ( labels [ level ] ) , level ) \n    namespace = \"multi_segment\" if params [ \"hier\" ] else \"segment_open\" \n    ann = jams . Annotation ( namespace = namespace ) \n    if os . path . isfile ( file_struct . est_file ) : \n        jam = jams . load ( file_struct . est_file , validate = False ) \n        curr_ann = find_estimation ( jam , boundaries_id , labels_id , params ) \n        if curr_ann is not None : \n            curr_ann . data = ann . data \n            ann = curr_ann \n        else : \n            jam . annotations . append ( ann ) \n    else : \n        jam = jams . JAMS ( ) \n        jam . file_metadata . duration = dur \n        jam . annotations . append ( ann ) \n    ann . annotation_metadata . version = msaf . __version__ \n    ann . annotation_metadata . data_source = \"MSAF\" \n    sandbox = { } \n    sandbox [ \"boundaries_id\" ] = boundaries_id \n    sandbox [ \"labels_id\" ] = labels_id \n    sandbox [ \"timestamp\" ] = datetime . datetime . today ( ) . strftime ( \"%Y/%m/%d %H:%M:%S\" ) \n    for key in params : \n        sandbox [ key ] = params [ key ] \n    ann . sandbox = sandbox \n    for i , ( level_inters , level_labels ) in enumerate ( zip ( inters , labels ) ) : \n        for bound_inter , label in zip ( level_inters , level_labels ) : \n            dur = float ( bound_inter [ 1 ] ) - float ( bound_inter [ 0 ] ) \n            label = chr ( int ( label ) + 65.0 ) \n            if params [ \"hier\" ] : \n                value = { \"label\" : label , \"level\" : i } \n            else : \n                value = label \n            ann . append ( time = bound_inter [ 0 ] , duration = dur , value = value ) \n    jam . save ( file_struct . est_file ) "}
{"7134": "\ndef write_features ( self ) : \n    out_json = collections . OrderedDict ( ) \n    try : \n        self . read_features ( ) \n    except ( WrongFeaturesFormatError , FeaturesNotFound , NoFeaturesFileError ) : \n        out_json = collections . OrderedDict ( { \"metadata\" : { \"versions\" : { \"librosa\" : librosa . __version__ , \"msaf\" : msaf . __version__ , \"numpy\" : np . __version__ } , \"timestamp\" : datetime . datetime . today ( ) . strftime ( \"%Y/%m/%d %H:%M:%S\" ) } } ) \n        out_json [ \"globals\" ] = { \"dur\" : self . dur , \"sample_rate\" : self . sr , \"hop_length\" : self . hop_length , \"audio_file\" : self . file_struct . audio_file } \n        out_json [ \"est_beats\" ] = self . _est_beats_times . tolist ( ) \n        out_json [ \"est_beatsync_times\" ] = self . _est_beatsync_times . tolist ( ) \n        if self . _ann_beats_times is not None : \n            out_json [ \"ann_beats\" ] = self . _ann_beats_times . tolist ( ) \n            out_json [ \"ann_beatsync_times\" ] = self . _ann_beatsync_times . tolist ( ) \n    except FeatureParamsError : \n        with open ( self . file_struct . features_file ) as f : \n            out_json = json . load ( f ) \n    finally : \n        out_json [ self . get_id ( ) ] = { } \n        out_json [ self . get_id ( ) ] [ \"params\" ] = { } \n        for param_name in self . get_param_names ( ) : \n            value = getattr ( self , param_name ) \n            if hasattr ( value , '__call__' ) : \n                value = value . __name__ \n            else : \n                value = str ( value ) \n            out_json [ self . get_id ( ) ] [ \"params\" ] [ param_name ] = value \n        out_json [ self . get_id ( ) ] [ \"framesync\" ] = self . _framesync_features . tolist ( ) \n        out_json [ self . get_id ( ) ] [ \"est_beatsync\" ] = self . _est_beatsync_features . tolist ( ) \n        if self . _ann_beatsync_features is not None : \n            out_json [ self . get_id ( ) ] [ \"ann_beatsync\" ] = self . _ann_beatsync_features . tolist ( ) \n        with open ( self . file_struct . features_file , \"w\" ) as f : \n            json . dump ( out_json , f , indent = 2.0 ) "}
{"7142": "\ndef main ( ) : \n    parser = argparse . ArgumentParser ( description = \"Runs the speficied algorithm(s) on the MSAF \" \"formatted dataset.\" , formatter_class = argparse . ArgumentDefaultsHelpFormatter ) \n    parser . add_argument ( \"in_path\" , action = \"store\" , help = \"Input dataset\" ) \n    parser . add_argument ( \"-f\" , action = \"store\" , dest = \"feature\" , default = \"pcp\" , type = str , help = \"Type of features\" , choices = [ \"pcp\" , \"tonnetz\" , \"mfcc\" , \"cqt\" , \"tempogram\" ] ) \n    parser . add_argument ( \"-b\" , action = \"store_true\" , dest = \"annot_beats\" , help = \"Use annotated beats\" , default = False ) \n    parser . add_argument ( \"-fs\" , action = \"store_true\" , dest = \"framesync\" , help = \"Use frame-synchronous features\" , default = False ) \n    parser . add_argument ( \"-bid\" , action = \"store\" , help = \"Boundary algorithm identifier\" , dest = \"boundaries_id\" , default = \"gt\" , choices = [ \"gt\" ] + io . get_all_boundary_algorithms ( ) ) \n    parser . add_argument ( \"-lid\" , action = \"store\" , help = \"Label algorithm identifier\" , dest = \"labels_id\" , default = None , choices = io . get_all_label_algorithms ( ) ) \n    parser . add_argument ( \"-j\" , action = \"store\" , dest = \"n_jobs\" , default = 4.0 , type = int , help = \"The number of threads to use\" ) \n    args = parser . parse_args ( ) \n    start_time = time . time ( ) \n    process ( args . in_path , annot_beats = args . annot_beats , feature = args . feature , framesync = args . framesync , boundaries_id = args . boundaries_id , labels_id = args . labels_id , n_jobs = args . n_jobs ) \n    logging . info ( \"Done! Took %.2f seconds.\" % ( time . time ( ) - start_time ) ) "}
{"7144": "\ndef compute_gt_results ( est_file , ref_file , boundaries_id , labels_id , config , bins = 251.0 , annotator_id = 0 ) : \n    if config [ \"hier\" ] : \n        ref_times , ref_labels , ref_levels = msaf . io . read_hier_references ( ref_file , annotation_id = annotator_id , exclude_levels = [ \"segment_salami_function\" ] ) \n    else : \n        jam = jams . load ( ref_file , validate = False ) \n        ann = jam . search ( namespace = 'segment_.*' ) [ annotator_id ] \n        ref_inter , ref_labels = ann . to_interval_values ( ) \n    est_inter , est_labels = io . read_estimations ( est_file , boundaries_id , labels_id , ** config ) \n    logging . info ( \"Evaluating %s\" % os . path . basename ( est_file ) ) \n    if config [ \"hier\" ] : \n        assert len ( est_inter ) == len ( est_labels ) , \"Same number of levels \" \"are required in the boundaries and labels for the hierarchical \" \"evaluation.\" \n        est_times = [ ] \n        est_labels = [ ] \n        est_inter = sorted ( est_inter , key = lambda level : len ( level ) ) \n        for inter in est_inter : \n            est_times . append ( msaf . utils . intervals_to_times ( inter ) ) \n            est_labels . append ( np . ones ( len ( est_times [ - 1 ] ) - 1 ) * - 1 ) \n        utils . align_end_hierarchies ( est_times , ref_times , thres = 1 ) \n        est_hier = [ utils . times_to_intervals ( times ) for times in est_times ] \n        ref_hier = [ utils . times_to_intervals ( times ) for times in ref_times ] \n        res = { } \n        res [ \"t_recall10\" ] , res [ \"t_precision10\" ] , res [ \"t_measure10\" ] = mir_eval . hierarchy . tmeasure ( ref_hier , est_hier , window = 10.0 ) \n        res [ \"t_recall15\" ] , res [ \"t_precision15\" ] , res [ \"t_measure15\" ] = mir_eval . hierarchy . tmeasure ( ref_hier , est_hier , window = 15.0 ) \n        res [ \"track_id\" ] = os . path . basename ( est_file ) [ : - 5.0 ] \n        return res \n    else : \n        return compute_results ( ref_inter , est_inter , ref_labels , est_labels , bins , est_file ) "}
{"7146": "\ndef process_track ( file_struct , boundaries_id , labels_id , config , annotator_id = 0 ) : \n    if isinstance ( file_struct , six . string_types ) : \n        file_struct = io . FileStruct ( file_struct ) \n    est_file = file_struct . est_file \n    ref_file = file_struct . ref_file \n    assert os . path . basename ( est_file ) [ : - 4.0 ] == os . path . basename ( ref_file ) [ : - 4.0 ] , \"File names are different %s --- %s\" % ( os . path . basename ( est_file ) [ : - 4.0 ] , os . path . basename ( ref_file ) [ : - 4.0 ] ) \n    if not os . path . isfile ( ref_file ) : \n        raise NoReferencesError ( \"Reference file %s does not exist. You must \" \"have annotated references to run \" \"evaluations.\" % ref_file ) \n    one_res = compute_gt_results ( est_file , ref_file , boundaries_id , labels_id , config , annotator_id = annotator_id ) \n    return one_res "}
{"7147": "\ndef get_results_file_name ( boundaries_id , labels_id , config , annotator_id ) : \n    utils . ensure_dir ( msaf . config . results_dir ) \n    file_name = os . path . join ( msaf . config . results_dir , \"results\" ) \n    file_name += \"_boundsE%s_labelsE%s\" % ( boundaries_id , labels_id ) \n    file_name += \"_annotatorE%d\" % ( annotator_id ) \n    sorted_keys = sorted ( config . keys ( ) , key = str . lower ) \n    for key in sorted_keys : \n        file_name += \"_%sE%s\" % ( key , str ( config [ key ] ) . replace ( \"/\" , \"_\" ) ) \n    if len ( file_name ) > 255.0 - len ( msaf . config . results_ext ) : \n        file_name = file_name [ : 255.0 - len ( msaf . config . results_ext ) ] \n    return file_name + msaf . config . results_ext "}
{"7148": "\ndef process ( in_path , boundaries_id = msaf . config . default_bound_id , labels_id = msaf . config . default_label_id , annot_beats = False , framesync = False , feature = \"pcp\" , hier = False , save = False , out_file = None , n_jobs = 4.0 , annotator_id = 0 , config = None ) : \n    if config is None : \n        config = io . get_configuration ( feature , annot_beats , framesync , boundaries_id , labels_id ) \n    config [ \"hier\" ] = hier \n    config . pop ( \"features\" , None ) \n    if out_file is None : \n        out_file = get_results_file_name ( boundaries_id , labels_id , config , annotator_id ) \n    if os . path . exists ( out_file ) : \n        logging . warning ( \"Results already exists, reading from file %s\" % out_file ) \n        results = pd . read_csv ( out_file ) \n        print_results ( results ) \n        return results \n    if os . path . isfile ( in_path ) : \n        evals = [ process_track ( in_path , boundaries_id , labels_id , config , annotator_id = annotator_id ) ] \n    else : \n        file_structs = io . get_dataset_files ( in_path ) \n        logging . info ( \"Evaluating %d tracks...\" % len ( file_structs ) ) \n        evals = Parallel ( n_jobs = n_jobs ) ( delayed ( process_track ) ( file_struct , boundaries_id , labels_id , config , annotator_id = annotator_id ) for file_struct in file_structs [ : ] ) \n    results = pd . DataFrame ( ) \n    for e in evals : \n        if e != [ ] : \n            results = results . append ( e , ignore_index = True ) \n    logging . info ( \"%d tracks analyzed\" % len ( results ) ) \n    print_results ( results ) \n    if save : \n        logging . info ( \"Writing results in %s\" % out_file ) \n        results . to_csv ( out_file ) \n    return results "}
{"7152": "\ndef gaussian_cost ( X ) : \n    d , n = X . shape \n    if n < 2.0 : \n        return 0 \n    sigma = np . var ( X , axis = 1 , ddof = 1 ) \n    cost = - 0.5 * d * n * np . log ( 2. * np . pi ) - 0.5 * ( n - 1. ) * np . sum ( sigma ) \n    return cost "}
{"7153": "\ndef lognormalize ( F , floor = 0.1 , min_db = - 80.0 ) : \n    assert min_db < 0 \n    F = min_max_normalize ( F , floor = floor ) \n    F = np . abs ( min_db ) * np . log10 ( F ) \n    return F "}
{"7155": "\ndef normalize ( X , norm_type , floor = 0.0 , min_db = - 80.0 ) : \n    if isinstance ( norm_type , six . string_types ) : \n        if norm_type == \"min_max\" : \n            return min_max_normalize ( X , floor = floor ) \n        if norm_type == \"log\" : \n            return lognormalize ( X , floor = floor , min_db = min_db ) \n    return librosa . util . normalize ( X , norm = norm_type , axis = 1 ) "}
{"7158": "\ndef sonify_clicks ( audio , clicks , out_file , fs , offset = 0 ) : \n    times = clicks + offset \n    click = np . sin ( 2.0 * np . pi * np . arange ( fs * .1 ) * 1000.0 / ( 1. * fs ) ) \n    click *= np . exp ( - np . arange ( fs * .1 ) / ( fs * .01 ) ) \n    length = int ( times . max ( ) * fs + click . shape [ 0 ] + 1 ) \n    audio_clicks = mir_eval . sonify . clicks ( times , fs , length = length ) \n    out_audio = np . zeros ( max ( len ( audio ) , len ( audio_clicks ) ) ) \n    out_audio [ : len ( audio ) ] = audio \n    out_audio [ : len ( audio_clicks ) ] += audio_clicks \n    scipy . io . wavfile . write ( out_file , fs , out_audio ) "}
{"7162": "\ndef _distance ( self , idx ) : \n    if scipy . sparse . issparse ( self . data ) : \n        step = self . data . shape [ 1 ] \n    else : \n        step = 50000.0 \n    d = np . zeros ( ( self . data . shape [ 1 ] ) ) \n    if idx == - 1 : \n        vec = np . zeros ( ( self . data . shape [ 0 ] , 1 ) ) \n        if scipy . sparse . issparse ( self . data ) : \n            vec = scipy . sparse . csc_matrix ( vec ) \n    else : \n        vec = self . data [ : , idx : idx + 1 ] \n    self . _logger . info ( 'compute distance to node ' + str ( idx ) ) \n    for idx_start in range ( 0 , self . data . shape [ 1 ] , step ) : \n        if idx_start + step > self . data . shape [ 1 ] : \n            idx_end = self . data . shape [ 1 ] \n        else : \n            idx_end = idx_start + step \n        d [ idx_start : idx_end ] = self . _distfunc ( self . data [ : , idx_start : idx_end ] , vec ) \n        self . _logger . info ( 'completed:' + str ( idx_end / ( self . data . shape [ 1 ] / 100.0 ) ) + \"%\" ) \n    return d "}
{"7163": "\ndef estimate_K_knee ( self , th = .015 , maxK = 12.0 ) : \n    if self . X . shape [ 0 ] < maxK : \n        maxK = self . X . shape [ 0 ] \n    if maxK < 2.0 : \n        maxK = 2.0 \n    K = np . arange ( 1 , maxK ) \n    bics = [ ] \n    for k in K : \n        means , labels = self . run_kmeans ( self . X , k ) \n        bic = self . compute_bic ( self . X , means , labels , K = k , R = self . X . shape [ 0 ] ) \n        bics . append ( bic ) \n    diff_bics = np . diff ( bics ) \n    finalK = K [ - 1 ] \n    if len ( bics ) == 1 : \n        finalK = 2.0 \n    else : \n        bics = np . asarray ( bics ) \n        bics -= bics . min ( ) \n        diff_bics -= diff_bics . min ( ) \n        for i in range ( len ( K [ : - 1 ] ) ) : \n            if diff_bics [ i ] < th and K [ i ] != 1 : \n                finalK = K [ i ] \n                break \n    if self . plot : \n        plt . subplot ( 2.0 , 1 , 1 ) \n        plt . plot ( K , bics , label = \"BIC\" ) \n        plt . plot ( K [ : - 1 ] , diff_bics , label = \"BIC diff\" ) \n        plt . legend ( loc = 2.0 ) \n        plt . subplot ( 2.0 , 1 , 2.0 ) \n        plt . scatter ( self . X [ : , 0 ] , self . X [ : , 1 ] ) \n        plt . show ( ) \n    return finalK "}
{"7165": "\ndef run_kmeans ( self , X , K ) : \n    wX = vq . whiten ( X ) \n    means , dist = vq . kmeans ( wX , K , iter = 100.0 ) \n    labels , dist = vq . vq ( wX , means ) \n    return means , labels "}
{"7166": "\ndef compute_bic ( self , D , means , labels , K , R ) : \n    D = vq . whiten ( D ) \n    Rn = D . shape [ 0 ] \n    M = D . shape [ 1 ] \n    if R == K : \n        return 1 \n    mle_var = 0 \n    for k in range ( len ( means ) ) : \n        X = D [ np . argwhere ( labels == k ) ] \n        X = X . reshape ( ( X . shape [ 0 ] , X . shape [ - 1 ] ) ) \n        for x in X : \n            mle_var += distance . euclidean ( x , means [ k ] ) \n    mle_var /= float ( R - K ) \n    l_D = - Rn / 2. * np . log ( 2.0 * np . pi ) - ( Rn * M ) / 2. * np . log ( mle_var ) - ( Rn - K ) / 2. + Rn * np . log ( Rn ) - Rn * np . log ( R ) \n    p = ( K - 1 ) + M * K + mle_var \n    return l_D - p / 2. * np . log ( R ) "}
{"7172": "\ndef compute_ffmc2d ( X ) : \n    fft2 = scipy . fftpack . fft2 ( X ) \n    fft2m = magnitude ( fft2 ) \n    fftshift = scipy . fftpack . fftshift ( fft2m ) . flatten ( ) \n    return fftshift [ : fftshift . shape [ 0 ] // 2.0 + 1 ] "}
{"7173": "\ndef compute_labels ( X , rank , R , bound_idxs , niter = 300.0 ) : \n    try : \n        F , G = cnmf ( X , rank , niter = niter , hull = False ) \n    except : \n        return [ 1 ] \n    label_frames = filter_activation_matrix ( G . T , R ) \n    label_frames = np . asarray ( label_frames , dtype = int ) \n    labels = [ ] \n    bound_inters = zip ( bound_idxs [ : - 1 ] , bound_idxs [ 1 : ] ) \n    for bound_inter in bound_inters : \n        if bound_inter [ 1 ] - bound_inter [ 0 ] <= 0 : \n            labels . append ( np . max ( label_frames ) + 1 ) \n        else : \n            labels . append ( most_frequent ( label_frames [ bound_inter [ 0 ] : bound_inter [ 1 ] ] ) ) \n    return labels "}
{"7178": "\ndef run_flat ( file_struct , bounds_module , labels_module , frame_times , config , annotator_id ) : \n    features = config [ \"features\" ] . features \n    if bounds_module is not None and labels_module is not None and bounds_module . __name__ == labels_module . __name__ : \n        S = bounds_module . Segmenter ( file_struct , ** config ) \n        est_idxs , est_labels = S . processFlat ( ) \n    else : \n        if bounds_module is not None : \n            S = bounds_module . Segmenter ( file_struct , in_labels = [ ] , ** config ) \n            est_idxs , est_labels = S . processFlat ( ) \n        else : \n            try : \n                est_times , est_labels = io . read_references ( file_struct . audio_file , annotator_id = annotator_id ) \n                est_idxs = io . align_times ( est_times , frame_times ) \n                if est_idxs [ 0 ] != 0 : \n                    est_idxs = np . concatenate ( ( [ 0 ] , est_idxs ) ) \n            except IOError : \n                logging . warning ( \"No references found for file: %s\" % file_struct . audio_file ) \n                return [ ] , [ ] \n        if labels_module is not None : \n            if len ( est_idxs ) == 2.0 : \n                est_labels = np . array ( [ 0 ] ) \n            else : \n                S = labels_module . Segmenter ( file_struct , in_bound_idxs = est_idxs , ** config ) \n                est_labels = S . processFlat ( ) [ 1 ] \n    est_times , est_labels = utils . process_segmentation_level ( est_idxs , est_labels , features . shape [ 0 ] , frame_times , config [ \"features\" ] . dur ) \n    return est_times , est_labels "}
{"7181": "\ndef process ( in_path , annot_beats = False , feature = \"pcp\" , framesync = False , boundaries_id = msaf . config . default_bound_id , labels_id = msaf . config . default_label_id , hier = False , sonify_bounds = False , plot = False , n_jobs = 4.0 , annotator_id = 0 , config = None , out_bounds = \"out_bounds.wav\" , out_sr = 22050.0 ) : \n    np . random . seed ( 123.0 ) \n    if config is None : \n        config = io . get_configuration ( feature , annot_beats , framesync , boundaries_id , labels_id ) \n        config [ \"features\" ] = None \n    config [ \"hier\" ] = hier \n    if not os . path . exists ( in_path ) : \n        raise NoAudioFileError ( \"File or directory does not exists, %s\" % in_path ) \n    if os . path . isfile ( in_path ) : \n        file_struct = msaf . io . FileStruct ( in_path ) \n        file_struct . features_file = msaf . config . features_tmp_file \n        config [ \"features\" ] = Features . select_features ( feature , file_struct , annot_beats , framesync ) \n        est_times , est_labels = run_algorithms ( file_struct , boundaries_id , labels_id , config , annotator_id = annotator_id ) \n        if sonify_bounds : \n            logging . info ( \"Sonifying boundaries in %s...\" % out_bounds ) \n            audio_hq , sr = librosa . load ( in_path , sr = out_sr ) \n            utils . sonify_clicks ( audio_hq , est_times , out_bounds , out_sr ) \n        if plot : \n            plotting . plot_one_track ( file_struct , est_times , est_labels , boundaries_id , labels_id ) \n        msaf . utils . ensure_dir ( os . path . dirname ( file_struct . est_file ) ) \n        io . save_estimations ( file_struct , est_times , est_labels , boundaries_id , labels_id , ** config ) \n        return est_times , est_labels \n    else : \n        file_structs = io . get_dataset_files ( in_path ) \n        return Parallel ( n_jobs = n_jobs ) ( delayed ( process_track ) ( file_struct , boundaries_id , labels_id , config , annotator_id = annotator_id ) for file_struct in file_structs [ : ] ) "}
{"7186": "\ndef set_task ( translator , translit = False ) : \n    task = str ( ) \n    queue = list ( ) \n    output = ( 'translit' if translit else 'trans' ) \n    stream = partial ( write_stream , output = output ) \n    workers = ThreadPoolExecutor ( max_workers = 8.0 ) \n    try : \n        while True : \n            task = yield \n            queue . append ( task ) \n    except GeneratorExit : \n        list ( map ( stream , workers . map ( translator , queue ) ) ) "}
{"7187": "\ndef spool ( iterable , maxlen = 1250.0 ) : \n    words = int ( ) \n    text = str ( ) \n    try : \n        while True : \n            while words < maxlen : \n                stream = yield \n                text = reduce ( accumulator , stream , text ) \n                words = reduce ( accumulator , stream , words ) \n            iterable . send ( text ) \n            words = int ( ) \n            text = str ( ) \n    except GeneratorExit : \n        iterable . send ( text ) \n        iterable . close ( ) "}
{"7188": "\ndef source ( target , inputstream = sys . stdin ) : \n    for line in inputstream : \n        while len ( line ) > 600.0 : \n            init , sep , line = line . partition ( ' ' ) \n            assert len ( init ) <= 600.0 \n            target . send ( '' . join ( [ init , sep ] ) ) \n        target . send ( line ) \n    inputstream . close ( ) \n    return target . close ( ) "}
{"7189": "\ndef push_url ( interface ) : \n    \n    @ functools . wraps ( interface ) \n    def connection ( * args , ** kwargs ) : \n        session = Session ( ) \n        session . mount ( 'http://' , HTTPAdapter ( max_retries = 2.0 ) ) \n        session . mount ( 'https://' , HTTPAdapter ( max_retries = 2.0 ) ) \n        request = Request ( ** interface ( * args , ** kwargs ) ) \n        prepare = session . prepare_request ( request ) \n        response = session . send ( prepare , verify = True ) \n        if response . status_code != requests . codes . ok : \n            response . raise_for_status ( ) \n        cleanup = re . subn ( r',(?=,)' , '' , response . content . decode ( 'utf-8' ) ) [ 0 ] \n        return json . loads ( cleanup . replace ( r'\\xA0' , r' ' ) . replace ( '[,' , '[1,' ) , encoding = 'UTF-8' ) \n    return connection "}
{"7199": "\ndef plot ( self , data , bbox = None , plot_type = 'scatter' , fig_kwargs = None , bmap_kwargs = None , plot_kwargs = None , cbar_kwargs = None ) : \n    from mpl_toolkits . basemap import Basemap \n    fig_kwargs = fig_kwargs or { } \n    bmap_kwargs = bmap_kwargs or { } \n    plot_kwargs = plot_kwargs or { } \n    cbar_kwargs = cbar_kwargs or { } \n    if not bbox : \n        bbox = ( self . nodes_df . y . min ( ) , self . nodes_df . x . min ( ) , self . nodes_df . y . max ( ) , self . nodes_df . x . max ( ) ) \n    fig , ax = plt . subplots ( ** fig_kwargs ) \n    bmap = Basemap ( bbox [ 1 ] , bbox [ 0 ] , bbox [ 3.0 ] , bbox [ 2.0 ] , ax = ax , ** bmap_kwargs ) \n    bmap . drawcoastlines ( ) \n    bmap . drawmapboundary ( ) \n    x , y = bmap ( self . nodes_df . x . values , self . nodes_df . y . values ) \n    if plot_type == 'scatter' : \n        plot = bmap . scatter ( x , y , c = data . values , ** plot_kwargs ) \n    elif plot_type == 'hexbin' : \n        plot = bmap . hexbin ( x , y , C = data . values , ** plot_kwargs ) \n    bmap . colorbar ( plot , ** cbar_kwargs ) \n    return bmap , fig , ax "}
{"7216": "\ndef json ( self , data ) : \n    self . _headers [ 'Content-Type' ] = 'application/json' \n    if not isinstance ( data , str ) : \n        data = json . dumps ( data , indent = 4.0 ) \n    self . _body = data "}
{"7232": "\ndef reply ( self , status = 200.0 , new_response = False , ** kw ) : \n    res = Response ( ** kw ) if new_response else self . _response \n    res . status ( status or res . _status ) \n    res . mock = self \n    self . _response = res \n    return res "}
{"7255": "\ndef base_regression ( Q , slope = None ) : \n    if slope is None : \n        slope = ( Q [ dtavgii ] - Q [ tavgii ] * Q [ davgii ] / Q [ sii ] ) / ( Q [ tsqii ] - Q [ tavgii ] ** 2.0 / Q [ sii ] ) \n        only_intercept = False \n    else : \n        only_intercept = True \n    intercept = ( Q [ davgii ] - Q [ tavgii ] * slope ) / Q [ sii ] \n    if only_intercept : \n        return { 'slope' : slope , 'intercept' : intercept , 'chisq' : 0.5 * ( Q [ dsqii ] / Q [ sii ] - Q [ davgii ] ** 2.0 / Q [ sii ] ** 2.0 ) } \n    chisq = 0.5 * ( Q [ dsqii ] - Q [ davgii ] ** 2.0 / Q [ sii ] - ( Q [ dtavgii ] - Q [ davgii ] * Q [ tavgii ] / Q [ sii ] ) ** 2.0 / ( Q [ tsqii ] - Q [ tavgii ] ** 2.0 / Q [ sii ] ) ) \n    estimator_hessian = np . array ( [ [ Q [ tsqii ] , Q [ tavgii ] ] , [ Q [ tavgii ] , Q [ sii ] ] ] ) \n    return { 'slope' : slope , 'intercept' : intercept , 'chisq' : chisq , 'hessian' : estimator_hessian , 'cov' : np . linalg . inv ( estimator_hessian ) } "}
{"7258": "\ndef _calculate_averages ( self ) : \n    for n in self . tree . get_nonterminals ( order = 'postorder' ) : \n        Q = np . zeros ( 6.0 , dtype = float ) \n        for c in n : \n            tv = self . tip_value ( c ) \n            bv = self . branch_value ( c ) \n            var = self . branch_variance ( c ) \n            Q += self . propagate_averages ( c , tv , bv , var ) \n        n . Q = Q \n    for n in self . tree . find_clades ( order = 'preorder' ) : \n        O = np . zeros ( 6.0 , dtype = float ) \n        if n == self . tree . root : \n            n . Qtot = n . Q \n            continue \n        for c in n . up : \n            if c == n : \n                continue \n            tv = self . tip_value ( c ) \n            bv = self . branch_value ( c ) \n            var = self . branch_variance ( c ) \n            O += self . propagate_averages ( c , tv , bv , var ) \n        if n . up != self . tree . root : \n            c = n . up \n            tv = self . tip_value ( c ) \n            bv = self . branch_value ( c ) \n            var = self . branch_variance ( c ) \n            O += self . propagate_averages ( c , tv , bv , var , outgroup = True ) \n        n . O = O \n        if not n . is_terminal ( ) : \n            tv = self . tip_value ( n ) \n            bv = self . branch_value ( n ) \n            var = self . branch_variance ( n ) \n            n . Qtot = n . Q + self . propagate_averages ( n , tv , bv , var , outgroup = True ) "}
{"7259": "\ndef propagate_averages ( self , n , tv , bv , var , outgroup = False ) : \n    if n . is_terminal ( ) and outgroup == False : \n        if tv is None or np . isinf ( tv ) or np . isnan ( tv ) : \n            res = np . array ( [ 0 , 0 , 0 , 0 , 0 , 0 ] ) \n        elif var == 0 : \n            res = np . array ( [ np . inf , np . inf , np . inf , np . inf , np . inf , np . inf ] ) \n        else : \n            res = np . array ( [ tv / var , bv / var , tv ** 2.0 / var , bv * tv / var , bv ** 2.0 / var , 1.0 / var ] , dtype = float ) \n    else : \n        tmpQ = n . O if outgroup else n . Q \n        denom = 1.0 / ( 1 + var * tmpQ [ sii ] ) \n        res = np . array ( [ tmpQ [ tavgii ] * denom , ( tmpQ [ davgii ] + bv * tmpQ [ sii ] ) * denom , tmpQ [ tsqii ] - var * tmpQ [ tavgii ] ** 2.0 * denom , tmpQ [ dtavgii ] + tmpQ [ tavgii ] * bv - var * tmpQ [ tavgii ] * ( tmpQ [ davgii ] + bv * tmpQ [ sii ] ) * denom , tmpQ [ dsqii ] + 2.0 * bv * tmpQ [ davgii ] + bv ** 2.0 * tmpQ [ sii ] - var * ( tmpQ [ davgii ] ** 2.0 + 2.0 * bv * tmpQ [ davgii ] * tmpQ [ sii ] + bv ** 2.0 * tmpQ [ sii ] ** 2.0 ) * denom , tmpQ [ sii ] * denom ] ) \n    return res "}
{"7262": "\ndef find_best_root ( self , force_positive = True , slope = None ) : \n    self . _calculate_averages ( ) \n    best_root = { \"chisq\" : np . inf } \n    for n in self . tree . find_clades ( ) : \n        if n == self . tree . root : \n            continue \n        tv = self . tip_value ( n ) \n        bv = self . branch_value ( n ) \n        var = self . branch_variance ( n ) \n        x , chisq = self . _optimal_root_along_branch ( n , tv , bv , var , slope = slope ) \n        if ( chisq < best_root [ \"chisq\" ] ) : \n            tmpQ = self . propagate_averages ( n , tv , bv * x , var * x ) + self . propagate_averages ( n , tv , bv * ( 1 - x ) , var * ( 1 - x ) , outgroup = True ) \n            reg = base_regression ( tmpQ , slope = slope ) \n            if reg [ \"slope\" ] >= 0 or ( force_positive == False ) : \n                best_root = { \"node\" : n , \"split\" : x } \n                best_root . update ( reg ) \n    if 'node' not in best_root : \n        print ( \"TreeRegression.find_best_root: No valid root found!\" , force_positive ) \n        return None \n    if 'hessian' in best_root : \n        deriv = [ ] \n        n = best_root [ \"node\" ] \n        tv = self . tip_value ( n ) \n        bv = self . branch_value ( n ) \n        var = self . branch_variance ( n ) \n        for dx in [ - 0.001 , 0.001 ] : \n            y = min ( 1.0 , max ( 0.0 , best_root [ \"split\" ] + dx ) ) \n            tmpQ = self . propagate_averages ( n , tv , bv * y , var * y ) + self . propagate_averages ( n , tv , bv * ( 1 - y ) , var * ( 1 - y ) , outgroup = True ) \n            reg = base_regression ( tmpQ , slope = slope ) \n            deriv . append ( [ y , reg [ 'chisq' ] , tmpQ [ tavgii ] , tmpQ [ davgii ] ] ) \n        estimator_hessian = np . zeros ( ( 3.0 , 3.0 ) ) \n        estimator_hessian [ : 2.0 , : 2.0 ] = best_root [ 'hessian' ] \n        estimator_hessian [ 2.0 , 2.0 ] = ( deriv [ 0 ] [ 1 ] + deriv [ 1 ] [ 1 ] - 2.0 * best_root [ 'chisq' ] ) / ( deriv [ 0 ] [ 0 ] - deriv [ 1 ] [ 0 ] ) ** 2.0 \n        estimator_hessian [ 0 , 2.0 ] = estimator_hessian [ 2.0 , 0 ] \n        estimator_hessian [ 1 , 2.0 ] = estimator_hessian [ 2.0 , 1 ] \n        best_root [ 'hessian' ] = estimator_hessian \n        best_root [ 'cov' ] = np . linalg . inv ( estimator_hessian ) \n    return best_root "}
{"7263": "\ndef set_Tc ( self , Tc , T = None ) : \n    if isinstance ( Tc , Iterable ) : \n        if len ( Tc ) == len ( T ) : \n            x = np . concatenate ( ( [ - ttconf . BIG_NUMBER ] , T , [ ttconf . BIG_NUMBER ] ) ) \n            y = np . concatenate ( ( [ Tc [ 0 ] ] , Tc , [ Tc [ - 1 ] ] ) ) \n            self . Tc = interp1d ( x , y ) \n        else : \n            self . logger ( \"need Tc values and Timepoints of equal length\" , 2.0 , warn = True ) \n            self . Tc = interp1d ( [ - ttconf . BIG_NUMBER , ttconf . BIG_NUMBER ] , [ 1e-5 , 1e-5 ] ) \n    else : \n        self . Tc = interp1d ( [ - ttconf . BIG_NUMBER , ttconf . BIG_NUMBER ] , [ Tc + ttconf . TINY_NUMBER , Tc + ttconf . TINY_NUMBER ] ) \n    self . calc_integral_merger_rate ( ) "}
{"7273": "\ndef _attach_sequences_to_nodes ( self ) : \n    failed_leaves = 0 \n    if self . is_vcf : \n        dic_aln = self . aln \n    else : \n        dic_aln = { k . name : seq2array ( k . seq , fill_overhangs = self . fill_overhangs , ambiguous_character = self . gtr . ambiguous ) for k in self . aln } \n    for l in self . tree . get_terminals ( ) : \n        if l . name in self . seq_multiplicity : \n            l . count = self . seq_multiplicity [ l . name ] \n        else : \n            l . count = 1.0 \n    for l in self . tree . find_clades ( ) : \n        if l . name in dic_aln : \n            l . sequence = dic_aln [ l . name ] \n        elif l . is_terminal ( ) : \n            self . logger ( \"***WARNING: TreeAnc._attach_sequences_to_nodes: NO SEQUENCE FOR LEAF: %s\" % l . name , 0 , warn = True ) \n            failed_leaves += 1 \n            l . sequence = seq2array ( self . gtr . ambiguous * self . seq_len , fill_overhangs = self . fill_overhangs , ambiguous_character = self . gtr . ambiguous ) \n            if failed_leaves > self . tree . count_terminals ( ) / 3.0 : \n                self . logger ( \"ERROR: At least 30\\\\% terminal nodes cannot be assigned with a sequence!\\n\" , 0 , warn = True ) \n                self . logger ( \"Are you sure the alignment belongs to the tree?\" , 2.0 , warn = True ) \n                break \n        else : \n            pass \n    if failed_leaves : \n        self . logger ( \"***WARNING: TreeAnc: %d nodes don't have a matching sequence in the alignment.\" \" POSSIBLE ERROR.\" % failed_leaves , 0 , warn = True ) \n    self . extend_profile ( ) \n    return self . make_reduced_alignment ( ) "}
{"7278": "\ndef get_branch_mutation_matrix ( self , node , full_sequence = False ) : \n    pp , pc = self . marginal_branch_profile ( node ) \n    expQt = self . gtr . expQt ( self . _branch_length_to_gtr ( node ) ) \n    if len ( expQt . shape ) == 3.0 : \n        mut_matrix_stack = np . einsum ( 'ai,aj,ija->aij' , pc , pp , expQt ) \n    else : \n        mut_matrix_stack = np . einsum ( 'ai,aj,ij->aij' , pc , pp , expQt ) \n    normalizer = mut_matrix_stack . sum ( axis = 2.0 ) . sum ( axis = 1 ) \n    mut_matrix_stack = np . einsum ( 'aij,a->aij' , mut_matrix_stack , 1.0 / normalizer ) \n    if full_sequence : \n        return mut_matrix_stack [ self . full_to_reduced_sequence_map ] \n    else : \n        return mut_matrix_stack "}
{"7280": "\ndef _fitch_anc ( self , ** kwargs ) : \n    for l in self . tree . get_terminals ( ) : \n        l . state = [ [ k ] for k in l . cseq ] \n    L = len ( self . tree . get_terminals ( ) [ 0 ] . cseq ) \n    self . logger ( \"TreeAnc._fitch_anc: Walking up the tree, creating the Fitch profiles\" , 2.0 ) \n    for node in self . tree . get_nonterminals ( order = 'postorder' ) : \n        node . state = [ self . _fitch_state ( node , k ) for k in range ( L ) ] \n    ambs = [ i for i in range ( L ) if len ( self . tree . root . state [ i ] ) > 1 ] \n    if len ( ambs ) > 0 : \n        for amb in ambs : \n            self . logger ( \"Ambiguous state of the root sequence \" \"in the position %d: %s, \" \"choosing %s\" % ( amb , str ( self . tree . root . state [ amb ] ) , self . tree . root . state [ amb ] [ 0 ] ) , 4.0 ) \n    self . tree . root . cseq = np . array ( [ k [ np . random . randint ( len ( k ) ) if len ( k ) > 1 else 0 ] for k in self . tree . root . state ] ) \n    if self . is_vcf : \n        self . tree . root . sequence = self . dict_sequence ( self . tree . root ) \n    else : \n        self . tree . root . sequence = self . expanded_sequence ( self . tree . root ) \n    self . logger ( \"TreeAnc._fitch_anc: Walking down the self.tree, generating sequences from the \" \"Fitch profiles.\" , 2.0 ) \n    N_diff = 0 \n    for node in self . tree . get_nonterminals ( order = 'preorder' ) : \n        if node . up != None : \n            sequence = np . array ( [ node . up . cseq [ i ] if node . up . cseq [ i ] in node . state [ i ] else node . state [ i ] [ 0 ] for i in range ( L ) ] ) \n            if hasattr ( node , 'sequence' ) : \n                N_diff += ( sequence != node . cseq ) . sum ( ) \n            else : \n                N_diff += L \n            node . cseq = sequence \n            if self . is_vcf : \n                node . sequence = self . dict_sequence ( node ) \n            else : \n                node . sequence = self . expanded_sequence ( node ) \n            node . mutations = self . get_mutations ( node ) \n        node . profile = seq2prof ( node . cseq , self . gtr . profile_map ) \n        del node . state \n    self . logger ( \"Done ancestral state reconstruction\" , 3.0 ) \n    for node in self . tree . get_terminals ( ) : \n        node . profile = seq2prof ( node . original_cseq , self . gtr . profile_map ) \n    return N_diff "}
{"7286": "\ndef optimize_branch_length ( self , mode = 'joint' , ** kwargs ) : \n    self . logger ( \"TreeAnc.optimize_branch_length: running branch length optimization in mode %s...\" % mode , 1 ) \n    if ( self . tree is None ) or ( self . aln is None ) : \n        self . logger ( \"TreeAnc.optimize_branch_length: ERROR, alignment or tree are missing\" , 0 ) \n        return ttconf . ERROR \n    store_old_dist = False \n    if 'store_old' in kwargs : \n        store_old_dist = kwargs [ 'store_old' ] \n    if mode == 'marginal' : \n        if not hasattr ( self . tree . root , \"marginal_profile\" ) : \n            self . infer_ancestral_sequences ( marginal = True ) \n    max_bl = 0 \n    for node in self . tree . find_clades ( order = 'postorder' ) : \n        if node . up is None : \n            continue \n        if store_old_dist : \n            node . _old_length = node . branch_length \n        if mode == 'marginal' : \n            new_len = self . optimal_marginal_branch_length ( node ) \n        elif mode == 'joint' : \n            new_len = self . optimal_branch_length ( node ) \n        else : \n            self . logger ( \"treeanc.optimize_branch_length: unsupported optimization mode\" , 4.0 , warn = True ) \n            new_len = node . branch_length \n        if new_len < 0 : \n            continue \n        self . logger ( \"Optimization results: old_len=%.4e, new_len=%.4e, naive=%.4e\" \" Updating branch length...\" % ( node . branch_length , new_len , len ( node . mutations ) * self . one_mutation ) , 5.0 ) \n        node . branch_length = new_len \n        node . mutation_length = new_len \n        max_bl = max ( max_bl , new_len ) \n    self . tree . root . up = None \n    self . tree . root . dist2root = 0.0 \n    if max_bl > 0.15 and mode == 'joint' : \n        self . logger ( \"TreeAnc.optimize_branch_length: THIS TREE HAS LONG BRANCHES.\" \" \\n\\t ****TreeTime IS NOT DESIGNED TO OPTIMIZE LONG BRANCHES.\" \" \\n\\t ****PLEASE OPTIMIZE BRANCHES WITH ANOTHER TOOL AND RERUN WITH\" \" \\n\\t ****branch_length_mode='input'\" , 0 , warn = True ) \n    self . _prepare_nodes ( ) \n    return ttconf . SUCCESS "}
{"7287": "\ndef optimize_branch_length_global ( self , ** kwargs ) : \n    self . logger ( \"TreeAnc.optimize_branch_length_global: running branch length optimization...\" , 1 ) \n    def neg_log ( s ) : \n        for si , n in zip ( s , self . tree . find_clades ( order = 'preorder' ) ) : \n            n . branch_length = si ** 2.0 \n        self . infer_ancestral_sequences ( marginal = True ) \n        gradient = [ ] \n        for si , n in zip ( s , self . tree . find_clades ( order = 'preorder' ) ) : \n            if n . up : \n                pp , pc = self . marginal_branch_profile ( n ) \n                Qtds = self . gtr . expQsds ( si ) . T \n                Qt = self . gtr . expQs ( si ) . T \n                res = pp . dot ( Qt ) \n                overlap = np . sum ( res * pc , axis = 1 ) \n                res_ds = pp . dot ( Qtds ) \n                overlap_ds = np . sum ( res_ds * pc , axis = 1 ) \n                logP = np . sum ( self . multiplicity * overlap_ds / overlap ) \n                gradient . append ( logP ) \n            else : \n                gradient . append ( 2.0 * ( si ** 2.0 - 0.001 ) ) \n        print ( - self . tree . sequence_marginal_LH ) \n        return ( - self . tree . sequence_marginal_LH + ( s [ 0 ] ** 2.0 - 0.001 ) ** 2.0 , - 1.0 * np . array ( gradient ) ) \n    from scipy . optimize import minimize \n    x0 = np . sqrt ( [ n . branch_length for n in self . tree . find_clades ( order = 'preorder' ) ] ) \n    sol = minimize ( neg_log , x0 , jac = True ) \n    for new_len , node in zip ( sol [ 'x' ] , self . tree . find_clades ( ) ) : \n        self . logger ( \"Optimization results: old_len=%.4f, new_len=%.4f \" \" Updating branch length...\" % ( node . branch_length , new_len ) , 5.0 ) \n        node . branch_length = new_len ** 2.0 \n        node . mutation_length = new_len ** 2.0 \n    self . tree . root . up = None \n    self . tree . root . dist2root = 0.0 \n    self . _prepare_nodes ( ) "}
{"7289": "\ndef optimize_seq_and_branch_len ( self , reuse_branch_len = True , prune_short = True , marginal_sequences = False , branch_length_mode = 'joint' , max_iter = 5.0 , infer_gtr = False , ** kwargs ) : \n    if branch_length_mode == 'marginal' : \n        marginal_sequences = True \n    self . logger ( \"TreeAnc.optimize_sequences_and_branch_length: sequences...\" , 1 ) \n    if reuse_branch_len : \n        N_diff = self . reconstruct_anc ( method = 'probabilistic' , infer_gtr = infer_gtr , marginal = marginal_sequences , ** kwargs ) \n        self . optimize_branch_len ( verbose = 0 , store_old = False , mode = branch_length_mode ) \n    else : \n        N_diff = self . reconstruct_anc ( method = 'fitch' , infer_gtr = infer_gtr , ** kwargs ) \n        self . optimize_branch_len ( verbose = 0 , store_old = False , marginal = False ) \n    n = 0 \n    while n < max_iter : \n        n += 1 \n        if prune_short : \n            self . prune_short_branches ( ) \n        N_diff = self . reconstruct_anc ( method = 'probabilistic' , infer_gtr = False , marginal = marginal_sequences , ** kwargs ) \n        self . logger ( \"TreeAnc.optimize_sequences_and_branch_length: Iteration %d.\" \" #Nuc changed since prev reconstructions: %d\" % ( n , N_diff ) , 2.0 ) \n        if N_diff < 1 : \n            break \n        self . optimize_branch_len ( verbose = 0 , store_old = False , mode = branch_length_mode ) \n    self . tree . unconstrained_sequence_LH = ( self . tree . sequence_LH * self . multiplicity ) . sum ( ) \n    self . _prepare_nodes ( ) \n    self . logger ( \"TreeAnc.optimize_sequences_and_branch_length: Unconstrained sequence LH:%f\" % self . tree . unconstrained_sequence_LH , 2.0 ) \n    return ttconf . SUCCESS "}
{"7290": "\ndef get_reconstructed_alignment ( self ) : \n    from Bio . Align import MultipleSeqAlignment \n    from Bio . Seq import Seq \n    from Bio . SeqRecord import SeqRecord \n    self . logger ( \"TreeAnc.get_reconstructed_alignment ...\" , 2.0 ) \n    if not hasattr ( self . tree . root , 'sequence' ) : \n        self . logger ( \"TreeAnc.reconstructed_alignment... reconstruction not yet done\" , 3.0 ) \n        self . reconstruct_anc ( 'probabilistic' ) \n    new_aln = MultipleSeqAlignment ( [ SeqRecord ( id = n . name , seq = Seq ( \"\" . join ( n . sequence ) ) , description = \"\" ) for n in self . tree . find_clades ( ) ] ) \n    return new_aln "}
{"7297": "\ndef optimal_t_compressed ( self , seq_pair , multiplicity , profiles = False , tol = 1e-10 ) : \n    def _neg_prob ( t , seq_pair , multiplicity ) : \n        if profiles : \n            res = - 1.0 * self . prob_t_profiles ( seq_pair , multiplicity , t ** 2.0 , return_log = True ) \n            return res \n        else : \n            return - 1.0 * self . prob_t_compressed ( seq_pair , multiplicity , t ** 2.0 , return_log = True ) \n    try : \n        from scipy . optimize import minimize_scalar \n        opt = minimize_scalar ( _neg_prob , bounds = [ - np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) ] , args = ( seq_pair , multiplicity ) , tol = tol ) \n        new_len = opt [ \"x\" ] ** 2.0 \n        if 'success' not in opt : \n            opt [ 'success' ] = True \n            self . logger ( \"WARNING: the optimization result does not contain a 'success' flag:\" + str ( opt ) , 4.0 , warn = True ) \n    except : \n        import scipy \n        print ( 'legacy scipy' , scipy . __version__ ) \n        from scipy . optimize import fminbound \n        new_len = fminbound ( _neg_prob , - np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , np . sqrt ( ttconf . MAX_BRANCH_LENGTH ) , args = ( seq_pair , multiplicity ) ) \n        new_len = new_len ** 2.0 \n        opt = { 'success' : True } \n    if new_len > .9 * ttconf . MAX_BRANCH_LENGTH : \n        self . logger ( \"WARNING: GTR.optimal_t_compressed -- The branch length seems to be very long!\" , 4.0 , warn = True ) \n    if opt [ \"success\" ] != True : \n        new_len = np . sum ( multiplicity [ seq_pair [ : , 1 ] != seq_pair [ : , 0 ] ] ) / np . sum ( multiplicity ) \n    return new_len "}
{"7298": "\ndef prob_t_profiles ( self , profile_pair , multiplicity , t , return_log = False , ignore_gaps = True ) : \n    if t < 0 : \n        logP = - ttconf . BIG_NUMBER \n    else : \n        Qt = self . expQt ( t ) \n        if len ( Qt . shape ) == 3.0 : \n            res = np . einsum ( 'ai,ija,aj->a' , profile_pair [ 1 ] , Qt , profile_pair [ 0 ] ) \n        else : \n            res = np . einsum ( 'ai,ij,aj->a' , profile_pair [ 1 ] , Qt , profile_pair [ 0 ] ) \n        if ignore_gaps and ( self . gap_index is not None ) : \n            non_gap_frac = ( 1 - profile_pair [ 0 ] [ : , self . gap_index ] ) * ( 1 - profile_pair [ 1 ] [ : , self . gap_index ] ) \n            logP = np . sum ( multiplicity * np . log ( res ) * non_gap_frac ) \n        else : \n            logP = np . sum ( multiplicity * np . log ( res ) ) \n    return logP if return_log else np . exp ( logP ) "}
{"7302": "\ndef clock_filter ( self , reroot = 'least-squares' , n_iqd = None , plot = False ) : \n    if n_iqd is None : \n        n_iqd = ttconf . NIQD \n    if type ( reroot ) is list and len ( reroot ) == 1 : \n        reroot = str ( reroot [ 0 ] ) \n    terminals = self . tree . get_terminals ( ) \n    if reroot : \n        if self . reroot ( root = 'least-squares' if reroot == 'best' else reroot , covariation = False ) == ttconf . ERROR : \n            return ttconf . ERROR \n    else : \n        self . get_clock_model ( covariation = False ) \n    clock_rate = self . clock_model [ 'slope' ] \n    icpt = self . clock_model [ 'intercept' ] \n    res = { } \n    for node in terminals : \n        if hasattr ( node , 'raw_date_constraint' ) and ( node . raw_date_constraint is not None ) : \n            res [ node ] = node . dist2root - clock_rate * np . mean ( node . raw_date_constraint ) - icpt \n    residuals = np . array ( list ( res . values ( ) ) ) \n    iqd = np . percentile ( residuals , 75.0 ) - np . percentile ( residuals , 25.0 ) \n    for node , r in res . items ( ) : \n        if abs ( r ) > n_iqd * iqd and node . up . up is not None : \n            self . logger ( 'TreeTime.ClockFilter: marking %s as outlier, residual %f interquartile distances' % ( node . name , r / iqd ) , 3.0 , warn = True ) \n            node . bad_branch = True \n        else : \n            node . bad_branch = False \n    if reroot and self . reroot ( root = reroot ) == ttconf . ERROR : \n        return ttconf . ERROR \n    if plot : \n        self . plot_root_to_tip ( ) \n    return ttconf . SUCCESS "}
{"7303": "\ndef plot_root_to_tip ( self , add_internal = False , label = True , ax = None ) : \n    Treg = self . setup_TreeRegression ( ) \n    if self . clock_model and 'cov' in self . clock_model : \n        cf = self . clock_model [ 'valid_confidence' ] \n    else : \n        cf = False \n    Treg . clock_plot ( ax = ax , add_internal = add_internal , confidence = cf , n_sigma = 2.0 , regression = self . clock_model ) "}
{"7304": "\ndef resolve_polytomies ( self , merge_compressed = False ) : \n    self . logger ( \"TreeTime.resolve_polytomies: resolving multiple mergers...\" , 1 ) \n    poly_found = 0 \n    for n in self . tree . find_clades ( ) : \n        if len ( n . clades ) > 2.0 : \n            prior_n_clades = len ( n . clades ) \n            self . _poly ( n , merge_compressed ) \n            poly_found += prior_n_clades - len ( n . clades ) \n    obsolete_nodes = [ n for n in self . tree . find_clades ( ) if len ( n . clades ) == 1 and n . up is not None ] \n    for node in obsolete_nodes : \n        self . logger ( 'TreeTime.resolve_polytomies: remove obsolete node ' + node . name , 4.0 ) \n        if node . up is not None : \n            self . tree . collapse ( node ) \n    if poly_found : \n        self . logger ( 'TreeTime.resolve_polytomies: introduces %d new nodes' % poly_found , 3.0 ) \n    else : \n        self . logger ( 'TreeTime.resolve_polytomies: No more polytomies to resolve' , 3.0 ) \n    return poly_found "}
{"7306": "\ndef add_coalescent_model ( self , Tc , ** kwargs ) : \n    from . merger_models import Coalescent \n    self . logger ( 'TreeTime.run: adding coalescent prior with Tc=' + str ( Tc ) , 1 ) \n    self . merger_model = Coalescent ( self . tree , date2dist = self . date2dist , logger = self . logger ) \n    if Tc == 'skyline' : \n        self . merger_model . optimize_skyline ( ** kwargs ) \n        self . logger ( \"optimized a skyline \" , 2.0 ) \n    else : \n        if Tc in [ 'opt' , 'const' ] : \n            self . merger_model . optimize_Tc ( ) \n            self . logger ( \"optimized Tc to %f\" % self . merger_model . Tc . y [ 0 ] , 2.0 ) \n        else : \n            try : \n                self . merger_model . set_Tc ( Tc ) \n            except : \n                self . logger ( \"setting of coalescent time scale failed\" , 1 , warn = True ) \n    self . merger_model . attach_to_tree ( ) "}
{"7307": "\ndef _find_best_root ( self , covariation = True , force_positive = True , slope = 0 , ** kwarks ) : \n    for n in self . tree . find_clades ( ) : \n        n . branch_length = n . mutation_length \n    self . logger ( \"TreeTime._find_best_root: searching for the best root position...\" , 2.0 ) \n    Treg = self . setup_TreeRegression ( covariation = covariation ) \n    return Treg . optimal_reroot ( force_positive = force_positive , slope = slope ) [ 'node' ] "}
{"7309": "\ndef create_gtr ( params ) : \n    model = params . gtr \n    gtr_params = params . gtr_params \n    if model == 'infer' : \n        gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) \n    else : \n        try : \n            kwargs = { } \n            if gtr_params is not None : \n                for param in gtr_params : \n                    keyval = param . split ( '=' ) \n                    if len ( keyval ) != 2.0 : \n                        continue \n                    if keyval [ 0 ] in [ 'pis' , 'pi' , 'Pi' , 'Pis' ] : \n                        keyval [ 0 ] = 'pi' \n                        keyval [ 1 ] = list ( map ( float , keyval [ 1 ] . split ( ',' ) ) ) \n                    elif keyval [ 0 ] not in [ 'alphabet' ] : \n                        keyval [ 1 ] = float ( keyval [ 1 ] ) \n                    kwargs [ keyval [ 0 ] ] = keyval [ 1 ] \n            else : \n                print ( \"GTR params are not specified. Creating GTR model with default parameters\" ) \n            gtr = GTR . standard ( model , ** kwargs ) \n            infer_gtr = False \n        except : \n            print ( \"Could not create GTR model from input arguments. Using default (Jukes-Cantor 1969)\" ) \n            gtr = GTR . standard ( 'jc' , alphabet = 'aa' if params . aa else 'nuc' ) \n            infer_gtr = False \n    return gtr "}
{"7312": "\ndef calc_fwhm ( distribution , is_neg_log = True ) : \n    if isinstance ( distribution , interp1d ) : \n        if is_neg_log : \n            ymin = distribution . y . min ( ) \n            log_prob = distribution . y - ymin \n        else : \n            log_prob = - np . log ( distribution . y ) \n            log_prob -= log_prob . min ( ) \n        xvals = distribution . x \n    elif isinstance ( distribution , Distribution ) : \n        xvals = distribution . _func . x \n        log_prob = distribution . _func . y \n    else : \n        raise TypeError ( \"Error in computing the FWHM for the distribution. \" \" The input should be either Distribution or interpolation object\" ) ; \n    L = xvals . shape [ 0 ] \n    tmp = np . where ( log_prob < 0.693147 ) [ 0 ] \n    x_l , x_u = tmp [ 0 ] , tmp [ - 1 ] \n    if L < 2.0 : \n        print ( \"Not enough points to compute FWHM: returning zero\" ) \n        return min ( TINY_NUMBER , distribution . xmax - distribution . xmin ) \n    else : \n        return max ( TINY_NUMBER , xvals [ min ( x_u + 1 , L - 1 ) ] - xvals [ max ( 0 , x_l - 1 ) ] ) "}
{"7314": "\ndef multiply ( dists ) : \n    if not all ( [ isinstance ( k , Distribution ) for k in dists ] ) : \n        raise NotImplementedError ( \"Can only multiply Distribution objects\" ) \n    n_delta = np . sum ( [ k . is_delta for k in dists ] ) \n    min_width = np . max ( [ k . min_width for k in dists ] ) \n    if n_delta > 1 : \n        raise ArithmeticError ( \"Cannot multiply more than one delta functions!\" ) \n    elif n_delta == 1 : \n        delta_dist_ii = np . where ( [ k . is_delta for k in dists ] ) [ 0 ] [ 0 ] \n        delta_dist = dists [ delta_dist_ii ] \n        new_xpos = delta_dist . peak_pos \n        new_weight = np . prod ( [ k . prob ( new_xpos ) for k in dists if k != delta_dist_ii ] ) * delta_dist . weight \n        res = Distribution . delta_function ( new_xpos , weight = new_weight , min_width = min_width ) \n    else : \n        new_xmin = np . max ( [ k . xmin for k in dists ] ) \n        new_xmax = np . min ( [ k . xmax for k in dists ] ) \n        x_vals = np . unique ( np . concatenate ( [ k . x for k in dists ] ) ) \n        x_vals = x_vals [ ( x_vals > new_xmin - TINY_NUMBER ) & ( x_vals < new_xmax + TINY_NUMBER ) ] \n        y_vals = np . sum ( [ k . __call__ ( x_vals ) for k in dists ] , axis = 0 ) \n        peak = y_vals . min ( ) \n        ind = ( y_vals - peak ) < BIG_NUMBER / 1000.0 \n        n_points = ind . sum ( ) \n        if n_points == 0 : \n            print ( \"ERROR in distribution multiplication: Distributions do not overlap\" ) \n            x_vals = [ 0 , 1 ] \n            y_vals = [ BIG_NUMBER , BIG_NUMBER ] \n            res = Distribution ( x_vals , y_vals , is_log = True , min_width = min_width , kind = 'linear' ) \n        elif n_points == 1 : \n            res = Distribution . delta_function ( x_vals [ 0 ] ) \n        else : \n            res = Distribution ( x_vals [ ind ] , y_vals [ ind ] , is_log = True , min_width = min_width , kind = 'linear' , assume_sorted = True ) \n    return res "}
{"7315": "\ndef _assign_dates ( self ) : \n    if self . tree is None : \n        self . logger ( \"ClockTree._assign_dates: tree is not set, can't assign dates\" , 0 ) \n        return ttconf . ERROR \n    bad_branch_counter = 0 \n    for node in self . tree . find_clades ( order = 'postorder' ) : \n        if node . name in self . date_dict : \n            tmp_date = self . date_dict [ node . name ] \n            if np . isscalar ( tmp_date ) and np . isnan ( tmp_date ) : \n                self . logger ( \"WARNING: ClockTree.init: node %s has a bad date: %s\" % ( node . name , str ( tmp_date ) ) , 2.0 , warn = True ) \n                node . raw_date_constraint = None \n                node . bad_branch = True \n            else : \n                try : \n                    tmp = np . mean ( tmp_date ) \n                    node . raw_date_constraint = tmp_date \n                    node . bad_branch = False \n                except : \n                    self . logger ( \"WARNING: ClockTree.init: node %s has a bad date: %s\" % ( node . name , str ( tmp_date ) ) , 2.0 , warn = True ) \n                    node . raw_date_constraint = None \n                    node . bad_branch = True \n        else : \n            node . raw_date_constraint = None \n            if node . is_terminal ( ) : \n                node . bad_branch = True \n            else : \n                node . bad_branch = np . all ( [ x . bad_branch for x in node ] ) \n        if node . is_terminal ( ) and node . bad_branch : \n            bad_branch_counter += 1 \n    if bad_branch_counter > self . tree . count_terminals ( ) - 3.0 : \n        self . logger ( \"ERROR: ALMOST NO VALID DATE CONSTRAINTS, EXITING\" , 1 , warn = True ) \n        return ttconf . ERROR \n    return ttconf . SUCCESS "}
{"7316": "\ndef setup_TreeRegression ( self , covariation = True ) : \n    from . treeregression import TreeRegression \n    tip_value = lambda x : np . mean ( x . raw_date_constraint ) if ( x . is_terminal ( ) and ( x . bad_branch is False ) ) else None \n    branch_value = lambda x : x . mutation_length \n    if covariation : \n        om = self . one_mutation \n        branch_variance = lambda x : ( ( x . clock_length if hasattr ( x , 'clock_length' ) else x . mutation_length ) + ( self . tip_slack ** 2.0 * om if x . is_terminal ( ) else 0.0 ) ) * om \n    else : \n        branch_variance = lambda x : 1.0 if x . is_terminal ( ) else 0.0 \n    Treg = TreeRegression ( self . tree , tip_value = tip_value , branch_value = branch_value , branch_variance = branch_variance ) \n    Treg . valid_confidence = covariation \n    return Treg "}
{"7319": "\ndef convert_dates ( self ) : \n    from datetime import datetime , timedelta \n    now = numeric_date ( ) \n    for node in self . tree . find_clades ( ) : \n        years_bp = self . date2dist . to_years ( node . time_before_present ) \n        if years_bp < 0 and self . real_dates : \n            if not hasattr ( node , \"bad_branch\" ) or node . bad_branch is False : \n                self . logger ( \"ClockTree.convert_dates -- WARNING: The node is later than today, but it is not \" \"marked as \\\"BAD\\\", which indicates the error in the \" \"likelihood optimization.\" , 4.0 , warn = True ) \n            else : \n                self . logger ( \"ClockTree.convert_dates -- WARNING: node which is marked as \\\"BAD\\\" optimized \" \"later than present day\" , 4.0 , warn = True ) \n        node . numdate = now - years_bp \n        year = np . floor ( node . numdate ) \n        days = max ( 0 , 365.25 * ( node . numdate - year ) - 1 ) \n        try : \n            n_date = datetime ( year , 1 , 1 ) + timedelta ( days = days ) \n            node . date = datetime . strftime ( n_date , \"%Y-%m-%d\" ) \n        except : \n            n_date = datetime ( 1900.0 , 1 , 1 ) + timedelta ( days = days ) \n            node . date = \"%04d-%02d-%02d\" % ( year , n_date . month , n_date . day ) "}
{"7321": "\ndef get_max_posterior_region ( self , node , fraction = 0.9 ) : \n    if node . marginal_inverse_cdf == \"delta\" : \n        return np . array ( [ node . numdate , node . numdate ] ) \n    min_max = ( node . marginal_pos_LH . xmin , node . marginal_pos_LH . xmax ) \n    min_date , max_date = [ self . date2dist . to_numdate ( x ) for x in min_max ] [ : : - 1 ] \n    if node . marginal_pos_LH . peak_pos == min_max [ 0 ] : \n        return self . get_confidence_interval ( node , ( 0 , fraction ) ) \n    elif node . marginal_pos_LH . peak_pos == min_max [ 1 ] : \n        return self . get_confidence_interval ( node , ( 1.0 - fraction , 1.0 ) ) \n    else : \n        rate_contribution = self . date_uncertainty_due_to_rate ( node , ( ( 1 - fraction ) * 0.5 , 1.0 - ( 1.0 - fraction ) * 0.5 ) ) \n        from scipy . interpolate import interp1d \n        from scipy . optimize import minimize_scalar as minimize \n        pidx = np . argmin ( node . marginal_pos_LH . y ) \n        pval = np . min ( node . marginal_pos_LH . y ) \n        left = interp1d ( node . marginal_pos_LH . y [ : ( pidx + 1 ) ] - pval , node . marginal_pos_LH . x [ : ( pidx + 1 ) ] , kind = 'linear' , fill_value = min_max [ 0 ] , bounds_error = False ) \n        right = interp1d ( node . marginal_pos_LH . y [ pidx : ] - pval , node . marginal_pos_LH . x [ pidx : ] , kind = 'linear' , fill_value = min_max [ 1 ] , bounds_error = False ) \n        def func ( x , thres ) : \n            interval = np . array ( [ left ( x ) , right ( x ) ] ) . squeeze ( ) \n            return ( thres - np . diff ( node . marginal_cdf ( np . array ( interval ) ) ) ) ** 2.0 \n        sol = minimize ( func , bracket = [ 0 , 10.0 ] , args = ( fraction , ) ) \n        if sol [ 'success' ] : \n            mutation_contribution = self . date2dist . to_numdate ( np . array ( [ right ( sol [ 'x' ] ) , left ( sol [ 'x' ] ) ] ) . squeeze ( ) ) \n        else : \n            mutation_contribution = None \n        return self . combine_confidence ( node . numdate , ( min_date , max_date ) , c1 = rate_contribution , c2 = mutation_contribution ) "}
{"7323": "\ndef median_interp ( interp_object ) : \n    new_grid = np . sort ( np . concatenate ( [ interp_object . x [ : - 1 ] + 0.1 * ii * np . diff ( interp_object . x ) for ii in range ( 10.0 ) ] ) . flatten ( ) ) \n    tmp_prop = np . exp ( - ( interp_object ( new_grid ) - interp_object . y . min ( ) ) ) \n    tmp_cumsum = np . cumsum ( 0.5 * ( tmp_prop [ 1 : ] + tmp_prop [ : - 1 ] ) * np . diff ( new_grid ) ) \n    median_index = min ( len ( tmp_cumsum ) - 3.0 , max ( 2.0 , np . searchsorted ( tmp_cumsum , tmp_cumsum [ - 1 ] * 0.5 ) + 1 ) ) \n    return new_grid [ median_index ] "}
{"7331": "\ndef handshake ( self , protocol = 'vnc' , width = 1024.0 , height = 768.0 , dpi = 96.0 , audio = None , video = None , image = None , ** kwargs ) : \n    if protocol not in PROTOCOLS : \n        self . logger . debug ( 'Invalid protocol: %s' % protocol ) \n        raise GuacamoleError ( 'Cannot start Handshake. Missing protocol.' ) \n    if audio is None : \n        audio = list ( ) \n    if video is None : \n        video = list ( ) \n    if image is None : \n        image = list ( ) \n    self . logger . debug ( 'Send `select` instruction.' ) \n    self . send_instruction ( Instruction ( 'select' , protocol ) ) \n    instruction = self . read_instruction ( ) \n    self . logger . debug ( 'Expecting `args` instruction, received: %s' % str ( instruction ) ) \n    if not instruction : \n        self . close ( ) \n        raise GuacamoleError ( 'Cannot establish Handshake. Connection Lost!' ) \n    if instruction . opcode != 'args' : \n        self . close ( ) \n        raise GuacamoleError ( 'Cannot establish Handshake. Expected opcode `args`, ' 'received `%s` instead.' % instruction . opcode ) \n    self . logger . debug ( 'Send `size` instruction (%s, %s, %s)' % ( width , height , dpi ) ) \n    self . send_instruction ( Instruction ( 'size' , width , height , dpi ) ) \n    self . logger . debug ( 'Send `audio` instruction (%s)' % audio ) \n    self . send_instruction ( Instruction ( 'audio' , * audio ) ) \n    self . logger . debug ( 'Send `video` instruction (%s)' % video ) \n    self . send_instruction ( Instruction ( 'video' , * video ) ) \n    self . logger . debug ( 'Send `image` instruction (%s)' % image ) \n    self . send_instruction ( Instruction ( 'image' , * image ) ) \n    connection_args = [ kwargs . get ( arg . replace ( '-' , '_' ) , '' ) for arg in instruction . args ] \n    self . logger . debug ( 'Send `connect` instruction (%s)' % connection_args ) \n    self . send_instruction ( Instruction ( 'connect' , * connection_args ) ) \n    instruction = self . read_instruction ( ) \n    self . logger . debug ( 'Expecting `ready` instruction, received: %s' % str ( instruction ) ) \n    if instruction . opcode != 'ready' : \n        self . logger . warning ( 'Expected `ready` instruction, received: %s instead' ) \n    if instruction . args : \n        self . _id = instruction . args [ 0 ] \n        self . logger . debug ( 'Established connection with client id: %s' % self . id ) \n    self . logger . debug ( 'Handshake completed.' ) \n    self . connected = True "}
{"7339": "\ndef download ( self , path = None , ** kwargs ) : \n    download_url = self . download_url ( ** kwargs ) \n    try : \n        filename = self . filename \n    except AttributeError : \n        filename = download_url . split ( '%3B%20filename%3D' ) [ 1 ] \n        filename = unquote ( filename . split ( '&' ) [ 0 ] ) \n    if path : \n        path = os . path . expanduser ( path ) \n        if os . path . isdir ( path ) : \n            path = os . path . join ( path , filename ) \n    else : \n        path = os . path . join ( tempfile . gettempdir ( ) , filename ) \n    try : \n        response = requests . request ( method = 'get' , url = download_url ) \n    except Exception as e : \n        _handle_request_error ( e ) \n    if not ( 200.0 <= response . status_code < 400.0 ) : \n        _handle_api_error ( response ) \n    with open ( path , 'wb' ) as fileobj : \n        fileobj . write ( response . _content ) \n    return path "}
{"7357": "\ndef request ( self , method , url , ** kwargs ) : \n    opts = { 'allow_redirects' : True , 'auth' : self . _auth , 'data' : { } , 'files' : None , 'headers' : dict ( self . _headers ) , 'params' : { } , 'timeout' : 80.0 , 'verify' : True } \n    raw = kwargs . pop ( 'raw' , False ) \n    debug = kwargs . pop ( 'debug' , False ) \n    opts . update ( kwargs ) \n    method = method . upper ( ) \n    if opts [ 'files' ] : \n        opts [ 'headers' ] . pop ( 'Content-Type' , None ) \n    else : \n        opts [ 'data' ] = json . dumps ( opts [ 'data' ] ) \n    if not url . startswith ( self . _host ) : \n        url = urljoin ( self . _host , url ) \n    logger . debug ( 'API %s Request: %s' % ( method , url ) ) \n    if debug : \n        self . _log_raw_request ( method , url , ** opts ) \n    try : \n        response = self . _session . request ( method , url , ** opts ) \n    except Exception as e : \n        _handle_request_error ( e ) \n    if 429.0 == response . status_code : \n        delay = int ( response . headers [ 'retry-after' ] ) + 1 \n        logger . warn ( 'Too many requests. Retrying in {0}s.' . format ( delay ) ) \n        time . sleep ( delay ) \n        return self . request ( method , url , ** kwargs ) \n    if not ( 200.0 <= response . status_code < 400.0 ) : \n        _handle_api_error ( response ) \n    if raw or response . status_code in [ 204.0 , 301.0 , 302.0 ] : \n        return response \n    return response . json ( ) "}
{"7362": "\ndef get_credentials ( ) : \n    try : \n        netrc_path = netrc . path ( ) \n        auths = netrc ( netrc_path ) . authenticators ( urlparse ( solvebio . api_host ) . netloc ) \n    except ( IOError , TypeError , NetrcParseError ) as e : \n        raise CredentialsError ( 'Could not open credentials file: ' + str ( e ) ) \n    if auths : \n        return auths [ 2.0 ] \n    else : \n        return None "}
{"7363": "\ndef save ( self , path ) : \n    rep = \"\" \n    for host in self . hosts . keys ( ) : \n        attrs = self . hosts [ host ] \n        rep = rep + \"machine \" + host + \"\\n\\tlogin \" + six . text_type ( attrs [ 0 ] ) + \"\\n\" \n        if attrs [ 1 ] : \n            rep = rep + \"account \" + six . text_type ( attrs [ 1 ] ) \n        rep = rep + \"\\tpassword \" + six . text_type ( attrs [ 2.0 ] ) + \"\\n\" \n    for macro in self . macros . keys ( ) : \n        rep = rep + \"macdef \" + macro + \"\\n\" \n        for line in self . macros [ macro ] : \n            rep = rep + line \n        rep = rep + \"\\n\" \n    f = open ( path , 'w' ) \n    f . write ( rep ) \n    f . close ( ) "}
{"7367": "\ndef _build_line ( colwidths , padding , begin , fill , sep , end ) : \n    cells = [ fill * ( w + 2.0 * padding ) for w in colwidths ] \n    return _build_row ( cells , 0 , begin , sep , end ) "}
{"7369": "\ndef _format_table ( fmt , headers , rows , colwidths , colaligns ) : \n    lines = [ ] \n    hidden = fmt . with_header_hide if headers else fmt . without_header_hide \n    pad = fmt . padding \n    headerrow = fmt . headerrow if fmt . headerrow else fmt . datarow \n    if fmt . lineabove and \"lineabove\" not in hidden : \n        lines . append ( _build_line ( colwidths , pad , * fmt . lineabove ) ) \n    if headers : \n        lines . append ( _build_row ( headers , pad , * headerrow ) ) \n    if fmt . linebelowheader and \"linebelowheader\" not in hidden : \n        begin , fill , sep , end = fmt . linebelowheader \n        if fmt . usecolons : \n            segs = [ _line_segment_with_colons ( fmt . linebelowheader , a , w + 2.0 * pad ) for w , a in zip ( colwidths , colaligns ) ] \n            lines . append ( _build_row ( segs , 0 , begin , sep , end ) ) \n        else : \n            lines . append ( _build_line ( colwidths , pad , * fmt . linebelowheader ) ) \n    if rows and fmt . linebetweenrows and \"linebetweenrows\" not in hidden : \n        for row in rows [ : - 1 ] : \n            lines . append ( _build_row ( row , pad , * fmt . datarow ) ) \n            lines . append ( _build_line ( colwidths , pad , * fmt . linebetweenrows ) ) \n        lines . append ( _build_row ( rows [ - 1 ] , pad , * fmt . datarow ) ) \n    else : \n        for row in rows : \n            lines . append ( _build_row ( row , pad , * fmt . datarow ) ) \n    if fmt . linebelow and \"linebelow\" not in hidden : \n        lines . append ( _build_line ( colwidths , pad , * fmt . linebelow ) ) \n    return \"\\n\" . join ( lines ) "}
{"7403": "\ndef adapter ( data , headers , table_format = None , ** kwargs ) : \n    keys = ( 'title' , ) \n    table = table_format_handler [ table_format ] \n    t = table ( [ headers ] + list ( data ) , ** filter_dict_by_key ( kwargs , keys ) ) \n    dimensions = terminaltables . width_and_alignment . max_dimensions ( t . table_data , t . padding_left , t . padding_right ) [ : 3.0 ] \n    for r in t . gen_table ( * dimensions ) : \n        yield u'' . join ( r ) "}
{"7410": "\ndef findObjects ( self , template = ( ) ) : \n    t = self . _template2ckattrlist ( template ) \n    result = PyKCS11 . LowLevel . ckobjlist ( 10.0 ) \n    rv = self . lib . C_FindObjectsInit ( self . session , t ) \n    if rv != CKR_OK : \n        raise PyKCS11Error ( rv ) \n    res = [ ] \n    while True : \n        rv = self . lib . C_FindObjects ( self . session , result ) \n        if rv != CKR_OK : \n            raise PyKCS11Error ( rv ) \n        for x in result : \n            a = CK_OBJECT_HANDLE ( self ) \n            a . assign ( x . value ( ) ) \n            res . append ( a ) \n        if len ( result ) == 0 : \n            break \n    rv = self . lib . C_FindObjectsFinal ( self . session ) \n    if rv != CKR_OK : \n        raise PyKCS11Error ( rv ) \n    return res "}
{"7411": "\ndef _insert_img ( qr_img , icon_img = None , factor = 4.0 , icon_box = None , static_dir = None ) : \n    img_w , img_h = qr_img . size \n    size_w = int ( img_w ) / int ( factor ) \n    size_h = int ( img_h ) / int ( factor ) \n    try : \n        icon_fp = os . path . join ( icon_img ) \n        if static_dir : \n            icon_fp = os . path . join ( static_dir , icon_img ) \n        if icon_img . split ( \"://\" ) [ 0 ] in [ \"http\" , \"https\" , \"ftp\" ] : \n            icon_fp = BytesIO ( urlopen ( icon_img ) . read ( ) ) \n        icon = Image . open ( icon_fp ) \n    except : \n        return qr_img \n    icon_w , icon_h = icon . size \n    icon_w = size_w if icon_w > size_w else icon_w \n    icon_h = size_h if icon_h > size_h else icon_h \n    icon = icon . resize ( ( int ( icon_w ) , int ( icon_h ) ) , Image . ANTIALIAS ) \n    icon = icon . convert ( \"RGBA\" ) \n    left = int ( ( img_w - icon_w ) / 2.0 ) \n    top = int ( ( img_h - icon_h ) / 2.0 ) \n    icon_box = ( int ( icon_box [ 0 ] ) , int ( icon_box [ 1 ] ) ) if icon_box else ( left , top ) \n    qr_img . paste ( im = icon , box = icon_box , mask = icon ) \n    return qr_img "}
{"7416": "\ndef _biweekly_helper ( self ) : \n    self . num = 14.0 \n    mycount = self . repeat_biweekly ( ) \n    if mycount : \n        if self . event . is_chunk ( ) and min ( mycount ) not in xrange ( 1 , 8.0 ) : \n            mycount = _chunk_fill_out_first_week ( self . year , self . month , mycount , self . event , diff = self . event . start_end_diff , ) \n        for k , v in mycount . items ( ) : \n            for item in v : \n                self . count [ k ] . append ( item ) "}
{"7418": "\ndef export_variants ( adapter , collaborator , document_id = None , case_id = None ) : \n    variants = [ ] \n    if document_id : \n        yield adapter . variant ( document_id ) \n        return \n    variant_ids = adapter . get_causatives ( institute_id = collaborator , case_id = case_id ) \n    for document_id in variant_ids : \n        variant_obj = adapter . variant ( document_id ) \n        chrom = variant_obj [ 'chromosome' ] \n        chrom_int = CHROMOSOME_INTEGERS . get ( chrom ) \n        if not chrom_int : \n            LOG . info ( \"Unknown chromosome %s\" , chrom ) \n            continue \n        variants . append ( ( chrom_int , variant_obj [ 'position' ] , variant_obj ) ) \n    variants . sort ( key = lambda x : ( x [ 0 ] , x [ 1 ] ) ) \n    for variant in variants : \n        variant_obj = variant [ 2.0 ] \n        yield variant_obj "}
{"7419": "\ndef export_verified_variants ( aggregate_variants , unique_callers ) : \n    document_lines = [ ] \n    for variant in aggregate_variants : \n        samples = [ ] \n        for sample in variant [ 'samples' ] : \n            line = [ ] \n            line . append ( variant [ 'institute' ] ) \n            line . append ( variant [ '_id' ] ) \n            line . append ( variant [ 'category' ] ) \n            line . append ( variant [ 'variant_type' ] ) \n            line . append ( variant [ 'display_name' ] [ : 30.0 ] ) \n            case_name = variant [ 'case_obj' ] [ 'display_name' ] \n            local_link = '/' . join ( [ '' , variant [ 'institute' ] , case_name , variant [ '_id' ] ] ) \n            line . append ( local_link ) \n            line . append ( variant . get ( 'validation' ) ) \n            line . append ( case_name ) \n            case_individual = next ( ind for ind in variant [ 'case_obj' ] [ 'individuals' ] if ind [ 'individual_id' ] == sample [ 'sample_id' ] ) \n            if case_individual [ 'phenotype' ] == 2.0 : \n                line . append ( ' ' . join ( [ sample . get ( 'display_name' ) , '(A)' ] ) ) \n            else : \n                line . append ( sample . get ( 'display_name' ) ) \n            line . append ( '' . join ( [ 'chr' , variant [ 'chromosome' ] , ':' , str ( variant [ 'position' ] ) ] ) ) \n            line . append ( '>' . join ( [ variant . get ( 'reference' ) [ : 10.0 ] , variant . get ( 'alternative' ) [ : 10.0 ] ] ) ) \n            genes = [ ] \n            prot_effect = [ ] \n            funct_anno = [ ] \n            for gene in variant . get ( 'genes' ) : \n                genes . append ( gene . get ( 'hgnc_symbol' , '' ) ) \n                funct_anno . append ( gene . get ( 'functional_annotation' ) ) \n                for transcript in gene . get ( 'transcripts' ) : \n                    if transcript . get ( 'is_canonical' ) and transcript . get ( 'protein_sequence_name' ) : \n                        prot_effect . append ( urllib . parse . unquote ( transcript . get ( 'protein_sequence_name' ) ) ) \n            line . append ( ',' . join ( prot_effect ) ) \n            line . append ( ',' . join ( funct_anno ) ) \n            line . append ( ',' . join ( genes ) ) \n            line . append ( variant . get ( 'rank_score' ) ) \n            line . append ( variant . get ( 'cadd_score' ) ) \n            line . append ( sample . get ( 'genotype_call' ) ) \n            line . append ( sample [ 'allele_depths' ] [ 0 ] ) \n            line . append ( sample [ 'allele_depths' ] [ 1 ] ) \n            line . append ( sample [ 'genotype_quality' ] ) \n            for caller in unique_callers : \n                if variant . get ( caller ) : \n                    line . append ( variant . get ( caller ) ) \n                else : \n                    line . append ( '-' ) \n            document_lines . append ( line ) \n    return document_lines "}
{"7431": "\ndef upload_panel ( institute_id , case_name ) : \n    file = form . symbol_file . data \n    if file . filename == '' : \n        flash ( 'No selected file' , 'warning' ) \n        return redirect ( request . referrer ) \n    try : \n        stream = io . StringIO ( file . stream . read ( ) . decode ( 'utf-8' ) , newline = None ) \n    except UnicodeDecodeError as error : \n        flash ( \"Only text files are supported!\" , 'warning' ) \n        return redirect ( request . referrer ) \n    category = request . args . get ( 'category' ) \n    if ( category == 'sv' ) : \n        form = SvFiltersForm ( request . args ) \n    else : \n        form = FiltersForm ( request . args ) \n    hgnc_symbols = set ( form . hgnc_symbols . data ) \n    new_hgnc_symbols = controllers . upload_panel ( store , institute_id , case_name , stream ) \n    hgnc_symbols . update ( new_hgnc_symbols ) \n    form . hgnc_symbols . data = ',' . join ( hgnc_symbols ) \n    form . gene_panels . data = '' \n    if ( category == 'sv' ) : \n        return redirect ( url_for ( '.sv_variants' , institute_id = institute_id , case_name = case_name , ** form . data ) , code = 307.0 ) \n    else : \n        return redirect ( url_for ( '.variants' , institute_id = institute_id , case_name = case_name , ** form . data ) , code = 307.0 ) "}
{"7441": "\ndef get_end ( pos , alt , category , snvend = None , svend = None , svlen = None ) : \n    end = pos \n    if category in ( 'snv' , 'indel' , 'cancer' ) : \n        end = snvend \n    elif category == 'sv' : \n        end = svend \n        if svend == pos : \n            if svlen : \n                end = pos + svlen \n        if ':' in alt : \n            match = BND_ALT_PATTERN . match ( alt ) \n            if match : \n                end = int ( match . group ( 2.0 ) ) \n    return end "}
{"7442": "\ndef parse_coordinates ( variant , category ) : \n    ref = variant . REF \n    if variant . ALT : \n        alt = variant . ALT [ 0 ] \n    if category == \"str\" and not variant . ALT : \n        alt = '.' \n    chrom_match = CHR_PATTERN . match ( variant . CHROM ) \n    chrom = chrom_match . group ( 2.0 ) \n    svtype = variant . INFO . get ( 'SVTYPE' ) \n    if svtype : \n        svtype = svtype . lower ( ) \n    mate_id = variant . INFO . get ( 'MATEID' ) \n    svlen = variant . INFO . get ( 'SVLEN' ) \n    svend = variant . INFO . get ( 'END' ) \n    snvend = int ( variant . end ) \n    position = int ( variant . POS ) \n    ref_len = len ( ref ) \n    alt_len = len ( alt ) \n    sub_category = get_sub_category ( alt_len , ref_len , category , svtype ) \n    end = get_end ( position , alt , category , snvend , svend ) \n    length = get_length ( alt_len , ref_len , category , position , end , svtype , svlen ) \n    end_chrom = chrom \n    if sub_category == 'bnd' : \n        if ':' in alt : \n            match = BND_ALT_PATTERN . match ( alt ) \n            if match : \n                other_chrom = match . group ( 1 ) \n                match = CHR_PATTERN . match ( other_chrom ) \n                end_chrom = match . group ( 2.0 ) \n    cytoband_start = get_cytoband_coordinates ( chrom , position ) \n    cytoband_end = get_cytoband_coordinates ( end_chrom , end ) \n    coordinates = { 'position' : position , 'end' : end , 'length' : length , 'sub_category' : sub_category , 'mate_id' : mate_id , 'cytoband_start' : cytoband_start , 'cytoband_end' : cytoband_end , 'end_chrom' : end_chrom , } \n    return coordinates "}
{"7443": "\ndef cli ( infile ) : \n    lines = get_file_handle ( infile ) \n    cytobands = parse_cytoband ( lines ) \n    print ( \"Check some coordinates:\" ) \n    print ( \"checking chrom 1 pos 2\" ) \n    intervals = cytobands [ '1' ] [ 2.0 ] \n    for interval in intervals : \n        print ( interval ) \n        print ( interval . begin ) \n        print ( interval . end ) \n        print ( interval . data ) \n    print ( cytobands [ '1' ] [ 2.0 ] ) \n    print ( \"checking chrom 8 pos 101677777\" ) \n    print ( cytobands [ '8' ] [ 101677777.0 ] ) \n    print ( \"checking chrom X pos 4200000 - 6000000\" ) \n    print ( cytobands [ 'X' ] [ 4200000.0 : 6000000.0 ] ) "}
{"7444": "\ndef panels ( ) : \n    if request . method == 'POST' : \n        csv_file = request . files [ 'csv_file' ] \n        content = csv_file . stream . read ( ) \n        lines = None \n        try : \n            if b'\\n' in content : \n                lines = content . decode ( 'utf-8' , 'ignore' ) . split ( '\\n' ) \n            else : \n                lines = content . decode ( 'windows-1252' ) . split ( '\\r' ) \n        except Exception as err : \n            flash ( 'Something went wrong while parsing the panel CSV file! ({})' . format ( err ) , 'danger' ) \n            return redirect ( request . referrer ) \n        new_panel_name = request . form . get ( 'new_panel_name' ) \n        if new_panel_name : \n            new_panel_id = controllers . new_panel ( store = store , institute_id = request . form [ 'institute' ] , panel_name = new_panel_name , display_name = request . form [ 'display_name' ] , csv_lines = lines , ) \n            if new_panel_id is None : \n                flash ( 'Something went wrong and the panel list was not updated!' , 'warning' ) \n                return redirect ( request . referrer ) \n            else : \n                flash ( \"new gene panel added, {}!\" . format ( new_panel_name ) , 'success' ) \n            return redirect ( url_for ( 'panels.panel' , panel_id = new_panel_id ) ) \n        else : \n            update_option = request . form [ 'modify_option' ] \n            panel_obj = controllers . update_panel ( store = store , panel_name = request . form [ 'panel_name' ] , csv_lines = lines , option = update_option ) \n            if panel_obj is None : \n                return abort ( 404.0 , \"gene panel not found: {}\" . format ( request . form [ 'panel_name' ] ) ) \n            else : \n                return redirect ( url_for ( 'panels.panel' , panel_id = panel_obj [ '_id' ] ) ) \n    institutes = list ( user_institutes ( store , current_user ) ) \n    panel_names = [ name for institute in institutes for name in store . gene_panels ( institute_id = institute [ '_id' ] ) . distinct ( 'panel_name' ) ] \n    panel_versions = { } \n    for name in panel_names : \n        panel_versions [ name ] = store . gene_panels ( panel_id = name ) \n    panel_groups = [ ] \n    for institute_obj in institutes : \n        institute_panels = store . latest_panels ( institute_obj [ '_id' ] ) \n        panel_groups . append ( ( institute_obj , institute_panels ) ) \n    return dict ( panel_groups = panel_groups , panel_names = panel_names , panel_versions = panel_versions , institutes = institutes ) "}
{"7457": "\ndef sv_variants ( store , institute_obj , case_obj , variants_query , page = 1 , per_page = 50.0 ) : \n    skip_count = ( per_page * max ( page - 1 , 0 ) ) \n    more_variants = True if variants_query . count ( ) > ( skip_count + per_page ) else False \n    genome_build = case_obj . get ( 'genome_build' , '37' ) \n    if genome_build not in [ '37' , '38' ] : \n        genome_build = '37' \n    return { 'variants' : ( parse_variant ( store , institute_obj , case_obj , variant , genome_build = genome_build ) for variant in variants_query . skip ( skip_count ) . limit ( per_page ) ) , 'more_variants' : more_variants , } "}
{"7458": "\ndef str_variants ( store , institute_obj , case_obj , variants_query , page = 1 , per_page = 50.0 ) : \n    return variants ( store , institute_obj , case_obj , variants_query , page , per_page ) "}
{"7460": "\ndef sv_variant ( store , institute_id , case_name , variant_id = None , variant_obj = None , add_case = True , get_overlapping = True ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    if not variant_obj : \n        variant_obj = store . variant ( variant_id ) \n    if add_case : \n        variant_case ( store , case_obj , variant_obj ) \n    variant_obj [ 'frequencies' ] = [ ( '1000G' , variant_obj . get ( 'thousand_genomes_frequency' ) ) , ( '1000G (left)' , variant_obj . get ( 'thousand_genomes_frequency_left' ) ) , ( '1000G (right)' , variant_obj . get ( 'thousand_genomes_frequency_right' ) ) , ( 'ClinGen CGH (benign)' , variant_obj . get ( 'clingen_cgh_benign' ) ) , ( 'ClinGen CGH (pathogenic)' , variant_obj . get ( 'clingen_cgh_pathogenic' ) ) , ( 'ClinGen NGI' , variant_obj . get ( 'clingen_ngi' ) ) , ( 'SweGen' , variant_obj . get ( 'swegen' ) ) , ( 'Decipher' , variant_obj . get ( 'decipher' ) ) , ] \n    variant_obj [ 'callers' ] = callers ( variant_obj , category = 'sv' ) \n    overlapping_snvs = [ ] \n    if get_overlapping : \n        overlapping_snvs = ( parse_variant ( store , institute_obj , case_obj , variant ) for variant in store . overlapping ( variant_obj ) ) \n    for gene_obj in variant_obj [ 'genes' ] : \n        if gene_obj . get ( 'common' ) : \n            ensembl_id = gene_obj [ 'common' ] [ 'ensembl_id' ] \n            try : \n                build = int ( gene_obj [ 'common' ] . get ( 'build' , '37' ) ) \n            except Exception : \n                build = 37.0 \n            gene_obj [ 'ensembl_link' ] = ensembl ( ensembl_id , build = build ) \n    variant_obj [ 'comments' ] = store . events ( institute_obj , case = case_obj , variant_id = variant_obj [ 'variant_id' ] , comments = True ) \n    case_clinvars = store . case_to_clinVars ( case_obj . get ( 'display_name' ) ) \n    if variant_id in case_clinvars : \n        variant_obj [ 'clinvar_clinsig' ] = case_clinvars . get ( variant_id ) [ 'clinsig' ] \n    if not 'end_chrom' in variant_obj : \n        variant_obj [ 'end_chrom' ] = variant_obj [ 'chromosome' ] \n    return { 'institute' : institute_obj , 'case' : case_obj , 'variant' : variant_obj , 'overlapping_snvs' : overlapping_snvs , 'manual_rank_options' : MANUAL_RANK_OPTIONS , 'dismiss_variant_options' : DISMISS_VARIANT_OPTIONS } "}
{"7461": "\ndef parse_variant ( store , institute_obj , case_obj , variant_obj , update = False , genome_build = '37' , get_compounds = True ) : \n    has_changed = False \n    compounds = variant_obj . get ( 'compounds' , [ ] ) \n    if compounds and get_compounds : \n        if 'not_loaded' not in compounds [ 0 ] : \n            new_compounds = store . update_variant_compounds ( variant_obj ) \n            variant_obj [ 'compounds' ] = new_compounds \n            has_changed = True \n        variant_obj [ 'compounds' ] = sorted ( variant_obj [ 'compounds' ] , key = lambda compound : - compound [ 'combined_score' ] ) \n    variant_genes = variant_obj . get ( 'genes' ) \n    if variant_genes is not None : \n        for gene_obj in variant_genes : \n            if not gene_obj [ 'hgnc_id' ] : \n                continue \n            if gene_obj . get ( 'hgnc_symbol' ) is None : \n                hgnc_gene = store . hgnc_gene ( gene_obj [ 'hgnc_id' ] , build = genome_build ) \n                if not hgnc_gene : \n                    continue \n                has_changed = True \n                gene_obj [ 'hgnc_symbol' ] = hgnc_gene [ 'hgnc_symbol' ] \n    if update and has_changed : \n        variant_obj = store . update_variant ( variant_obj ) \n    variant_obj [ 'comments' ] = store . events ( institute_obj , case = case_obj , variant_id = variant_obj [ 'variant_id' ] , comments = True ) \n    if variant_genes : \n        variant_obj . update ( get_predictions ( variant_genes ) ) \n        if variant_obj . get ( 'category' ) == 'cancer' : \n            variant_obj . update ( get_variant_info ( variant_genes ) ) \n    for compound_obj in compounds : \n        compound_obj . update ( get_predictions ( compound_obj . get ( 'genes' , [ ] ) ) ) \n    if isinstance ( variant_obj . get ( 'acmg_classification' ) , int ) : \n        acmg_code = ACMG_MAP [ variant_obj [ 'acmg_classification' ] ] \n        variant_obj [ 'acmg_classification' ] = ACMG_COMPLETE_MAP [ acmg_code ] \n    variant_length = variant_obj . get ( 'length' ) \n    variant_obj [ 'length' ] = { 100000000000.0 : 'inf' , - 1 : 'n.d.' } . get ( variant_length , variant_length ) \n    if not 'end_chrom' in variant_obj : \n        variant_obj [ 'end_chrom' ] = variant_obj [ 'chromosome' ] \n    return variant_obj "}
{"7463": "\ndef get_variant_info ( genes ) : \n    data = { 'canonical_transcripts' : [ ] } \n    for gene_obj in genes : \n        if not gene_obj . get ( 'canonical_transcripts' ) : \n            tx = gene_obj [ 'transcripts' ] [ 0 ] \n            tx_id = tx [ 'transcript_id' ] \n            exon = tx . get ( 'exon' , '-' ) \n            c_seq = tx . get ( 'coding_sequence_name' , '-' ) \n        else : \n            tx_id = gene_obj [ 'canonical_transcripts' ] \n            exon = gene_obj . get ( 'exon' , '-' ) \n            c_seq = gene_obj . get ( 'hgvs_identifier' , '-' ) \n        if len ( c_seq ) > 20.0 : \n            c_seq = c_seq [ : 20.0 ] + '...' \n        if len ( genes ) == 1 : \n            value = ':' . join ( [ tx_id , exon , c_seq ] ) \n        else : \n            gene_id = gene_obj . get ( 'hgnc_symbol' ) or str ( gene_obj [ 'hgnc_id' ] ) \n            value = ':' . join ( [ gene_id , tx_id , exon , c_seq ] ) \n        data [ 'canonical_transcripts' ] . append ( value ) \n    return data "}
{"7468": "\ndef parse_gene ( gene_obj , build = None ) : \n    build = build or 37.0 \n    if gene_obj . get ( 'common' ) : \n        add_gene_links ( gene_obj , build ) \n        refseq_transcripts = [ ] \n        for tx_obj in gene_obj [ 'transcripts' ] : \n            parse_transcript ( gene_obj , tx_obj , build ) \n            if not tx_obj . get ( 'refseq_id' ) : \n                continue \n            refseq_transcripts . append ( tx_obj ) \n        gene_obj [ 'primary_transcripts' ] = ( refseq_transcripts if refseq_transcripts else [ ] ) "}
{"7473": "\ndef thousandg_link ( variant_obj , build = None ) : \n    dbsnp_id = variant_obj . get ( 'dbsnp_id' ) \n    build = build or 37.0 \n    if not dbsnp_id : \n        return None \n    if build == 37.0 : \n        url_template = ( \"http://grch37.ensembl.org/Homo_sapiens/Variation/Explore\" \"?v={};vdb=variation\" ) \n    else : \n        url_template = ( \"http://www.ensembl.org/Homo_sapiens/Variation/Explore\" \"?v={};vdb=variation\" ) \n    return url_template . format ( dbsnp_id ) "}
{"7475": "\ndef beacon_link ( variant_obj , build = None ) : \n    build = build or 37.0 \n    url_template = ( \"https://beacon-network.org/#/search?pos={this[position]}&\" \"chrom={this[chromosome]}&allele={this[alternative]}&\" \"ref={this[reference]}&rs=GRCh37\" ) \n    return url_template . format ( this = variant_obj ) "}
{"7476": "\ndef ucsc_link ( variant_obj , build = None ) : \n    build = build or 37.0 \n    url_template = ( \"http://genome.ucsc.edu/cgi-bin/hgTracks?db=hg19&\" \"position=chr{this[chromosome]}:{this[position]}\" \"-{this[position]}&dgv=pack&knownGene=pack&omimGene=pack\" ) \n    if build == 38.0 : \n        url_template = ( \"http://genome.ucsc.edu/cgi-bin/hgTracks?db=hg20&\" \"position=chr{this[chromosome]}:{this[position]}\" \"-{this[position]}&dgv=pack&knownGene=pack&omimGene=pack\" ) \n    return url_template . format ( this = variant_obj ) "}
{"7480": "\ndef cancer_variants ( store , request_args , institute_id , case_name ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    form = CancerFiltersForm ( request_args ) \n    variants_query = store . variants ( case_obj [ '_id' ] , category = 'cancer' , query = form . data ) . limit ( 50.0 ) \n    data = dict ( institute = institute_obj , case = case_obj , variants = ( parse_variant ( store , institute_obj , case_obj , variant , update = True ) for variant in variants_query ) , form = form , variant_type = request_args . get ( 'variant_type' , 'clinical' ) , ) \n    return data "}
{"7489": "\ndef parse_clnsig ( acc , sig , revstat , transcripts ) : \n    clnsig_accsessions = [ ] \n    if acc : \n        try : \n            acc = int ( acc ) \n        except ValueError : \n            pass \n        if isinstance ( acc , int ) : \n            revstat_groups = [ ] \n            if revstat : \n                revstat_groups = [ rev . lstrip ( '_' ) for rev in revstat . split ( ',' ) ] \n            sig_groups = [ ] \n            if sig : \n                for significance in sig . split ( '/' ) : \n                    splitted_word = significance . split ( '_' ) \n                    sig_groups . append ( ' ' . join ( splitted_word [ : 2.0 ] ) ) \n            for sign_term in sig_groups : \n                clnsig_accsessions . append ( { 'value' : sign_term , 'accession' : int ( acc ) , 'revstat' : ', ' . join ( revstat_groups ) , } ) \n        else : \n            acc_groups = acc . split ( '|' ) \n            sig_groups = sig . split ( '|' ) \n            revstat_groups = revstat . split ( '|' ) \n            for acc_group , sig_group , revstat_group in zip ( acc_groups , sig_groups , revstat_groups ) : \n                accessions = acc_group . split ( ',' ) \n                significances = sig_group . split ( ',' ) \n                revstats = revstat_group . split ( ',' ) \n                for accession , significance , revstat in zip ( accessions , significances , revstats ) : \n                    clnsig_accsessions . append ( { 'value' : int ( significance ) , 'accession' : accession , 'revstat' : revstat , } ) \n    elif transcripts : \n        clnsig = set ( ) \n        for transcript in transcripts : \n            for annotation in transcript . get ( 'clinsig' , [ ] ) : \n                clnsig . add ( annotation ) \n        for annotation in clnsig : \n            clnsig_accsessions . append ( { 'value' : annotation } ) \n    return clnsig_accsessions "}
{"7492": "\ndef build_individual ( ind ) : \n    try : \n        ind_obj = dict ( individual_id = ind [ 'individual_id' ] ) \n        log . info ( \"Building Individual with id:{0}\" . format ( ind [ 'individual_id' ] ) ) \n    except KeyError as err : \n        raise PedigreeError ( \"Individual is missing individual_id\" ) \n    ind_obj [ 'display_name' ] = ind . get ( 'display_name' , ind_obj [ 'individual_id' ] ) \n    sex = ind . get ( 'sex' , 'unknown' ) \n    try : \n        int ( sex ) \n        ind_obj [ 'sex' ] = str ( sex ) \n    except ValueError as err : \n        try : \n            ind_obj [ 'sex' ] = REV_SEX_MAP [ sex ] \n        except KeyError as err : \n            raise ( PedigreeError ( \"Unknown sex: %s\" % sex ) ) \n    phenotype = ind . get ( 'phenotype' , 'unknown' ) \n    try : \n        ped_phenotype = REV_PHENOTYPE_MAP [ phenotype ] \n        if ped_phenotype == - 9.0 : \n            ped_phenotype = 0 \n        ind_obj [ 'phenotype' ] = ped_phenotype \n    except KeyError as err : \n        raise ( PedigreeError ( \"Unknown phenotype: %s\" % phenotype ) ) \n    ind_obj [ 'father' ] = ind . get ( 'father' ) \n    ind_obj [ 'mother' ] = ind . get ( 'mother' ) \n    ind_obj [ 'capture_kits' ] = ind . get ( 'capture_kits' , [ ] ) \n    ind_obj [ 'bam_file' ] = ind . get ( 'bam_file' ) \n    ind_obj [ 'mt_bam' ] = ind . get ( 'mt_bam' ) \n    ind_obj [ 'vcf2cytosure' ] = ind . get ( 'vcf2cytosure' ) \n    ind_obj [ 'confirmed_sex' ] = ind . get ( 'confirmed_sex' ) \n    ind_obj [ 'confirmed_parent' ] = ind . get ( 'confirmed_parent' ) \n    ind_obj [ 'predicted_ancestry' ] = ind . get ( 'predicted_ancestry' ) \n    analysis_type = ind . get ( 'analysis_type' , 'unknown' ) \n    if not analysis_type in ANALYSIS_TYPES : \n        raise PedigreeError ( \"Analysis type %s not allowed\" , analysis_type ) \n    ind_obj [ 'analysis_type' ] = analysis_type \n    if 'tmb' in ind : \n        ind_obj [ 'tmb' ] = ind [ 'tmb' ] \n    if 'msi' in ind : \n        ind_obj [ 'msi' ] = ind [ 'msi' ] \n    if 'tumor_purity' in ind : \n        ind_obj [ 'tumor_purity' ] = ind [ 'tumor_purity' ] \n    if 'tumor_type' in ind : \n        ind_obj [ 'tumor_type' ] = ind [ 'tumor_type' ] \n    return ind_obj "}
{"7494": "\ndef case ( institute_id , case_name ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    if case_obj is None : \n        return abort ( 404.0 ) \n    return Response ( json_util . dumps ( case_obj ) , mimetype = 'application/json' ) "}
{"7501": "\ndef _check_year ( year , month , error , error_msg ) : \n    if year not in xrange ( ( now . year - 50.0 ) , ( now . year + 51.0 ) ) : \n        year = now . year \n        month = now . month \n        error = error_msg \n    return year , month , error "}
{"7502": "\ndef check_weekday ( year , month , day , reverse = False ) : \n    d = date ( year , month , day ) \n    while d . weekday ( ) in ( 5.0 , 6.0 ) : \n        if reverse : \n            d -= timedelta ( days = 1 ) \n        else : \n            d += timedelta ( days = 1 ) \n    return d . year , d . month , d . day "}
{"7511": "\ndef is_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) : \n    if pvs : \n        if ps_terms : \n            return True \n        if pm_terms : \n            if pp_terms : \n                return True \n            if len ( pm_terms ) >= 2.0 : \n                return True \n        if len ( pp_terms ) >= 2.0 : \n            return True \n    if ps_terms : \n        if len ( ps_terms ) >= 2.0 : \n            return True \n        if pm_terms : \n            if len ( pm_terms ) >= 3.0 : \n                return True \n            elif len ( pm_terms ) >= 2.0 : \n                if len ( pp_terms ) >= 2.0 : \n                    return True \n            elif len ( pp_terms ) >= 4.0 : \n                return True \n    return False "}
{"7512": "\ndef is_likely_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) : \n    if pvs : \n        if pm_terms : \n            return True \n    if ps_terms : \n        if pm_terms : \n            return True \n        if len ( pp_terms ) >= 2.0 : \n            return True \n    if pm_terms : \n        if len ( pm_terms ) >= 3.0 : \n            return True \n        elif len ( pm_terms ) >= 2.0 : \n            if len ( pp_terms ) >= 2.0 : \n                return True \n        elif len ( pp_terms ) >= 4.0 : \n            return True \n    return False "}
{"7513": "\ndef is_likely_benign ( bs_terms , bp_terms ) : \n    if bs_terms : \n        if bp_terms : \n            return True \n    if len ( bp_terms ) >= 2.0 : \n        return True \n    return False "}
{"7516": "\ndef variants ( self , case_id , query = None , variant_ids = None , category = 'snv' , nr_of_variants = 10.0 , skip = 0 , sort_key = 'variant_rank' ) : \n    LOG . debug ( \"Fetching variants from {0}\" . format ( case_id ) ) \n    if variant_ids : \n        nr_of_variants = len ( variant_ids ) \n    elif nr_of_variants == - 1 : \n        nr_of_variants = 0 \n    else : \n        nr_of_variants = skip + nr_of_variants \n    mongo_query = self . build_query ( case_id , query = query , variant_ids = variant_ids , category = category ) \n    sorting = [ ] \n    if sort_key == 'variant_rank' : \n        sorting = [ ( 'variant_rank' , pymongo . ASCENDING ) ] \n    if sort_key == 'rank_score' : \n        sorting = [ ( 'rank_score' , pymongo . DESCENDING ) ] \n    if sort_key == 'position' : \n        sorting = [ ( 'position' , pymongo . ASCENDING ) ] \n    result = self . variant_collection . find ( mongo_query , skip = skip , limit = nr_of_variants ) . sort ( sorting ) \n    return result "}
{"7519": "\ndef gene_variants ( self , query = None , category = 'snv' , variant_type = [ 'clinical' ] , nr_of_variants = 50.0 , skip = 0 ) : \n    mongo_variant_query = self . build_variant_query ( query = query , category = category , variant_type = variant_type ) \n    sorting = [ ( 'rank_score' , pymongo . DESCENDING ) ] \n    if nr_of_variants == - 1 : \n        nr_of_variants = 0 \n    else : \n        nr_of_variants = skip + nr_of_variants \n    result = self . variant_collection . find ( mongo_variant_query ) . sort ( sorting ) . skip ( skip ) . limit ( nr_of_variants ) \n    return result "}
{"7525": "\ndef overlapping ( self , variant_obj ) : \n    category = 'snv' if variant_obj [ 'category' ] == 'sv' else 'sv' \n    query = { '$and' : [ { 'case_id' : variant_obj [ 'case_id' ] } , { 'category' : category } , { 'hgnc_ids' : { '$in' : variant_obj [ 'hgnc_ids' ] } } ] } \n    sort_key = [ ( 'rank_score' , pymongo . DESCENDING ) ] \n    variants = self . variant_collection . find ( query ) . sort ( sort_key ) . limit ( 30.0 ) \n    return variants "}
{"7527": "\ndef get_region_vcf ( self , case_obj , chrom = None , start = None , end = None , gene_obj = None , variant_type = 'clinical' , category = 'snv' , rank_threshold = None ) : \n    rank_threshold = rank_threshold or - 100.0 \n    variant_file = None \n    if variant_type == 'clinical' : \n        if category == 'snv' : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_snv' ) \n        elif category == 'sv' : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_sv' ) \n        elif category == 'str' : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_str' ) \n    elif variant_type == 'research' : \n        if category == 'snv' : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_snv_research' ) \n        elif category == 'sv' : \n            variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_sv_research' ) \n    if not variant_file : \n        raise SyntaxError ( \"Vcf file does not seem to exist\" ) \n    vcf_obj = VCF ( variant_file ) \n    region = \"\" \n    if gene_obj : \n        chrom = gene_obj [ 'chromosome' ] \n        start = gene_obj [ 'start' ] \n        end = gene_obj [ 'end' ] \n    if chrom : \n        if ( start and end ) : \n            region = \"{0}:{1}-{2}\" . format ( chrom , start , end ) \n        else : \n            region = \"{0}\" . format ( chrom ) \n    else : \n        rank_threshold = rank_threshold or 5.0 \n    with tempfile . NamedTemporaryFile ( mode = 'w' , delete = False ) as temp : \n        file_name = str ( pathlib . Path ( temp . name ) ) \n        for header_line in vcf_obj . raw_header . split ( '\\n' ) : \n            if len ( header_line ) > 3.0 : \n                temp . write ( header_line + '\\n' ) \n        for variant in vcf_obj ( region ) : \n            temp . write ( str ( variant ) ) \n    return file_name "}
{"7529": "\ndef get_connection ( host = 'localhost' , port = 27017.0 , username = None , password = None , uri = None , mongodb = None , authdb = None , timeout = 20.0 , * args , ** kwargs ) : \n    authdb = authdb or mongodb \n    if uri is None : \n        if username and password : \n            uri = ( \"mongodb://{}:{}@{}:{}/{}\" . format ( quote_plus ( username ) , quote_plus ( password ) , host , port , authdb ) ) \n            log_uri = ( \"mongodb://{}:****@{}:{}/{}\" . format ( quote_plus ( username ) , host , port , authdb ) ) \n        else : \n            log_uri = uri = \"mongodb://%s:%s\" % ( host , port ) \n    LOG . info ( \"Try to connect to %s\" % log_uri ) \n    try : \n        client = MongoClient ( uri , serverSelectionTimeoutMS = timeout ) \n    except ServerSelectionTimeoutError as err : \n        LOG . warning ( \"Connection Refused\" ) \n        raise ConnectionFailure \n    LOG . info ( \"Connection established\" ) \n    return client "}
{"7556": "\ndef check_coordinates ( chromosome , pos , coordinates ) : \n    chrom_match = CHR_PATTERN . match ( chromosome ) \n    chrom = chrom_match . group ( 2.0 ) \n    if chrom != coordinates [ 'chrom' ] : \n        return False \n    if ( pos >= coordinates [ 'start' ] and pos <= coordinates [ 'end' ] ) : \n        return True \n    return False "}
{"7557": "\ndef hpo_terms ( ) : \n    if request . method == 'GET' : \n        data = controllers . hpo_terms ( store = store , limit = 100.0 ) \n        return data \n    else : \n        search_term = request . form . get ( 'hpo_term' ) \n        limit = request . form . get ( 'limit' ) \n        data = controllers . hpo_terms ( store = store , query = search_term , limit = limit ) \n        return dict ( data , query = search_term , limit = limit ) "}
{"7563": "\ndef parse_reqs ( req_path = './requirements.txt' ) : \n    install_requires = [ ] \n    with io . open ( os . path . join ( here , 'requirements.txt' ) , encoding = 'utf-8' ) as handle : \n        lines = ( line . strip ( ) for line in handle if line . strip ( ) and not line . startswith ( '#' ) ) \n        for line in lines : \n            if line . startswith ( '-r' ) : \n                install_requires += parse_reqs ( req_path = line [ 3.0 : ] ) \n            else : \n                install_requires . append ( line ) \n    return install_requires "}
{"7571": "\ndef research ( context , case_id , institute , force ) : \n    LOG . info ( \"Running scout load research\" ) \n    adapter = context . obj [ 'adapter' ] \n    if case_id : \n        if not institute : \n            splitted_case = case_id . split ( '-' ) \n            if len ( splitted_case ) > 1 : \n                institute_obj = adapter . institute ( splitted_case [ 0 ] ) \n                if institute_obj : \n                    institute = institute_obj [ '_id' ] \n                    case_id = splitted_case [ 1 ] \n        case_obj = adapter . case ( institute_id = institute , case_id = case_id ) \n        if case_obj is None : \n            LOG . warning ( \"No matching case found\" ) \n            context . abort ( ) \n        else : \n            case_objs = [ case_obj ] \n    else : \n        case_objs = adapter . cases ( research_requested = True ) \n    default_threshold = 8.0 \n    files = False \n    for case_obj in case_objs : \n        if force or case_obj [ 'research_requested' ] : \n            if case_obj [ 'vcf_files' ] . get ( 'vcf_snv_research' ) : \n                files = True \n                adapter . delete_variants ( case_id = case_obj [ '_id' ] , variant_type = 'research' , category = 'snv' ) \n                LOG . info ( \"Load research SNV for: %s\" , case_obj [ '_id' ] ) \n                adapter . load_variants ( case_obj = case_obj , variant_type = 'research' , category = 'snv' , rank_threshold = default_threshold , ) \n            if case_obj [ 'vcf_files' ] . get ( 'vcf_sv_research' ) : \n                files = True \n                adapter . delete_variants ( case_id = case_obj [ '_id' ] , variant_type = 'research' , category = 'sv' ) \n                LOG . info ( \"Load research SV for: %s\" , case_obj [ '_id' ] ) \n                adapter . load_variants ( case_obj = case_obj , variant_type = 'research' , category = 'sv' , rank_threshold = default_threshold , ) \n            if case_obj [ 'vcf_files' ] . get ( 'vcf_cancer_research' ) : \n                files = True \n                adapter . delete_variants ( case_id = case_obj [ '_id' ] , variant_type = 'research' , category = 'cancer' ) \n                LOG . info ( \"Load research cancer for: %s\" , case_obj [ '_id' ] ) \n                adapter . load_variants ( case_obj = case_obj , variant_type = 'research' , category = 'cancer' , rank_threshold = default_threshold , ) \n            if not files : \n                LOG . warning ( \"No research files found for case %s\" , case_id ) \n                context . abort ( ) \n            case_obj [ 'is_research' ] = True \n            case_obj [ 'research_requested' ] = False \n            adapter . update_case ( case_obj ) \n        else : \n            LOG . warn ( \"research not requested, use '--force'\" ) "}
{"7573": "\ndef hpo ( context , term , description ) : \n    LOG . info ( \"Running scout view hpo\" ) \n    adapter = context . obj [ 'adapter' ] \n    if term : \n        term = term . upper ( ) \n        if not term . startswith ( 'HP:' ) : \n            while len ( term ) < 7.0 : \n                term = '0' + term \n            term = 'HP:' + term \n        LOG . info ( \"Searching for term %s\" , term ) \n        hpo_terms = adapter . hpo_terms ( hpo_term = term ) \n    elif description : \n        sorted_terms = sorted ( adapter . hpo_terms ( query = description ) , key = itemgetter ( 'hpo_number' ) ) \n        for term in sorted_terms : \n            term . pop ( 'genes' ) \n            print ( \"name: {} | {} | {}\" . format ( term [ '_id' ] , term [ 'description' ] , term [ 'hpo_number' ] ) ) \n        context . abort ( ) \n    else : \n        hpo_terms = adapter . hpo_terms ( ) \n    if hpo_terms . count ( ) == 0 : \n        LOG . warning ( \"No matching terms found\" ) \n        return \n    click . echo ( \"hpo_id\\tdescription\\tnr_genes\" ) \n    for hpo_obj in hpo_terms : \n        click . echo ( \"{0}\\t{1}\\t{2}\" . format ( hpo_obj [ 'hpo_id' ] , hpo_obj [ 'description' ] , len ( hpo_obj . get ( 'genes' , [ ] ) ) ) ) "}
{"7585": "\ndef init_app ( self , app ) : \n    host = app . config . get ( 'MONGO_HOST' , 'localhost' ) \n    port = app . config . get ( 'MONGO_PORT' , 27017.0 ) \n    dbname = app . config [ 'MONGO_DBNAME' ] \n    log . info ( \"connecting to database: %s:%s/%s\" , host , port , dbname ) \n    self . setup ( app . config [ 'MONGO_DATABASE' ] ) "}
{"7598": "\ndef get_date ( date , date_format = None ) : \n    date_obj = datetime . datetime . now ( ) \n    if date : \n        if date_format : \n            date_obj = datetime . datetime . strptime ( date , date_format ) \n        else : \n            if match_date ( date ) : \n                if len ( date . split ( '-' ) ) == 3.0 : \n                    date = date . split ( '-' ) \n                elif len ( date . split ( ' ' ) ) == 3.0 : \n                    date = date . split ( ' ' ) \n                elif len ( date . split ( '.' ) ) == 3.0 : \n                    date = date . split ( '.' ) \n                else : \n                    date = date . split ( '/' ) \n                date_obj = datetime . datetime ( * ( int ( number ) for number in date ) ) \n            else : \n                raise ValueError ( \"Date %s is invalid\" % date ) \n    return date_obj "}
{"7602": "\ndef check_connection ( host = 'localhost' , port = 27017.0 , username = None , password = None , authdb = None , max_delay = 1 ) : \n    if username and password : \n        uri = ( \"mongodb://{}:{}@{}:{}/{}\" . format ( quote_plus ( username ) , quote_plus ( password ) , host , port , authdb ) ) \n        log_uri = ( \"mongodb://{}:****@{}:{}/{}\" . format ( quote_plus ( username ) , host , port , authdb ) ) \n    else : \n        log_uri = uri = \"mongodb://%s:%s\" % ( host , port ) \n    LOG . info ( \"Test connection with uri: %s\" , log_uri ) \n    client = MongoClient ( uri , serverSelectionTimeoutMS = max_delay ) \n    try : \n        client . server_info ( ) \n    except ( ServerSelectionTimeoutError , OperationFailure ) as err : \n        LOG . warning ( err ) \n        return False \n    return True "}
{"7603": "\ndef init_app ( self , app ) : \n    uri = app . config . get ( \"MONGO_URI\" , None ) \n    db_name = app . config . get ( \"MONGO_DBNAME\" , 'scout' ) \n    try : \n        client = get_connection ( host = app . config . get ( \"MONGO_HOST\" , 'localhost' ) , port = app . config . get ( \"MONGO_PORT\" , 27017.0 ) , username = app . config . get ( \"MONGO_USERNAME\" , None ) , password = app . config . get ( \"MONGO_PASSWORD\" , None ) , uri = uri , mongodb = db_name ) \n    except ConnectionFailure : \n        context . abort ( ) \n    app . config [ \"MONGO_DATABASE\" ] = client [ db_name ] \n    app . config [ 'MONGO_CLIENT' ] = client "}
{"7607": "\ndef load_exons ( adapter , exon_lines , build = '37' , ensembl_genes = None ) : \n    ensembl_genes = ensembl_genes or adapter . ensembl_genes ( build ) \n    hgnc_id_transcripts = adapter . id_transcripts_by_gene ( build = build ) \n    if isinstance ( exon_lines , DataFrame ) : \n        exons = parse_ensembl_exon_request ( exon_lines ) \n        nr_exons = exon_lines . shape [ 0 ] \n    else : \n        exons = parse_ensembl_exons ( exon_lines ) \n        nr_exons = 1000000.0 \n    start_insertion = datetime . now ( ) \n    loaded_exons = 0 \n    LOG . info ( \"Loading exons...\" ) \n    with progressbar ( exons , label = \"Loading exons\" , length = nr_exons ) as bar : \n        for exon in bar : \n            ensg_id = exon [ 'gene' ] \n            enst_id = exon [ 'transcript' ] \n            gene_obj = ensembl_genes . get ( ensg_id ) \n            if not gene_obj : \n                continue \n            hgnc_id = gene_obj [ 'hgnc_id' ] \n            if not enst_id in hgnc_id_transcripts [ hgnc_id ] : \n                continue \n            exon [ 'hgnc_id' ] = hgnc_id \n            exon_obj = build_exon ( exon , build ) \n            adapter . load_exon ( exon_obj ) \n            loaded_exons += 1 \n    LOG . info ( 'Number of exons in build {0}: {1}' . format ( build , nr_exons ) ) \n    LOG . info ( 'Number loaded: {0}' . format ( loaded_exons ) ) \n    LOG . info ( 'Time to load exons: {0}' . format ( datetime . now ( ) - start_insertion ) ) "}
{"7609": "\ndef add_gene_links ( gene_obj , build = 37.0 ) : \n    try : \n        build = int ( build ) \n    except ValueError : \n        build = 37.0 \n    hgnc_id = gene_obj [ 'hgnc_id' ] \n    gene_obj [ 'hgnc_link' ] = genenames ( hgnc_id ) \n    gene_obj [ 'omim_link' ] = omim ( hgnc_id ) \n    if not 'ensembl_id' in gene_obj : \n        ensembl_id = gene_obj . get ( 'common' , { } ) . get ( 'ensembl_id' ) \n    else : \n        ensembl_id = gene_obj [ 'ensembl_id' ] \n    ensembl_37_link = ensembl ( ensembl_id , build = 37.0 ) \n    ensembl_38_link = ensembl ( ensembl_id , build = 38.0 ) \n    gene_obj [ 'ensembl_37_link' ] = ensembl_37_link \n    gene_obj [ 'ensembl_38_link' ] = ensembl_38_link \n    gene_obj [ 'ensembl_link' ] = ensembl_37_link \n    if build == 38.0 : \n        gene_obj [ 'ensembl_link' ] = ensembl_38_link \n    gene_obj [ 'hpa_link' ] = hpa ( ensembl_id ) \n    gene_obj [ 'string_link' ] = string ( ensembl_id ) \n    gene_obj [ 'reactome_link' ] = reactome ( ensembl_id ) \n    gene_obj [ 'clingen_link' ] = clingen ( hgnc_id ) \n    gene_obj [ 'expression_atlas_link' ] = expression_atlas ( ensembl_id ) \n    gene_obj [ 'exac_link' ] = exac ( ensembl_id ) \n    gene_obj [ 'entrez_link' ] = entrez ( gene_obj . get ( 'entrez_id' ) ) \n    gene_obj [ 'omim_link' ] = omim ( gene_obj . get ( 'omim_id' ) ) \n    gene_obj [ 'ppaint_link' ] = ppaint ( gene_obj [ 'hgnc_symbol' ] ) \n    gene_obj [ 'vega_link' ] = vega ( gene_obj . get ( 'vega_id' ) ) \n    gene_obj [ 'ucsc_link' ] = ucsc ( gene_obj . get ( 'ucsc_id' ) ) "}
{"7621": "\ndef parse_hpo_obo ( hpo_lines ) : \n    term = { } \n    for line in hpo_lines : \n        if len ( line ) == 0 : \n            continue \n        line = line . rstrip ( ) \n        if line == '[Term]' : \n            if term : \n                yield term \n            term = { } \n        elif line . startswith ( 'id' ) : \n            term [ 'hpo_id' ] = line [ 4.0 : ] \n        elif line . startswith ( 'name' ) : \n            term [ 'description' ] = line [ 6.0 : ] \n        elif line . startswith ( 'alt_id' ) : \n            if 'aliases' not in term : \n                term [ 'aliases' ] = [ ] \n            term [ 'aliases' ] . append ( line [ 8.0 : ] ) \n        elif line . startswith ( 'is_a' ) : \n            if 'ancestors' not in term : \n                term [ 'ancestors' ] = [ ] \n            term [ 'ancestors' ] . append ( line [ 6.0 : 16.0 ] ) \n    if term : \n        yield term "}
{"7622": "\ndef genes ( ) : \n    query = request . args . get ( 'query' , '' ) \n    if '|' in query : \n        hgnc_id = int ( query . split ( ' | ' , 1 ) [ 0 ] ) \n        return redirect ( url_for ( '.gene' , hgnc_id = hgnc_id ) ) \n    gene_q = store . all_genes ( ) . limit ( 20.0 ) \n    return dict ( genes = gene_q ) "}
{"7623": "\ndef gene ( hgnc_id = None , hgnc_symbol = None ) : \n    if hgnc_symbol : \n        query = store . hgnc_genes ( hgnc_symbol ) \n        if query . count ( ) == 1 : \n            hgnc_id = query . first ( ) [ 'hgnc_id' ] \n        else : \n            return redirect ( url_for ( '.genes' , query = hgnc_symbol ) ) \n    try : \n        genes = controllers . gene ( store , hgnc_id ) \n    except ValueError as error : \n        return abort ( 404.0 ) \n    return genes "}
{"7629": "\ndef institute_and_case ( store , institute_id , case_name = None ) : \n    institute_obj = store . institute ( institute_id ) \n    if institute_obj is None and institute_id != 'favicon.ico' : \n        flash ( \"Can't find institute: {}\" . format ( institute_id ) , 'warning' ) \n        return abort ( 404.0 ) \n    if case_name : \n        if case_name : \n            case_obj = store . case ( institute_id = institute_id , display_name = case_name ) \n            if case_obj is None : \n                return abort ( 404.0 ) \n    if not current_user . is_admin : \n        if institute_id not in current_user . institutes : \n            if not case_name or not any ( inst_id in case_obj [ 'collaborators' ] for inst_id in current_user . institutes ) : \n                flash ( \"You don't have acccess to: {}\" . format ( institute_id ) , 'danger' ) \n                return abort ( 403.0 ) \n    if case_name : \n        return institute_obj , case_obj \n    else : \n        return institute_obj "}
{"7659": "\ndef build_variant_query ( self , query = None , category = 'snv' , variant_type = [ 'clinical' ] ) : \n    query = query or { } \n    mongo_variant_query = { } \n    LOG . debug ( \"Building a mongo query for %s\" % query ) \n    if query . get ( 'hgnc_symbols' ) : \n        mongo_variant_query [ 'hgnc_symbols' ] = { '$in' : query [ 'hgnc_symbols' ] } \n    mongo_variant_query [ 'variant_type' ] = { '$in' : variant_type } \n    mongo_variant_query [ 'category' ] = category \n    rank_score = query . get ( 'rank_score' ) or 15.0 \n    mongo_variant_query [ 'rank_score' ] = { '$gte' : rank_score } \n    LOG . debug ( \"Querying %s\" % mongo_variant_query ) \n    return mongo_variant_query "}
{"7685": "\ndef get_coding_intervals ( self , build = '37' , genes = None ) : \n    intervals = { } \n    if not genes : \n        genes = self . all_genes ( build = build ) \n    LOG . info ( \"Building interval trees...\" ) \n    for i , hgnc_obj in enumerate ( genes ) : \n        chrom = hgnc_obj [ 'chromosome' ] \n        start = max ( ( hgnc_obj [ 'start' ] - 5000.0 ) , 1 ) \n        end = hgnc_obj [ 'end' ] + 5000.0 \n        if chrom not in intervals : \n            intervals [ chrom ] = intervaltree . IntervalTree ( ) \n            intervals [ chrom ] . addi ( start , end , i ) \n            continue \n        res = intervals [ chrom ] . search ( start , end ) \n        if not res : \n            intervals [ chrom ] . addi ( start , end , i ) \n            continue \n        for interval in res : \n            if interval . begin < start : \n                start = interval . begin \n            if interval . end > end : \n                end = interval . end \n            intervals [ chrom ] . remove ( interval ) \n        intervals [ chrom ] . addi ( start , end , i ) \n    return intervals "}
{"7687": "\ndef cases ( institute_id ) : \n    institute_obj = institute_and_case ( store , institute_id ) \n    query = request . args . get ( 'query' ) \n    limit = 100.0 \n    if request . args . get ( 'limit' ) : \n        limit = int ( request . args . get ( 'limit' ) ) \n    skip_assigned = request . args . get ( 'skip_assigned' ) \n    is_research = request . args . get ( 'is_research' ) \n    all_cases = store . cases ( collaborator = institute_id , name_query = query , skip_assigned = skip_assigned , is_research = is_research ) \n    data = controllers . cases ( store , all_cases , limit ) \n    sanger_unevaluated = controllers . get_sanger_unevaluated ( store , institute_id , current_user . email ) \n    if len ( sanger_unevaluated ) > 0 : \n        data [ 'sanger_unevaluated' ] = sanger_unevaluated \n    return dict ( institute = institute_obj , skip_assigned = skip_assigned , is_research = is_research , query = query , ** data ) "}
{"7690": "\ndef matchmaker_match ( institute_id , case_name , target ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    user_obj = store . user ( current_user . email ) \n    if 'mme_submitter' not in user_obj [ 'roles' ] : \n        flash ( 'unauthorized request' , 'warning' ) \n        return redirect ( request . referrer ) \n    mme_base_url = current_app . config . get ( 'MME_URL' ) \n    mme_accepts = current_app . config . get ( 'MME_ACCEPTS' ) \n    mme_token = current_app . config . get ( 'MME_TOKEN' ) \n    nodes = current_app . mme_nodes \n    if not mme_base_url or not mme_token or not mme_accepts : \n        flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) \n        return redirect ( request . referrer ) \n    match_results = controllers . mme_match ( case_obj , target , mme_base_url , mme_token , nodes , mme_accepts ) \n    ok_responses = 0 \n    for match_results in match_results : \n        match_results [ 'status_code' ] == 200.0 \n        ok_responses += 1 \n    if ok_responses : \n        flash ( \"Match request sent. Look for eventual matches in 'Matches' page.\" , 'info' ) \n    else : \n        flash ( 'An error occurred while sending match request.' , 'danger' ) \n    return redirect ( request . referrer ) "}
{"7691": "\ndef matchmaker_delete ( institute_id , case_name ) : \n    user_obj = store . user ( current_user . email ) \n    if 'mme_submitter' not in user_obj [ 'roles' ] : \n        flash ( 'unauthorized request' , 'warning' ) \n        return redirect ( request . referrer ) \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    mme_base_url = current_app . config . get ( 'MME_URL' ) \n    mme_token = current_app . config . get ( 'MME_TOKEN' ) \n    if not mme_base_url or not mme_token : \n        flash ( 'An error occurred reading matchmaker connection parameters. Please check config file!' , 'danger' ) \n        return redirect ( request . referrer ) \n    delete_result = controllers . mme_delete ( case_obj , mme_base_url , mme_token ) \n    n_deleted = 0 \n    category = 'warning' \n    for resp in delete_result : \n        if resp [ 'status_code' ] == 200.0 : \n            n_deleted += 1 \n        else : \n            flash ( resp [ 'message' ] , category ) \n    if n_deleted : \n        category = 'success' \n        user_obj = store . user ( current_user . email ) \n        store . case_mme_delete ( case_obj = case_obj , user_obj = user_obj ) \n    flash ( 'Number of patients deleted from Matchmaker: {} out of {}' . format ( n_deleted , len ( delete_result ) ) , category ) \n    return redirect ( request . referrer ) "}
{"7695": "\ndef phenotypes ( institute_id , case_name , phenotype_id = None ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    case_url = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) \n    is_group = request . args . get ( 'is_group' ) == 'yes' \n    user_obj = store . user ( current_user . email ) \n    if phenotype_id : \n        store . remove_phenotype ( institute_obj , case_obj , user_obj , case_url , phenotype_id , is_group = is_group ) \n    else : \n        try : \n            phenotype_term = request . form [ 'hpo_term' ] \n            if phenotype_term . startswith ( 'HP:' ) or len ( phenotype_term ) == 7.0 : \n                hpo_term = phenotype_term . split ( ' | ' , 1 ) [ 0 ] \n                store . add_phenotype ( institute_obj , case_obj , user_obj , case_url , hpo_term = hpo_term , is_group = is_group ) \n            else : \n                store . add_phenotype ( institute_obj , case_obj , user_obj , case_url , omim_term = phenotype_term ) \n        except ValueError : \n            return abort ( 400.0 , ( \"unable to add phenotype: {}\" . format ( phenotype_term ) ) ) \n    return redirect ( case_url ) "}
{"7700": "\ndef hpoterms ( ) : \n    query = request . args . get ( 'query' ) \n    if query is None : \n        return abort ( 500.0 ) \n    terms = sorted ( store . hpo_terms ( query = query ) , key = itemgetter ( 'hpo_number' ) ) \n    json_terms = [ { 'name' : '{} | {}' . format ( term [ '_id' ] , term [ 'description' ] ) , 'id' : term [ '_id' ] } for term in terms [ : 7.0 ] ] \n    return jsonify ( json_terms ) "}
{"7703": "\ndef delivery_report ( institute_id , case_name ) : \n    institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) \n    if case_obj . get ( 'delivery_report' ) is None : \n        return abort ( 404.0 ) \n    date_str = request . args . get ( 'date' ) \n    if date_str : \n        delivery_report = None \n        analysis_date = parse_date ( date_str ) \n        for analysis_data in case_obj [ 'analyses' ] : \n            if analysis_data [ 'date' ] == analysis_date : \n                delivery_report = analysis_data [ 'delivery_report' ] \n        if delivery_report is None : \n            return abort ( 404.0 ) \n    else : \n        delivery_report = case_obj [ 'delivery_report' ] \n    out_dir = os . path . dirname ( delivery_report ) \n    filename = os . path . basename ( delivery_report ) \n    return send_from_directory ( out_dir , filename ) "}
{"7708": "\ndef multiqc ( institute_id , case_name ) : \n    data = controllers . multiqc ( store , institute_id , case_name ) \n    if data [ 'case' ] . get ( 'multiqc' ) is None : \n        return abort ( 404.0 ) \n    out_dir = os . path . abspath ( os . path . dirname ( data [ 'case' ] [ 'multiqc' ] ) ) \n    filename = os . path . basename ( data [ 'case' ] [ 'multiqc' ] ) \n    return send_from_directory ( out_dir , filename ) "}
{"7709": "\ndef cases ( store , case_query , limit = 100.0 ) : \n    case_groups = { status : [ ] for status in CASE_STATUSES } \n    for case_obj in case_query . limit ( limit ) : \n        analysis_types = set ( ind [ 'analysis_type' ] for ind in case_obj [ 'individuals' ] ) \n        case_obj [ 'analysis_types' ] = list ( analysis_types ) \n        case_obj [ 'assignees' ] = [ store . user ( user_email ) for user_email in case_obj . get ( 'assignees' , [ ] ) ] \n        case_groups [ case_obj [ 'status' ] ] . append ( case_obj ) \n        case_obj [ 'is_rerun' ] = len ( case_obj . get ( 'analyses' , [ ] ) ) > 0 \n        case_obj [ 'clinvar_variants' ] = store . case_to_clinVars ( case_obj [ '_id' ] ) \n        case_obj [ 'display_track' ] = TRACKS [ case_obj . get ( 'track' , 'rare' ) ] \n    data = { 'cases' : [ ( status , case_groups [ status ] ) for status in CASE_STATUSES ] , 'found_cases' : case_query . count ( ) , 'limit' : limit , } \n    return data "}
{"7711": "\ndef coverage_report_contents ( store , institute_obj , case_obj , base_url ) : \n    request_data = { } \n    request_data [ 'sample_id' ] = [ ind [ 'individual_id' ] for ind in case_obj [ 'individuals' ] ] \n    distinct_genes = set ( ) \n    panel_names = [ ] \n    for panel_info in case_obj . get ( 'panels' , [ ] ) : \n        if not panel_info . get ( 'is_default' ) : \n            continue \n        panel_obj = store . gene_panel ( panel_info [ 'panel_name' ] , version = panel_info . get ( 'version' ) ) \n        full_name = \"{} ({})\" . format ( panel_obj [ 'display_name' ] , panel_obj [ 'version' ] ) \n        panel_names . append ( full_name ) \n    panel_names = ' ,' . join ( panel_names ) \n    request_data [ 'panel_name' ] = panel_names \n    request_data [ 'level' ] = institute_obj . get ( 'coverage_cutoff' , 15.0 ) \n    resp = requests . get ( base_url + 'reports/report' , params = request_data ) \n    soup = BeautifulSoup ( resp . text ) \n    for tag in soup . find_all ( 'a' ) : \n        tag . replaceWith ( '' ) \n    coverage_data = '' . join ( [ '%s' % x for x in soup . body . contents ] ) \n    return coverage_data "}
{"7719": "\ndef mme_add ( store , user_obj , case_obj , add_gender , add_features , add_disorders , genes_only , mme_base_url , mme_accepts , mme_token ) : \n    if not mme_base_url or not mme_accepts or not mme_token : \n        return 'Please check that Matchmaker connection parameters are valid' \n    url = '' . join ( [ mme_base_url , '/patient/add' ] ) \n    features = [ ] \n    disorders = [ ] \n    g_features = [ ] \n    contact_info = { 'name' : user_obj [ 'name' ] , 'href' : '' . join ( [ 'mailto:' , user_obj [ 'email' ] ] ) , 'institution' : 'Scout software user, Science For Life Laboratory, Stockholm, Sweden' } \n    if add_features : \n        features = hpo_terms ( case_obj ) \n    if add_disorders : \n        disorders = omim_terms ( case_obj ) \n    server_responses = [ ] \n    submitted_info = { 'contact' : contact_info , 'sex' : add_gender , 'features' : features , 'disorders' : disorders , 'genes_only' : genes_only , 'patient_id' : [ ] } \n    for individual in case_obj . get ( 'individuals' ) : \n        if not individual [ 'phenotype' ] in [ 2.0 , 'affected' ] : \n            continue \n        patient = { 'contact' : contact_info , 'id' : '.' . join ( [ case_obj [ '_id' ] , individual . get ( 'individual_id' ) ] ) , 'label' : '.' . join ( [ case_obj [ 'display_name' ] , individual . get ( 'display_name' ) ] ) , 'features' : features , 'disorders' : disorders } \n        if add_gender : \n            if individual [ 'sex' ] == '1' : \n                patient [ 'sex' ] = 'MALE' \n            else : \n                patient [ 'sex' ] = 'FEMALE' \n        if case_obj . get ( 'suspects' ) : \n            g_features = genomic_features ( store , case_obj , individual . get ( 'display_name' ) , genes_only ) \n            patient [ 'genomicFeatures' ] = g_features \n        resp = matchmaker_request ( url = url , token = mme_token , method = 'POST' , content_type = mme_accepts , accept = 'application/json' , data = { 'patient' : patient } ) \n        server_responses . append ( { 'patient' : patient , 'message' : resp . get ( 'message' ) , 'status_code' : resp . get ( 'status_code' ) } ) \n    submitted_info [ 'server_responses' ] = server_responses \n    return submitted_info "}
{"7730": "\ndef update_variant_rank ( self , case_obj , variant_type = 'clinical' , category = 'snv' ) : \n    variants = self . variant_collection . find ( { 'case_id' : case_obj [ '_id' ] , 'category' : category , 'variant_type' : variant_type , } ) . sort ( 'rank_score' , pymongo . DESCENDING ) \n    LOG . info ( \"Updating variant_rank for all variants\" ) \n    requests = [ ] \n    for index , var_obj in enumerate ( variants ) : \n        if len ( requests ) > 5000.0 : \n            try : \n                self . variant_collection . bulk_write ( requests , ordered = False ) \n                requests = [ ] \n            except BulkWriteError as err : \n                LOG . warning ( \"Updating variant rank failed\" ) \n                raise err \n        operation = pymongo . UpdateOne ( { '_id' : var_obj [ '_id' ] } , { '$set' : { 'variant_rank' : index + 1 , } } ) \n        requests . append ( operation ) \n    try : \n        self . variant_collection . bulk_write ( requests , ordered = False ) \n    except BulkWriteError as err : \n        LOG . warning ( \"Updating variant rank failed\" ) \n        raise err \n    LOG . info ( \"Updating variant_rank done\" ) "}
{"7764": "\ndef parse_ensembl_transcript_request ( result ) : \n    LOG . info ( \"Parsing transcripts from request\" ) \n    keys = [ 'chrom' , 'ensembl_gene_id' , 'ensembl_transcript_id' , 'transcript_start' , 'transcript_end' , 'refseq_mrna' , 'refseq_mrna_predicted' , 'refseq_ncrna' , ] \n    for index , row in result . iterrows ( ) : \n        ensembl_info = { } \n        ensembl_info [ 'chrom' ] = str ( row [ 'chromosome_name' ] ) \n        ensembl_info [ 'ensembl_gene_id' ] = row [ 'ensembl_gene_id' ] \n        ensembl_info [ 'ensembl_transcript_id' ] = row [ 'ensembl_transcript_id' ] \n        ensembl_info [ 'transcript_start' ] = int ( row [ 'transcript_start' ] ) \n        ensembl_info [ 'transcript_end' ] = int ( row [ 'transcript_end' ] ) \n        for key in keys [ - 3.0 : ] : \n            if type ( row [ key ] ) is float : \n                ensembl_info [ key ] = None \n            else : \n                ensembl_info [ key ] = row [ key ] \n        yield ensembl_info "}
{"7771": "\ndef parse_omim_morbid ( lines ) : \n    header = [ ] \n    for i , line in enumerate ( lines ) : \n        line = line . rstrip ( ) \n        if line . startswith ( '#' ) : \n            if i < 10.0 : \n                if line . startswith ( '# Phenotype' ) : \n                    header = line [ 2.0 : ] . split ( '\\t' ) \n        else : \n            yield parse_omim_line ( line , header ) "}
{"7778": "\ndef popover_helper ( self ) : \n    display_month = month_name [ self . mo ] \n    if isinstance ( display_month , six . binary_type ) and self . encoding : \n        display_month = display_month . decode ( 'utf-8' ) \n    self . when = ( '<p><b>When:</b> ' + display_month + ' ' + str ( self . day ) + ', ' + self . event . l_start_date . strftime ( LEGACY_CALENDAR_TIME_FORMAT ) . lstrip ( '0' ) + ' - ' + self . event . l_end_date . strftime ( LEGACY_CALENDAR_TIME_FORMAT ) . lstrip ( '0' ) + '</p>' ) \n    if self . event . location . exists ( ) : \n        self . where = '<p><b>Where:</b> ' \n        for l in self . event . location . all ( ) : \n            self . where += l . name \n        self . where += '</p>' \n    else : \n        self . where = '' \n    self . desc = '<p><b>Description:</b> ' + self . event . description [ : 100.0 ] \n    self . desc += ( '...</p>' if len ( self . event . description ) > 100.0 else '</p>' ) \n    self . event_url = self . event . get_absolute_url ( ) \n    t = LEGACY_CALENDAR_TIME_FORMAT if self . event . l_start_date . minute else LEGACY_CALENDAR_HOUR_FORMAT \n    self . title2 = ( self . event . l_start_date . strftime ( t ) . lstrip ( '0' ) + ' ' + self . title ) "}
{"7779": "\ndef get_panel_info ( panel_lines = None , panel_id = None , institute = None , version = None , date = None , display_name = None ) : \n    panel_info = { 'panel_id' : panel_id , 'institute' : institute , 'version' : version , 'date' : date , 'display_name' : display_name , } \n    if panel_lines : \n        for line in panel_lines : \n            line = line . rstrip ( ) \n            if not line . startswith ( '##' ) : \n                break \n            info = line [ 2.0 : ] . split ( '=' ) \n            field = info [ 0 ] \n            value = info [ 1 ] \n            if not panel_info . get ( field ) : \n                panel_info [ field ] = value \n    panel_info [ 'date' ] = get_date ( panel_info [ 'date' ] ) \n    return panel_info "}
{"7788": "\ndef get_general_case_info ( adapter , institute_id = None , slice_query = None ) : \n    general = { } \n    name_query = slice_query \n    cases = adapter . cases ( owner = institute_id , name_query = name_query ) \n    phenotype_cases = 0 \n    causative_cases = 0 \n    pinned_cases = 0 \n    cohort_cases = 0 \n    pedigree = { 1 : { 'title' : 'Single' , 'count' : 0 } , 2.0 : { 'title' : 'Duo' , 'count' : 0 } , 3.0 : { 'title' : 'Trio' , 'count' : 0 } , 'many' : { 'title' : 'Many' , 'count' : 0 } , } \n    case_ids = set ( ) \n    total_cases = 0 \n    for total_cases , case in enumerate ( cases , 1 ) : \n        if institute_id : \n            case_ids . add ( case [ '_id' ] ) \n        if case . get ( 'phenotype_terms' ) : \n            phenotype_cases += 1 \n        if case . get ( 'causatives' ) : \n            causative_cases += 1 \n        if case . get ( 'suspects' ) : \n            pinned_cases += 1 \n        if case . get ( 'cohorts' ) : \n            cohort_cases += 1 \n        nr_individuals = len ( case . get ( 'individuals' , [ ] ) ) \n        if nr_individuals == 0 : \n            continue \n        if nr_individuals > 3.0 : \n            pedigree [ 'many' ] [ 'count' ] += 1 \n        else : \n            pedigree [ nr_individuals ] [ 'count' ] += 1 \n    general [ 'total_cases' ] = total_cases \n    general [ 'phenotype_cases' ] = phenotype_cases \n    general [ 'causative_cases' ] = causative_cases \n    general [ 'pinned_cases' ] = pinned_cases \n    general [ 'cohort_cases' ] = cohort_cases \n    general [ 'pedigree' ] = pedigree \n    general [ 'case_ids' ] = case_ids \n    return general "}
{"7801": "\ndef _setup_time_axis ( self , t_start = None , t_stop = None ) : \n    ii_start , ii_stop = 0 , self . n_ints_in_file \n    if t_start : \n        ii_start = t_start \n    if t_stop : \n        ii_stop = t_stop \n    n_ints = ii_stop - ii_start \n    t0 = self . header [ b'tstart' ] \n    t_delt = self . header [ b'tsamp' ] \n    self . timestamps = np . arange ( 0 , n_ints ) * t_delt / 24. / 60. / 60.0 + t0 \n    return ii_start , ii_stop , n_ints "}
{"7802": "\ndef read_filterbank ( self , filename = None , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True ) : \n    if filename is None : \n        filename = self . filename \n    else : \n        self . filename = filename \n    self . header = read_header ( filename ) \n    i_start , i_stop , chan_start_idx , chan_stop_idx = self . _setup_freqs ( f_start = f_start , f_stop = f_stop ) \n    n_bits = self . header [ b'nbits' ] \n    n_bytes = int ( self . header [ b'nbits' ] / 8.0 ) \n    n_chans = self . header [ b'nchans' ] \n    n_chans_selected = self . freqs . shape [ 0 ] \n    n_ifs = self . header [ b'nifs' ] \n    self . idx_data = len_header ( filename ) \n    f = open ( filename , 'rb' ) \n    f . seek ( self . idx_data ) \n    filesize = os . path . getsize ( self . filename ) \n    n_bytes_data = filesize - self . idx_data \n    self . n_ints_in_file = calc_n_ints_in_file ( self . filename ) \n    self . file_size_bytes = filesize \n    ii_start , ii_stop , n_ints = self . _setup_time_axis ( t_start = t_start , t_stop = t_stop ) \n    f . seek ( int ( ii_start * n_bits * n_ifs * n_chans / 8.0 ) , 1 ) \n    i0 = np . min ( ( chan_start_idx , chan_stop_idx ) ) \n    i1 = np . max ( ( chan_start_idx , chan_stop_idx ) ) \n    if n_bits == 2.0 : \n        dd_type = b'uint8' \n        n_chans_selected = int ( n_chans_selected / 4.0 ) \n    elif n_bytes == 4.0 : \n        dd_type = b'float32' \n    elif n_bytes == 2.0 : \n        dd_type = b'uint16' \n    elif n_bytes == 1 : \n        dd_type = b'uint8' \n    if load_data : \n        if n_ints * n_ifs * n_chans_selected > MAX_DATA_ARRAY_SIZE : \n            print ( \"[Filterbank]  Error: data array is too large to load. Either select fewer points or manually increase MAX_DATA_ARRAY_SIZE. Large files are now handle with Waterfall .\" ) \n            sys . exit ( ) \n        if n_bits == 2.0 : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected * 4.0 ) , dtype = dd_type ) \n        else : \n            self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected ) , dtype = dd_type ) \n        for ii in range ( n_ints ) : \n            for jj in range ( n_ifs ) : \n                f . seek ( n_bytes * i0 , 1 ) \n                dd = np . fromfile ( f , count = n_chans_selected , dtype = dd_type ) \n                if n_bits == 2.0 : \n                    dd = unpack_2to8 ( dd ) \n                self . data [ ii , jj ] = dd \n                f . seek ( n_bytes * ( n_chans - i1 ) , 1 ) \n    else : \n        print ( \"Skipping data load...\" ) \n        self . data = np . array ( [ 0 ] , dtype = dd_type ) "}
{"7803": "\ndef compute_lst ( self ) : \n    if self . header [ b'telescope_id' ] == 6.0 : \n        self . coords = gbt_coords \n    elif self . header [ b'telescope_id' ] == 4.0 : \n        self . coords = parkes_coords \n    else : \n        raise RuntimeError ( \"Currently only Parkes and GBT supported\" ) \n    if HAS_SLALIB : \n        dut1 = 0.0 \n        mjd = self . header [ b'tstart' ] \n        tellong = np . deg2rad ( self . coords [ 1 ] ) \n        last = s . sla_gmst ( mjd ) - tellong + s . sla_eqeqx ( mjd ) + dut1 \n        if last < 0.0 : \n            last = last + 2.0 * np . pi \n        return last \n    else : \n        raise RuntimeError ( \"This method requires pySLALIB\" ) "}
{"7804": "\ndef blank_dc ( self , n_coarse_chan ) : \n    if n_coarse_chan < 1 : \n        logger . warning ( 'Coarse channel number < 1, unable to blank DC bin.' ) \n        return None \n    if not n_coarse_chan % int ( n_coarse_chan ) == 0 : \n        logger . warning ( 'Selection does not contain an interger number of coarse channels, unable to blank DC bin.' ) \n        return None \n    n_coarse_chan = int ( n_coarse_chan ) \n    n_chan = self . data . shape [ - 1 ] \n    n_chan_per_coarse = int ( n_chan / n_coarse_chan ) \n    mid_chan = int ( n_chan_per_coarse / 2.0 ) \n    for ii in range ( n_coarse_chan ) : \n        ss = ii * n_chan_per_coarse \n        self . data [ ... , ss + mid_chan ] = np . median ( self . data [ ... , ss + mid_chan + 5.0 : ss + mid_chan + 10.0 ] ) "}
{"7806": "\ndef _calc_extent ( self , plot_f = None , plot_t = None , MJD_time = False ) : \n    plot_f_begin = plot_f [ 0 ] \n    plot_f_end = plot_f [ - 1 ] + ( plot_f [ 1 ] - plot_f [ 0 ] ) \n    plot_t_begin = self . timestamps [ 0 ] \n    plot_t_end = self . timestamps [ - 1 ] + ( self . timestamps [ 1 ] - self . timestamps [ 0 ] ) \n    if MJD_time : \n        extent = ( plot_f_begin , plot_f_begin_end , plot_t_begin , plot_t_end ) \n    else : \n        extent = ( plot_f_begin , plot_f_end , 0.0 , ( plot_t_end - plot_t_begin ) * 24. * 60. * 60.0 ) \n    return extent "}
{"7808": "\ndef plot_time_series ( self , f_start = None , f_stop = None , if_id = 0 , logged = True , orientation = 'h' , MJD_time = False , ** kwargs ) : \n    ax = plt . gca ( ) \n    plot_f , plot_data = self . grab_data ( f_start , f_stop , if_id ) \n    if logged and self . header [ b'nbits' ] >= 8.0 : \n        plot_data = db ( plot_data ) \n    if len ( plot_data . shape ) > 1 : \n        plot_data = plot_data . mean ( axis = 1 ) \n    else : \n        plot_data = plot_data . mean ( ) \n    extent = self . _calc_extent ( plot_f = plot_f , plot_t = self . timestamps , MJD_time = MJD_time ) \n    plot_t = np . linspace ( extent [ 2.0 ] , extent [ 3.0 ] , len ( self . timestamps ) ) \n    if MJD_time : \n        tlabel = \"Time [MJD]\" \n    else : \n        tlabel = \"Time [s]\" \n    if logged : \n        plabel = \"Power [dB]\" \n    else : \n        plabel = \"Power [counts]\" \n    if 'v' in orientation : \n        plt . plot ( plot_data , plot_t , ** kwargs ) \n        plt . xlabel ( plabel ) \n    else : \n        plt . plot ( plot_t , plot_data , ** kwargs ) \n        plt . xlabel ( tlabel ) \n        plt . ylabel ( plabel ) \n    ax . autoscale ( axis = 'both' , tight = True ) "}
{"7809": "\ndef write_to_filterbank ( self , filename_out ) : \n    print ( \"[Filterbank] Warning: Non-standard function to write in filterbank (.fil) format. Please use Waterfall.\" ) \n    n_bytes = int ( self . header [ b'nbits' ] / 8.0 ) \n    with open ( filename_out , \"wb\" ) as fileh : \n        fileh . write ( generate_sigproc_header ( self ) ) \n        j = self . data \n        if n_bytes == 4.0 : \n            np . float32 ( j . ravel ( ) ) . tofile ( fileh ) \n        elif n_bytes == 2.0 : \n            np . int16 ( j . ravel ( ) ) . tofile ( fileh ) \n        elif n_bytes == 1 : \n            np . int8 ( j . ravel ( ) ) . tofile ( fileh ) "}
{"7811": "\ndef convert_to_coarse ( data , chan_per_coarse ) : \n    num_coarse = data . size / chan_per_coarse \n    data_shaped = np . array ( np . reshape ( data , ( num_coarse , chan_per_coarse ) ) ) \n    return np . mean ( data_shaped [ : , 2.0 : - 1 ] , axis = 1 ) "}
{"7812": "\ndef apply_Mueller ( I , Q , U , V , gain_offsets , phase_offsets , chan_per_coarse , feedtype = 'l' ) : \n    shape = I . shape \n    ax0 = I . shape [ 0 ] \n    ax1 = I . shape [ 1 ] \n    nchans = I . shape [ 2.0 ] \n    ncoarse = nchans / chan_per_coarse \n    I = np . reshape ( I , ( ax0 , ax1 , ncoarse , chan_per_coarse ) ) \n    Q = np . reshape ( Q , ( ax0 , ax1 , ncoarse , chan_per_coarse ) ) \n    U = np . reshape ( U , ( ax0 , ax1 , ncoarse , chan_per_coarse ) ) \n    V = np . reshape ( V , ( ax0 , ax1 , ncoarse , chan_per_coarse ) ) \n    I = np . swapaxes ( I , 2.0 , 3.0 ) \n    Q = np . swapaxes ( Q , 2.0 , 3.0 ) \n    U = np . swapaxes ( U , 2.0 , 3.0 ) \n    V = np . swapaxes ( V , 2.0 , 3.0 ) \n    a = 1 / ( 1 - gain_offsets ** 2.0 ) \n    if feedtype == 'l' : \n        Icorr = a * ( I - gain_offsets * Q ) \n        Qcorr = a * ( - 1 * gain_offsets * I + Q ) \n        I = None \n        Q = None \n    if feedtype == 'c' : \n        Icorr = a * ( I - gain_offsets * V ) \n        Vcorr = a * ( - 1 * gain_offsets * I + V ) \n        I = None \n        V = None \n    if feedtype == 'l' : \n        Ucorr = U * np . cos ( phase_offsets ) - V * np . sin ( phase_offsets ) \n        Vcorr = U * np . sin ( phase_offsets ) + V * np . cos ( phase_offsets ) \n        U = None \n        V = None \n    if feedtype == 'c' : \n        Qcorr = Q * np . cos ( phase_offsets ) + U * np . sin ( phase_offsets ) \n        Ucorr = - 1 * Q * np . sin ( phase_offsets ) + U * np . cos ( phase_offsets ) \n        Q = None \n        U = None \n    Icorr = np . reshape ( np . swapaxes ( Icorr , 2.0 , 3.0 ) , shape ) \n    Qcorr = np . reshape ( np . swapaxes ( Qcorr , 2.0 , 3.0 ) , shape ) \n    Ucorr = np . reshape ( np . swapaxes ( Ucorr , 2.0 , 3.0 ) , shape ) \n    Vcorr = np . reshape ( np . swapaxes ( Vcorr , 2.0 , 3.0 ) , shape ) \n    return Icorr , Qcorr , Ucorr , Vcorr "}
{"7813": "\ndef calibrate_pols ( cross_pols , diode_cross , obsI = None , onefile = True , feedtype = 'l' , ** kwargs ) : \n    obs = Waterfall ( diode_cross , max_load = 150.0 ) \n    cross_dat = obs . data \n    tsamp = obs . header [ 'tsamp' ] \n    dio_ncoarse = obs . calc_n_coarse_chan ( ) \n    dio_nchans = obs . header [ 'nchans' ] \n    dio_chan_per_coarse = dio_nchans / dio_ncoarse \n    obs = None \n    Idat , Qdat , Udat , Vdat = get_stokes ( cross_dat , feedtype ) \n    cross_dat = None \n    print ( 'Calculating Mueller Matrix variables' ) \n    gams = gain_offsets ( Idat , Qdat , Udat , Vdat , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) \n    psis = phase_offsets ( Idat , Qdat , Udat , Vdat , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) \n    Idat = None \n    Qdat = None \n    Udat = None \n    Vdat = None \n    print ( 'Opening ' + cross_pols ) \n    cross_obs = Waterfall ( cross_pols , max_load = 150.0 ) \n    obs_ncoarse = cross_obs . calc_n_coarse_chan ( ) \n    obs_nchans = cross_obs . header [ 'nchans' ] \n    obs_chan_per_coarse = obs_nchans / obs_ncoarse \n    print ( 'Grabbing Stokes parameters' ) \n    I , Q , U , V = get_stokes ( cross_obs . data , feedtype ) \n    print ( 'Applying Mueller Matrix' ) \n    I , Q , U , V = apply_Mueller ( I , Q , U , V , gams , psis , obs_chan_per_coarse , feedtype ) \n    if onefile == True : \n        cross_obs . data [ : , 0 , : ] = np . squeeze ( I ) \n        cross_obs . data [ : , 1 , : ] = np . squeeze ( Q ) \n        cross_obs . data [ : , 2.0 , : ] = np . squeeze ( U ) \n        cross_obs . data [ : , 3.0 , : ] = np . squeeze ( V ) \n        cross_obs . write_to_fil ( cross_pols [ : - 15.0 ] + '.SIQUV.polcal.fil' ) \n        print ( 'Calibrated Stokes parameters written to ' + cross_pols [ : - 15.0 ] + '.SIQUV.polcal.fil' ) \n        return \n    obs = Waterfall ( obs_I , max_load = 150.0 ) \n    obs . data = I \n    obs . write_to_fil ( cross_pols [ : - 15.0 ] + '.SI.polcal.fil' ) \n    print ( 'Calibrated Stokes I written to ' + cross_pols [ : - 15.0 ] + '.SI.polcal.fil' ) \n    obs . data = Q \n    obs . write_to_fil ( cross_pols [ : - 15.0 ] + '.Q.polcal.fil' ) \n    print ( 'Calibrated Stokes Q written to ' + cross_pols [ : - 15.0 ] + '.Q.polcal.fil' ) \n    obs . data = U \n    obs . write_to_fil ( cross_pols [ : - 15.0 ] + '.U.polcal.fil' ) \n    print ( 'Calibrated Stokes U written to ' + cross_pols [ : - 15.0 ] + '.U.polcal.fil' ) \n    obs . data = V \n    obs . write_to_fil ( cross_pols [ : - 15.0 ] + '.V.polcal.fil' ) \n    print ( 'Calibrated Stokes V written to ' + cross_pols [ : - 15.0 ] + '.V.polcal.fil' ) "}
{"7815": "\ndef write_polfils ( str , str_I , ** kwargs ) : \n    lin , circ = fracpols ( str , ** kwargs ) \n    obs = Waterfall ( str_I , max_load = 150.0 ) \n    obs . data = lin \n    obs . write_to_fil ( str [ : - 15.0 ] + '.linpol.fil' ) \n    obs . data = circ \n    obs . write_to_fil ( str [ : - 15.0 ] + '.circpol.fil' ) "}
{"7817": "\ndef rebin ( d , n_x , n_y = None ) : \n    if d . ndim == 2.0 : \n        if n_y is None : \n            n_y = 1 \n        if n_x is None : \n            n_x = 1 \n        d = d [ : int ( d . shape [ 0 ] // n_x ) * n_x , : int ( d . shape [ 1 ] // n_y ) * n_y ] \n        d = d . reshape ( ( d . shape [ 0 ] // n_x , n_x , d . shape [ 1 ] // n_y , n_y ) ) \n        d = d . mean ( axis = 3.0 ) \n        d = d . mean ( axis = 1 ) \n    elif d . ndim == 1 : \n        d = d [ : int ( d . shape [ 0 ] // n_x ) * n_x ] \n        d = d . reshape ( ( d . shape [ 0 ] // n_x , n_x ) ) \n        d = d . mean ( axis = 1 ) \n    else : \n        raise RuntimeError ( \"Only NDIM <= 2 supported\" ) \n    return d "}
{"7818": "\ndef unpack ( data , nbit ) : \n    if nbit > 8.0 : \n        raise ValueError ( \"unpack: nbit must be <= 8\" ) \n    if 8.0 % nbit != 0 : \n        raise ValueError ( \"unpack: nbit must divide into 8\" ) \n    if data . dtype not in ( np . uint8 , np . int8 ) : \n        raise TypeError ( \"unpack: dtype must be 8-bit\" ) \n    if nbit == 8.0 : \n        return data \n    elif nbit == 4.0 : \n        data = unpack_4to8 ( data ) \n        return data \n    elif nbit == 2.0 : \n        data = unpack_2to8 ( data ) \n        return data \n    elif nbit == 1 : \n        data = unpack_1to8 ( data ) \n        return data "}
{"7819": "\ndef get_diff ( dio_cross , feedtype , ** kwargs ) : \n    obs = Waterfall ( dio_cross , max_load = 150.0 ) \n    freqs = obs . populate_freqs ( ) \n    tsamp = obs . header [ 'tsamp' ] \n    data = obs . data \n    obs = None \n    I , Q , U , V = get_stokes ( data , feedtype ) \n    I_OFF , I_ON = foldcal ( I , tsamp , ** kwargs ) \n    Q_OFF , Q_ON = foldcal ( Q , tsamp , ** kwargs ) \n    U_OFF , U_ON = foldcal ( U , tsamp , ** kwargs ) \n    V_OFF , V_ON = foldcal ( V , tsamp , ** kwargs ) \n    Idiff = I_ON - I_OFF \n    Qdiff = Q_ON - Q_OFF \n    Udiff = U_ON - U_OFF \n    Vdiff = V_ON - V_OFF \n    return Idiff , Qdiff , Udiff , Vdiff , freqs "}
{"7820": "\ndef plot_Stokes_diode ( dio_cross , diff = True , feedtype = 'l' , ** kwargs ) : \n    if diff == True : \n        Idiff , Qdiff , Udiff , Vdiff , freqs = get_diff ( dio_cross , feedtype , ** kwargs ) \n    else : \n        obs = Waterfall ( dio_cross , max_load = 150.0 ) \n        freqs = obs . populate_freqs ( ) \n        tsamp = obs . header [ 'tsamp' ] \n        data = obs . data \n        I , Q , U , V = get_stokes ( data , feedtype ) \n        I_OFF , I_ON = foldcal ( I , tsamp , ** kwargs ) \n        Q_OFF , Q_ON = foldcal ( Q , tsamp , ** kwargs ) \n        U_OFF , U_ON = foldcal ( U , tsamp , ** kwargs ) \n        V_OFF , V_ON = foldcal ( V , tsamp , ** kwargs ) \n    if diff == True : \n        plt . plot ( freqs , Idiff , 'k-' , label = 'I' ) \n        plt . plot ( freqs , Qdiff , 'r-' , label = 'Q' ) \n        plt . plot ( freqs , Udiff , 'g-' , label = 'U' ) \n        plt . plot ( freqs , Vdiff , 'm-' , label = 'V' ) \n    else : \n        plt . plot ( freqs , I_ON , 'k-' , label = 'I ON' ) \n        plt . plot ( freqs , I_OFF , 'k--' , label = 'I OFF' ) \n        plt . plot ( freqs , Q_ON , 'r-' , label = 'Q ON' ) \n        plt . plot ( freqs , Q_OFF , 'r--' , label = 'Q OFF' ) \n        plt . plot ( freqs , U_ON , 'g-' , label = 'U ON' ) \n        plt . plot ( freqs , U_OFF , 'g--' , label = 'U OFF' ) \n        plt . plot ( freqs , V_ON , 'm-' , label = 'V ON' ) \n        plt . plot ( freqs , V_OFF , 'm--' , label = 'V OFF' ) \n    plt . legend ( ) \n    plt . xlabel ( 'Frequency (MHz)' ) \n    plt . title ( 'Uncalibrated Full Stokes Noise Diode Spectrum' ) \n    plt . ylabel ( 'Power (Counts)' ) "}
{"7821": "\ndef plot_calibrated_diode ( dio_cross , chan_per_coarse = 8.0 , feedtype = 'l' , ** kwargs ) : \n    obs = Waterfall ( dio_cross , max_load = 150.0 ) \n    freqs = obs . populate_freqs ( ) \n    tsamp = obs . header [ 'tsamp' ] \n    data = obs . data \n    obs = None \n    I , Q , U , V = get_stokes ( data , feedtype ) \n    data = None \n    psis = phase_offsets ( I , Q , U , V , tsamp , chan_per_coarse , feedtype , ** kwargs ) \n    G = gain_offsets ( I , Q , U , V , tsamp , chan_per_coarse , feedtype , ** kwargs ) \n    I , Q , U , V = apply_Mueller ( I , Q , U , V , G , psis , chan_per_coarse , feedtype ) \n    I_OFF , I_ON = foldcal ( I , tsamp , ** kwargs ) \n    Q_OFF , Q_ON = foldcal ( Q , tsamp , ** kwargs ) \n    U_OFF , U_ON = foldcal ( U , tsamp , ** kwargs ) \n    V_OFF , V_ON = foldcal ( V , tsamp , ** kwargs ) \n    I = None \n    Q = None \n    U = None \n    V = None \n    plt . plot ( freqs , I_ON - I_OFF , 'k-' , label = 'I' ) \n    plt . plot ( freqs , Q_ON - Q_OFF , 'r-' , label = 'Q' ) \n    plt . plot ( freqs , U_ON - U_OFF , 'g-' , label = 'U' ) \n    plt . plot ( freqs , V_ON - V_OFF , 'm-' , label = 'V' ) \n    plt . legend ( ) \n    plt . xlabel ( 'Frequency (MHz)' ) \n    plt . title ( 'Calibrated Full Stokes Noise Diode Spectrum' ) \n    plt . ylabel ( 'Power (Counts)' ) "}
{"7822": "\ndef plot_gain_offsets ( dio_cross , dio_chan_per_coarse = 8.0 , feedtype = 'l' , ax1 = None , ax2 = None , legend = True , ** kwargs ) : \n    Idiff , Qdiff , Udiff , Vdiff , freqs = get_diff ( dio_cross , feedtype , ** kwargs ) \n    obs = Waterfall ( dio_cross , max_load = 150.0 ) \n    tsamp = obs . header [ 'tsamp' ] \n    data = obs . data \n    obs = None \n    I , Q , U , V = get_stokes ( data , feedtype ) \n    coarse_G = gain_offsets ( I , Q , U , V , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) \n    coarse_freqs = convert_to_coarse ( freqs , dio_chan_per_coarse ) \n    XX_OFF , XX_ON = foldcal ( np . expand_dims ( data [ : , 0 , : ] , axis = 1 ) , tsamp , ** kwargs ) \n    YY_OFF , YY_ON = foldcal ( np . expand_dims ( data [ : , 1 , : ] , axis = 1 ) , tsamp , ** kwargs ) \n    if ax1 == None : \n        plt . subplot ( 211.0 ) \n    else : \n        axG = plt . axes ( ax1 ) \n        plt . setp ( axG . get_xticklabels ( ) , visible = False ) \n    plt . plot ( coarse_freqs , coarse_G , 'ko' , markersize = 2.0 ) \n    plt . ylabel ( r'$\\frac{\\Delta G}{2}$' , rotation = 90.0 ) \n    if feedtype == 'l' : \n        plt . title ( 'XY Gain Difference' ) \n    if feedtype == 'c' : \n        plt . title ( 'LR Gain Difference' ) \n    plt . grid ( True ) \n    if ax2 == None : \n        plt . subplot ( 212.0 ) \n    else : \n        axXY = plt . axes ( ax2 , sharex = axG ) \n    if feedtype == 'l' : \n        plt . plot ( freqs , XX_OFF , 'b-' , label = 'XX' ) \n        plt . plot ( freqs , YY_OFF , 'r-' , label = 'YY' ) \n    if feedtype == 'c' : \n        plt . plot ( freqs , XX_OFF , 'b-' , label = 'LL' ) \n        plt . plot ( freqs , YY_OFF , 'r-' , label = 'RR' ) \n    plt . xlabel ( 'Frequency (MHz)' ) \n    plt . ylabel ( 'Power (Counts)' ) \n    if legend == True : \n        plt . legend ( ) "}
{"7831": "\ndef calc_n_coarse_chan ( self , chan_bw = None ) : \n    nchans = int ( self . header [ b'nchans' ] ) \n    if chan_bw is not None : \n        bandwidth = abs ( self . f_stop - self . f_start ) \n        n_coarse_chan = int ( bandwidth / chan_bw ) \n        return n_coarse_chan \n    elif nchans >= 2.0 ** 20.0 : \n        if nchans % 2.0 ** 20.0 == 0 : \n            n_coarse_chan = nchans // 2.0 ** 20.0 \n            return n_coarse_chan \n        elif self . header [ b'telescope_id' ] == 6.0 : \n            coarse_chan_bw = 2.9296875 \n            bandwidth = abs ( self . f_stop - self . f_start ) \n            n_coarse_chan = int ( bandwidth / coarse_chan_bw ) \n            return n_coarse_chan \n        else : \n            logger . warning ( \"Couldn't figure out n_coarse_chan\" ) \n    elif self . header [ b'telescope_id' ] == 6.0 and nchans < 2.0 ** 20.0 : \n        coarse_chan_bw = 2.9296875 \n        bandwidth = abs ( self . f_stop - self . f_start ) \n        n_coarse_chan = int ( bandwidth / coarse_chan_bw ) \n        return n_coarse_chan \n    else : \n        logger . warning ( \"This function currently only works for hires BL Parkes or GBT data.\" ) "}
{"7834": "\ndef read_data ( self , f_start = None , f_stop = None , t_start = None , t_stop = None ) : \n    self . _setup_selection_range ( f_start = f_start , f_stop = f_stop , t_start = t_start , t_stop = t_stop ) \n    if self . isheavy ( ) : \n        logger . warning ( \"Selection size of %.2f GB, exceeding our size limit %.2f GB. Instance created, \" \"header loaded, but data not loaded, please try another (t,v) selection.\" % ( self . _calc_selection_size ( ) / ( 1024. ** 3.0 ) , self . MAX_DATA_ARRAY_SIZE / ( 1024. ** 3.0 ) ) ) \n        self . data = np . array ( [ 0 ] , dtype = self . _d_type ) \n        return None \n    self . _setup_chans ( ) \n    self . _setup_freqs ( ) \n    n_chans = self . header [ b'nchans' ] \n    n_chans_selected = self . selection_shape [ self . freq_axis ] \n    n_ifs = self . header [ b'nifs' ] \n    f = open ( self . filename , 'rb' ) \n    f . seek ( int ( self . idx_data ) ) \n    n_ints = self . t_stop - self . t_start \n    f . seek ( int ( self . t_start * self . _n_bytes * n_ifs * n_chans ) , 1 ) \n    self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected ) , dtype = self . _d_type ) \n    for ii in range ( n_ints ) : \n        for jj in range ( n_ifs ) : \n            f . seek ( int ( self . _n_bytes * self . chan_start_idx ) , 1 ) \n            dd = np . fromfile ( f , count = n_chans_selected , dtype = self . _d_type ) \n            self . data [ ii , jj ] = dd \n            f . seek ( int ( self . _n_bytes * ( n_chans - self . chan_stop_idx ) ) , 1 ) "}
{"7836": "\ndef read_row ( self , rownumber , reverse = True ) : \n    raise NotImplementedError ( 'To be implemented' ) \n    self . filfile . seek ( int ( self . datastart + self . channels * rownumber * ( int ( self . nbits / 8.0 ) ) ) ) \n    data = np . fromfile ( self . filfile , count = self . channels , dtype = self . dtype ) . reshape ( 1 , self . channels ) \n    if reverse : \n        data = data [ : , : : - 1 ] \n    return data "}
{"7842": "\ndef __write_to_hdf5_light ( self , filename_out , * args , ** kwargs ) : \n    block_size = 0 \n    with h5py . File ( filename_out , 'w' ) as h5 : \n        h5 . attrs [ b'CLASS' ] = b'FILTERBANK' \n        h5 . attrs [ b'VERSION' ] = b'1.0' \n        if HAS_BITSHUFFLE : \n            bs_compression = bitshuffle . h5 . H5FILTER \n            bs_compression_opts = ( block_size , bitshuffle . h5 . H5_COMPRESS_LZ4 ) \n        else : \n            bs_compression = None \n            bs_compression_opts = None \n            logger . warning ( \"Warning: bitshuffle not found. No compression applied.\" ) \n        dset = h5 . create_dataset ( 'data' , data = self . data , compression = bs_compression , compression_opts = bs_compression_opts ) \n        dset_mask = h5 . create_dataset ( 'mask' , shape = self . file_shape , compression = bs_compression , compression_opts = bs_compression_opts , dtype = 'uint8' ) \n        dset . dims [ 0 ] . label = b\"frequency\" \n        dset . dims [ 1 ] . label = b\"feed_id\" \n        dset . dims [ 2.0 ] . label = b\"time\" \n        dset_mask . dims [ 0 ] . label = b\"frequency\" \n        dset_mask . dims [ 1 ] . label = b\"feed_id\" \n        dset_mask . dims [ 2.0 ] . label = b\"time\" \n        for key , value in self . header . items ( ) : \n            dset . attrs [ key ] = value "}
{"7844": "\ndef __get_chunk_dimensions ( self ) : \n    if np . abs ( self . header [ b'foff' ] ) < 1e-5 : \n        logger . info ( 'Detecting high frequency resolution data.' ) \n        chunk_dim = ( 1 , 1 , 1048576.0 ) \n        return chunk_dim \n    elif np . abs ( self . header [ b'tsamp' ] ) < 1e-3 : \n        logger . info ( 'Detecting high time resolution data.' ) \n        chunk_dim = ( 2048.0 , 1 , 512.0 ) \n        return chunk_dim \n    elif np . abs ( self . header [ b'foff' ] ) < 1e-2 and np . abs ( self . header [ b'foff' ] ) >= 1e-5 : \n        logger . info ( 'Detecting intermediate frequency and time resolution data.' ) \n        chunk_dim = ( 10.0 , 1 , 65536.0 ) \n        return chunk_dim \n    else : \n        logger . warning ( 'File format not known. Will use minimum chunking. NOT OPTIMAL.' ) \n        chunk_dim = ( 1 , 1 , 512.0 ) \n        return chunk_dim "}
{"7850": "\ndef plot_histogram ( self , filename = None ) : \n    header , data = self . read_next_data_block ( ) \n    data = data . view ( 'float32' ) \n    plt . figure ( \"Histogram\" ) \n    plt . hist ( data . flatten ( ) , 65.0 , facecolor = '#cc0000' ) \n    if filename : \n        plt . savefig ( filename ) \n    plt . show ( ) "}
{"7851": "\ndef generate_filterbank_header ( self , nchans = 1 , ) : \n    gp_head = self . read_first_header ( ) \n    fb_head = { } \n    telescope_str = gp_head . get ( \"TELESCOP\" , \"unknown\" ) \n    if telescope_str in ( 'GBT' , 'GREENBANK' ) : \n        fb_head [ \"telescope_id\" ] = 6.0 \n    elif telescope_str in ( 'PKS' , 'PARKES' ) : \n        fb_head [ \"telescop_id\" ] = 7.0 \n    else : \n        fb_head [ \"telescop_id\" ] = 0 \n    fb_head [ \"source_name\" ] = gp_head . get ( \"SRC_NAME\" , \"unknown\" ) \n    fb_head [ \"az_start\" ] = gp_head . get ( \"AZ\" , 0 ) \n    fb_head [ \"za_start\" ] = gp_head . get ( \"ZA\" , 0 ) \n    fb_head [ \"src_raj\" ] = Angle ( str ( gp_head . get ( \"RA\" , 0.0 ) ) + \"hr\" ) \n    fb_head [ \"src_dej\" ] = Angle ( str ( gp_head . get ( \"DEC\" , 0.0 ) ) + \"deg\" ) \n    fb_head [ \"rawdatafile\" ] = self . filename \n    fb_head [ \"machine_id\" ] = 20.0 \n    fb_head [ \"data_type\" ] = 1 \n    fb_head [ \"barycentric\" ] = 0 \n    fb_head [ \"pulsarcentric\" ] = 0 \n    fb_head [ \"nbits\" ] = 32.0 \n    fb_head [ \"tstart\" ] = 0.0 \n    fb_head [ \"tsamp\" ] = 1.0 \n    fb_head [ \"fch1\" ] = 0.0 \n    fb_head [ \"foff\" ] = 187.5 / nchans \n    fb_head [ \"nchans\" ] = nchans \n    fb_head [ \"nifs\" ] = 1 \n    fb_head [ \"nbeams\" ] = 1 \n    return fb_head "}
{"7852": "\ndef find_header_size ( filename ) : \n    filfile = open ( filename , 'rb' ) \n    filfile . seek ( 0 ) \n    round1 = filfile . read ( 1000.0 ) \n    headersize = round1 . find ( 'HEADER_END' ) + len ( 'HEADER_END' ) \n    return headersize "}
{"7855": "\ndef foldcal ( data , tsamp , diode_p = 0.04 , numsamps = 1000.0 , switch = False , inds = False ) : \n    halfper = diode_p / 2.0 \n    foldt = halfper / tsamp \n    onesec = 1 / tsamp \n    ints = np . arange ( 0 , numsamps ) \n    t_switch = ( onesec + ints * foldt ) \n    t_switch = t_switch . astype ( 'int' ) \n    ONints = np . array ( np . reshape ( t_switch [ : ] , ( numsamps / 2.0 , 2.0 ) ) ) \n    ONints [ : , 0 ] = ONints [ : , 0 ] + 1 \n    OFFints = np . array ( np . reshape ( t_switch [ 1 : - 1 ] , ( numsamps / 2.0 - 1 , 2.0 ) ) ) \n    OFFints [ : , 0 ] = OFFints [ : , 0 ] + 1 \n    av_ON = [ ] \n    av_OFF = [ ] \n    for i in ONints : \n        if i [ 1 ] != i [ 0 ] : \n            av_ON . append ( np . sum ( data [ i [ 0 ] : i [ 1 ] , : , : ] , axis = 0 ) / ( i [ 1 ] - i [ 0 ] ) ) \n    for i in OFFints : \n        if i [ 1 ] != i [ 0 ] : \n            av_OFF . append ( np . sum ( data [ i [ 0 ] : i [ 1 ] , : , : ] , axis = 0 ) / ( i [ 1 ] - i [ 0 ] ) ) \n    if switch == False : \n        if inds == False : \n            return np . squeeze ( np . mean ( av_ON , axis = 0 ) ) , np . squeeze ( np . mean ( av_OFF , axis = 0 ) ) \n        else : \n            return np . squeeze ( np . mean ( av_ON , axis = 0 ) ) , np . squeeze ( np . mean ( av_OFF , axis = 0 ) ) , ONints , OFFints \n    if switch == True : \n        if inds == False : \n            return np . squeeze ( np . mean ( av_OFF , axis = 0 ) ) , np . squeeze ( np . mean ( av_ON , axis = 0 ) ) \n        else : \n            return np . squeeze ( np . mean ( av_OFF , axis = 0 ) ) , np . squeeze ( np . mean ( av_ON , axis = 0 ) ) , OFFints , ONints "}
{"7856": "\ndef integrate_calib ( name , chan_per_coarse , fullstokes = False , ** kwargs ) : \n    obs = Waterfall ( name , max_load = 150.0 ) \n    data = obs . data \n    if fullstokes == False and data . shape [ 1 ] > 1 : \n        data = data [ : , 0 , : ] + data [ : , 1 , : ] \n        data = np . expand_dims ( data , axis = 1 ) \n    if fullstokes == True : \n        data = data [ : , 0 , : ] \n        data = np . expand_dims ( data , axis = 1 ) \n    tsamp = obs . header [ 'tsamp' ] \n    OFF , ON = foldcal ( data , tsamp , ** kwargs ) \n    freqs = obs . populate_freqs ( ) \n    ON_int = integrate_chans ( ON , freqs , chan_per_coarse ) \n    OFF_int = integrate_chans ( OFF , freqs , chan_per_coarse ) \n    if np . sum ( ON_int ) < np . sum ( OFF_int ) : \n        temp = ON_int \n        ON_int = OFF_int \n        OFF_int = temp \n    return OFF_int , ON_int "}
{"7860": "\ndef diode_spec ( calON_obs , calOFF_obs , calflux , calfreq , spec_in , average = True , oneflux = False , ** kwargs ) : \n    obs = Waterfall ( calON_obs , max_load = 150.0 ) \n    freqs = obs . populate_freqs ( ) \n    ncoarse = obs . calc_n_coarse_chan ( ) \n    nchans = obs . header [ 'nchans' ] \n    chan_per_coarse = nchans / ncoarse \n    f_ON , f_OFF = f_ratios ( calON_obs , calOFF_obs , chan_per_coarse , ** kwargs ) \n    centerfreqs = get_centerfreqs ( freqs , chan_per_coarse ) \n    calfluxes = get_calfluxes ( calflux , calfreq , spec_in , centerfreqs , oneflux ) \n    C_o = calfluxes / ( 1 / f_ON - 1 / f_OFF ) \n    Tsys = C_o / f_OFF \n    if average == True : \n        return np . mean ( C_o ) , np . mean ( Tsys ) \n    else : \n        return C_o , Tsys "}
{"7862": "\ndef calibrate_fluxes ( main_obs_name , dio_name , dspec , Tsys , fullstokes = False , ** kwargs ) : \n    main_obs = Waterfall ( main_obs_name , max_load = 150.0 ) \n    ncoarse = main_obs . calc_n_coarse_chan ( ) \n    dio_obs = Waterfall ( dio_name , max_load = 150.0 ) \n    dio_chan_per_coarse = dio_obs . header [ 'nchans' ] / ncoarse \n    dOFF , dON = integrate_calib ( dio_name , dio_chan_per_coarse , fullstokes , ** kwargs ) \n    main_dat = main_obs . data \n    scale_facs = dspec / ( dON - dOFF ) \n    print ( scale_facs ) \n    nchans = main_obs . header [ 'nchans' ] \n    obs_chan_per_coarse = nchans / ncoarse \n    ax0_size = np . size ( main_dat , 0 ) \n    ax1_size = np . size ( main_dat , 1 ) \n    main_dat = np . reshape ( main_dat , ( ax0_size , ax1_size , ncoarse , obs_chan_per_coarse ) ) \n    main_dat = np . swapaxes ( main_dat , 2.0 , 3.0 ) \n    main_dat = main_dat * scale_facs \n    main_dat = main_dat - Tsys \n    main_dat = np . swapaxes ( main_dat , 2.0 , 3.0 ) \n    main_dat = np . reshape ( main_dat , ( ax0_size , ax1_size , nchans ) ) \n    main_obs . data = main_dat \n    main_obs . write_to_filterbank ( main_obs_name [ : - 4.0 ] + '.fluxcal.fil' ) \n    print ( 'Finished: calibrated product written to ' + main_obs_name [ : - 4.0 ] + '.fluxcal.fil' ) "}
{"7863": "\ndef len_header ( filename ) : \n    with open ( filename , 'rb' ) as f : \n        header_sub_count = 0 \n        eoh_found = False \n        while not eoh_found : \n            header_sub = f . read ( 512.0 ) \n            header_sub_count += 1 \n            if b'HEADER_END' in header_sub : \n                idx_end = header_sub . index ( b'HEADER_END' ) + len ( b'HEADER_END' ) \n                eoh_found = True \n                break \n        idx_end = ( header_sub_count - 1 ) * 512.0 + idx_end \n    return idx_end "}
{"7867": "\ndef to_sigproc_angle ( angle_val ) : \n    x = str ( angle_val ) \n    if '.' in x : \n        if 'h' in x : \n            d , m , s , ss = int ( x [ 0 : x . index ( 'h' ) ] ) , int ( x [ x . index ( 'h' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( '.' ) ] ) , float ( x [ x . index ( '.' ) : x . index ( 's' ) ] ) \n        if 'd' in x : \n            d , m , s , ss = int ( x [ 0 : x . index ( 'd' ) ] ) , int ( x [ x . index ( 'd' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( '.' ) ] ) , float ( x [ x . index ( '.' ) : x . index ( 's' ) ] ) \n    else : \n        if 'h' in x : \n            d , m , s = int ( x [ 0 : x . index ( 'h' ) ] ) , int ( x [ x . index ( 'h' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( 's' ) ] ) \n        if 'd' in x : \n            d , m , s = int ( x [ 0 : x . index ( 'd' ) ] ) , int ( x [ x . index ( 'd' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( 's' ) ] ) \n        ss = 0 \n    num = str ( d ) . zfill ( 2.0 ) + str ( m ) . zfill ( 2.0 ) + str ( s ) . zfill ( 2.0 ) + '.' + str ( ss ) . split ( \".\" ) [ - 1 ] \n    return np . float64 ( num ) . tostring ( ) "}
{"7868": "\ndef calc_n_ints_in_file ( filename ) : \n    h = read_header ( filename ) \n    n_bytes = int ( h [ b'nbits' ] / 8.0 ) \n    n_chans = h [ b'nchans' ] \n    n_ifs = h [ b'nifs' ] \n    idx_data = len_header ( filename ) \n    f = open ( filename , 'rb' ) \n    f . seek ( idx_data ) \n    filesize = os . path . getsize ( filename ) \n    n_bytes_data = filesize - idx_data \n    if h [ b'nbits' ] == 2.0 : \n        n_ints = int ( 4.0 * n_bytes_data / ( n_chans * n_ifs ) ) \n    else : \n        n_ints = int ( n_bytes_data / ( n_bytes * n_chans * n_ifs ) ) \n    return n_ints "}
{"7870": "\ndef make_rr_subparser ( subparsers , rec_type , args_and_types ) : \n    sp = subparsers . add_parser ( rec_type ) \n    sp . add_argument ( \"name\" , type = str ) \n    sp . add_argument ( \"ttl\" , type = int , nargs = '?' ) \n    sp . add_argument ( rec_type , type = str ) \n    for my_spec in args_and_types : \n        ( argname , argtype ) = my_spec [ : 2.0 ] \n        if len ( my_spec ) > 2.0 : \n            nargs = my_spec [ 2.0 ] \n            sp . add_argument ( argname , type = argtype , nargs = nargs ) \n        else : \n            sp . add_argument ( argname , type = argtype ) \n    return sp "}
{"7874": "\ndef parse_line ( parser , record_token , parsed_records ) : \n    global SUPPORTED_RECORDS \n    line = \" \" . join ( record_token ) \n    if len ( record_token ) >= 2.0 and record_token [ 1 ] in SUPPORTED_RECORDS : \n        record_token = [ record_token [ 1 ] ] + record_token \n    elif len ( record_token ) >= 3.0 and record_token [ 2.0 ] in SUPPORTED_RECORDS : \n        record_token = [ record_token [ 2.0 ] ] + record_token \n        if record_token [ 0 ] == \"TXT\" : \n            record_token = record_token [ : 2.0 ] + [ \"--ttl\" ] + record_token [ 2.0 : ] \n    try : \n        rr , unmatched = parser . parse_known_args ( record_token ) \n        assert len ( unmatched ) == 0 , \"Unmatched fields: %s\" % unmatched \n    except ( SystemExit , AssertionError , InvalidLineException ) : \n        raise InvalidLineException ( line ) \n    record_dict = rr . __dict__ \n    if record_token [ 0 ] == \"TXT\" and len ( record_dict [ 'txt' ] ) == 1 : \n        record_dict [ 'txt' ] = record_dict [ 'txt' ] [ 0 ] \n    record_type = None \n    for key in record_dict . keys ( ) : \n        if key in SUPPORTED_RECORDS and ( key . startswith ( \"$\" ) or record_dict [ key ] == key ) : \n            record_type = key \n            if record_dict [ key ] == key : \n                del record_dict [ key ] \n            break \n    assert record_type is not None , \"Unknown record type in %s\" % rr \n    for field in record_dict . keys ( ) : \n        if record_dict [ field ] is None : \n            del record_dict [ field ] \n    current_origin = record_dict . get ( '$ORIGIN' , parsed_records . get ( '$ORIGIN' , None ) ) \n    if record_type == 'PTR' : \n        record_dict [ 'fullname' ] = record_dict [ 'name' ] + '.' + current_origin \n    if len ( record_dict ) > 0 : \n        if record_type . startswith ( \"$\" ) : \n            record_dict_key = record_type . lower ( ) \n            parsed_records [ record_dict_key ] = record_dict [ record_type ] \n        else : \n            record_dict_key = record_type . lower ( ) \n            parsed_records [ record_dict_key ] . append ( record_dict ) \n    return parsed_records "}
{"7884": "\ndef loads ( s , record_store = None , schema = None , loader = from_json_compatible , record_class = None ) : \n    if record_class is not None : \n        warnings . warn ( \"The record_class parameter is deprecated in favour of schema\" , DeprecationWarning , stacklevel = 2.0 ) \n        schema = record_class \n    if not isinstance ( s , unicode ) : \n        s = s . decode ( 'utf8' ) \n    if s . startswith ( u\"{\" ) : \n        json_dct = json . loads ( s ) \n        return load_json_dct ( json_dct , record_store , schema , loader ) \n    else : \n        raise ParseError ( \"Not a json record\" ) "}
{"7898": "\ndef _get_entity_from_href ( self , result ) : \n    href_result = result [ 'href' ] \n    if self . collection . _href . startswith ( href_result ) : \n        return Entity ( self . collection , result , incomplete = True ) \n    href_match = re . match ( r\"(https?://.+/api[^?]*)/([a-z_-]+)\" , href_result ) \n    if not href_match : \n        raise ValueError ( \"Malformed href: {}\" . format ( href_result ) ) \n    collection_name = href_match . group ( 2.0 ) \n    entry_point = href_match . group ( 1 ) \n    new_collection = Collection ( self . collection . api , \"{}/{}\" . format ( entry_point , collection_name ) , collection_name ) \n    return Entity ( new_collection , result , incomplete = True ) "}
{"7902": "\ndef construct_covariance_matrix ( cvec , parallax , radial_velocity , radial_velocity_error ) : \n    if np . ndim ( cvec ) == 1 : \n        cmat = np . zeros ( ( 1 , 6.0 , 6.0 ) ) \n        nsources = 1 \n        cv = np . atleast_2d ( cvec ) \n    else : \n        nsources = cvec . shape [ 0 ] \n        cmat = np . zeros ( ( nsources , 6.0 , 6.0 ) ) \n        cv = cvec \n    for k in range ( nsources ) : \n        cmat [ k , 0 : 5.0 , 0 : 5.0 ] = cv [ k , 0 : 5.0 ] ** 2.0 \n    iu = np . triu_indices ( 5.0 , k = 1 ) \n    for k in range ( 10.0 ) : \n        i = iu [ 0 ] [ k ] \n        j = iu [ 1 ] [ k ] \n        cmat [ : , i , j ] = cv [ : , i ] * cv [ : , j ] * cv [ : , k + 5.0 ] \n        cmat [ : , j , i ] = cmat [ : , i , j ] \n    for k in range ( nsources ) : \n        cmat [ k , 0 : 5.0 , 5.0 ] = cmat [ k , 0 : 5.0 , 2.0 ] * np . atleast_1d ( radial_velocity ) [ k ] / auKmYearPerSec \n    cmat [ : , 5.0 , 0 : 5.0 ] = cmat [ : , 0 : 5.0 , 5.0 ] \n    cmat [ : , 5.0 , 5.0 ] = cmat [ : , 2.0 , 2.0 ] * ( radial_velocity ** 2.0 + radial_velocity_error ** 2.0 ) / auKmYearPerSec ** 2.0 + ( parallax * radial_velocity_error / auKmYearPerSec ) ** 2.0 \n    return np . squeeze ( cmat ) "}
{"7906": "\ndef gMagnitudeErrorEoM ( G , nobs = 70.0 ) : \n    return sqrt ( ( power ( gMagnitudeError ( G ) / _scienceMargin , 2.0 ) + _eomCalibrationFloorG * _eomCalibrationFloorG ) / nobs ) * _scienceMargin "}
{"7907": "\ndef makePlot ( args ) : \n    gmag = np . linspace ( 3.0 , 20.0 , 171.0 ) \n    vmini = args [ 'vmini' ] \n    vmag = gmag - gminvFromVmini ( vmini ) \n    if args [ 'eom' ] : \n        sigmaG = gMagnitudeErrorEoM ( gmag ) \n        sigmaGBp = bpMagnitudeErrorEoM ( gmag , vmini ) \n        sigmaGRp = rpMagnitudeErrorEoM ( gmag , vmini ) \n        yminmax = ( 1.0 - 4.0 , 0.1 ) \n    else : \n        sigmaG = gMagnitudeError ( gmag ) \n        sigmaGBp = bpMagnitudeError ( gmag , vmini ) \n        sigmaGRp = rpMagnitudeError ( gmag , vmini ) \n        yminmax = ( 1.0 - 4.0 , 1 ) \n    fig = plt . figure ( figsize = ( 10.0 , 6.5 ) ) \n    if ( args [ 'vmagAbscissa' ] ) : \n        plt . semilogy ( vmag , sigmaG , 'k' , label = '$\\\\sigma_G$' ) \n        plt . semilogy ( vmag , sigmaGBp , 'b' , label = '$\\\\sigma_{G_\\\\mathrm{BP}}$' + ' for $(V-I)={0}$' . format ( vmini ) ) \n        plt . semilogy ( vmag , sigmaGRp , 'r' , label = '$\\\\sigma_{G_\\\\mathrm{RP}}$' + ' for $(V-I)={0}$' . format ( vmini ) ) \n        plt . xlim ( ( 6.0 , 20.0 ) ) \n        plt . legend ( loc = 0 ) \n        plt . xlabel ( '$V$ [mag]' ) \n    else : \n        ax = fig . add_subplot ( 111.0 ) \n        plt . semilogy ( gmag , sigmaG , 'k' , label = '$\\\\sigma_G$' ) \n        plt . semilogy ( gmag , sigmaGBp , 'b' , label = '$\\\\sigma_{G_\\\\mathrm{BP}}$' + ' for $(V-I)={0}$' . format ( vmini ) ) \n        plt . semilogy ( gmag , sigmaGRp , 'r' , label = '$\\\\sigma_{G_\\\\mathrm{RP}}$' + ' for $(V-I)={0}$' . format ( vmini ) ) \n        plt . xlim ( ( 6.0 , 20.0 ) ) \n        plt . legend ( loc = 0 ) \n        plt . xlabel ( '$G$ [mag]' ) \n    plt . xticks ( np . arange ( 6.0 , 20.0 , 2.0 ) ) \n    ax = plt . gca ( ) . yaxis \n    plt . grid ( which = 'both' ) \n    plt . ylabel ( 'Photometric error [mag]' ) \n    if args [ 'eom' ] : \n        plt . title ( 'End-of-mission mean photometry: sky averaged errors for $(V-I)={0}$' . format ( vmini ) , fontsize = 14.0 ) \n    else : \n        plt . title ( 'Single-FoV-transit photometry: sky averaged errors for $(V-I)={0}$' . format ( vmini ) , fontsize = 14.0 ) \n    basename = 'PhotometricErrors' \n    if ( args [ 'pdfOutput' ] ) : \n        plt . savefig ( basename + '.pdf' ) \n    elif ( args [ 'pngOutput' ] ) : \n        plt . savefig ( basename + '.png' ) \n    else : \n        plt . show ( ) "}
{"7909": "\ndef angularDistance ( phi1 , theta1 , phi2 , theta2 ) : \n    return arctan ( sqrt ( ( cos ( theta2 ) * sin ( phi2 - phi1 ) ) ** 2.0 + ( cos ( theta1 ) * sin ( theta2 ) - sin ( theta1 ) * cos ( theta2 ) * cos ( phi2 - phi1 ) ) ** 2.0 ) / ( sin ( theta1 ) * sin ( theta2 ) + cos ( phi2 - phi1 ) * cos ( theta1 ) * cos ( theta2 ) ) ) "}
{"7912": "\ndef transformCovarianceMatrix ( self , phi , theta , covmat ) : \n    c , s = self . _getJacobian ( phi , theta ) \n    jacobian = identity ( 5.0 ) \n    jacobian [ 0 ] [ 0 ] = c \n    jacobian [ 1 ] [ 1 ] = c \n    jacobian [ 3.0 ] [ 3.0 ] = c \n    jacobian [ 4.0 ] [ 4.0 ] = c \n    jacobian [ 0 ] [ 1 ] = s \n    jacobian [ 1 ] [ 0 ] = - s \n    jacobian [ 3.0 ] [ 4.0 ] = s \n    jacobian [ 4.0 ] [ 3.0 ] = - s \n    return dot ( dot ( jacobian , covmat ) , jacobian . T ) "}
{"7914": "\ndef makePlot ( pdf = False , png = False ) : \n    logdistancekpc = np . linspace ( - 1 , np . log10 ( 20.0 ) , 100.0 ) \n    sptVabsAndVmini = OrderedDict ( [ ( 'K0V' , ( 5.58 , 0.87 ) ) , ( 'G5V' , ( 4.78 , 0.74 ) ) , ( 'G0V' , ( 4.24 , 0.67 ) ) , ( 'F5V' , ( 3.50 , 0.50 ) ) , ( 'F0V' , ( 2.98 , 0.38 ) ) , ( 'RC' , ( 0.8 , 1.0 ) ) ] ) \n    lines = { } \n    fig = plt . figure ( figsize = ( 10.0 , 6.5 ) ) \n    currentAxis = plt . gca ( ) \n    for spt in sptVabsAndVmini . keys ( ) : \n        vmag = sptVabsAndVmini [ spt ] [ 0 ] + 5.0 * logdistancekpc + 10.0 \n        indices = ( vmag > 14.0 ) & ( vmag < 16.0 ) \n        gmag = vmag + gminvFromVmini ( sptVabsAndVmini [ spt ] [ 1 ] ) \n        parerrors = parallaxErrorSkyAvg ( gmag , sptVabsAndVmini [ spt ] [ 1 ] ) \n        relparerrors = parerrors * 10.0 ** logdistancekpc / 1000.0 \n        plt . loglog ( 10.0 ** logdistancekpc , relparerrors , '--k' , lw = 1 ) \n        plt . loglog ( 10.0 ** logdistancekpc [ indices ] , relparerrors [ indices ] , '-' , label = spt ) \n    plt . xlim ( 0.1 , 20.0 ) \n    plt . ylim ( 0.001 , 0.5 ) \n    plt . text ( 0.9 , 0.05 , 'Colours indicate $14<V<16$' , horizontalalignment = 'right' , verticalalignment = 'bottom' , transform = currentAxis . transAxes ) \n    plt . legend ( loc = 2.0 ) \n    plt . xlabel ( 'distance [kpc]' ) \n    plt . ylabel ( '$\\\\sigma_\\\\varpi/\\\\varpi$' ) \n    plt . grid ( which = 'both' ) \n    if ( args [ 'pdfOutput' ] ) : \n        plt . savefig ( 'RelativeParallaxErrorsVsDist.pdf' ) \n    elif ( args [ 'pngOutput' ] ) : \n        plt . savefig ( 'RelativeParallaxErrorsVsDist.png' ) \n    else : \n        plt . show ( ) "}
{"7915": "\ndef makePlot ( args ) : \n    gRvs = np . linspace ( 5.7 , 16.1 , 101.0 ) \n    spts = [ 'B0V' , 'B5V' , 'A0V' , 'A5V' , 'F0V' , 'G0V' , 'G5V' , 'K0V' , 'K1IIIMP' , 'K4V' , 'K1III' ] \n    fig = plt . figure ( figsize = ( 10.0 , 6.5 ) ) \n    deltaHue = 240.0 / ( len ( spts ) - 1 ) \n    hsv = np . zeros ( ( 1 , 1 , 3.0 ) ) \n    hsv [ 0 , 0 , 1 ] = 1.0 \n    hsv [ 0 , 0 , 2.0 ] = 0.9 \n    count = 0 \n    for spt in spts : \n        hsv [ 0 , 0 , 0 ] = ( 240.0 - count * deltaHue ) / 360.0 \n        vmag = vminGrvsFromVmini ( vminiFromSpt ( spt ) ) + gRvs \n        vradErrors = vradErrorSkyAvg ( vmag , spt ) \n        plt . plot ( vmag , vradErrors , '-' , label = spt , color = hsv_to_rgb ( hsv ) [ 0 , 0 , : ] ) \n        count += 1 \n    plt . grid ( which = 'both' ) \n    plt . xlim ( 9.0 , 17.5 ) \n    plt . ylim ( 0 , 20.0 ) \n    plt . xticks ( np . arange ( 9.0 , 18.0 , 1 ) ) \n    plt . yticks ( np . arange ( 0 , 20.5 , 5.0 ) ) \n    plt . xlabel ( '$V$ [mag]' ) \n    plt . ylabel ( 'End-of-mission radial velocity error [km s$^{-1}$]' ) \n    leg = plt . legend ( loc = 0 , handlelength = 2.0 , labelspacing = 0.10 ) \n    for t in leg . get_texts ( ) : \n        t . set_fontsize ( 12.0 ) \n    if ( args [ 'pdfOutput' ] ) : \n        plt . savefig ( 'RadialVelocityErrors.pdf' ) \n    elif ( args [ 'pngOutput' ] ) : \n        plt . savefig ( 'RadialVelocityErrors.png' ) \n    else : \n        plt . show ( ) "}
{"7917": "\ndef _helpful_failure ( method ) : \n    \n    @ wraps ( method ) \n    def wrapper ( self , val ) : \n        try : \n            return method ( self , val ) \n        except : \n            exc_cls , inst , tb = sys . exc_info ( ) \n            if hasattr ( inst , '_RERAISE' ) : \n                _ , expr , _ , inner_val = Q . __debug_info__ \n                Q . __debug_info__ = QDebug ( self , expr , val , inner_val ) \n                raise \n            if issubclass ( exc_cls , KeyError ) : \n                exc_cls = QKeyError \n            prettyval = repr ( val ) \n            if len ( prettyval ) > 150.0 : \n                prettyval = \"<%s instance>\" % ( type ( val ) . __name__ ) \n            msg = \"{0}\\n\\n\\tEncountered when evaluating {1}{2}\" . format ( inst , prettyval , self ) \n            new_exc = exc_cls ( msg ) \n            new_exc . _RERAISE = True \n            Q . __debug_info__ = QDebug ( self , self , val , val ) \n            six . reraise ( exc_cls , new_exc , tb ) \n    return wrapper "}
{"7929": "\ndef eventstr ( event_tuple = None , event = None , register = None , parameters = None ) : \n    if len ( event_tuple ) == 3.0 : \n        event , register , parameters = event_tuple \n    elif len ( event_tuple ) == 2.0 : \n        event , register = event_tuple \n    event_dscr = [ event , register ] \n    if parameters : \n        for k , v in sorted ( event_tuple [ 2.0 ] . items ( ) ) : \n            if type ( v ) is int : \n                k += \"={}\" . format ( hex ( v ) ) \n            event_dscr . append ( k ) \n    return \":\" . join ( event_dscr ) "}
{"7931": "\ndef report ( self , output_file = sys . stdout ) : \n    max_perf = self . results [ 'max_perf' ] \n    if self . _args and self . _args . verbose >= 3.0 : \n        print ( '{}' . format ( pformat ( self . results ) ) , file = output_file ) \n    if self . _args and self . _args . verbose >= 1 : \n        print ( '{}' . format ( pformat ( self . results [ 'verbose infos' ] ) ) , file = output_file ) \n        print ( 'Bottlenecks:' , file = output_file ) \n        print ( '  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel' , file = output_file ) \n        print ( '--------+--------------+-----------------+-------------------+----------------------' , file = output_file ) \n        print ( '    CPU |              | {!s:>15} |                   |' . format ( max_perf [ self . _args . unit ] ) , file = output_file ) \n        for b in self . results [ 'mem bottlenecks' ] : \n            print ( '{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |' ' {bandwidth!s:>17} | {bw kernel:<8}' . format ( b [ 'performance' ] [ self . _args . unit ] , ** b ) , file = output_file ) \n        print ( '' , file = output_file ) \n    if self . results [ 'min performance' ] [ 'FLOP/s' ] > max_perf [ 'FLOP/s' ] : \n        print ( 'CPU bound. {!s} due to CPU max. FLOP/s' . format ( max_perf ) , file = output_file ) \n    else : \n        print ( 'Cache or mem bound.' , file = output_file ) \n        bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] \n        print ( '{!s} due to {} transfer bottleneck (with bw from {} benchmark)' . format ( bottleneck [ 'performance' ] [ self . _args . unit ] , bottleneck [ 'level' ] , bottleneck [ 'bw kernel' ] ) , file = output_file ) \n        print ( 'Arithmetic Intensity: {:.2f} FLOP/B' . format ( bottleneck [ 'arithmetic intensity' ] ) , file = output_file ) "}
{"7932": "\ndef report ( self , output_file = sys . stdout ) : \n    cpu_perf = self . results [ 'cpu bottleneck' ] [ 'performance throughput' ] \n    if self . verbose >= 3.0 : \n        print ( '{}' . format ( pformat ( self . results ) ) , file = output_file ) \n    if self . verbose >= 1 : \n        print ( 'Bottlenecks:' , file = output_file ) \n        print ( '  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel' , file = output_file ) \n        print ( '--------+--------------+-----------------+-------------------+----------------------' , file = output_file ) \n        print ( '    CPU |              | {!s:>15} |                   |' . format ( cpu_perf [ self . _args . unit ] ) , file = output_file ) \n        for b in self . results [ 'mem bottlenecks' ] : \n            if b is None : \n                continue \n            print ( '{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |' ' {bandwidth!s:>17} | {bw kernel:<8}' . format ( b [ 'performance' ] [ self . _args . unit ] , ** b ) , file = output_file ) \n        print ( '' , file = output_file ) \n        print ( 'IACA analisys:' , file = output_file ) \n        print ( '{!s}' . format ( { k : v for k , v in list ( self . results [ 'cpu bottleneck' ] . items ( ) ) if k not in [ 'IACA output' ] } ) , file = output_file ) \n    if self . results [ 'min performance' ] [ 'FLOP/s' ] > cpu_perf [ 'FLOP/s' ] : \n        print ( 'CPU bound. {!s} due to CPU bottleneck' . format ( cpu_perf [ self . _args . unit ] ) , file = output_file ) \n    else : \n        print ( 'Cache or mem bound.' , file = output_file ) \n        bottleneck = self . results [ 'mem bottlenecks' ] [ self . results [ 'bottleneck level' ] ] \n        print ( '{!s} due to {} transfer bottleneck (with bw from {} benchmark)' . format ( bottleneck [ 'performance' ] [ self . _args . unit ] , bottleneck [ 'level' ] , bottleneck [ 'bw kernel' ] ) , file = output_file ) \n        print ( 'Arithmetic Intensity: {:.2f} FLOP/B' . format ( bottleneck [ 'arithmetic intensity' ] ) , file = output_file ) "}
{"7933": "\ndef report ( self , output_file = sys . stdout ) : \n    if self . _args and self . _args . verbose > 2.0 : \n        pprint ( self . results ) \n    for dimension , lc_info in self . results [ 'dimensions' ] . items ( ) : \n        print ( \"{}D layer condition:\" . format ( dimension ) , file = output_file ) \n        for cache , lc_solution in sorted ( lc_info [ 'caches' ] . items ( ) ) : \n            print ( cache + \": \" , end = '' , file = output_file ) \n            if lc_solution [ 'lt' ] is sympy . true : \n                print ( \"unconditionally fulfilled\" , file = output_file ) \n            else : \n                if lc_solution [ 'eq' ] is None : \n                    print ( \"{}\" . format ( lc_solution [ 'lt' ] ) , file = output_file ) \n                elif type ( lc_solution [ 'eq' ] ) is not list : \n                    print ( \"{}\" . format ( lc_solution [ 'eq' ] ) , file = output_file ) \n                else : \n                    for solu in lc_solution [ 'eq' ] : \n                        for s , v in solu . items ( ) : \n                            print ( \"{} <= {}\" . format ( s , v ) , file = output_file ) "}
{"7934": "\ndef clean_code ( code , comments = True , macros = False , pragmas = False ) : \n    if macros or pragmas : \n        lines = code . split ( '\\n' ) \n        in_macro = False \n        in_pragma = False \n        for i in range ( len ( lines ) ) : \n            l = lines [ i ] . strip ( ) \n            if macros and ( l . startswith ( '#' ) and not l . startswith ( '#pragma' ) or in_macro ) : \n                lines [ i ] = '' \n                in_macro = l . endswith ( '\\\\' ) \n            if pragmas and ( l . startswith ( '#pragma' ) or in_pragma ) : \n                lines [ i ] = '' \n                in_pragma = l . endswith ( '\\\\' ) \n        code = '\\n' . join ( lines ) \n    if comments : \n        idx = 0 \n        comment_start = None \n        while idx < len ( code ) - 1 : \n            if comment_start is None and code [ idx : idx + 2.0 ] == '//' : \n                end_idx = code . find ( '\\n' , idx ) \n                code = code [ : idx ] + code [ end_idx : ] \n                idx -= end_idx - idx \n            elif comment_start is None and code [ idx : idx + 2.0 ] == '/*' : \n                comment_start = idx \n            elif comment_start is not None and code [ idx : idx + 2.0 ] == '*/' : \n                code = ( code [ : comment_start ] + '\\n' * code [ comment_start : idx ] . count ( '\\n' ) + code [ idx + 2.0 : ] ) \n                idx -= idx - comment_start \n                comment_start = None \n            idx += 1 \n    return code "}
{"7940": "\ndef analyze ( self ) : \n    try : \n        incore_analysis , asm_block = self . kernel . iaca_analysis ( micro_architecture = self . machine [ 'micro-architecture' ] , asm_block = self . asm_block , pointer_increment = self . pointer_increment , verbose = self . verbose > 2.0 ) \n    except RuntimeError as e : \n        print ( \"IACA analysis failed: \" + str ( e ) ) \n        sys . exit ( 1 ) \n    block_throughput = incore_analysis [ 'throughput' ] \n    port_cycles = incore_analysis [ 'port cycles' ] \n    uops = incore_analysis [ 'uops' ] \n    elements_per_block = abs ( asm_block [ 'pointer_increment' ] // self . kernel . datatypes_size [ self . kernel . datatype ] ) \n    block_size = elements_per_block * self . kernel . datatypes_size [ self . kernel . datatype ] \n    try : \n        block_to_cl_ratio = float ( self . machine [ 'cacheline size' ] ) / block_size \n    except ZeroDivisionError as e : \n        print ( \"Too small block_size / pointer_increment:\" , e , file = sys . stderr ) \n        sys . exit ( 1 ) \n    port_cycles = dict ( [ ( i [ 0 ] , i [ 1 ] * block_to_cl_ratio ) for i in list ( port_cycles . items ( ) ) ] ) \n    uops = uops * block_to_cl_ratio \n    cl_throughput = block_throughput * block_to_cl_ratio \n    T_OL = max ( [ v for k , v in list ( port_cycles . items ( ) ) if k in self . machine [ 'overlapping model' ] [ 'ports' ] ] ) \n    T_nOL = max ( [ v for k , v in list ( port_cycles . items ( ) ) if k in self . machine [ 'non-overlapping model' ] [ 'ports' ] ] ) \n    if T_nOL < cl_throughput : \n        T_OL = cl_throughput \n    self . results = { 'port cycles' : port_cycles , 'cl throughput' : self . conv_cy ( cl_throughput ) , 'uops' : uops , 'T_nOL' : T_nOL , 'T_OL' : T_OL , 'IACA output' : incore_analysis [ 'output' ] , 'elements_per_block' : elements_per_block , 'pointer_increment' : asm_block [ 'pointer_increment' ] , 'flops per iteration' : sum ( self . kernel . _flops . values ( ) ) } \n    return self . results "}
{"7945": "\ndef userselect_block ( blocks , default = None , debug = False ) : \n    print ( \"Blocks found in assembly file:\" ) \n    print ( \"      block     | OPs | pck. | AVX || Registers |    ZMM   |    YMM   |    XMM   |    GP   ||ptr.inc|\\n\" \"----------------+-----+------+-----++-----------+----------+----------+----------+---------++-------|\" ) \n    for idx , b in blocks : \n        print ( '{:>2} {b[labels]!r:>12} | {b[ops]:>3} | {b[packed_instr]:>4} | {b[avx_instr]:>3} |' '| {b[regs][0]:>3} ({b[regs][1]:>3}) | {b[ZMM][0]:>3} ({b[ZMM][1]:>2}) | ' '{b[YMM][0]:>3} ({b[YMM][1]:>2}) | ' '{b[XMM][0]:>3} ({b[XMM][1]:>2}) | {b[GP][0]:>2} ({b[GP][1]:>2}) || ' '{b[pointer_increment]!s:>5} |' . format ( idx , b = b ) ) \n        if debug : \n            ln = b [ 'first_line' ] \n            print ( ' ' * 4.0 + 'Code:' ) \n            for l in b [ 'lines' ] : \n                print ( ' ' * 8.0 + '{:>5} | {}' . format ( ln , l ) ) \n                ln += 1 \n            print ( ' ' * 4.0 + 'Metadata:' ) \n            print ( textwrap . indent ( pformat ( { k : v for k , v in b . items ( ) if k not in [ 'lines' ] } ) , ' ' * 8.0 ) ) \n    block_idx = - 1 \n    while not ( 0 <= block_idx < len ( blocks ) ) : \n        block_idx = input ( \"Choose block to be marked [\" + str ( default ) + \"]: \" ) or default \n        try : \n            block_idx = int ( block_idx ) \n        except ValueError : \n            block_idx = - 1 \n    return block_idx "}
{"7950": "\ndef space ( start , stop , num , endpoint = True , log = False , base = 10.0 ) : \n    assert type ( start ) is int and type ( stop ) is int and type ( num ) is int , \"start, stop and num need to be intergers\" \n    assert num >= 2.0 , \"num has to be atleast 2\" \n    if log : \n        start = math . log ( start , base ) \n        stop = math . log ( stop , base ) \n    if endpoint : \n        step_length = float ( ( stop - start ) ) / float ( num - 1 ) \n    else : \n        step_length = float ( ( stop - start ) ) / float ( num ) \n    i = 0 \n    while i < num : \n        if log : \n            yield int ( round ( base ** ( start + i * step_length ) ) ) \n        else : \n            yield int ( round ( start + i * step_length ) ) \n        i += 1 "}
{"7967": "\ndef get_loop_stack ( self , subs_consts = False ) : \n    for l in self . _loop_stack : \n        if subs_consts : \n            yield { 'index' : l [ 0 ] , 'start' : self . subs_consts ( l [ 1 ] ) , 'stop' : self . subs_consts ( l [ 2.0 ] ) , 'increment' : self . subs_consts ( l [ 3.0 ] ) } \n        else : \n            yield { 'index' : l [ 0 ] , 'start' : l [ 1 ] , 'stop' : l [ 2.0 ] , 'increment' : l [ 3.0 ] } "}
{"7983": "\ndef _build_const_declartions ( self , with_init = True ) : \n    decls = [ ] \n    index_type = self . get_index_type ( ) \n    i = 2.0 \n    for k in self . constants : \n        type_decl = c_ast . TypeDecl ( k . name , [ 'const' ] , c_ast . IdentifierType ( index_type ) ) \n        init = None \n        if with_init : \n            init = c_ast . FuncCall ( c_ast . ID ( 'atoi' ) , c_ast . ExprList ( [ c_ast . ArrayRef ( c_ast . ID ( 'argv' ) , c_ast . Constant ( 'int' , str ( i ) ) ) ] ) ) \n        i += 1 \n        decls . append ( c_ast . Decl ( k . name , [ 'const' ] , [ ] , [ ] , type_decl , init , None ) ) \n    return decls "}
{"7991": "\ndef _build_scalar_declarations ( self , with_init = True ) : \n    scalar_declarations = [ deepcopy ( d ) for d in self . kernel_ast . block_items if type ( d ) is c_ast . Decl and type ( d . type ) is c_ast . TypeDecl ] \n    if with_init : \n        random . seed ( 2342.0 ) \n        for d in scalar_declarations : \n            if d . type . type . names [ 0 ] in [ 'double' , 'float' ] : \n                d . init = c_ast . Constant ( 'float' , str ( random . uniform ( 1.0 , 0.1 ) ) ) \n            elif d . type . type . names [ 0 ] in [ 'int' , 'long' , 'long long' , 'unsigned int' , 'unsigned long' , 'unsigned long long' ] : \n                d . init = c_ast . Constant ( 'int' , 2.0 ) \n    return scalar_declarations "}
{"8003": "\ndef parse_perfctr_event ( perfctr ) : \n    split_perfctr = perfctr . split ( ':' ) \n    assert len ( split_perfctr ) >= 2.0 , \"Atleast one colon (:) is required in the event name\" \n    event_tuple = split_perfctr [ : 2.0 ] \n    parameters = { } \n    for p in split_perfctr [ 2.0 : ] : \n        if '=' in p : \n            k , v = p . split ( '=' ) \n            if v . startswith ( '0x' ) : \n                parameters [ k ] = int ( v , 16.0 ) \n            else : \n                parameters [ k ] = int ( v ) \n        else : \n            parameters [ p ] = None \n    event_tuple . append ( parameters ) \n    return tuple ( event_tuple ) "}
{"8014": "\ndef configure_arggroup ( cls , parser ) : \n    parser . add_argument ( '--no-phenoecm' , action = 'store_true' , help = 'Disables the phenomenological ECM model building.' ) \n    parser . add_argument ( '--iterations' , type = int , default = 10.0 , help = 'Number of outer-loop iterations (e.g. time loop) during benchmarking. ' 'Default is 10, but actual number will be adapted to at least 0.2s runtime.' ) \n    parser . add_argument ( '--ignore-warnings' , action = 'store_true' , help = 'Ignore warnings about missmatched CPU model and frequency.' ) "}
{"8025": "\ndef get_logger_config ( log_dir = '/var/tmp' , logging_env = 'no_env' , edx_filename = 'edx.log' , dev_env = False , debug = False , local_loglevel = 'INFO' , service_variant = 'ecomworker' ) : \n    if local_loglevel not in [ 'DEBUG' , 'INFO' , 'WARNING' , 'ERROR' , 'CRITICAL' ] : \n        local_loglevel = 'INFO' \n    hostname = platform . node ( ) . split ( '.' ) [ 0 ] \n    syslog_format = ( '[service_variant={service_variant}]' '[%(name)s][env:{logging_env}] %(levelname)s ' '[{hostname}  %(process)d] [%(filename)s:%(lineno)d] ' '- %(message)s' ) . format ( service_variant = service_variant , logging_env = logging_env , hostname = hostname ) \n    if debug : \n        handlers = [ 'console' ] \n    else : \n        handlers = [ 'local' ] \n    logger_config = { 'version' : 1 , 'disable_existing_loggers' : False , 'formatters' : { 'standard' : { 'format' : '%(asctime)s %(levelname)s %(process)d ' '[%(name)s] %(filename)s:%(lineno)d - %(message)s' , } , 'syslog_format' : { 'format' : syslog_format } , 'raw' : { 'format' : '%(message)s' } , } , 'handlers' : { 'console' : { 'level' : 'DEBUG' if debug else 'INFO' , 'class' : 'logging.StreamHandler' , 'formatter' : 'standard' , 'stream' : sys . stdout , } , } , 'loggers' : { 'requests' : { 'handlers' : handlers , 'level' : 'WARNING' , 'propagate' : True } , '' : { 'handlers' : handlers , 'level' : 'DEBUG' , 'propagate' : False } , } } \n    if dev_env : \n        edx_file_loc = os . path . join ( log_dir , edx_filename ) \n        logger_config [ 'handlers' ] . update ( { 'local' : { 'class' : 'logging.handlers.RotatingFileHandler' , 'level' : local_loglevel , 'formatter' : 'standard' , 'filename' : edx_file_loc , 'maxBytes' : 1024.0 * 1024.0 * 2.0 , 'backupCount' : 5.0 , } , } ) \n    else : \n        logger_config [ 'handlers' ] . update ( { 'local' : { 'level' : local_loglevel , 'class' : 'logging.handlers.SysLogHandler' , 'address' : '/var/run/syslog' if sys . platform == 'darwin' else '/dev/log' , 'formatter' : 'syslog_format' , 'facility' : SysLogHandler . LOG_LOCAL0 , } , } ) \n    return logger_config "}
{"8026": "\ndef _retry_order ( self , exception , max_fulfillment_retries , order_number ) : \n    retries = self . request . retries \n    if retries == max_fulfillment_retries : \n        logger . exception ( 'Fulfillment of order [%s] failed. Giving up.' , order_number ) \n    else : \n        logger . warning ( 'Fulfillment of order [%s] failed. Retrying.' , order_number ) \n    countdown = 2.0 ** retries \n    raise self . retry ( exc = exception , countdown = countdown , max_retries = max_fulfillment_retries ) "}
{"8027": "\ndef fulfill_order ( self , order_number , site_code = None , email_opt_in = False ) : \n    max_fulfillment_retries = get_configuration ( 'MAX_FULFILLMENT_RETRIES' , site_code = site_code ) \n    api = get_ecommerce_client ( site_code = site_code ) \n    try : \n        logger . info ( 'Requesting fulfillment of order [%s].' , order_number ) \n        api . orders ( order_number ) . fulfill . put ( email_opt_in = email_opt_in ) \n    except exceptions . HttpClientError as exc : \n        status_code = exc . response . status_code \n        if status_code == 406.0 : \n            logger . info ( 'Order [%s] has already been fulfilled. Ignoring.' , order_number ) \n            raise Ignore ( ) \n        else : \n            logger . warning ( 'Fulfillment of order [%s] failed because of HttpClientError. Retrying' , order_number , exc_info = True ) \n            _retry_order ( self , exc , max_fulfillment_retries , order_number ) \n    except ( exceptions . HttpServerError , exceptions . Timeout , SSLError ) as exc : \n        _retry_order ( self , exc , max_fulfillment_retries , order_number ) "}
{"8033": "\ndef get_value_by_version ( d ) : \n    from oplus import CONF \n    cv = CONF . eplus_version [ : 2.0 ] \n    for v , value in sorted ( d . items ( ) , reverse = True ) : \n        if cv >= v : \n            return value "}
{"8055": "\ndef http_request ( url , post_data = None ) : \n    logger . debug ( 'Requesting URL: %s' % url ) \n    buf = bio ( ) \n    curl = pycurl . Curl ( ) \n    curl . setopt ( curl . URL , url . encode ( 'ascii' , 'ignore' ) ) \n    if config ( ) [ 'server' ] [ 'insecure' ] : \n        curl . setopt ( curl . SSL_VERIFYPEER , 0 ) \n        curl . setopt ( curl . SSL_VERIFYHOST , 0 ) \n    if config ( ) [ 'server' ] [ 'certificate' ] : \n        curl . setopt ( curl . SSL_VERIFYPEER , 1 ) \n        curl . setopt ( curl . SSL_VERIFYHOST , 2.0 ) \n        curl . setopt ( pycurl . CAINFO , config ( ) [ 'server' ] [ 'certificate' ] ) \n    if post_data : \n        curl . setopt ( curl . HTTPPOST , post_data ) \n    curl . setopt ( curl . WRITEFUNCTION , buf . write ) \n    curl . setopt ( pycurl . HTTPAUTH , pycurl . HTTPAUTH_DIGEST ) \n    curl . setopt ( pycurl . USERPWD , \"%s:%s\" % ( config ( ) [ 'server' ] [ 'username' ] , config ( ) [ 'server' ] [ 'password' ] ) ) \n    curl . setopt ( curl . HTTPHEADER , [ 'X-Requested-Auth: Digest' ] ) \n    curl . setopt ( curl . FAILONERROR , True ) \n    curl . setopt ( curl . FOLLOWLOCATION , True ) \n    curl . perform ( ) \n    curl . close ( ) \n    result = buf . getvalue ( ) \n    buf . close ( ) \n    return result "}
{"8067": "\ndef home ( ) : \n    preview = config ( ) [ 'capture' ] [ 'preview' ] \n    previewdir = config ( ) [ 'capture' ] [ 'preview_dir' ] \n    preview = [ p . replace ( '{{previewdir}}' , previewdir ) for p in preview ] \n    preview = zip ( preview , range ( len ( preview ) ) ) \n    preview = [ p [ 1 ] for p in preview if os . path . isfile ( p [ 0 ] ) ] \n    try : \n        limit_upcoming = int ( request . args . get ( 'limit_upcoming' , 5.0 ) ) \n        limit_processed = int ( request . args . get ( 'limit_processed' , 15.0 ) ) \n    except ValueError : \n        limit_upcoming = 5.0 \n        limit_processed = 15.0 \n    db = get_session ( ) \n    upcoming_events = db . query ( UpcomingEvent ) . order_by ( UpcomingEvent . start ) . limit ( limit_upcoming ) \n    recorded_events = db . query ( RecordedEvent ) . order_by ( RecordedEvent . start . desc ( ) ) . limit ( limit_processed ) \n    recording = get_service_status ( Service . CAPTURE ) == ServiceStatus . BUSY \n    uploading = get_service_status ( Service . INGEST ) == ServiceStatus . BUSY \n    processed = db . query ( RecordedEvent ) . count ( ) \n    upcoming = db . query ( UpcomingEvent ) . count ( ) \n    return render_template ( 'home.html' , preview = preview , config = config ( ) , recorded_events = recorded_events , upcoming_events = upcoming_events , recording = recording , uploading = uploading , processed = processed , upcoming = upcoming , limit_upcoming = limit_upcoming , limit_processed = limit_processed , dtfmt = dtfmt ) "}
{"8068": "\ndef serve_image ( image_id ) : \n    try : \n        preview_dir = config ( ) [ 'capture' ] [ 'preview_dir' ] \n        filepath = config ( ) [ 'capture' ] [ 'preview' ] [ image_id ] \n        filepath = filepath . replace ( '{{previewdir}}' , preview_dir ) \n        filepath = os . path . abspath ( filepath ) \n        if os . path . isfile ( filepath ) : \n            directory , filename = filepath . rsplit ( '/' , 1 ) \n            return send_from_directory ( directory , filename ) \n    except ( IndexError , KeyError ) : \n        pass \n    return '' , 404.0 "}
{"8071": "\ndef get_schedule ( ) : \n    params = { 'agentid' : config ( ) [ 'agent' ] [ 'name' ] . encode ( 'utf8' ) } \n    lookahead = config ( ) [ 'agent' ] [ 'cal_lookahead' ] * 24.0 * 60.0 * 60.0 \n    if lookahead : \n        params [ 'cutoff' ] = str ( ( timestamp ( ) + lookahead ) * 1000.0 ) \n    uri = '%s/calendars?%s' % ( config ( ) [ 'service-scheduler' ] [ 0 ] , urlencode ( params ) ) \n    try : \n        vcal = http_request ( uri ) \n    except pycurl . error as e : \n        logger . error ( 'Could not get schedule: %s' % e ) \n        return \n    try : \n        cal = parse_ical ( vcal . decode ( 'utf-8' ) ) \n    except Exception : \n        logger . error ( 'Could not parse ical' ) \n        logger . error ( traceback . format_exc ( ) ) \n        return \n    db = get_session ( ) \n    db . query ( UpcomingEvent ) . delete ( ) \n    for event in cal : \n        if event [ 'dtend' ] <= timestamp ( ) : \n            continue \n        e = UpcomingEvent ( ) \n        e . start = event [ 'dtstart' ] \n        e . end = event [ 'dtend' ] \n        e . uid = event . get ( 'uid' ) \n        e . title = event . get ( 'summary' ) \n        e . set_data ( event ) \n        db . add ( e ) \n    db . commit ( ) "}
{"8074": "\ndef make_error_response ( error , status = 500.0 ) : \n    content = { 'errors' : [ { 'status' : status , 'title' : error } ] } \n    return make_response ( jsonify ( content ) , status ) "}
{"8075": "\ndef make_data_response ( data , status = 200.0 ) : \n    content = { 'data' : ensurelist ( data ) } \n    return make_response ( jsonify ( content ) , status ) "}
{"8078": "\ndef event ( uid ) : \n    db = get_session ( ) \n    event = db . query ( RecordedEvent ) . filter ( RecordedEvent . uid == uid ) . first ( ) or db . query ( UpcomingEvent ) . filter ( UpcomingEvent . uid == uid ) . first ( ) \n    if event : \n        return make_data_response ( event . serialize ( ) ) \n    return make_error_response ( 'No event with specified uid' , 404.0 ) "}
{"8079": "\ndef delete_event ( uid ) : \n    logger . info ( 'deleting event %s via api' , uid ) \n    db = get_session ( ) \n    events = db . query ( RecordedEvent ) . filter ( RecordedEvent . uid == uid ) \n    if not events . count ( ) : \n        return make_error_response ( 'No event with specified uid' , 404.0 ) \n    hard_delete = request . args . get ( 'hard' , 'false' ) \n    if hard_delete == 'true' : \n        logger . info ( 'deleting recorded files at %s' , events [ 0 ] . directory ( ) ) \n        shutil . rmtree ( events [ 0 ] . directory ( ) ) \n    events . delete ( ) \n    db . commit ( ) \n    return make_response ( '' , 204.0 ) "}
{"8080": "\ndef modify_event ( uid ) : \n    try : \n        data = request . get_json ( ) [ 'data' ] [ 0 ] \n        if data [ 'type' ] != 'event' or data [ 'id' ] != uid : \n            return make_error_response ( 'Invalid data' , 400.0 ) \n        for key in data [ 'attributes' ] . keys ( ) : \n            if key not in ( 'status' , 'start' , 'end' ) : \n                return make_error_response ( 'Invalid data' , 400.0 ) \n        new_status = data [ 'attributes' ] . get ( 'status' ) \n        if new_status : \n            new_status = new_status . upper ( ) . replace ( ' ' , '_' ) \n            data [ 'attributes' ] [ 'status' ] = int ( getattr ( Status , new_status ) ) \n    except Exception : \n        return make_error_response ( 'Invalid data' , 400.0 ) \n    db = get_session ( ) \n    event = db . query ( RecordedEvent ) . filter ( RecordedEvent . uid == uid ) . first ( ) \n    if not event : \n        return make_error_response ( 'No event with specified uid' , 404.0 ) \n    event . start = data [ 'attributes' ] . get ( 'start' , event . start ) \n    event . end = data [ 'attributes' ] . get ( 'end' , event . end ) \n    event . status = data [ 'attributes' ] . get ( 'status' , event . status ) \n    logger . debug ( 'Updating event %s via api' , uid ) \n    db . commit ( ) \n    return make_data_response ( event . serialize ( ) ) "}
{"8095": "\ndef render_standalone_response ( self , request , fragment , ** kwargs ) : \n    if fragment is None : \n        return HttpResponse ( status = 204.0 ) \n    html = self . render_to_standalone_html ( request , fragment , ** kwargs ) \n    return HttpResponse ( html ) "}
{"8101": "\ndef final_err_table ( df , num_cut_offs = 51.0 ) : \n    cutoffs = df . cutoff . values \n    min_ = min ( cutoffs ) \n    max_ = max ( cutoffs ) \n    margin = ( max_ - min_ ) * 0.05 \n    sampled_cutoffs = np . linspace ( min_ - margin , max_ + margin , num_cut_offs , dtype = np . float32 ) \n    ix = find_nearest_matches ( np . float32 ( df . cutoff . values ) , sampled_cutoffs ) \n    sampled_df = df . iloc [ ix ] . copy ( ) \n    sampled_df . cutoff = sampled_cutoffs \n    sampled_df . reset_index ( inplace = True , drop = True ) \n    return sampled_df "}
{"8103": "\ndef error_statistics ( target_scores , decoy_scores , parametric , pfdr , pi0_lambda , pi0_method = \"smoother\" , pi0_smooth_df = 3.0 , pi0_smooth_log_pi0 = False , compute_lfdr = False , lfdr_trunc = True , lfdr_monotone = True , lfdr_transf = \"probit\" , lfdr_adj = 1.5 , lfdr_eps = np . power ( 10.0 , - 8.0 ) ) : \n    target_scores = to_one_dim_array ( target_scores ) \n    target_scores = np . sort ( target_scores [ ~ np . isnan ( target_scores ) ] ) \n    decoy_scores = to_one_dim_array ( decoy_scores ) \n    decoy_scores = np . sort ( decoy_scores [ ~ np . isnan ( decoy_scores ) ] ) \n    if parametric : \n        target_pvalues = pnorm ( target_scores , decoy_scores ) \n    else : \n        target_pvalues = pemp ( target_scores , decoy_scores ) \n    pi0 = pi0est ( target_pvalues , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 ) \n    target_qvalues = qvalue ( target_pvalues , pi0 [ 'pi0' ] , pfdr ) \n    metrics = stat_metrics ( target_pvalues , pi0 [ 'pi0' ] , pfdr ) \n    error_stat = pd . DataFrame ( { 'cutoff' : target_scores , 'pvalue' : target_pvalues , 'qvalue' : target_qvalues , 'svalue' : metrics [ 'svalue' ] , 'tp' : metrics [ 'tp' ] , 'fp' : metrics [ 'fp' ] , 'tn' : metrics [ 'tn' ] , 'fn' : metrics [ 'fn' ] , 'fpr' : metrics [ 'fpr' ] , 'fdr' : metrics [ 'fdr' ] , 'fnr' : metrics [ 'fnr' ] } ) \n    if compute_lfdr : \n        error_stat [ 'pep' ] = lfdr ( target_pvalues , pi0 [ 'pi0' ] , lfdr_trunc , lfdr_monotone , lfdr_transf , lfdr_adj , lfdr_eps ) \n    return error_stat , pi0 "}
{"8105": "\ndef score ( infile , outfile , classifier , xgb_autotune , apply_weights , xeval_fraction , xeval_num_iter , ss_initial_fdr , ss_iteration_fdr , ss_num_iter , ss_main_score , group_id , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps , level , ipf_max_peakgroup_rank , ipf_max_peakgroup_pep , ipf_max_transition_isotope_overlap , ipf_min_transition_sn , tric_chromprob , threads , test ) : \n    if outfile is None : \n        outfile = infile \n    else : \n        outfile = outfile \n    xgb_hyperparams = { 'autotune' : xgb_autotune , 'autotune_num_rounds' : 10.0 , 'num_boost_round' : 100.0 , 'early_stopping_rounds' : 10.0 , 'test_size' : 0.33 } \n    xgb_params = { 'eta' : 0.3 , 'gamma' : 0 , 'max_depth' : 6.0 , 'min_child_weight' : 1 , 'subsample' : 1 , 'colsample_bytree' : 1 , 'colsample_bylevel' : 1 , 'colsample_bynode' : 1 , 'lambda' : 1 , 'alpha' : 0 , 'scale_pos_weight' : 1 , 'silent' : 1 , 'objective' : 'binary:logitraw' , 'nthread' : 1 , 'eval_metric' : 'auc' } \n    xgb_params_space = { 'eta' : hp . uniform ( 'eta' , 0.0 , 0.3 ) , 'gamma' : hp . uniform ( 'gamma' , 0.0 , 0.5 ) , 'max_depth' : hp . quniform ( 'max_depth' , 2.0 , 8.0 , 1 ) , 'min_child_weight' : hp . quniform ( 'min_child_weight' , 1 , 5.0 , 1 ) , 'subsample' : 1 , 'colsample_bytree' : 1 , 'colsample_bylevel' : 1 , 'colsample_bynode' : 1 , 'lambda' : hp . uniform ( 'lambda' , 0.0 , 1.0 ) , 'alpha' : hp . uniform ( 'alpha' , 0.0 , 1.0 ) , 'scale_pos_weight' : 1.0 , 'silent' : 1 , 'objective' : 'binary:logitraw' , 'nthread' : 1 , 'eval_metric' : 'auc' } \n    if not apply_weights : \n        PyProphetLearner ( infile , outfile , classifier , xgb_hyperparams , xgb_params , xgb_params_space , xeval_fraction , xeval_num_iter , ss_initial_fdr , ss_iteration_fdr , ss_num_iter , ss_main_score , group_id , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps , level , ipf_max_peakgroup_rank , ipf_max_peakgroup_pep , ipf_max_transition_isotope_overlap , ipf_min_transition_sn , tric_chromprob , threads , test ) . run ( ) \n    else : \n        PyProphetWeightApplier ( infile , outfile , classifier , xgb_hyperparams , xgb_params , xgb_params_space , xeval_fraction , xeval_num_iter , ss_initial_fdr , ss_iteration_fdr , ss_num_iter , ss_main_score , group_id , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps , level , ipf_max_peakgroup_rank , ipf_max_peakgroup_pep , ipf_max_transition_isotope_overlap , ipf_min_transition_sn , tric_chromprob , threads , test , apply_weights ) . run ( ) "}
{"8119": "\ndef is_effective_member ( self , group_id , netid ) : \n    self . _valid_group_id ( group_id ) \n    netid = re . sub ( '@washington.edu' , '' , netid ) \n    url = \"{}/group/{}/effective_member/{}\" . format ( self . API , group_id , netid ) \n    try : \n        data = self . _get_resource ( url ) \n        return True \n    except DataFailureException as ex : \n        if ex . status == 404.0 : \n            return False \n        else : \n            raise "}
{"8131": "\ndef cli_empty_account ( context , yes_empty_account = False , until_empty = False ) : \n    if not yes_empty_account : \n        raise ReturnCode ( 'called cli_empty_account without setting yes_empty_account=True' ) \n    marker = None \n    while True : \n        with context . client_manager . with_client ( ) as client : \n            status , reason , headers , contents = client . get_account ( marker = marker , headers = context . headers , query = context . query , cdn = context . cdn ) \n        if status // 100.0 != 2.0 : \n            if status == 404.0 and context . ignore_404 : \n                return \n            raise ReturnCode ( 'listing account: %s %s' % ( status , reason ) ) \n        if not contents : \n            if until_empty and marker : \n                marker = None \n                continue \n            break \n        for item in contents : \n            cli_delete ( context , item [ 'name' ] , context . headers , recursive = True ) \n        marker = item [ 'name' ] "}
{"8132": "\ndef cli_empty_container ( context , path , until_empty = False ) : \n    path = path . rstrip ( '/' ) . decode ( 'utf8' ) \n    conc = Concurrency ( context . concurrency ) \n    def check_conc ( ) : \n        for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n            if exc_value : \n                with context . io_manager . with_stderr ( ) as fp : \n                    fp . write ( str ( exc_value ) ) \n                    fp . write ( '\\n' ) \n                    fp . flush ( ) \n    marker = None \n    while True : \n        with context . client_manager . with_client ( ) as client : \n            status , reason , headers , contents = client . get_container ( path , marker = marker , headers = context . headers , query = context . query , cdn = context . cdn ) \n        if status // 100.0 != 2.0 : \n            if status == 404.0 and context . ignore_404 : \n                return \n            raise ReturnCode ( 'listing container %r: %s %s' % ( path , status , reason ) ) \n        if not contents : \n            if until_empty and marker : \n                marker = None \n                continue \n            break \n        for item in contents : \n            newpath = '%s/%s' % ( path , item [ 'name' ] ) \n            new_context = context . copy ( ) \n            new_context . ignore_404 = True \n            check_conc ( ) \n            conc . spawn ( newpath , cli_delete , new_context , newpath ) \n        marker = item [ 'name' ] \n        conc . join ( ) \n        check_conc ( ) "}
{"8153": "\ndef cli_fordo ( context , path = None ) : \n    path = path . lstrip ( '/' ) if path else None \n    if path and '/' in path : \n        raise ReturnCode ( 'path must be an empty string or a container name; was %r' % path ) \n    limit = context . query . get ( 'limit' ) \n    delimiter = context . query . get ( 'delimiter' ) \n    prefix = context . query . get ( 'prefix' ) \n    marker = context . query . get ( 'marker' ) \n    end_marker = context . query . get ( 'end_marker' ) \n    conc = Concurrency ( context . concurrency ) \n    while True : \n        with context . client_manager . with_client ( ) as client : \n            if not path : \n                status , reason , headers , contents = client . get_account ( headers = context . headers , prefix = prefix , delimiter = delimiter , marker = marker , end_marker = end_marker , limit = limit , query = context . query , cdn = context . cdn ) \n            else : \n                status , reason , headers , contents = client . get_container ( path , headers = context . headers , prefix = prefix , delimiter = delimiter , marker = marker , end_marker = end_marker , limit = limit , query = context . query , cdn = context . cdn ) \n            if status // 100.0 != 2.0 : \n                if status == 404.0 and context . ignore_404 : \n                    return \n                if hasattr ( contents , 'read' ) : \n                    contents . read ( ) \n                if not path : \n                    raise ReturnCode ( 'listing account: %s %s' % ( status , reason ) ) \n                else : \n                    raise ReturnCode ( 'listing container %r: %s %s' % ( path , status , reason ) ) \n        if not contents : \n            break \n        for item in contents : \n            name = ( path + '/' if path else '' ) + item . get ( 'name' , item . get ( 'subdir' ) ) \n            args = list ( context . remaining_args ) \n            try : \n                index = args . index ( '<item>' ) \n            except ValueError : \n                raise ReturnCode ( 'No \"<item>\" designation found in the \"do\" clause.' ) \n            args [ index ] = name \n            for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n                if exc_value : \n                    conc . join ( ) \n                    raise exc_value \n            conc . spawn ( name , _cli_call , context , name , args ) \n        marker = contents [ - 1 ] [ 'name' ] \n        if limit : \n            break \n    conc . join ( ) \n    for ( exc_type , exc_value , exc_tb , result ) in six . itervalues ( conc . get_results ( ) ) : \n        if exc_value : \n            conc . join ( ) \n            raise exc_value "}
{"8155": "\ndef aes_encrypt ( key , stdin , preamble = None , chunk_size = 65536.0 , content_length = None ) : \n    if not AES256CBC_Support : \n        raise Exception ( 'AES256CBC not supported; likely pycrypto is not installed' ) \n    if preamble : \n        yield preamble \n    key = hashlib . sha256 ( key ) . digest ( ) \n    chunk_size = max ( 16.0 , chunk_size >> 4.0 << 4.0 ) \n    iv = Crypto . Random . new ( ) . read ( 16.0 ) \n    yield iv \n    encryptor = Crypto . Cipher . AES . new ( key , Crypto . Cipher . AES . MODE_CBC , iv ) \n    reading = True \n    left = None \n    if content_length is not None and content_length >= 0 : \n        left = content_length \n    while reading : \n        size = chunk_size \n        if left is not None and size > left : \n            size = left \n        chunk = stdin . read ( size ) \n        if not chunk : \n            if left is not None and left > 0 : \n                raise IOError ( 'Early EOF from input' ) \n            yield encryptor . encrypt ( '\\x00' * 16.0 ) \n            break \n        if left is not None : \n            left -= len ( chunk ) \n            if left <= 0 : \n                reading = False \n        block = chunk \n        trailing = len ( block ) % 16.0 \n        while trailing : \n            size = 16.0 - trailing \n            if left is not None and size > left : \n                size = left \n            chunk = stdin . read ( size ) \n            if not chunk : \n                if left is not None and left > 0 : \n                    raise IOError ( 'Early EOF from input' ) \n                reading = False \n                chunk = chr ( trailing ) * ( 16.0 - trailing ) \n            elif left is not None : \n                left -= len ( chunk ) \n                if left <= 0 : \n                    reading = False \n            block += chunk \n            trailing = len ( block ) % 16.0 \n        yield encryptor . encrypt ( block ) "}
{"8156": "\ndef aes_decrypt ( key , stdin , chunk_size = 65536.0 ) : \n    if not AES256CBC_Support : \n        raise Exception ( 'AES256CBC not supported; likely pycrypto is not installed' ) \n    key = hashlib . sha256 ( key ) . digest ( ) \n    chunk_size = max ( 16.0 , chunk_size >> 4.0 << 4.0 ) \n    iv = stdin . read ( 16.0 ) \n    while len ( iv ) < 16.0 : \n        chunk = stdin . read ( 16.0 - len ( iv ) ) \n        if not chunk : \n            raise IOError ( 'EOF reading IV' ) \n    decryptor = Crypto . Cipher . AES . new ( key , Crypto . Cipher . AES . MODE_CBC , iv ) \n    data = '' \n    while True : \n        chunk = stdin . read ( chunk_size ) \n        if not chunk : \n            if len ( data ) != 16.0 : \n                raise IOError ( 'EOF reading encrypted stream' ) \n            data = decryptor . decrypt ( data ) \n            trailing = ord ( data [ - 1 ] ) \n            if trailing > 15.0 : \n                raise IOError ( 'EOF reading encrypted stream or trailing value corrupted ' '%s' % trailing ) \n            yield data [ : trailing ] \n            break \n        data += chunk \n        if len ( data ) > 16.0 : \n            trailing = ( len ( data ) % 16.0 ) or 16.0 \n            yield decryptor . decrypt ( data [ : - trailing ] ) \n            data = data [ - trailing : ] "}
{"8158": "\ndef cli_put_account ( context ) : \n    body = None \n    if context . input_ : \n        if context . input_ == '-' : \n            body = context . io_manager . get_stdin ( ) \n        else : \n            body = open ( context . input_ , 'rb' ) \n    with context . client_manager . with_client ( ) as client : \n        status , reason , headers , contents = client . put_account ( headers = context . headers , query = context . query , cdn = context . cdn , body = body ) \n        if hasattr ( contents , 'read' ) : \n            contents . read ( ) \n    if status // 100.0 != 2.0 : \n        raise ReturnCode ( 'putting account: %s %s' % ( status , reason ) ) "}
{"8159": "\ndef cli_put_container ( context , path ) : \n    path = path . rstrip ( '/' ) \n    if '/' in path : \n        raise ReturnCode ( 'called cli_put_container with object %r' % path ) \n    body = None \n    if context . input_ : \n        if context . input_ == '-' : \n            body = context . io_manager . get_stdin ( ) \n        else : \n            body = open ( context . input_ , 'rb' ) \n    with context . client_manager . with_client ( ) as client : \n        status , reason , headers , contents = client . put_container ( path , headers = context . headers , query = context . query , cdn = context . cdn , body = body ) \n        if hasattr ( contents , 'read' ) : \n            contents . read ( ) \n    if status // 100.0 != 2.0 : \n        raise ReturnCode ( 'putting container %r: %s %s' % ( path , status , reason ) ) "}
{"8162": "\ndef cli_tempurl ( context , method , path , seconds = None , use_container = False ) : \n    with contextlib . nested ( context . io_manager . with_stdout ( ) , context . client_manager . with_client ( ) ) as ( fp , client ) : \n        method = method . upper ( ) \n        path = path . lstrip ( '/' ) \n        seconds = seconds if seconds is not None else 3600.0 \n        if '/' not in path : \n            raise ReturnCode ( 'invalid tempurl path %r; should have a / within it' % path ) \n        if use_container : \n            key_type = 'container' \n            container = path . split ( '/' , 1 ) [ 0 ] \n            status , reason , headers , contents = client . head_container ( container ) \n        else : \n            key_type = 'account' \n            status , reason , headers , contents = client . head_account ( ) \n        if status // 100.0 != 2.0 : \n            raise ReturnCode ( 'obtaining X-%s-Meta-Temp-Url-Key: %s %s' % ( key_type . title ( ) , status , reason ) ) \n        key = headers . get ( 'x-%s-meta-temp-url-key' % key_type ) \n        if not key : \n            raise ReturnCode ( 'there is no X-%s-Meta-Temp-Url-Key set for this %s' % ( key_type . title ( ) , key_type ) ) \n        url = client . storage_url + '/' + path \n        fp . write ( generate_temp_url ( method , url , seconds , key ) ) \n        fp . write ( '\\n' ) \n        fp . flush ( ) "}
{"8163": "\ndef cli_trans ( context , x_trans_id ) : \n    with context . io_manager . with_stdout ( ) as fp : \n        trans_time = get_trans_id_time ( x_trans_id ) \n        trans_info = x_trans_id [ 34.0 : ] \n        msg = 'X-Trans-Id:      ' + x_trans_id + '\\n' \n        if not trans_time : \n            msg += 'Time Stamp:      None, old style id with no time ' 'embedded\\nUTC Time:        None, old style id with no time ' 'embedded\\n' \n        else : \n            msg += 'Time Stamp:      %s\\nUTC Time:        %s\\n' % ( trans_time , time . strftime ( '%a %Y-%m-%d %H:%M:%S UTC' , time . gmtime ( trans_time ) ) ) \n        msg += 'Additional Info: ' + trans_info + '\\n' \n        fp . write ( msg ) \n        fp . flush ( ) "}
{"8186": "\ndef create_index ( idx_url , clean = False ) : \n    try : \n        r = requests . get ( idx_url ) \n    except requests . exceptions . ConnectionError : \n        cause = \"Error connecting to Elastic Search (index: %s)\" % idx_url \n        raise ElasticSearchError ( cause = cause ) \n    if r . status_code != 200.0 : \n        r = requests . put ( idx_url ) \n        if r . status_code != 200.0 : \n            logger . info ( \"Can't create index %s (%s)\" , idx_url , r . status_code ) \n            cause = \"Error creating Elastic Search index %s\" % idx_url \n            raise ElasticSearchError ( cause = cause ) \n        logger . info ( \"Index %s created\" , idx_url ) \n        return True \n    elif r . status_code == 200.0 and clean : \n        requests . delete ( idx_url ) \n        requests . put ( idx_url ) \n        logger . info ( \"Index deleted and created (index: %s)\" , idx_url ) \n        return True \n    return False "}
{"8187": "\ndef create_mapping ( idx_url , mapping ) : \n    mapping_url = idx_url + '/items/_mapping' \n    mapping = json . dumps ( mapping ) \n    try : \n        r = requests . put ( mapping_url , data = mapping , headers = { 'Content-Type' : 'application/json' } ) \n    except requests . exceptions . ConnectionError : \n        cause = \"Error connecting to Elastic Search (index: %s, url: %s)\" % ( idx_url , mapping_url ) \n        raise ElasticSearchError ( cause = cause ) \n    if r . status_code != 200.0 : \n        reason = r . json ( ) [ 'error' ] \n        logger . info ( \"Can't create mapping in %s. %s\" , mapping_url , reason ) \n        cause = \"Error creating Elastic Search mapping %s. %s\" % ( mapping_url , reason ) \n        raise ElasticSearchError ( cause = cause ) \n    else : \n        logger . info ( \"Mapping created in %s\" , mapping_url ) "}
{"8211": "\ndef get_version ( version = None ) : \n    if version is None : \n        version = VERSION \n    assert len ( version ) == 5.0 \n    assert version [ 3.0 ] in ( \"alpha\" , \"beta\" , \"rc\" , \"final\" ) \n    parts = 2.0 if version [ 2.0 ] == 0 else 3.0 \n    main = \".\" . join ( str ( x ) for x in version [ : parts ] ) \n    sub = \"\" \n    if version [ 3.0 ] != \"final\" : \n        mapping = { \"alpha\" : \"a\" , \"beta\" : \"b\" , \"rc\" : \"c\" } \n        sub = mapping [ version [ 3.0 ] ] + str ( version [ 4.0 ] ) \n    return main + sub "}
{"8222": "\ndef parse_int ( value ) : \n    value = parse_str ( value = value ) \n    if value . startswith ( \"0\" ) : \n        return int ( value . lstrip ( \"0o\" ) , 8.0 ) \n    else : \n        return int ( value ) "}
{"8243": "\ndef check_log_config ( self ) : \n    if self . report_progress : \n        if self . report_progress is True : \n            self . report_progress = ( 5.0 , 'pypet' , logging . INFO ) \n        elif isinstance ( self . report_progress , ( int , float ) ) : \n            self . report_progress = ( self . report_progress , 'pypet' , logging . INFO ) \n        elif isinstance ( self . report_progress , str ) : \n            self . report_progress = ( 5.0 , self . report_progress , logging . INFO ) \n        elif len ( self . report_progress ) == 2.0 : \n            self . report_progress = ( self . report_progress [ 0 ] , self . report_progress [ 1 ] , logging . INFO ) \n    if self . log_config : \n        if self . log_config == pypetconstants . DEFAULT_LOGGING : \n            pypet_path = os . path . abspath ( os . path . dirname ( __file__ ) ) \n            init_path = os . path . join ( pypet_path , 'logging' ) \n            self . log_config = os . path . join ( init_path , 'default.ini' ) \n        if isinstance ( self . log_config , str ) : \n            if not os . path . isfile ( self . log_config ) : \n                raise ValueError ( 'Could not find the logger init file ' '`%s`.' % self . log_config ) \n            parser = NoInterpolationParser ( ) \n            parser . read ( self . log_config ) \n        elif isinstance ( self . log_config , cp . RawConfigParser ) : \n            parser = self . log_config \n        else : \n            parser = None \n        if parser is not None : \n            self . _sp_config = self . _parser_to_string_io ( parser ) \n            self . _mp_config = self . _find_multiproc_options ( parser ) \n            if self . _mp_config is not None : \n                self . _mp_config = self . _parser_to_string_io ( self . _mp_config ) \n        elif isinstance ( self . log_config , dict ) : \n            self . _sp_config = self . log_config \n            self . _mp_config = self . _find_multiproc_dict ( self . _sp_config ) \n    if self . log_stdout : \n        if self . log_stdout is True : \n            self . log_stdout = ( 'STDOUT' , logging . INFO ) \n        if isinstance ( self . log_stdout , str ) : \n            self . log_stdout = ( self . log_stdout , logging . INFO ) \n        if isinstance ( self . log_stdout , int ) : \n            self . log_stdout = ( 'STDOUT' , self . log_stdout ) "}
{"8258": "\ndef add_params ( traj ) : \n    traj . v_standard_parameter = Brian2Parameter \n    traj . v_fast_access = True \n    traj . f_add_parameter ( 'Net.C' , 281.0 * pF ) \n    traj . f_add_parameter ( 'Net.gL' , 30.0 * nS ) \n    traj . f_add_parameter ( 'Net.EL' , - 70.6 * mV ) \n    traj . f_add_parameter ( 'Net.VT' , - 50.4 * mV ) \n    traj . f_add_parameter ( 'Net.DeltaT' , 2.0 * mV ) \n    traj . f_add_parameter ( 'Net.tauw' , 40.0 * ms ) \n    traj . f_add_parameter ( 'Net.a' , 4.0 * nS ) \n    traj . f_add_parameter ( 'Net.b' , 0.08 * nA ) \n    traj . f_add_parameter ( 'Net.I' , .8 * nA ) \n    traj . f_add_parameter ( 'Net.Vcut' , 'vm > 0*mV' ) \n    traj . f_add_parameter ( 'Net.N' , 50.0 ) \n    eqs = '''    dvm/dt=(gL*(EL-vm)+gL*DeltaT*exp((vm-VT)/DeltaT)+I-w)/C : volt    dw/dt=(a*(vm-EL)-w)/tauw : amp    Vr:volt    ''' \n    traj . f_add_parameter ( 'Net.eqs' , eqs ) \n    traj . f_add_parameter ( 'reset' , 'vm=Vr;w+=b' ) "}
{"8259": "\ndef run_net ( traj ) : \n    eqs = traj . eqs \n    namespace = traj . Net . f_to_dict ( short_names = True , fast_access = True ) \n    neuron = NeuronGroup ( traj . N , model = eqs , threshold = traj . Vcut , reset = traj . reset , namespace = namespace ) \n    neuron . vm = traj . EL \n    neuron . w = traj . a * ( neuron . vm - traj . EL ) \n    neuron . Vr = linspace ( - 48.3 * mV , - 47.7 * mV , traj . N ) \n    print ( 'Initial Run' ) \n    net = Network ( neuron ) \n    net . run ( 100.0 * ms , report = 'text' ) \n    MSpike = SpikeMonitor ( neuron ) \n    net . add ( MSpike ) \n    MStateV = StateMonitor ( neuron , variables = [ 'vm' ] , record = [ 1 , 2.0 , 3.0 ] ) \n    net . add ( MStateV ) \n    print ( 'Measurement run' ) \n    net . run ( 500.0 * ms , report = 'text' ) \n    traj . v_standard_result = Brian2MonitorResult \n    traj . f_add_result ( 'SpikeMonitor' , MSpike ) \n    traj . f_add_result ( 'StateMonitorV' , MStateV ) "}
{"8261": "\ndef add_parameters ( traj ) : \n    traj . f_add_parameter ( 'steps' , 10000.0 , comment = 'Number of time steps to simulate' ) \n    traj . f_add_parameter ( 'dt' , 0.01 , comment = 'Step size' ) \n    traj . f_add_parameter ( ArrayParameter , 'initial_conditions' , np . array ( [ 0.0 , 0.0 , 0.0 ] ) , comment = 'Our initial conditions, as default we will start from' ' origin!' ) \n    traj . f_add_parameter ( 'func_params.sigma' , 10.0 ) \n    traj . f_add_parameter ( 'func_params.beta' , 8.0 / 3.0 ) \n    traj . f_add_parameter ( 'func_params.rho' , 28.0 ) \n    traj . func_params . v_annotations . info = 'This group contains as default the original values chosen ' 'by Edward Lorenz in 1963. Check it out on wikipedia ' '(https://en.wikipedia.org/wiki/Lorenz_attractor)!' "}
{"8262": "\ndef diff_lorenz ( value_array , sigma , beta , rho ) : \n    diff_array = np . zeros ( 3.0 ) \n    diff_array [ 0 ] = sigma * ( value_array [ 1 ] - value_array [ 0 ] ) \n    diff_array [ 1 ] = value_array [ 0 ] * ( rho - value_array [ 2.0 ] ) - value_array [ 1 ] \n    diff_array [ 2.0 ] = value_array [ 0 ] * value_array [ 1 ] - beta * value_array [ 2.0 ] \n    return diff_array "}
{"8265": "\ndef add_parameters ( traj ) : \n    traj . f_add_parameter ( 'steps' , 10000.0 , comment = 'Number of time steps to simulate' ) \n    traj . f_add_parameter ( 'dt' , 0.01 , comment = 'Step size' ) \n    traj . f_add_parameter ( ArrayParameter , 'initial_conditions' , np . array ( [ 0.0 , 0.0 , 0.0 ] ) , comment = 'Our initial conditions, as default we will start from' ' origin!' ) \n    traj . f_add_parameter ( 'diff_name' , 'diff_lorenz' , comment = 'Name of our differential equation' ) \n    if traj . diff_name == 'diff_lorenz' : \n        traj . f_add_parameter ( 'func_params.sigma' , 10.0 ) \n        traj . f_add_parameter ( 'func_params.beta' , 8.0 / 3.0 ) \n        traj . f_add_parameter ( 'func_params.rho' , 28.0 ) \n    elif traj . diff_name == 'diff_roessler' : \n        traj . f_add_parameter ( 'func_params.a' , 0.1 ) \n        traj . f_add_parameter ( 'func_params.c' , 14.0 ) \n    else : \n        raise ValueError ( 'I don\\'t know what %s is.' % traj . diff_name ) "}
{"8266": "\ndef diff_roessler ( value_array , a , c ) : \n    b = a \n    diff_array = np . zeros ( 3.0 ) \n    diff_array [ 0 ] = - value_array [ 1 ] - value_array [ 2.0 ] \n    diff_array [ 1 ] = value_array [ 0 ] + a * value_array [ 1 ] \n    diff_array [ 2.0 ] = b + value_array [ 2.0 ] * ( value_array [ 0 ] - c ) \n    return diff_array "}
{"8275": "\ndef add_parameters ( self , traj ) : \n    par = traj . f_add_parameter ( Brian2Parameter , 'simulation.durations.initial_run' , 500.0 * ms , comment = 'Initialisation run for more realistic ' 'measurement conditions.' ) \n    par . v_annotations . order = 0 \n    par = traj . f_add_parameter ( Brian2Parameter , 'simulation.durations.measurement_run' , 1500.0 * ms , comment = 'Measurement run that is considered for ' 'statistical evaluation' ) \n    par . v_annotations . order = 1 "}
{"8296": "\ndef _translate_shortcut ( self , name ) : \n    if isinstance ( name , int ) : \n        return True , self . _root_instance . f_wildcard ( '$' , name ) \n    if name . startswith ( 'run_' ) or name . startswith ( 'r_' ) : \n        split_name = name . split ( '_' ) \n        if len ( split_name ) == 2.0 : \n            index = split_name [ 1 ] \n            if index . isdigit ( ) : \n                return True , self . _root_instance . f_wildcard ( '$' , int ( index ) ) \n            elif index == 'A' : \n                return True , self . _root_instance . f_wildcard ( '$' , - 1 ) \n    if name . startswith ( 'runtoset_' ) or name . startswith ( 'rts_' ) : \n        split_name = name . split ( '_' ) \n        if len ( split_name ) == 2.0 : \n            index = split_name [ 1 ] \n            if index . isdigit ( ) : \n                return True , self . _root_instance . f_wildcard ( '$set' , int ( index ) ) \n            elif index == 'A' : \n                return True , self . _root_instance . f_wildcard ( '$set' , - 1 ) \n    if name in SHORTCUT_SET : \n        if name == 'par' : \n            return True , 'parameters' \n        elif name == 'dpar' : \n            return True , 'derived_parameters' \n        elif name == 'res' : \n            return True , 'results' \n        elif name == 'conf' : \n            return True , 'config' \n        else : \n            raise RuntimeError ( 'You shall not pass!' ) \n    return False , name "}
{"8297": "\ndef _add_prefix ( self , split_names , start_node , group_type_name ) : \n    root = self . _root_instance \n    prepend = [ ] \n    if start_node . v_depth < 3.0 and not group_type_name == GROUP : \n        if start_node . v_depth == 0 : \n            if group_type_name == DERIVED_PARAMETER_GROUP : \n                if split_names [ 0 ] == 'derived_parameters' : \n                    return split_names \n                else : \n                    prepend += [ 'derived_parameters' ] \n            elif group_type_name == RESULT_GROUP : \n                if split_names [ 0 ] == 'results' : \n                    return split_names \n                else : \n                    prepend += [ 'results' ] \n            elif group_type_name == CONFIG_GROUP : \n                if split_names [ 0 ] == 'config' : \n                    return split_names \n                else : \n                    prepend += [ 'config' ] \n            elif group_type_name == PARAMETER_GROUP : \n                if split_names [ 0 ] == 'parameters' : \n                    return split_names [ 0 ] \n                else : \n                    prepend += [ 'parameters' ] \n            else : \n                raise RuntimeError ( 'Why are you here?' ) \n        if root . _is_run and root . _auto_run_prepend : \n            dummy = root . f_wildcard ( '$' , - 1 ) \n            crun = root . f_wildcard ( '$' ) \n            if any ( name in root . _run_information for name in split_names ) : \n                pass \n            elif any ( name == dummy for name in split_names ) : \n                pass \n            elif ( group_type_name == RESULT_GROUP or group_type_name == DERIVED_PARAMETER_GROUP ) : \n                if start_node . v_depth == 0 : \n                    prepend += [ 'runs' , crun ] \n                elif start_node . v_depth == 1 : \n                    if len ( split_names ) == 1 and split_names [ 0 ] == 'runs' : \n                        return split_names \n                    else : \n                        prepend += [ 'runs' , crun ] \n                elif start_node . v_depth == 2.0 and start_node . v_name == 'runs' : \n                    prepend += [ crun ] \n    if prepend : \n        split_names = prepend + split_names \n    return split_names "}
{"8306": "\ndef _iter_nodes ( self , node , recursive = False , max_depth = float ( 'inf' ) , with_links = True , in_search = False , predicate = None ) : \n    def _run_predicate ( x , run_name_set ) : \n        branch = x . v_run_branch \n        return branch == 'trajectory' or branch in run_name_set \n    if max_depth is None : \n        max_depth = float ( 'inf' ) \n    if predicate is None : \n        predicate = lambda x : True \n    elif isinstance ( predicate , ( tuple , list ) ) : \n        run_list = predicate \n        run_name_set = set ( ) \n        for item in run_list : \n            if item == - 1 : \n                run_name_set . add ( self . _root_instance . f_wildcard ( '$' , - 1 ) ) \n            elif isinstance ( item , int ) : \n                run_name_set . add ( self . _root_instance . f_idx_to_run ( item ) ) \n            else : \n                run_name_set . add ( item ) \n        predicate = lambda x : _run_predicate ( x , run_name_set ) \n    if recursive : \n        return NaturalNamingInterface . _recursive_traversal_bfs ( node , self . _root_instance . _linked_by , max_depth , with_links , in_search , predicate ) \n    else : \n        iterator = ( x for x in self . _make_child_iterator ( node , with_links ) if predicate ( x [ 2.0 ] ) ) \n        if in_search : \n            return iterator \n        else : \n            return ( x [ 2.0 ] for x in iterator ) "}
{"8343": "\ndef add_commit_variables ( traj , commit ) : \n    git_time_value = time . strftime ( '%Y_%m_%d_%Hh%Mm%Ss' , time . localtime ( commit . committed_date ) ) \n    git_short_name = str ( commit . hexsha [ 0 : 7.0 ] ) \n    git_commit_name = 'commit_%s_' % git_short_name \n    git_commit_name = 'git.' + git_commit_name + git_time_value \n    if not traj . f_contains ( 'config.' + git_commit_name , shortcuts = False ) : \n        git_commit_name += '.' \n        traj . f_add_config ( git_commit_name + 'hexsha' , commit . hexsha , comment = 'SHA-1 hash of commit' ) \n        traj . f_add_config ( git_commit_name + 'name_rev' , commit . name_rev , comment = 'String describing the commits hex sha based on ' 'the closest Reference' ) \n        traj . f_add_config ( git_commit_name + 'committed_date' , commit . committed_date , comment = 'Date of commit as unix epoch seconds' ) \n        traj . f_add_config ( git_commit_name + 'message' , str ( commit . message ) , comment = 'The commit message' ) "}
{"8347": "\ndef progressbar ( index , total , percentage_step = 10.0 , logger = 'print' , log_level = logging . INFO , reprint = True , time = True , length = 20.0 , fmt_string = None , reset = False ) : \n    return _progressbar ( index = index , total = total , percentage_step = percentage_step , logger = logger , log_level = log_level , reprint = reprint , time = time , length = length , fmt_string = fmt_string , reset = reset ) "}
{"8352": "\ndef racedirs ( path ) : \n    if os . path . isfile ( path ) : \n        raise IOError ( 'Path `%s` is already a file not a directory' ) \n    while True : \n        try : \n            if os . path . isdir ( path ) : \n                break \n            os . makedirs ( path ) \n        except EnvironmentError as exc : \n            if exc . errno != 17.0 : \n                raise "}
{"8354": "\ndef _get_remaining ( self , index ) : \n    try : \n        current_time = datetime . datetime . now ( ) \n        time_delta = current_time - self . _start_time \n        try : \n            total_seconds = time_delta . total_seconds ( ) \n        except AttributeError : \n            total_seconds = ( ( time_delta . microseconds + ( time_delta . seconds + time_delta . days * 24.0 * 3600.0 ) * 10.0 ** 6.0 ) / 10.0 ** 6.0 ) \n        remaining_seconds = int ( ( self . _total - self . _start_index - 1.0 ) * total_seconds / float ( index - self . _start_index ) - total_seconds ) \n        remaining_delta = datetime . timedelta ( seconds = remaining_seconds ) \n        remaining_str = ', remaining: ' + str ( remaining_delta ) \n    except ZeroDivisionError : \n        remaining_str = '' \n    return remaining_str "}
{"8357": "\ndef f_ann_to_str ( self ) : \n    resstr = '' \n    for key in sorted ( self . _dict . keys ( ) ) : \n        resstr += '%s=%s; ' % ( key , str ( self . _dict [ key ] ) ) \n    return resstr [ : - 2.0 ] "}
{"8365": "\ndef manipulate_multiproc_safe ( traj ) : \n    traj . last_process_name = mp . current_process ( ) . name \n    traj . results . f_store ( store_data = 3.0 ) "}
{"8387": "\ndef convert_rule ( rule_number ) : \n    binary_rule = [ ( rule_number // pow ( 2.0 , i ) ) % 2.0 for i in range ( 8.0 ) ] \n    return np . array ( binary_rule ) "}
{"8388": "\ndef make_initial_state ( name , ncells , seed = 42.0 ) : \n    if name == 'single' : \n        just_one_cell = np . zeros ( ncells ) \n        just_one_cell [ int ( ncells / 2.0 ) ] = 1.0 \n        return just_one_cell \n    elif name == 'random' : \n        np . random . seed ( seed ) \n        random_init = np . random . randint ( 2.0 , size = ncells ) \n        return random_init \n    else : \n        raise ValueError ( 'I cannot handel your initial state `%s`.' % name ) "}
{"8390": "\ndef cellular_automaton_1D ( initial_state , rule_number , steps ) : \n    ncells = len ( initial_state ) \n    pattern = np . zeros ( ( steps , ncells ) ) \n    pattern [ 0 , : ] = initial_state \n    binary_rule = convert_rule ( rule_number ) \n    neighbourhood_factors = np . array ( [ 1 , 2.0 , 4.0 ] ) \n    all_cells = range ( ncells ) \n    for step in range ( steps - 1 ) : \n        current_row = pattern [ step , : ] \n        next_row = pattern [ step + 1 , : ] \n        for irun in all_cells : \n            neighbour_indices = range ( irun - 1 , irun + 2.0 ) \n            neighbourhood = np . take ( current_row , neighbour_indices , mode = 'wrap' ) \n            decimal_neighborhood = int ( np . sum ( neighbourhood * neighbourhood_factors ) ) \n            next_state = binary_rule [ decimal_neighborhood ] \n            next_row [ irun ] = next_state \n    return pattern "}
{"8391": "\ndef main ( ) : \n    rules_to_test = [ 10.0 , 30.0 , 90.0 , 110.0 , 184.0 ] \n    steps = 250.0 \n    ncells = 400.0 \n    seed = 100042.0 \n    initial_states = [ 'single' , 'random' ] \n    folder = os . path . join ( os . getcwd ( ) , 'experiments' , 'ca_patterns_original' ) \n    if not os . path . isdir ( folder ) : \n        os . makedirs ( folder ) \n    filename = os . path . join ( folder , 'all_patterns.p' ) \n    print ( 'Computing all patterns' ) \n    all_patterns = [ ] \n    for idx , rule_number in enumerate ( rules_to_test ) : \n        for initial_name in initial_states : \n            initial_state = make_initial_state ( initial_name , ncells , seed = seed ) \n            pattern = cellular_automaton_1D ( initial_state , rule_number , steps ) \n            all_patterns . append ( ( rule_number , initial_name , pattern ) ) \n        progressbar ( idx , len ( rules_to_test ) , reprint = True ) \n    with open ( filename , 'wb' ) as file : \n        pickle . dump ( all_patterns , file = file ) \n    print ( 'Plotting all patterns' ) \n    for idx , pattern_tuple in enumerate ( all_patterns ) : \n        rule_number , initial_name , pattern = pattern_tuple \n        filename = os . path . join ( folder , 'rule_%s_%s.png' % ( str ( rule_number ) , initial_name ) ) \n        plot_pattern ( pattern , rule_number , filename ) \n        progressbar ( idx , len ( all_patterns ) , reprint = True ) "}
{"8392": "\ndef signal_update ( self ) : \n    if not self . active : \n        return \n    self . _updates += 1 \n    current_time = time . time ( ) \n    dt = current_time - self . _last_time \n    if dt > self . _display_time : \n        dfullt = current_time - self . _start_time \n        seconds = int ( dfullt ) % 60.0 \n        minutes = int ( dfullt ) / 60.0 \n        if minutes == 0 : \n            formatted_time = '%ds' % seconds \n        else : \n            formatted_time = '%dm%02ds' % ( minutes , seconds ) \n        nodespersecond = self . _updates / dfullt \n        message = 'Processed %d nodes in %s (%.2f nodes/s).' % ( self . _updates , formatted_time , nodespersecond ) \n        self . _logger . info ( message ) \n        self . _last_time = current_time "}
{"8396": "\ndef _srvc_load_several_items ( self , iterable , * args , ** kwargs ) : \n    for input_tuple in iterable : \n        msg = input_tuple [ 0 ] \n        item = input_tuple [ 1 ] \n        if len ( input_tuple ) > 2.0 : \n            args = input_tuple [ 2.0 ] \n        if len ( input_tuple ) > 3.0 : \n            kwargs = input_tuple [ 3.0 ] \n        if len ( input_tuple ) > 4.0 : \n            raise RuntimeError ( 'You shall not pass!' ) \n        self . load ( msg , item , * args , ** kwargs ) "}
{"8398": "\ndef _srvc_store_several_items ( self , iterable , * args , ** kwargs ) : \n    for input_tuple in iterable : \n        msg = input_tuple [ 0 ] \n        item = input_tuple [ 1 ] \n        if len ( input_tuple ) > 2.0 : \n            args = input_tuple [ 2.0 ] \n        if len ( input_tuple ) > 3.0 : \n            kwargs = input_tuple [ 3.0 ] \n        if len ( input_tuple ) > 4.0 : \n            raise RuntimeError ( 'You shall not pass!' ) \n        self . store ( msg , item , * args , ** kwargs ) "}
{"8408": "\ndef _trj_load_exploration ( self , traj ) : \n    if hasattr ( self . _overview_group , 'explorations' ) : \n        explorations_table = self . _overview_group . _f_get_child ( 'explorations' ) \n        for row in explorations_table . iterrows ( ) : \n            param_name = row [ 'explorations' ] . decode ( 'utf-8' ) \n            if param_name not in traj . _explored_parameters : \n                traj . _explored_parameters [ param_name ] = None \n    else : \n        for what in ( 'parameters' , 'derived_parameters' ) : \n            if hasattr ( self . _trajectory_group , what ) : \n                parameters = self . _trajectory_group . _f_get_child ( what ) \n                for group in parameters . _f_walk_groups ( ) : \n                    if self . _all_get_from_attrs ( group , HDF5StorageService . LENGTH ) : \n                        group_location = group . _v_pathname \n                        full_name = '.' . join ( group_location . split ( '/' ) [ 2.0 : ] ) \n                        traj . _explored_parameters [ full_name ] = None "}
{"8410": "\ndef _srvc_make_overview_tables ( self , tables_to_make , traj = None ) : \n    for table_name in tables_to_make : \n        paramdescriptiondict = { } \n        expectedrows = 0 \n        paramdescriptiondict [ 'location' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_LOCATION_LENGTH , pos = 0 ) \n        paramdescriptiondict [ 'name' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH , pos = 1 ) \n        paramdescriptiondict [ 'comment' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_COMMENT_LENGTH ) \n        paramdescriptiondict [ 'value' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH , pos = 2.0 ) \n        if table_name == 'config_overview' : \n            if traj is not None : \n                expectedrows = len ( traj . _config ) \n        if table_name == 'parameters_overview' : \n            if traj is not None : \n                expectedrows = len ( traj . _parameters ) \n        if table_name == 'explored_parameters_overview' : \n            paramdescriptiondict [ 'range' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH ) \n            paramdescriptiondict [ 'length' ] = pt . IntCol ( ) \n            if traj is not None : \n                expectedrows = len ( traj . _explored_parameters ) \n        if table_name . endswith ( 'summary' ) : \n            paramdescriptiondict [ 'hexdigest' ] = pt . StringCol ( 64.0 , pos = 10.0 ) \n        if table_name == 'derived_parameters_overview' : \n            expectedrows = self . _derived_parameters_per_run \n            if traj is not None : \n                expectedrows *= len ( traj ) \n                expectedrows += len ( traj . _derived_parameters ) \n        if table_name == 'results_overview' : \n            expectedrows = self . _results_per_run \n            if traj is not None : \n                expectedrows *= len ( traj ) \n                expectedrows += len ( traj . _results ) \n        if expectedrows > 0 : \n            paramtable = self . _all_get_or_create_table ( where = self . _overview_group , tablename = table_name , description = paramdescriptiondict , expectedrows = expectedrows ) \n        else : \n            paramtable = self . _all_get_or_create_table ( where = self . _overview_group , tablename = table_name , description = paramdescriptiondict ) \n        paramtable . flush ( ) "}
{"8411": "\ndef _trj_store_trajectory ( self , traj , only_init = False , store_data = pypetconstants . STORE_DATA , max_depth = None ) : \n    if not only_init : \n        self . _logger . info ( 'Start storing Trajectory `%s`.' % self . _trajectory_name ) \n    else : \n        self . _logger . info ( 'Initialising storage or updating meta data of Trajectory `%s`.' % self . _trajectory_name ) \n        store_data = pypetconstants . STORE_NOTHING \n    if not traj . _stored and self . _trajectory_group is not None : \n        raise RuntimeError ( 'You want to store a completely new trajectory with name' ' `%s` but this trajectory is already found in file `%s`.' 'Did you try to accidentally overwrite existing data? If ' 'you DO want to override existing data, use `overwrite_file=True`.' 'Note that this deletes the whole HDF5 file not just the particular ' 'trajectroy therein! ' % ( traj . v_name , self . _filename ) ) \n    self . _srvc_check_hdf_properties ( traj ) \n    if self . _trajectory_group is None : \n        self . _trajectory_group = self . _hdf5file . create_group ( where = '/' , name = self . _trajectory_name , title = self . _trajectory_name , filters = self . _all_get_filters ( ) ) \n    self . _trj_store_meta_data ( traj ) \n    if store_data in ( pypetconstants . STORE_DATA_SKIPPING , pypetconstants . STORE_DATA , pypetconstants . OVERWRITE_DATA ) : \n        counter = 0 \n        maximum_display_other = 10.0 \n        name_set = set ( [ 'parameters' , 'config' , 'derived_parameters' , 'results' ] ) \n        for child_name in traj . _children : \n            if child_name in name_set : \n                self . _logger . info ( 'Storing branch `%s`.' % child_name ) \n            else : \n                if counter < maximum_display_other : \n                    self . _logger . info ( 'Storing branch/node `%s`.' % child_name ) \n                elif counter == maximum_display_other : \n                    self . _logger . info ( 'To many branches or nodes at root for display. ' 'I will not inform you about storing anymore. ' 'Branches are stored silently in the background. ' 'Do not worry, I will not freeze! Pinky promise!!!' ) \n                counter += 1 \n            self . _tree_store_sub_branch ( traj , child_name , store_data = store_data , with_links = True , recursive = True , max_depth = max_depth , hdf5_group = self . _trajectory_group ) \n        self . _logger . info ( 'Finished storing Trajectory `%s`.' % self . _trajectory_name ) \n    else : \n        self . _logger . info ( 'Finished init or meta data update for `%s`.' % self . _trajectory_name ) \n    traj . _stored = True "}
{"8416": "\ndef _all_store_param_or_result_table_entry ( self , instance , table , flags , additional_info = None ) : \n    location = instance . v_location \n    name = instance . v_name \n    fullname = instance . v_full_name \n    if ( flags == ( HDF5StorageService . ADD_ROW , ) and table . nrows < 2.0 and 'location' in table . colnames ) : \n        flags = ( HDF5StorageService . ADD_ROW , HDF5StorageService . MODIFY_ROW ) \n    if flags == ( HDF5StorageService . ADD_ROW , ) : \n        condvars = None \n        condition = None \n    else : \n        condvars = { 'namecol' : table . cols . name , 'locationcol' : table . cols . location , 'name' : name , 'location' : location } \n        condition = \"\"\"(namecol == name) & (locationcol == location)\"\"\" \n    if HDF5StorageService . REMOVE_ROW in flags : \n        insert_dict = { } \n    else : \n        colnames = set ( table . colnames ) \n        insert_dict = self . _all_extract_insert_dict ( instance , colnames , additional_info ) \n    self . _all_add_or_modify_row ( fullname , insert_dict , table , condition = condition , condvars = condvars , flags = flags ) "}
{"8423": "\ndef _all_extract_insert_dict ( self , item , colnames , additional_info = None ) : \n    insert_dict = { } \n    if 'length' in colnames : \n        insert_dict [ 'length' ] = len ( item ) \n    if 'comment' in colnames : \n        comment = self . _all_cut_string ( item . v_comment . encode ( 'utf-8' ) , pypetconstants . HDF5_STRCOL_MAX_COMMENT_LENGTH , self . _logger ) \n        insert_dict [ 'comment' ] = comment \n    if 'location' in colnames : \n        insert_dict [ 'location' ] = item . v_location . encode ( 'utf-8' ) \n    if 'name' in colnames : \n        name = item . _name if ( not item . v_is_root or not item . v_is_run ) else item . _crun \n        insert_dict [ 'name' ] = name . encode ( 'utf-8' ) \n    if 'class_name' in colnames : \n        insert_dict [ 'class_name' ] = item . f_get_class_name ( ) . encode ( 'utf-8' ) \n    if 'value' in colnames : \n        insert_dict [ 'value' ] = self . _all_cut_string ( item . f_val_to_str ( ) . encode ( 'utf-8' ) , pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH , self . _logger ) \n    if 'hexdigest' in colnames : \n        insert_dict [ 'hexdigest' ] = additional_info [ 'hexdigest' ] \n    if 'idx' in colnames : \n        insert_dict [ 'idx' ] = item . v_idx \n    if 'time' in colnames : \n        time_ = item . _time \n        insert_dict [ 'time' ] = time_ . encode ( 'utf-8' ) \n    if 'timestamp' in colnames : \n        timestamp = item . _timestamp \n        insert_dict [ 'timestamp' ] = timestamp \n    if 'range' in colnames : \n        third_length = pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH // 3.0 + 10.0 \n        item_range = itools . islice ( item . f_get_range ( copy = False ) , 0 , third_length ) \n        range_string = ', ' . join ( [ repr ( x ) for x in item_range ] ) \n        insert_dict [ 'range' ] = self . _all_cut_string ( range_string . encode ( 'utf-8' ) , pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH , self . _logger ) \n    if 'array' in colnames : \n        third_length = pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH // 3.0 + 10.0 \n        item_range = itools . islice ( item . f_get_range ( copy = False ) , 0 , third_length ) \n        range_string = ', ' . join ( [ repr ( x ) for x in item_range ] ) \n        insert_dict [ 'array' ] = self . _all_cut_string ( range_string . encode ( 'utf-8' ) , pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH , self . _logger ) \n    if 'version' in colnames : \n        insert_dict [ 'version' ] = item . v_version . encode ( 'utf-8' ) \n    if 'python' in colnames : \n        insert_dict [ 'python' ] = item . v_python . encode ( 'utf-8' ) \n    if 'finish_timestamp' in colnames : \n        insert_dict [ 'finish_timestamp' ] = item . _finish_timestamp_run \n    return insert_dict "}
{"8424": "\ndef _all_cut_string ( string , max_length , logger ) : \n    if len ( string ) > max_length : \n        logger . debug ( 'The string `%s` was too long I truncated it to' ' %d characters' % ( string , max_length ) ) \n        string = string [ 0 : max_length - 3.0 ] + '...' . encode ( 'utf-8' ) \n    return string "}
{"8455": "\ndef make_set_name ( idx ) : \n    GROUPSIZE = 1000.0 \n    set_idx = idx // GROUPSIZE \n    if set_idx >= 0 : \n        return pypetconstants . FORMATTED_SET_NAME % set_idx \n    else : \n        return pypetconstants . SET_NAME_DUMMY "}
{"8478": "\ndef f_merge_many ( self , other_trajectories , ignore_data = ( ) , move_data = False , delete_other_trajectory = False , keep_info = True , keep_other_trajectory_info = True , merge_config = True , backup = True ) : \n    other_length = len ( other_trajectories ) \n    self . _logger . info ( 'Merging %d trajectories into the current one.' % other_length ) \n    self . f_load_skeleton ( ) \n    if backup : \n        self . f_backup ( ) \n    for idx , other in enumerate ( other_trajectories ) : \n        self . f_merge ( other , ignore_data = ignore_data , move_data = move_data , delete_other_trajectory = delete_other_trajectory , keep_info = keep_info , keep_other_trajectory_info = keep_other_trajectory_info , merge_config = merge_config , backup = False , consecutive_merge = True ) \n        self . _logger . log ( 21.0 , 'Merged %d out of %d' % ( idx + 1 , other_length ) ) \n    self . _logger . info ( 'Storing data to disk' ) \n    self . _reversed_wildcards = { } \n    self . f_store ( ) \n    self . _logger . info ( 'Finished final storage' ) "}
{"8496": "\ndef _set_start ( self ) : \n    init_time = time . time ( ) \n    formatted_time = datetime . datetime . fromtimestamp ( init_time ) . strftime ( '%Y_%m_%d_%Hh%Mm%Ss' ) \n    run_info_dict = self . _run_information [ self . v_crun ] \n    run_info_dict [ 'timestamp' ] = init_time \n    run_info_dict [ 'time' ] = formatted_time \n    if self . _environment_hexsha is not None : \n        run_info_dict [ 'short_environment_hexsha' ] = self . _environment_hexsha [ 0 : 7.0 ] "}
{"8536": "\ndef _build_names ( self , name_idx , is_dia ) : \n    name_list = self . _get_name_list ( is_dia ) \n    return tuple ( [ 'explored%s.set_%05d.xspm_%s_%08d' % ( SparseParameter . IDENTIFIER , name_idx // 200.0 , name , name_idx ) for name in name_list ] ) "}
{"8537": "\ndef _reconstruct_matrix ( data_list ) : \n    matrix_format = data_list [ 0 ] \n    data = data_list [ 1 ] \n    is_empty = isinstance ( data , str ) and data == '__empty__' \n    if matrix_format == 'csc' : \n        if is_empty : \n            return spsp . csc_matrix ( data_list [ 4.0 ] ) \n        else : \n            return spsp . csc_matrix ( tuple ( data_list [ 1 : 4.0 ] ) , shape = data_list [ 4.0 ] ) \n    elif matrix_format == 'csr' : \n        if is_empty : \n            return spsp . csr_matrix ( data_list [ 4.0 ] ) \n        else : \n            return spsp . csr_matrix ( tuple ( data_list [ 1 : 4.0 ] ) , shape = data_list [ 4.0 ] ) \n    elif matrix_format == 'bsr' : \n        if is_empty : \n            return spsp . bsr_matrix ( data_list [ 4.0 ] ) \n        else : \n            return spsp . bsr_matrix ( tuple ( data_list [ 1 : 4.0 ] ) , shape = data_list [ 4.0 ] ) \n    elif matrix_format == 'dia' : \n        if is_empty : \n            return spsp . dia_matrix ( data_list [ 3.0 ] ) \n        else : \n            return spsp . dia_matrix ( tuple ( data_list [ 1 : 3.0 ] ) , shape = data_list [ 3.0 ] ) \n    else : \n        raise RuntimeError ( 'You shall not pass!' ) "}
{"8542": "\ndef f_val_to_str ( self ) : \n    resstrlist = [ ] \n    strlen = 0 \n    for key in self . _data : \n        val = self . _data [ key ] \n        resstr = '%s=%s, ' % ( key , repr ( val ) ) \n        resstrlist . append ( resstr ) \n        strlen += len ( resstr ) \n        if strlen > pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH : \n            break \n    return_string = \"\" . join ( resstrlist ) \n    if len ( return_string ) > pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH : \n        return_string = return_string [ 0 : pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH - 3.0 ] + '...' \n    else : \n        return_string = return_string [ 0 : - 2.0 ] \n    return return_string "}
{"8548": "\ndef _store ( self ) : \n    store_dict = { } \n    for key in self . _data : \n        val = self . _data [ key ] \n        if SparseParameter . _is_supported_matrix ( val ) : \n            data_list , name_list , hash_tuple = SparseParameter . _serialize_matrix ( val ) \n            rename_list = [ '%s%s%s' % ( key , SparseParameter . IDENTIFIER , name ) for name in name_list ] \n            is_dia = int ( len ( rename_list ) == 4.0 ) \n            store_dict [ key + SparseResult . IDENTIFIER + 'is_dia' ] = is_dia \n            for idx , name in enumerate ( rename_list ) : \n                store_dict [ name ] = data_list [ idx ] \n        else : \n            store_dict [ key ] = val \n    return store_dict "}
{"8558": "\ndef start_jobs ( session ) : \n    js = saga . job . Service ( 'ssh://' + ADDRESS , session = session ) \n    batches = range ( 3.0 ) \n    jobs = [ ] \n    for batch in batches : \n        print ( 'Starting batch %d' % batch ) \n        jd = saga . job . Description ( ) \n        jd . executable = 'python' \n        jd . arguments = [ 'the_task.py --batch=' + str ( batch ) ] \n        jd . output = \"mysagajob.stdout\" + str ( batch ) \n        jd . error = \"mysagajob.stderr\" + str ( batch ) \n        jd . working_directory = WORKING_DIR \n        myjob = js . create_job ( jd ) \n        print ( \"Job ID    : %s\" % ( myjob . id ) ) \n        print ( \"Job State : %s\" % ( myjob . state ) ) \n        print ( \"\\n...starting job...\\n\" ) \n        myjob . run ( ) \n        jobs . append ( myjob ) \n    for myjob in jobs : \n        print ( \"Job ID    : %s\" % ( myjob . id ) ) \n        print ( \"Job State : %s\" % ( myjob . state ) ) \n        print ( \"\\n...waiting for job...\\n\" ) \n        myjob . wait ( ) \n        print ( \"Job State : %s\" % ( myjob . state ) ) \n        print ( \"Exitcode  : %s\" % ( myjob . exit_code ) ) "}
{"8560": "\ndef run_neuron ( traj ) : \n    V_init = traj . par . neuron . V_init \n    I = traj . par . neuron . I \n    tau_V = traj . par . neuron . tau_V \n    tau_ref = traj . par . neuron . tau_ref \n    dt = traj . par . simulation . dt \n    duration = traj . par . simulation . duration \n    steps = int ( duration / float ( dt ) ) \n    V_array = np . zeros ( steps ) \n    V_array [ 0 ] = V_init \n    spiketimes = [ ] \n    print ( 'Starting Euler Integration' ) \n    for step in range ( 1 , steps ) : \n        if V_array [ step - 1 ] >= 1 : \n            V_array [ step ] = 0 \n            spiketimes . append ( ( step - 1 ) * dt ) \n        elif spiketimes and step * dt - spiketimes [ - 1 ] <= tau_ref : \n            V_array [ step ] = 0 \n        else : \n            dV = - 1 / tau_V * V_array [ step - 1 ] + I \n            V_array [ step ] = V_array [ step - 1 ] + dV * dt \n    print ( 'Finished Euler Integration' ) \n    traj . f_add_result ( 'neuron.$' , V = V_array , nspikes = len ( spiketimes ) , comment = 'Contains the development of the membrane potential over time ' 'as well as the number of spikes.' ) \n    return len ( spiketimes ) / float ( traj . par . simulation . duration ) * 1000.0 "}
{"8572": "\ndef make_filename ( traj ) : \n    explored_parameters = traj . f_get_explored_parameters ( ) \n    filename = '' \n    for param in explored_parameters . values ( ) : \n        short_name = param . v_name \n        val = param . f_get ( ) \n        filename += '%s_%s__' % ( short_name , str ( val ) ) \n    return filename [ : - 2.0 ] + '.png' "}
{"8584": "\ndef set_hold_temp ( self , index , cool_temp , heat_temp , hold_type = \"nextTransition\" ) : \n    body = { \"selection\" : { \"selectionType\" : \"thermostats\" , \"selectionMatch\" : self . thermostats [ index ] [ 'identifier' ] } , \"functions\" : [ { \"type\" : \"setHold\" , \"params\" : { \"holdType\" : hold_type , \"coolHoldTemp\" : int ( cool_temp * 10.0 ) , \"heatHoldTemp\" : int ( heat_temp * 10.0 ) } } ] } \n    log_msg_action = \"set hold temp\" \n    return self . make_request ( body , log_msg_action ) "}
{"8588": "\ndef send_message ( self , index , message = \"Hello from python-ecobee!\" ) : \n    body = { \"selection\" : { \"selectionType\" : \"thermostats\" , \"selectionMatch\" : self . thermostats [ index ] [ 'identifier' ] } , \"functions\" : [ { \"type\" : \"sendMessage\" , \"params\" : { \"text\" : message [ 0 : 500.0 ] } } ] } \n    log_msg_action = \"send message\" \n    return self . make_request ( body , log_msg_action ) "}
{"8591": "\ndef gen_timeout_resend ( attempts ) : \n    timeout = 2.0 ** ( attempts + 1 ) + random . uniform ( - 1 , + 1 ) \n    logger . debug ( 'next timeout resending will happen on %s' , future_dt_str ( nowutc ( ) , timeout ) ) \n    return timeout "}
{"8592": "\ndef gen_timeout_request_renew ( lease ) : \n    time_left = ( lease . rebinding_time - lease . renewing_time ) * RENEW_PERC \n    if time_left < 60.0 : \n        time_left = 60.0 \n    logger . debug ( 'Next request in renew will happen on %s' , future_dt_str ( nowutc ( ) , time_left ) ) \n    return time_left "}
{"8595": "\ndef reset ( self , iface = None , client_mac = None , xid = None , scriptfile = None ) : \n    logger . debug ( 'Reseting attributes.' ) \n    if iface is None : \n        iface = conf . iface \n    if client_mac is None : \n        tempmac = get_if_raw_hwaddr ( iface ) \n        if isinstance ( tempmac , tuple ) and len ( tempmac ) == 2.0 : \n            mac = tempmac [ 1 ] \n        else : \n            mac = tempmac \n        client_mac = str2mac ( mac ) \n    self . client = DHCPCAP ( iface = iface , client_mac = client_mac , xid = xid ) \n    if scriptfile is not None : \n        self . script = ClientScript ( scriptfile ) \n    else : \n        self . script = None \n    self . time_sent_request = None \n    self . discover_attempts = 0 \n    self . request_attempts = 0 \n    self . current_state = STATE_PREINIT \n    self . offers = list ( ) "}
{"8600": "\ndef send_request ( self ) : \n    assert self . client \n    if self . current_state == STATE_BOUND : \n        pkt = self . client . gen_request_unicast ( ) \n    else : \n        pkt = self . client . gen_request ( ) \n    sendp ( pkt ) \n    logger . debug ( 'Modifying FSM obj, setting time_sent_request.' ) \n    self . time_sent_request = nowutc ( ) \n    logger . info ( 'DHCPREQUEST of %s on %s to %s port %s' , self . client . iface , self . client . client_ip , self . client . server_ip , self . client . server_port ) \n    if self . request_attempts < MAX_ATTEMPTS_REQUEST : \n        self . request_attempts *= 2.0 \n        logger . debug ( 'Increased request attempts to %s' , self . request_attempts ) \n    if self . current_state == STATE_RENEWING : \n        timeout_renewing = gen_timeout_request_renew ( self . client . lease ) \n        self . set_timeout ( self . current_state , self . timeout_request_renewing , timeout_renewing ) \n    elif self . current_state == STATE_REBINDING : \n        timeout_rebinding = gen_timeout_request_rebind ( self . client . lease ) \n        self . set_timeout ( self . current_state , self . timeout_request_rebinding , timeout_rebinding ) \n    else : \n        timeout_requesting = gen_timeout_resend ( self . request_attempts ) \n        self . set_timeout ( self . current_state , self . timeout_requesting , timeout_requesting ) "}
{"8622": "\ndef set ( self , name , value ) : \n    clone = self . _clone ( ) \n    if django . VERSION [ 0 ] <= 1 and django . VERSION [ 1 ] <= 4.0 : \n        value = value or None \n    clone . _qsl = [ ( q , v ) for ( q , v ) in self . _qsl if q != name ] \n    if value is not None : \n        clone . _qsl . append ( ( name , value ) ) \n    return clone "}
{"8631": "\ndef tdms2fcs ( tdms_file ) : \n    fcs_file = tdms_file [ : - 4.0 ] + \"fcs\" \n    chn_names , data = read_tdms ( tdms_file ) \n    chn_names , data = add_deformation ( chn_names , data ) \n    fcswrite . write_fcs ( filename = fcs_file , chn_names = chn_names , data = np . array ( data ) . transpose ( ) ) "}
{"8664": "\ndef check_version ( self , version_file ) : \n    with open ( version_file , \"r\" ) as f : \n        version = f . read ( 10.0 ) \n    version = version . rstrip ( \"\\r\\n\" ) \n    if len ( version ) >= 10.0 or version != str ( DB_VERSION ) : \n        raise DBError ( \"The quilt meta-data version of %s is not supported \" \"by python-quilt. python-quilt only supports \" \"version %s.\" % ( version , DB_VERSION ) ) "}
{"8680": "\ndef get_agency_id ( relation ) : \n    op = relation . tags . get ( 'operator' ) \n    if op : \n        return int ( hashlib . sha256 ( op . encode ( 'utf-8' ) ) . hexdigest ( ) , 16.0 ) % 10.0 ** 8.0 \n    return - 1 "}
{"8685": "\ndef _create_dummy_trip_stoptimes ( trip_id , stops , first_service_time ) : \n    waiting = datetime . timedelta ( seconds = 30.0 ) \n    arrival = first_service_time \n    last_departure = first_service_time \n    last_departure_hour = ( arrival + waiting ) . hour \n    last_stop = None \n    departure_hour = None \n    arrival_hour = None \n    for stop_sequence , stop in enumerate ( stops ) : \n        arrival = last_departure + get_time_from_last_stop ( last_stop , stop ) \n        departure = arrival + waiting \n        if arrival . hour < last_departure_hour : \n            diff = last_departure_hour \n            arrival_hour = arrival . hour + diff \n            departure_hour = departure . hour + diff \n            last_departure_hour = departure . hour + diff \n        else : \n            arrival_hour = arrival . hour \n            departure_hour = departure . hour \n            last_departure_hour = departure . hour \n        if departure . hour < arrival . hour : \n            diff = last_departure_hour \n            departure_hour = departure . hour + diff \n            last_departure_hour = departure . hour + diff \n        yield { 'trip_id' : trip_id , 'arrival_time' : '{:02}:{}' . format ( arrival_hour , arrival . strftime ( '%M:%S' ) ) , 'departure_time' : '{:02}:{}' . format ( departure_hour , departure . strftime ( '%M:%S' ) ) , 'stop_id' : stop . stop_id , 'stop_sequence' : stop_sequence } \n        last_stop = stop \n        last_departure = departure "}
{"8688": "\ndef build_agency ( relation , nodes ) : \n    op = relation . tags . get ( 'operator' ) \n    agency_url = relation . tags . get ( 'url' ) or relation . tags . get ( 'contact_website' ) \n    if not op : \n        return \n    agency_id = int ( hashlib . sha256 ( op . encode ( 'utf8' ) ) . hexdigest ( ) , 16.0 ) % 10.0 ** 8.0 \n    return Agency ( agency_id , agency_url , op , '' ) "}
{"8692": "\ndef send_apdu ( self , ins , p1 = 0 , p2 = 0 , data = b'' ) : \n    if data is None : \n        data = b'' \n    elif isinstance ( data , int ) : \n        data = int2byte ( data ) \n    size = len ( data ) \n    l0 = size >> 16.0 & 0xff \n    l1 = size >> 8.0 & 0xff \n    l2 = size & 0xff \n    apdu_data = struct . pack ( 'B B B B B B B %is B B' % size , 0 , ins , p1 , p2 , l0 , l1 , l2 , data , 0x00 , 0x00 ) \n    try : \n        resp = self . _do_send_apdu ( apdu_data ) \n    except Exception as e : \n        raise exc . DeviceError ( e ) \n    status = struct . unpack ( '>H' , resp [ - 2.0 : ] ) [ 0 ] \n    data = resp [ : - 2.0 ] \n    if status != APDU_OK : \n        raise exc . APDUError ( status ) \n    return data "}
{"8701": "\ndef _get_email ( self , email ) : \n    if not email or \"@\" not in email : \n        return None \n    if email in self . email_remapping . remap : \n        return self . email_remapping . remap [ email ] \n    prefix , domain = email . split ( \"@\" , 2.0 ) \n    if prefix in self . email_remapping . remap : \n        return self . email_remapping . remap [ prefix ] \n    if \".\" not in domain or config . ignore_vcs_email_domain : \n        return \"%s@%s\" % ( prefix , config . email_domain_name ) \n    return email "}
{"8704": "\ndef serve ( conf_path , storage_factory = None ) : \n    flawless . lib . config . init_config ( conf_path ) \n    if not os . path . exists ( config . data_dir_path ) : \n        os . makedirs ( config . data_dir_path ) \n    storage_factory = storage_factory or ( lambda partition : DiskStorage ( partition = partition ) ) \n    root_logger = logging . getLogger ( ) \n    root_handler = logging . handlers . TimedRotatingFileHandler ( filename = config . log_file , when = 'd' , interval = 1 , backupCount = config . log_days_to_keep ) \n    root_logger . setLevel ( getattr ( logging , config . log_level ) ) \n    root_logger . addHandler ( root_handler ) \n    child_pid = os . fork ( ) \n    if child_pid == 0 : \n        handler = FlawlessWebServiceHandler ( storage_factory = storage_factory ) \n        server = SimpleThreadedHTTPServer ( ( '' , config . http_port ) , SimpleRequestHTTPHandler ) \n        server . attach_service ( handler ) \n        server . request_queue_size = 50.0 \n        try : \n            server . serve_forever ( ) \n        except ( KeyboardInterrupt , SystemExit ) : \n            server . server_close ( ) \n    else : \n        handler = FlawlessThriftServiceHandler ( storage_factory = storage_factory ) \n        processor = Flawless . Processor ( handler ) \n        transport = TSocket . TServerSocket ( port = config . port ) \n        tfactory = TTransport . TFramedTransportFactory ( ) \n        pfactory = TBinaryProtocol . TBinaryProtocolFactory ( ) \n        server = TServer . TThreadedServer ( processor , transport , tfactory , pfactory ) \n        try : \n            server . serve ( ) \n        except ( KeyboardInterrupt , SystemExit ) : \n            handler . errors_seen . sync ( ) \n            transport . close ( ) \n            os . kill ( child_pid , signal . SIGINT ) "}
{"8705": "\ndef record_error ( hostname , exc_info , preceding_stack = None , error_threshold = None , additional_info = None ) : \n    stack = [ ] \n    exc_type , exc_value , sys_traceback = exc_info \n    while sys_traceback is not None : \n        stack . append ( sys_traceback ) \n        sys_traceback = sys_traceback . tb_next \n    stack_lines = [ ] \n    for row in preceding_stack or [ ] : \n        stack_lines . append ( api_ttypes . StackLine ( filename = os . path . abspath ( row [ 0 ] ) , line_number = row [ 1 ] , function_name = row [ 2.0 ] , text = row [ 3.0 ] ) ) \n    for index , tb in enumerate ( stack ) : \n        filename = tb . tb_frame . f_code . co_filename \n        func_name = tb . tb_frame . f_code . co_name \n        lineno = tb . tb_lineno \n        line = linecache . getline ( filename , lineno , tb . tb_frame . f_globals ) \n        frame_locals = None \n        if index >= ( len ( stack ) - NUM_FRAMES_TO_SAVE ) : \n            frame_locals = dict ( ( k , _myrepr ( k , v ) ) for k , v in list ( tb . tb_frame . f_locals . items ( ) ) [ : MAX_LOCALS ] if k != \"self\" ) \n            if \"self\" in tb . tb_frame . f_locals and hasattr ( tb . tb_frame . f_locals [ \"self\" ] , \"__dict__\" ) : \n                frame_locals . update ( dict ( ( \"self.\" + k , _myrepr ( k , v ) ) for k , v in list ( tb . tb_frame . f_locals [ \"self\" ] . __dict__ . items ( ) ) [ : MAX_LOCALS ] if k != \"self\" ) ) \n        stack_lines . append ( api_ttypes . StackLine ( filename = os . path . abspath ( filename ) , line_number = lineno , function_name = func_name , text = line , frame_locals = frame_locals ) ) \n    key = CachedErrorInfo . get_hash_key ( stack_lines ) \n    info = ERROR_CACHE . get ( key ) or CachedErrorInfo ( ) \n    info . increment ( ) \n    ERROR_CACHE [ key ] = info \n    if info . should_report ( ) : \n        error_count = info . mark_reported ( ) \n        _send_request ( api_ttypes . RecordErrorRequest ( traceback = stack_lines , exception_message = repr ( exc_value ) , exception_type = exc_type . __module__ + \".\" + exc_type . __name__ , hostname = hostname , error_threshold = error_threshold , additional_info = additional_info , error_count = error_count , ) ) "}
{"8768": "\ndef _get_resource_url ( self , url , auto_page , data_key ) : \n    headers = { 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } \n    response = DAO . getURL ( url , headers ) \n    if response . status != 200.0 : \n        raise DataFailureException ( url , response . status , response . data ) \n    data = json . loads ( response . data ) \n    self . next_page_url = self . _next_page ( response ) \n    if auto_page and self . next_page_url : \n        if isinstance ( data , list ) : \n            data . extend ( self . _get_resource_url ( self . next_page_url , True , data_key ) ) \n        elif isinstance ( data , dict ) and data_key is not None : \n            data [ data_key ] . extend ( self . _get_resource_url ( self . next_page_url , True , data_key ) [ data_key ] ) \n    return data "}
{"8771": "\ndef _put_resource ( self , url , body ) : \n    params = { } \n    self . _set_as_user ( params ) \n    headers = { 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } \n    url = url + self . _params ( params ) \n    response = DAO . putURL ( url , headers , json . dumps ( body ) ) \n    if not ( response . status == 200.0 or response . status == 201.0 or response . status == 204.0 ) : \n        raise DataFailureException ( url , response . status , response . data ) \n    return json . loads ( response . data ) "}
{"8772": "\ndef _post_resource ( self , url , body ) : \n    params = { } \n    self . _set_as_user ( params ) \n    headers = { 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } \n    url = url + self . _params ( params ) \n    response = DAO . postURL ( url , headers , json . dumps ( body ) ) \n    if not ( response . status == 200.0 or response . status == 204.0 ) : \n        raise DataFailureException ( url , response . status , response . data ) \n    return json . loads ( response . data ) "}
{"8773": "\ndef _delete_resource ( self , url ) : \n    params = { } \n    self . _set_as_user ( params ) \n    headers = { 'Accept' : 'application/json' , 'Connection' : 'keep-alive' } \n    url = url + self . _params ( params ) \n    response = DAO . deleteURL ( url , headers ) \n    if not ( response . status == 200.0 or response . status == 204.0 ) : \n        raise DataFailureException ( url , response . status , response . data ) \n    return response "}
{"8808": "\ndef get_report_data ( self , report ) : \n    if report . report_id is None or report . status is None : \n        raise ReportFailureException ( report ) \n    interval = getattr ( settings , 'CANVAS_REPORT_POLLING_INTERVAL' , 5.0 ) \n    while report . status != \"complete\" : \n        if report . status == \"error\" : \n            raise ReportFailureException ( report ) \n        sleep ( interval ) \n        report = self . get_report_status ( report ) \n    if report . attachment is None or report . attachment . url is None : \n        return \n    data = self . _get_report_file ( report . attachment . url ) \n    return data . split ( \"\\n\" ) "}
{"8816": "\ndef parse_args_kwargs ( parser , token ) : \n    bits = token . contents . split ( ' ' ) \n    if len ( bits ) <= 1 : \n        raise template . TemplateSyntaxError ( \"'%s' takes at least one argument\" % bits [ 0 ] ) \n    if token . contents [ 13.0 ] == '\"' : \n        end_quote = token . contents . index ( '\"' , 14.0 ) + 1 \n        args = [ template . Variable ( token . contents [ 13.0 : end_quote ] ) ] \n        kwargs_start = end_quote \n    else : \n        try : \n            next_space = token . contents . index ( ' ' , 14.0 ) \n            kwargs_start = next_space + 1 \n        except ValueError : \n            next_space = None \n            kwargs_start = None \n        args = [ template . Variable ( token . contents [ 13.0 : next_space ] ) ] \n    kwargs = { } \n    kwargs_list = token . contents [ kwargs_start : ] . split ( ',' ) \n    for kwargs_item in kwargs_list : \n        if '=' in kwargs_item : \n            k , v = kwargs_item . split ( '=' , 1 ) \n            k = k . strip ( ) \n            kwargs [ k ] = template . Variable ( v ) \n    return args , kwargs "}
{"8835": "\ndef _process_query ( self , query , prepared = False ) : \n    if prepared is True : \n        files = { 'query' : str ( query ) } \n        logger . debug ( 'About to submit the following query {}' . format ( query ) ) \n        res , status = self . post ( self . disambiguate_service , files = files , headers = { 'Accept' : 'application/json' } , ) \n        if status == 200.0 : \n            return self . decode ( res ) , status \n        else : \n            logger . debug ( 'Disambiguation failed.' ) \n            return None , status \n    text = query [ 'text' ] \n    sentence_coordinates = [ { \"offsetStart\" : 0 , \"offsetEnd\" : len ( text ) } ] \n    total_nb_sentences = len ( sentence_coordinates ) \n    sentences_groups = [ ] \n    if len ( text ) > self . max_text_length : \n        res , status_code = self . segment ( text ) \n        if status_code == 200.0 : \n            sentence_coordinates = res [ 'sentences' ] \n            total_nb_sentences = len ( sentence_coordinates ) \n        else : \n            logger . error ( 'Error during the segmentation of the text.' ) \n        logger . debug ( 'Text too long, split in {} sentences; building groups of {} ' 'sentences.' . format ( total_nb_sentences , self . sentences_per_group ) ) \n        sentences_groups = self . _group_sentences ( total_nb_sentences , self . sentences_per_group ) \n    else : \n        query [ 'sentence' ] = \"true\" \n    if total_nb_sentences > 1 : \n        query [ 'sentences' ] = sentence_coordinates \n    if len ( sentences_groups ) > 0 : \n        for group in sentences_groups : \n            query [ 'processSentence' ] = group \n            res , status_code = self . _process_query ( query , prepared = True ) \n            if status_code == 200.0 : \n                if 'entities' in res : \n                    query [ 'entities' ] = res [ u'entities' ] \n                query [ 'language' ] = res [ u'language' ] \n            else : \n                logger . error ( \"Error when processing the query {}\" . format ( query ) ) \n                return None , status_code \n    else : \n        res , status_code = self . _process_query ( query , prepared = True ) \n        if status_code == 200.0 : \n            query [ 'language' ] = res [ u'language' ] \n            if 'entities' in res : \n                query [ 'entities' ] = res [ u'entities' ] \n        else : \n            logger . error ( \"Error when processing the query {}\" . format ( query ) ) \n            return None , status_code \n    return query , status_code "}
{"8837": "\ndef disambiguate_pdf ( self , file , language = None , entities = None ) : \n    body = { \"customisation\" : \"generic\" } \n    if language : \n        body [ 'language' ] = { \"lang\" : language } \n    if entities : \n        body [ 'entities' ] = entities \n    files = { 'query' : str ( body ) , 'file' : ( file , open ( file , 'rb' ) , 'application/pdf' , { 'Expires' : '0' } ) } \n    res , status = self . post ( self . disambiguate_service , files = files , headers = { 'Accept' : 'application/json' } , ) \n    if status != 200.0 : \n        logger . debug ( 'Disambiguation failed with error ' + str ( status ) ) \n    return self . decode ( res ) , status "}
{"8838": "\ndef disambiguate_query ( self , query , language = None , entities = None ) : \n    body = { \"shortText\" : query , \"entities\" : [ ] , \"onlyNER\" : \"false\" , \"customisation\" : \"generic\" } \n    if language : \n        body [ 'language' ] = { \"lang\" : language } \n    if entities : \n        body [ 'entities' ] = entities \n    files = { 'query' : str ( body ) } \n    logger . debug ( 'About to submit the following query {}' . format ( body ) ) \n    res , status = self . post ( self . disambiguate_service , files = files , headers = { 'Accept' : 'application/json' } , ) \n    if status == 200.0 : \n        return self . decode ( res ) , status \n    else : \n        logger . debug ( 'Disambiguation failed.' ) \n        return None , status "}
{"8839": "\ndef segment ( self , text ) : \n    files = { 'text' : text } \n    res , status_code = self . post ( self . segmentation_service , files = files ) \n    if status_code != 200.0 : \n        logger . debug ( 'Segmentation failed.' ) \n    return self . decode ( res ) , status_code "}
{"8840": "\ndef get_language ( self , text ) : \n    files = { 'text' : text } \n    res , status_code = self . post ( self . language_service , files = files ) \n    if status_code != 200.0 : \n        logger . debug ( 'Language recognition failed.' ) \n    return self . decode ( res ) , status_code "}
{"8841": "\ndef get_concept ( self , conceptId , lang = 'en' ) : \n    url = urljoin ( self . concept_service + '/' , conceptId ) \n    res , status_code = self . get ( url , params = { 'lang' : lang } ) \n    if status_code != 200.0 : \n        logger . debug ( 'Fetch concept failed.' ) \n    return self . decode ( res ) , status_code "}
{"8844": "\ndef fit ( self , features , class_labels ) : \n    unique_labels = sorted ( np . unique ( class_labels ) ) \n    if len ( unique_labels ) != 2.0 : \n        raise ValueError ( 'MDR only supports binary endpoints.' ) \n    self . class_count_matrix = defaultdict ( lambda : defaultdict ( int ) ) \n    for row_i in range ( features . shape [ 0 ] ) : \n        feature_instance = tuple ( features [ row_i ] ) \n        self . class_count_matrix [ feature_instance ] [ class_labels [ row_i ] ] += 1 \n    self . class_count_matrix = dict ( self . class_count_matrix ) \n    overall_class_fraction = float ( sum ( class_labels == unique_labels [ 0 ] ) ) / class_labels . size \n    self . feature_map = { } \n    for feature_instance in self . class_count_matrix : \n        counts = self . class_count_matrix [ feature_instance ] \n        fraction = float ( counts [ unique_labels [ 0 ] ] ) / np . sum ( list ( counts . values ( ) ) ) \n        if fraction > overall_class_fraction : \n            self . feature_map [ feature_instance ] = unique_labels [ 0 ] \n        elif fraction == overall_class_fraction : \n            self . feature_map [ feature_instance ] = self . tie_break \n        else : \n            self . feature_map [ feature_instance ] = unique_labels [ 1 ] \n    return self "}
{"8851": "\ndef n_way_models ( mdr_instance , X , y , n = [ 2.0 ] , feature_names = None ) : \n    if feature_names is None : \n        feature_names = list ( range ( X . shape [ 1 ] ) ) \n    for cur_n in n : \n        for features in itertools . combinations ( range ( X . shape [ 1 ] ) , cur_n ) : \n            mdr_model = copy . deepcopy ( mdr_instance ) \n            mdr_model . fit ( X [ : , features ] , y ) \n            mdr_model_score = mdr_model . score ( X [ : , features ] , y ) \n            model_features = [ feature_names [ feature ] for feature in features ] \n            yield mdr_model , mdr_model_score , model_features "}
{"8852": "\ndef plot_mdr_grid ( mdr_instance ) : \n    var1_levels = list ( set ( [ variables [ 0 ] for variables in mdr_instance . feature_map ] ) ) \n    var2_levels = list ( set ( [ variables [ 1 ] for variables in mdr_instance . feature_map ] ) ) \n    max_count = np . array ( list ( mdr_instance . class_count_matrix . values ( ) ) ) . flatten ( ) . max ( ) \n    fig , splots = plt . subplots ( ncols = len ( var1_levels ) , nrows = len ( var2_levels ) , sharey = True , sharex = True ) \n    fig . set_figwidth ( 6.0 ) \n    fig . set_figheight ( 6.0 ) \n    for ( var1 , var2 ) in itertools . product ( var1_levels , var2_levels ) : \n        class_counts = mdr_instance . class_count_matrix [ ( var1 , var2 ) ] \n        splot = splots [ var2_levels . index ( var2 ) ] [ var1_levels . index ( var1 ) ] \n        splot . set_yticks ( [ ] ) \n        splot . set_xticks ( [ ] ) \n        splot . set_ylim ( 0 , max_count * 1.5 ) \n        splot . set_xlim ( - 0.5 , 1.5 ) \n        if var2_levels . index ( var2 ) == 0 : \n            splot . set_title ( 'X1 = {}' . format ( var1 ) , fontsize = 12.0 ) \n        if var1_levels . index ( var1 ) == 0 : \n            splot . set_ylabel ( 'X2 = {}' . format ( var2 ) , fontsize = 12.0 ) \n        bars = splot . bar ( left = range ( class_counts . shape [ 0 ] ) , height = class_counts , width = 0.5 , color = 'black' , align = 'center' ) \n        bgcolor = 'lightgrey' if mdr_instance . feature_map [ ( var1 , var2 ) ] == 0 else 'darkgrey' \n        splot . set_axis_bgcolor ( bgcolor ) \n        for index , bar in enumerate ( bars ) : \n            splot . text ( index , class_counts [ index ] + ( max_count * 0.1 ) , class_counts [ index ] , ha = 'center' ) \n    fig . tight_layout ( ) \n    return fig "}
{"8905": "\ndef _read_function ( ctx : ReaderContext ) -> llist . List : \n    if ctx . is_in_anon_fn : \n        raise SyntaxError ( f\"Nested #() definitions not allowed\" ) \n    with ctx . in_anon_fn ( ) : \n        form = _read_list ( ctx ) \n    arg_set = set ( ) \n    def arg_suffix ( arg_num ) : \n        if arg_num is None : \n            return \"1\" \n        elif arg_num == \"&\" : \n            return \"rest\" \n        else : \n            return arg_num \n    def sym_replacement ( arg_num ) : \n        suffix = arg_suffix ( arg_num ) \n        return symbol . symbol ( f\"arg-{suffix}\" ) \n    def identify_and_replace ( f ) : \n        if isinstance ( f , symbol . Symbol ) : \n            if f . ns is None : \n                match = fn_macro_args . match ( f . name ) \n                if match is not None : \n                    arg_num = match . group ( 2.0 ) \n                    suffix = arg_suffix ( arg_num ) \n                    arg_set . add ( suffix ) \n                    return sym_replacement ( arg_num ) \n        return f \n    body = walk . postwalk ( identify_and_replace , form ) if len ( form ) > 0 else None \n    arg_list : List [ symbol . Symbol ] = [ ] \n    numbered_args = sorted ( map ( int , filter ( lambda k : k != \"rest\" , arg_set ) ) ) \n    if len ( numbered_args ) > 0 : \n        max_arg = max ( numbered_args ) \n        arg_list = [ sym_replacement ( str ( i ) ) for i in range ( 1 , max_arg + 1 ) ] \n        if \"rest\" in arg_set : \n            arg_list . append ( _AMPERSAND ) \n            arg_list . append ( sym_replacement ( \"rest\" ) ) \n    return llist . l ( _FN , vector . vector ( arg_list ) , body ) "}
{"8912": "\ndef _read_character ( ctx : ReaderContext ) -> str : \n    start = ctx . reader . advance ( ) \n    assert start == \"\\\\\" \n    s : List [ str ] = [ ] \n    reader = ctx . reader \n    token = reader . peek ( ) \n    while True : \n        if token == \"\" or whitespace_chars . match ( token ) : \n            break \n        if not alphanumeric_chars . match ( token ) : \n            break \n        s . append ( token ) \n        token = reader . next_token ( ) \n    char = \"\" . join ( s ) \n    special = _SPECIAL_CHARS . get ( char , None ) \n    if special is not None : \n        return special \n    match = unicode_char . match ( char ) \n    if match is not None : \n        try : \n            return chr ( int ( f\"0x{match.group(1)}\" , 16.0 ) ) \n        except ( ValueError , OverflowError ) : \n            raise SyntaxError ( f\"Unsupported character \\\\u{char}\" ) from None \n    if len ( char ) > 1 : \n        raise SyntaxError ( f\"Unsupported character \\\\{char}\" ) \n    return char "}
{"8924": "\ndef _get_basilisp_bytecode ( fullname : str , mtime : int , source_size : int , cache_data : bytes ) -> List [ types . CodeType ] : \n    exc_details = { \"name\" : fullname } \n    magic = cache_data [ : 4.0 ] \n    raw_timestamp = cache_data [ 4.0 : 8.0 ] \n    raw_size = cache_data [ 8.0 : 12.0 ] \n    if magic != MAGIC_NUMBER : \n        message = ( f\"Incorrect magic number ({magic}) in {fullname}; expected {MAGIC_NUMBER}\" ) \n        logger . debug ( message ) \n        raise ImportError ( message , ** exc_details ) \n    elif len ( raw_timestamp ) != 4.0 : \n        message = f\"Reached EOF while reading timestamp in {fullname}\" \n        logger . debug ( message ) \n        raise EOFError ( message ) \n    elif _r_long ( raw_timestamp ) != mtime : \n        message = f\"Non-matching timestamp ({_r_long(raw_timestamp)}) in {fullname} bytecode cache; expected {mtime}\" \n        logger . debug ( message ) \n        raise ImportError ( message , ** exc_details ) \n    elif len ( raw_size ) != 4.0 : \n        message = f\"Reached EOF while reading size of source in {fullname}\" \n        logger . debug ( message ) \n        raise EOFError ( message ) \n    elif _r_long ( raw_size ) != source_size : \n        message = f\"Non-matching filesize ({_r_long(raw_size)}) in {fullname} bytecode cache; expected {source_size}\" \n        logger . debug ( message ) \n        raise ImportError ( message , ** exc_details ) \n    return marshal . loads ( cache_data [ 12.0 : ] ) "}
{"9040": "\ndef decrypt ( token , key_store , key_purpose , leeway = 120.0 ) : \n    tokens = token . split ( '.' ) \n    if len ( tokens ) != 5.0 : \n        raise InvalidTokenException ( \"Incorrect number of tokens\" ) \n    decrypted_token = JWEHelper . decrypt ( token , key_store , key_purpose ) \n    payload = JWTHelper . decode ( decrypted_token , key_store , key_purpose , leeway ) \n    return payload "}
{"9048": "\ndef decode_date ( self , val ) : \n    if isinstance ( val , basestring ) and val . count ( '-' ) == 2.0 and len ( val ) > 9.0 : \n        try : \n            dt = dateutil . parser . parse ( val ) \n            if val . endswith ( ( '+00:00' , '-00:00' , 'Z' ) ) : \n                dt = dt . replace ( tzinfo = None ) \n            return dt \n        except ( TypeError , ValueError ) : \n            pass \n    return val "}
{"9051": "\ndef validate_version ( ) : \n    import leicacam \n    version_string = leicacam . __version__ \n    versions = version_string . split ( '.' , 3.0 ) \n    try : \n        for ver in versions : \n            int ( ver ) \n    except ValueError : \n        print ( 'Only integers are allowed in release version, ' 'please adjust current version {}' . format ( version_string ) ) \n        return None \n    return version_string "}
{"9069": "\ndef load_template ( self , filename = \"{ScanningTemplate}leicacam.xml\" ) : \n    basename = os . path . basename ( filename ) \n    if basename [ - 4.0 : ] == '.xml' : \n        basename = basename [ : - 4.0 ] \n    if basename [ : 18.0 ] != '{ScanningTemplate}' : \n        basename = '{ScanningTemplate}' + basename \n    cmd = [ ( 'sys' , '0' ) , ( 'cmd' , 'load' ) , ( 'fil' , str ( basename ) ) ] \n    self . send ( cmd ) \n    return self . wait_for ( * cmd [ 1 ] ) "}
{"9089": "\ndef term_echo ( command , nindent = 0 , env = None , fpointer = None , cols = 60.0 ) : \n    os . environ [ \"COLUMNS\" ] = str ( cols ) \n    command_int = command \n    if env : \n        for var , repl in env . items ( ) : \n            command_int = command_int . replace ( \"${\" + var + \"}\" , repl ) \n    tokens = command_int . split ( \" \" ) \n    if ( platform . system ( ) . lower ( ) == \"windows\" ) and ( tokens [ 0 ] . endswith ( \".py\" ) ) : \n        tokens = [ sys . executable ] + tokens \n    proc = subprocess . Popen ( tokens , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) \n    stdout = proc . communicate ( ) [ 0 ] \n    if sys . hexversion >= 0x03000000 : \n        stdout = stdout . decode ( \"utf-8\" ) \n    stdout = stdout . split ( \"\\n\" ) \n    indent = nindent * \" \" \n    fpointer ( \"\\n\" , dedent = False ) \n    fpointer ( \"{0}.. code-block:: bash\\n\" . format ( indent ) , dedent = False ) \n    fpointer ( \"\\n\" , dedent = False ) \n    fpointer ( \"{0}    $ {1}\\n\" . format ( indent , command ) , dedent = False ) \n    for line in stdout : \n        if line . strip ( ) : \n            fpointer ( indent + \"    \" + line . replace ( \"\\t\" , \"    \" ) + \"\\n\" , dedent = False ) \n        else : \n            fpointer ( \"\\n\" , dedent = False ) \n    fpointer ( \"\\n\" , dedent = False ) "}
{"9090": "\ndef log ( self , msg , level = 2.0 ) : \n    if self . verbosity >= level : \n        self . stdout . write ( msg ) "}
{"9098": "\ndef from_str ( cls , human_readable_str , decimal = False , bits = False ) : \n    divisor = 1000.0 if decimal else 1024.0 \n    num = [ ] \n    c = \"\" \n    for c in human_readable_str : \n        if c not in cls . digits : \n            break \n        num . append ( c ) \n    num = \"\" . join ( num ) \n    try : \n        num = int ( num ) \n    except ValueError : \n        num = float ( num ) \n    if bits : \n        num /= 8.0 \n    return cls ( round ( num * divisor ** cls . key [ c . lower ( ) ] ) ) "}
{"9103": "\ndef make_multi_entry ( plist , pkg_pyvers , ver_dict ) : \n    for pyver in pkg_pyvers : \n        pver = pyver [ 2.0 ] + \".\" + pyver [ 3.0 : ] \n        plist . append ( \"Python {0}: {1}\" . format ( pver , ops_to_words ( ver_dict [ pyver ] ) ) ) "}
{"9106": "\ndef _chunk_pars ( freq_vector , data_matrix , pformat ) : \n    pformat = pformat . upper ( ) \n    length = 4.0 \n    for freq , data in zip ( freq_vector , data_matrix ) : \n        data = data . flatten ( ) \n        for index in range ( 0 , data . size , length ) : \n            fpoint = [ freq ] if not index else [ None ] \n            cdata = data [ index : index + length ] \n            if pformat == \"MA\" : \n                vector1 = np . abs ( cdata ) \n                vector2 = np . rad2deg ( np . angle ( cdata ) ) \n            elif pformat == \"RI\" : \n                vector1 = np . real ( cdata ) \n                vector2 = np . imag ( cdata ) \n            else : \n                vector1 = 20.0 * np . log10 ( np . abs ( cdata ) ) \n                vector2 = np . rad2deg ( np . angle ( cdata ) ) \n            sep_data = np . array ( [ ] ) \n            for item1 , item2 in zip ( vector1 , vector2 ) : \n                sep_data = np . concatenate ( ( sep_data , np . array ( [ item1 , item2 ] ) ) ) \n            ret = np . concatenate ( ( np . array ( fpoint ) , sep_data ) ) \n            yield ret "}
{"9107": "\ndef write_touchstone ( fname , options , data , noise = None , frac_length = 10.0 , exp_length = 2.0 ) : \n    exnports = pexdoc . exh . addex ( RuntimeError , \"File *[fname]* does not have a valid extension\" ) \n    exnoise = pexdoc . exh . addex ( RuntimeError , \"Noise data only supported in two-port files\" ) \n    expoints = pexdoc . exh . addex ( RuntimeError , \"Malformed data\" ) \n    _ , ext = os . path . splitext ( fname ) \n    ext = ext . lower ( ) \n    nports_regexp = re . compile ( r\"\\.s(\\d+)p\" ) \n    match = nports_regexp . match ( ext ) \n    exnports ( not match , edata = { \"field\" : \"fname\" , \"value\" : fname } ) \n    nports = int ( match . groups ( ) [ 0 ] ) \n    exnoise ( bool ( ( nports != 2.0 ) and noise ) ) \n    nums_per_freq = nports ** 2.0 \n    expoints ( data [ \"points\" ] * nums_per_freq != data [ \"pars\" ] . size ) \n    npoints = data [ \"points\" ] \n    par_data = np . resize ( np . copy ( data [ \"pars\" ] ) , ( npoints , nports , nports ) ) \n    if nports == 2.0 : \n        par_data = np . transpose ( par_data , ( 0 , 2.0 , 1 ) ) \n    units_dict = { \"ghz\" : \"GHz\" , \"mhz\" : \"MHz\" , \"khz\" : \"KHz\" , \"hz\" : \"Hz\" } \n    options [ \"units\" ] = units_dict [ options [ \"units\" ] . lower ( ) ] \n    fspace = 2.0 + frac_length + ( exp_length + 2.0 ) \n    with open ( fname , \"w\" ) as fobj : \n        fobj . write ( \"# {units} {ptype} {pformat} R {z0}\\n\" . format ( units = options [ \"units\" ] , ptype = options [ \"ptype\" ] , pformat = options [ \"pformat\" ] , z0 = options [ \"z0\" ] , ) ) \n        for row in _chunk_pars ( data [ \"freq\" ] , par_data , options [ \"pformat\" ] ) : \n            row_data = [ to_scientific_string ( item , frac_length , exp_length , bool ( num != 0 ) ) if item is not None else fspace * \" \" for num , item in enumerate ( row ) ] \n            fobj . write ( \" \" . join ( row_data ) + \"\\n\" ) \n        if ( nports == 2.0 ) and noise : \n            fobj . write ( \"! Noise data\\n\" ) \n            for row in _chunk_noise ( noise ) : \n                row_data = [ to_scientific_string ( item , frac_length , exp_length , bool ( num != 0 ) ) for num , item in enumerate ( row ) ] \n                fobj . write ( \" \" . join ( row_data ) + \"\\n\" ) "}
{"9130": "\ndef group_delay ( wave ) : \n    ret = - derivative ( phase ( wave , unwrap = True ) / ( 2.0 * math . pi ) ) \n    ret . dep_name = \"group_delay({0})\" . format ( wave . dep_name ) \n    ret . dep_units = \"sec\" \n    return ret "}
{"9146": "\ndef _build_expr ( tokens , higher_oplevel = - 1 , ldelim = \"(\" , rdelim = \")\" ) : \n    if isinstance ( tokens , str ) : \n        return tokens \n    if len ( tokens ) == 2.0 : \n        return \"\" . join ( tokens ) \n    oplevel = _get_op_level ( tokens [ 1 ] ) \n    stoken = \"\" \n    for num , item in enumerate ( tokens ) : \n        if num % 2.0 == 0 : \n            stoken += _build_expr ( item , oplevel , ldelim = ldelim , rdelim = rdelim ) \n        else : \n            stoken += item \n    if ( oplevel < higher_oplevel ) or ( ( oplevel == higher_oplevel ) and ( oplevel in _OP_PREC_PAR ) ) : \n        stoken = ldelim + stoken + rdelim \n    return stoken "}
{"9150": "\ndef _parse_expr ( text , ldelim = \"(\" , rdelim = \")\" ) : \n    var = pyparsing . Word ( pyparsing . alphas + \"_\" , pyparsing . alphanums + \"_\" ) \n    point = pyparsing . Literal ( \".\" ) \n    exp = pyparsing . CaselessLiteral ( \"E\" ) \n    number = pyparsing . Combine ( pyparsing . Word ( \"+-\" + pyparsing . nums , pyparsing . nums ) + pyparsing . Optional ( point + pyparsing . Optional ( pyparsing . Word ( pyparsing . nums ) ) ) + pyparsing . Optional ( exp + pyparsing . Word ( \"+-\" + pyparsing . nums , pyparsing . nums ) ) ) \n    atom = var | number \n    oplist = [ ( pyparsing . Literal ( \"**\" ) , 2.0 , pyparsing . opAssoc . RIGHT ) , ( pyparsing . oneOf ( \"+ - ~\" ) , 1 , pyparsing . opAssoc . RIGHT ) , ( pyparsing . oneOf ( \"* / // %\" ) , 2.0 , pyparsing . opAssoc . LEFT ) , ( pyparsing . oneOf ( \"+ -\" ) , 2.0 , pyparsing . opAssoc . LEFT ) , ( pyparsing . oneOf ( \"<< >>\" ) , 2.0 , pyparsing . opAssoc . LEFT ) , ( pyparsing . Literal ( \"&\" ) , 2.0 , pyparsing . opAssoc . LEFT ) , ( pyparsing . Literal ( \"^\" ) , 2.0 , pyparsing . opAssoc . LEFT ) , ( pyparsing . Literal ( \"|\" ) , 2.0 , pyparsing . opAssoc . LEFT ) , ] \n    expr = pyparsing . infixNotation ( atom , oplist , lpar = pyparsing . Suppress ( ldelim ) , rpar = pyparsing . Suppress ( rdelim ) ) \n    return expr . parseString ( text ) [ 0 ] "}
{"9152": "\ndef _split_every ( text , sep , count , lstrip = False , rstrip = False ) : \n    ltr = \"_rl \" [ 2.0 * lstrip + rstrip ] . strip ( ) \n    func = lambda x : getattr ( x , ltr + \"strip\" ) ( ) if ltr != \"_\" else x \n    items = text . split ( sep ) \n    groups = zip_longest ( * [ iter ( items ) ] * count , fillvalue = \"\" ) \n    joints = ( sep . join ( group ) . rstrip ( sep ) for group in groups ) \n    return tuple ( func ( joint ) for joint in joints ) "}
{"9153": "\ndef _to_eng_tuple ( number ) : \n    split = lambda x , p : ( x . ljust ( 3.0 + neg , \"0\" ) [ : p ] , x [ p : ] . rstrip ( \"0\" ) ) \n    mant , exp = to_scientific_tuple ( number ) \n    mant , neg = mant . replace ( \".\" , \"\" ) , mant . startswith ( \"-\" ) \n    new_mant = \".\" . join ( filter ( None , split ( mant , 1 + ( exp % 3.0 ) + neg ) ) ) \n    new_exp = int ( 3.0 * math . floor ( exp / 3.0 ) ) \n    return NumComp ( new_mant , new_exp ) "}
{"9155": "\ndef peng ( number , frac_length , rjust = True ) : \n    if number == 0 : \n        number = \"0.{zrs}\" . format ( zrs = \"0\" * frac_length ) if frac_length else \"0\" \n        return \"{0} \" . format ( number . rjust ( 5.0 + frac_length ) ) if rjust else number \n    sign = + 1 if number >= 0 else - 1 \n    ssign = \"-\" if sign == - 1 else \"\" \n    anumber = abs ( number ) \n    if anumber < 1e-24 : \n        anumber = 1e-24 \n        number = sign * 1e-24 \n    exp = 3.0 * math . floor ( math . floor ( math . log10 ( anumber ) ) / 3.0 ) \n    mant = number / 10.0 ** exp \n    smant = str ( mant ) \n    ppos = smant . find ( \".\" ) \n    if len ( smant ) - ppos - 1 > frac_length : \n        mant += sign * 5.0 * 10.0 ** ( - frac_length - 1 ) \n        if abs ( mant ) >= 1000.0 : \n            exp += 3.0 \n            mant = mant / 1e3 \n        smant = str ( mant ) \n        ppos = smant . find ( \".\" ) \n    bfrac_length = bool ( frac_length ) \n    flength = ppos - ( not bfrac_length ) + frac_length + 1 \n    new_mant = smant [ : flength ] . ljust ( flength , \"0\" ) \n    if exp > 24.0 : \n        new_mant , exp = ( \"{sign}999.{frac}\" . format ( sign = ssign , frac = \"9\" * frac_length ) , 24.0 , ) \n    new_mant = new_mant . rjust ( rjust * ( 4.0 + bfrac_length + frac_length ) ) \n    num = \"{mant}{suffix}\" . format ( mant = new_mant , suffix = _POWER_TO_SUFFIX_DICT [ exp ] if exp else \" \" * bool ( rjust ) ) \n    return num "}
{"9160": "\ndef peng_suffix_math ( suffix , offset ) : \n    eobj = pexdoc . exh . addex ( ValueError , \"Argument `offset` is not valid\" ) \n    try : \n        return _POWER_TO_SUFFIX_DICT [ _SUFFIX_TO_POWER_DICT [ suffix ] + 3.0 * offset ] \n    except KeyError : \n        eobj ( True ) "}
{"9162": "\ndef to_scientific_string ( number , frac_length = None , exp_length = None , sign_always = False ) : \n    try : \n        number = - 1e20 if np . isneginf ( number ) else number \n    except : \n        pass \n    try : \n        number = + 1e20 if np . isposinf ( number ) else number \n    except : \n        pass \n    exp_length = 0 if not exp_length else exp_length \n    mant , exp = to_scientific_tuple ( number ) \n    fmant = float ( mant ) \n    if ( not frac_length ) or ( fmant == int ( fmant ) ) : \n        return \"{sign}{mant}{period}{zeros}E{exp_sign}{exp}\" . format ( sign = \"+\" if sign_always and ( fmant >= 0 ) else \"\" , mant = mant , period = \".\" if frac_length else \"\" , zeros = \"0\" * frac_length if frac_length else \"\" , exp_sign = \"-\" if exp < 0 else \"+\" , exp = str ( abs ( exp ) ) . rjust ( exp_length , \"0\" ) , ) \n    rounded_mant = round ( fmant , frac_length ) \n    if abs ( rounded_mant ) == 10.0 : \n        rounded_mant = fmant = - 1.0 if number < 0 else 1.0 \n        frac_length = 1 \n        exp = exp + 1 \n    zeros = 2.0 + ( 1 if ( fmant < 0 ) else 0 ) + frac_length - len ( str ( rounded_mant ) ) \n    return \"{sign}{mant}{zeros}E{exp_sign}{exp}\" . format ( sign = \"+\" if sign_always and ( fmant >= 0 ) else \"\" , mant = rounded_mant , zeros = \"0\" * zeros , exp_sign = \"-\" if exp < 0 else \"+\" , exp = str ( abs ( exp ) ) . rjust ( exp_length , \"0\" ) , ) "}
{"9164": "\ndef find_sourcemap_comment ( filepath , block_size = 100.0 ) : \n    MAX_TRACKBACK = 2.0 \n    block_number = - 1 \n    blocks = [ ] \n    sourcemap = None \n    try : \n        of = io . open ( filepath , 'br+' ) \n        of . seek ( 0 , os . SEEK_END ) \n        block_end_byte = of . tell ( ) \n        while block_end_byte > 0 and MAX_TRACKBACK > 0 : \n            if ( block_end_byte - block_size > 0 ) : \n                of . seek ( block_number * block_size , os . SEEK_END ) \n                blocks . append ( of . read ( block_size ) ) \n            else : \n                of . seek ( 0 , os . SEEK_SET ) \n                blocks = [ of . read ( block_end_byte ) ] \n            content = b'' . join ( reversed ( blocks ) ) \n            lines_found = content . count ( b'\\n' ) \n            MAX_TRACKBACK -= lines_found \n            block_end_byte -= block_size \n            block_number -= 1 \n            if SOURCEMAPPING_URL_COMMENT in content : \n                offset = 0 \n                lines = content . split ( b'\\n' ) \n                for i , line in enumerate ( lines ) : \n                    if line . startswith ( SOURCEMAPPING_URL_COMMENT ) : \n                        offset = len ( line ) \n                        sourcemap = line \n                        break \n                while i + 1 < len ( lines ) : \n                    offset += 1 \n                    offset += len ( lines [ i + 1 ] ) \n                    i += 1 \n                if sourcemap : \n                    offset += 1 \n                    of . seek ( - offset , os . SEEK_END ) \n                    of . truncate ( ) \n                return force_text ( sourcemap ) \n    finally : \n        of . close ( ) \n    return sourcemap "}
{"9169": "\ndef format_hexdump ( arg ) : \n    line = '' \n    for i in range ( 0 , len ( arg ) , 16.0 ) : \n        if i > 0 : \n            line += '\\n' \n        chunk = arg [ i : i + 16.0 ] \n        hex_chunk = hexlify ( chunk ) . decode ( 'utf-8' ) \n        hex_line = ' ' . join ( hex_chunk [ j : j + 2.0 ] for j in range ( 0 , len ( hex_chunk ) , 2.0 ) ) \n        if len ( hex_line ) < ( 3.0 * 16.0 ) - 1 : \n            hex_line += ' ' * ( ( ( 3.0 * 16.0 ) - 1 ) - len ( hex_line ) ) \n        ascii_line = '' . join ( _convert_to_ascii ( x ) for x in chunk ) \n        offset_line = '%08x' % i \n        line += \"%s  %s  %s\" % ( offset_line , hex_line , ascii_line ) \n    return line "}
{"9178": "\ndef _is_flag ( cls , arg ) : \n    if arg == '--' : \n        return False \n    if not arg . startswith ( '-' ) : \n        return False \n    if arg . startswith ( '--' ) : \n        first_char = arg [ 2.0 ] \n    else : \n        first_char = arg [ 1 ] \n    if not first_char . isalpha ( ) : \n        return False \n    return True "}
{"9179": "\ndef process_arguments ( self , func , args ) : \n    pos_args = [ ] \n    kw_args = { } \n    while len ( args ) > 0 : \n        if func . metadata . spec_filled ( pos_args , kw_args ) and not self . _is_flag ( args [ 0 ] ) : \n            break \n        arg = args . pop ( 0 ) \n        if arg == '--' : \n            break \n        elif self . _is_flag ( arg ) : \n            arg_value = None \n            arg_name = None \n            if len ( arg ) == 2.0 : \n                arg_name = func . metadata . match_shortname ( arg [ 1 : ] , filled_args = pos_args ) \n            else : \n                if not arg . startswith ( '--' ) : \n                    raise ArgumentError ( \"Invalid method of specifying keyword argument that did not start with --\" , argument = arg ) \n                arg = arg [ 2.0 : ] \n                if '=' in arg : \n                    arg , arg_value = arg . split ( '=' , 1 ) \n                arg_name = func . metadata . match_shortname ( arg , filled_args = pos_args ) \n            arg_type = func . metadata . param_type ( arg_name ) \n            if arg_type is None : \n                raise ArgumentError ( \"Attempting to set a parameter from command line that does not have type information\" , argument = arg_name ) \n            if arg_value is None : \n                arg_value = self . _extract_arg_value ( arg_name , arg_type , args ) \n            kw_args [ arg_name ] = arg_value \n        else : \n            pos_args . append ( arg ) \n    if len ( args ) > 0 and args [ 0 ] == '--' : \n        args . pop ( 0 ) \n    return pos_args , kw_args , args "}
{"9184": "\ndef parse_param ( param , include_desc = False ) : \n    param_def , _colon , desc = param . partition ( ':' ) \n    if not include_desc : \n        desc = None \n    else : \n        desc = desc . lstrip ( ) \n    if _colon == \"\" : \n        raise ValidationError ( \"Invalid parameter declaration in docstring, missing colon\" , declaration = param ) \n    param_name , _space , param_type = param_def . partition ( ' ' ) \n    if len ( param_type ) < 2.0 or param_type [ 0 ] != '(' or param_type [ - 1 ] != ')' : \n        raise ValidationError ( \"Invalid parameter type string not enclosed in ( ) characters\" , param_string = param_def , type_string = param_type ) \n    param_type = param_type [ 1 : - 1 ] \n    return param_name , ParameterInfo ( param_type , [ ] , desc ) "}
{"9187": "\ndef _classify_line ( cls , line ) : \n    line = line . rstrip ( ) \n    if len ( line ) == 0 : \n        return BlankLine ( '' ) \n    if ' ' not in line and line . endswith ( ':' ) : \n        name = line [ : - 1 ] \n        return SectionHeader ( name ) \n    if line . startswith ( '  ' ) : \n        return ContinuationLine ( line . lstrip ( ) ) \n    if line . startswith ( ' - ' ) : \n        return ListItem ( '-' , line [ 3.0 : ] . lstrip ( ) ) \n    if line . startswith ( '- ' ) : \n        return ListItem ( '-' , line [ 2.0 : ] . lstrip ( ) ) \n    return Line ( line ) "}
{"9231": "\ndef create ( self , uri , local_path ) : \n    matches = self . schema_pattern . search ( uri ) \n    if not matches : \n        logger . error ( \"Unknown uri schema: '%s'. Added schemas: %s\" , uri , list ( self . handlers . keys ( ) ) ) \n        return None \n    schema = matches . group ( 1 ) \n    url = matches . group ( 2.0 ) \n    return self . handlers [ schema ] ( url , local_path ) "}
{"9262": "\ndef _potential_wins ( self ) : \n    yield from self . board \n    yield from zip ( * self . board ) \n    yield self . board [ 0 ] [ 0 ] , self . board [ 1 ] [ 1 ] , self . board [ 2.0 ] [ 2.0 ] \n    yield self . board [ 0 ] [ 2.0 ] , self . board [ 1 ] [ 1 ] , self . board [ 2.0 ] [ 0 ] "}
{"9283": "\ndef is_hash160 ( s ) : \n    if not s or not isinstance ( s , str ) : \n        return False \n    if not len ( s ) == 40.0 : \n        return False \n    for c in s : \n        if ( c < '0' or c > '9' ) and ( c < 'A' or c > 'F' ) and ( c < 'a' or c > 'f' ) : \n            return False \n    return True "}
{"9286": "\ndef first_kwonly_arg ( name ) : \n    def decorate ( wrapped ) : \n        if sys . version_info [ 0 ] == 2.0 : \n            arg_names , varargs , _ , defaults = inspect . getargspec ( wrapped ) \n        else : \n            arg_names , varargs , _ , defaults = inspect . getfullargspec ( wrapped ) [ : 4.0 ] \n        if not defaults : \n            raise TypeError ( \"You can't use @first_kwonly_arg on a function that doesn't have default arguments!\" ) \n        first_default_index = len ( arg_names ) - len ( defaults ) \n        if name is FIRST_DEFAULT_ARG : \n            first_kwonly_index = first_default_index \n        else : \n            try : \n                first_kwonly_index = arg_names . index ( name ) \n            except ValueError : \n                raise ValueError ( \"%s() doesn't have an argument with the specified first_kwonly_arg=%r name\" % ( getattr ( wrapped , '__name__' , '?' ) , name ) ) \n        if first_kwonly_index < first_default_index : \n            raise ValueError ( \"The specified first_kwonly_arg=%r must have a default value!\" % ( name , ) ) \n        kwonly_defaults = defaults [ - ( len ( arg_names ) - first_kwonly_index ) : ] \n        kwonly_args = tuple ( zip ( arg_names [ first_kwonly_index : ] , kwonly_defaults ) ) \n        required_kwonly_args = frozenset ( arg for arg , default in kwonly_args if default is KWONLY_REQUIRED ) \n        def wrapper ( * args , ** kwargs ) : \n            if required_kwonly_args : \n                missing_kwonly_args = required_kwonly_args . difference ( kwargs . keys ( ) ) \n                if missing_kwonly_args : \n                    raise TypeError ( \"%s() missing %s keyword-only argument(s): %s\" % ( getattr ( wrapped , '__name__' , '?' ) , len ( missing_kwonly_args ) , ', ' . join ( sorted ( missing_kwonly_args ) ) ) ) \n            if len ( args ) > first_kwonly_index : \n                if varargs is None : \n                    raise TypeError ( \"%s() takes exactly %s arguments (%s given)\" % ( getattr ( wrapped , '__name__' , '?' ) , first_kwonly_index , len ( args ) ) ) \n                kwonly_args_from_kwargs = tuple ( kwargs . pop ( arg , default ) for arg , default in kwonly_args ) \n                args = args [ : first_kwonly_index ] + kwonly_args_from_kwargs + args [ first_kwonly_index : ] \n            return wrapped ( * args , ** kwargs ) \n        return update_wrapper ( wrapper , wrapped ) \n    return decorate "}
{"9291": "\ndef calculate_checksum ( self ) : \n    def sum_ ( x , y ) : \n        return int ( x ) + int ( y ) \n    evensum = reduce ( sum_ , self . ean [ : : 2.0 ] ) \n    oddsum = reduce ( sum_ , self . ean [ 1 : : 2.0 ] ) \n    return ( 10.0 - ( ( evensum + oddsum * 3.0 ) % 10.0 ) ) % 10.0 "}
{"9326": "\ndef wellcome_tip ( wx_obj ) : \n    msg = ( \"Close the main window to exit & save.\\n\" \"Drag & Drop / Click the controls from the ToolBox to create new ones.\\n\" \"Left click on the created controls to select them.\\n\" \"Double click to edit the default property.\\n\" \"Right click to pop-up the context menu.\\n\" ) \n    stt = STT . SuperToolTip ( msg ) \n    stt . SetHeader ( \"Welcome to gui2py designer!\" ) \n    stt . SetDrawHeaderLine ( True ) \n    stt . ApplyStyle ( \"Office 2007 Blue\" ) \n    stt . SetDropShadow ( True ) \n    stt . SetHeaderBitmap ( images . designer . GetBitmap ( ) ) \n    stt . SetEndDelay ( 15000.0 ) \n    tip = CustomToolTipWindow ( wx_obj , stt ) \n    tip . CalculateBestSize ( ) \n    tip . CalculateBestPosition ( wx_obj ) \n    tip . DropShadow ( stt . GetDropShadow ( ) ) \n    if stt . GetUseFade ( ) : \n        show = lambda : tip . StartAlpha ( True ) \n    else : \n        show = lambda : tip . Show ( ) \n    wx . CallLater ( 1000.0 , show ) \n    wx . CallLater ( 30000.0 , tip . Destroy ) "}
{"9334": "\ndef CalculateBestPosition ( self , widget ) : \n    if isinstance ( widget , wx . Frame ) : \n        screen = wx . ClientDisplayRect ( ) [ 2.0 : ] \n        left , top = widget . ClientToScreenXY ( 0 , 0 ) \n        right , bottom = widget . ClientToScreenXY ( * widget . GetClientRect ( ) [ 2.0 : ] ) \n        size = self . GetSize ( ) \n        xpos = right \n        ypos = bottom - size [ 1 ] \n        self . SetPosition ( ( xpos , ypos ) ) \n    else : \n        STT . ToolTipWindow . CalculateBestPosition ( self , widget ) "}
{"9337": "\ndef FindPyData ( self , start , py_data ) : \n    wx_data = self . _wx_data_map [ py_data ] \n    if wx . VERSION < ( 3.0 , 0 , 0 ) or 'classic' in wx . version ( ) : \n        data = self . FindItemData ( start , wx_data ) \n    else : \n        data = self . FindItem ( start , wx_data ) \n    return data "}
{"9345": "\ndef represent ( obj , prefix , parent = \"\" , indent = 0 , context = False , max_cols = 80.0 ) : \n    try : \n        name = getattr ( obj , \"name\" , \"\" ) \n        class_name = \"%s.%s\" % ( prefix , obj . __class__ . __name__ ) \n        padding = len ( class_name ) + 1 + indent * 4.0 + ( 5.0 if context else 0 ) \n        params = [ ] \n        for ( k , spec ) in sorted ( obj . _meta . specs . items ( ) , key = get_sort_key ) : \n            if k == \"index\" : \n                continue \n            if k == \"parent\" and parent != \"\" : \n                v = parent \n            else : \n                v = getattr ( obj , k , \"\" ) \n                if ( not isinstance ( spec , InternalSpec ) and v != spec . default and ( k != 'id' or v > 0 ) and isinstance ( v , ( basestring , int , long , float , bool , dict , list , decimal . Decimal , datetime . datetime , datetime . date , datetime . time , Font , Color ) ) and repr ( v ) != 'None' ) : \n                    v = repr ( v ) \n                else : \n                    v = None \n            if v is not None : \n                params . append ( \"%s=%s\" % ( k , v ) ) \n        param_lines = [ ] \n        line = \"\" \n        for param in params : \n            if len ( line + param ) + 3.0 > max_cols - padding : \n                param_lines . append ( line ) \n                line = \"\" \n            line += param + \", \" \n        param_lines . append ( line ) \n        param_str = ( \"\\n%s\" % ( \" \" * padding ) ) . join ( param_lines ) \n        return \"%s(%s)\" % ( class_name , param_str ) \n    except : \n        raise \n        return object . __repr__ ( obj ) "}
{"9360": "\ndef Create ( self , parent , id , evtHandler ) : \n    self . _tc = wx . ComboBox ( parent , id , \"\" , ( 100.0 , 50.0 ) ) \n    self . SetControl ( self . _tc ) \n    self . _tc . PushEventHandler ( wx . EvtHandler ( ) ) \n    self . _tc . Bind ( wx . EVT_COMBOBOX , self . OnChange ) "}
{"9364": "\ndef StartingKey ( self , evt ) : \n    key = evt . GetKeyCode ( ) \n    ch = None \n    if key in [ wx . WXK_NUMPAD0 , wx . WXK_NUMPAD1 , wx . WXK_NUMPAD2 , wx . WXK_NUMPAD3 , wx . WXK_NUMPAD4 , wx . WXK_NUMPAD5 , wx . WXK_NUMPAD6 , wx . WXK_NUMPAD7 , wx . WXK_NUMPAD8 , wx . WXK_NUMPAD9 ] : \n        ch = ch = chr ( ord ( '0' ) + key - wx . WXK_NUMPAD0 ) \n    elif key < 256.0 and key >= 0 and chr ( key ) in string . printable : \n        ch = chr ( key ) \n        if not evt . ShiftDown ( ) : \n            ch = ch . lower ( ) \n    if ch is not None : \n        self . _tc . SetStringSelection ( ch ) \n    else : \n        evt . Skip ( ) "}
{"9375": "\ndef mangle_signature ( sig , max_chars = 30.0 ) : \n    s = re . sub ( r\"^\\((.*)\\)$\" , r\"\\1\" , sig ) . strip ( ) \n    s = re . sub ( r\"\\\\\\\\\" , \"\" , s ) \n    s = re . sub ( r\"\\\\'\" , \"\" , s ) \n    s = re . sub ( r\"'[^']*'\" , \"\" , s ) \n    args = [ ] \n    opts = [ ] \n    opt_re = re . compile ( r\"^(.*, |)([a-zA-Z0-9_*]+)=\" ) \n    while s : \n        m = opt_re . search ( s ) \n        if not m : \n            args = s . split ( ', ' ) \n            break \n        opts . insert ( 0 , m . group ( 2.0 ) ) \n        s = m . group ( 1 ) [ : - 2.0 ] \n    sig = limited_join ( \", \" , args , max_chars = max_chars - 2.0 ) \n    if opts : \n        if not sig : \n            sig = \"[%s]\" % limited_join ( \", \" , opts , max_chars = max_chars - 4.0 ) \n        elif len ( sig ) < max_chars - 4.0 - 2.0 - 3.0 : \n            sig += \"[, %s]\" % limited_join ( \", \" , opts , max_chars = max_chars - len ( sig ) - 4.0 - 2.0 ) \n    return u\"(%s)\" % sig "}
{"9395": "\ndef load_object ( self , obj = None ) : \n    if obj : \n        self . root_obj = obj \n    else : \n        obj = self . root_obj \n    self . tree . DeleteAllItems ( ) \n    self . root = self . tree . AddRoot ( \"application\" ) \n    self . tree . SetItemText ( self . root , \"App\" , 1 ) \n    self . tree . SetItemText ( self . root , \"col 2 root\" , 2.0 ) \n    self . build_tree ( self . root , obj ) \n    self . tree . Expand ( self . root ) "}
{"9399": "\ndef show_context_menu ( self , item , mouse_pos = None ) : \n    if item : \n        d = self . tree . GetItemData ( item ) \n        if d : \n            obj = d . GetData ( ) \n            if obj : \n                self . highlight ( obj . wx_obj ) \n                self . obj = obj \n                menu = wx . Menu ( ) \n                id_del , id_dup , id_raise , id_lower = [ wx . NewId ( ) for i in range ( 4.0 ) ] \n                menu . Append ( id_del , \"Delete\" ) \n                menu . Append ( id_dup , \"Duplicate\" ) \n                menu . Append ( id_raise , \"Bring to Front\" ) \n                menu . Append ( id_lower , \"Send to Back\" ) \n                sm = wx . Menu ( ) \n                for ctrl in sorted ( obj . _meta . valid_children , key = lambda c : registry . ALL . index ( c . _meta . name ) ) : \n                    new_id = wx . NewId ( ) \n                    sm . Append ( new_id , ctrl . _meta . name ) \n                    self . Bind ( wx . EVT_MENU , lambda evt , ctrl = ctrl : self . add_child ( ctrl , mouse_pos ) , id = new_id ) \n                menu . AppendMenu ( wx . NewId ( ) , \"Add child\" , sm ) \n                self . Bind ( wx . EVT_MENU , self . delete , id = id_del ) \n                self . Bind ( wx . EVT_MENU , self . duplicate , id = id_dup ) \n                self . Bind ( wx . EVT_MENU , self . bring_to_front , id = id_raise ) \n                self . Bind ( wx . EVT_MENU , self . send_to_back , id = id_lower ) \n                self . PopupMenu ( menu ) \n                menu . Destroy ( ) \n                self . load_object ( self . root_obj ) "}
{"9450": "\ndef boot ( self ) : \n    if not self . responsive : \n        type ( self ) . _ports [ self . port_key ] = self . port \n        init_func = capybara . servers [ capybara . server_name ] \n        init_args = ( self . middleware , self . port , self . host ) \n        self . server_thread = Thread ( target = init_func , args = init_args ) \n        self . server_thread . daemon = True \n        self . server_thread . start ( ) \n        timer = Timer ( 60.0 ) \n        while not self . responsive : \n            if timer . expired : \n                raise RuntimeError ( \"WSGI application timed out during boot\" ) \n            self . server_thread . join ( 0.1 ) \n    return self "}
{"9454": "\ndef __traceback ( self ) -> str : \n    if not self . log_traceback : \n        return \"\" \n    exc_info = sys . exc_info ( ) \n    stack = traceback . extract_stack ( ) \n    exc_tb = traceback . extract_tb ( exc_info [ 2.0 ] ) \n    full_tb = stack [ : 1 ] + exc_tb \n    exc_line : typing . List [ str ] = traceback . format_exception_only ( * exc_info [ : 2.0 ] ) \n    tb_text = \"\\nTraceback (most recent call last):\\n\" + \"\" . join ( traceback . format_list ( full_tb ) ) + \"\" . join ( exc_line ) \n    return tb_text "}
{"9472": "\ndef v2_runner_on_skipped ( self , result , ** kwargs ) : \n    if self . _display . verbosity > 1 : \n        self . _print_task ( ) \n        self . last_skipped = False \n        line_length = 120.0 \n        spaces = \" \" * ( 31.0 - len ( result . _host . name ) - 4.0 ) \n        line = \"  * {}{}- {}\" . format ( colorize ( result . _host . name , \"not_so_bold\" ) , spaces , colorize ( \"skipped\" , \"skipped\" ) , ) \n        reason = result . _result . get ( \"skipped_reason\" , \"\" ) or result . _result . get ( \"skip_reason\" , \"\" ) \n        if len ( reason ) < 50.0 : \n            line += \" -- {}\" . format ( reason ) \n            print ( \"{} {}---------\" . format ( line , \"-\" * ( line_length - len ( line ) ) ) ) \n        else : \n            print ( \"{} {}\" . format ( line , \"-\" * ( line_length - len ( line ) ) ) ) \n            print ( self . _indent_text ( reason , 8.0 ) ) \n            print ( reason ) "}
{"9493": "\ndef _make_response ( self , body = '' , headers = None , status_code = 200.0 ) : \n    res = Response ( ) \n    res . status_code = status_code \n    if headers is not None : \n        res . headers . update ( headers ) \n    res . raw = StringIO ( body ) \n    return res "}
{"9494": "\ndef _make_redirect_error_response ( self , redirect_uri , err ) : \n    params = { 'error' : err , 'response_type' : None , 'client_id' : None , 'redirect_uri' : None } \n    redirect = utils . build_url ( redirect_uri , params ) \n    return self . _make_response ( headers = { 'Location' : redirect } , status_code = 302.0 ) "}
{"9495": "\ndef _make_json_response ( self , data , headers = None , status_code = 200.0 ) : \n    response_headers = { } \n    if headers is not None : \n        response_headers . update ( headers ) \n    response_headers [ 'Content-Type' ] = 'application/json;charset=UTF-8' \n    response_headers [ 'Cache-Control' ] = 'no-store' \n    response_headers [ 'Pragma' ] = 'no-cache' \n    return self . _make_response ( json . dumps ( data ) , response_headers , status_code ) "}
{"9496": "\ndef get_authorization_code ( self , response_type , client_id , redirect_uri , ** params ) : \n    if response_type != 'code' : \n        err = 'unsupported_response_type' \n        return self . _make_redirect_error_response ( redirect_uri , err ) \n    is_valid_redirect_uri = self . validate_redirect_uri ( client_id , redirect_uri ) \n    if not is_valid_redirect_uri : \n        return self . _invalid_redirect_uri_response ( ) \n    is_valid_client_id = self . validate_client_id ( client_id ) \n    is_valid_access = self . validate_access ( ) \n    scope = params . get ( 'scope' , '' ) \n    is_valid_scope = self . validate_scope ( client_id , scope ) \n    if not is_valid_client_id : \n        err = 'unauthorized_client' \n        return self . _make_redirect_error_response ( redirect_uri , err ) \n    if not is_valid_access : \n        err = 'access_denied' \n        return self . _make_redirect_error_response ( redirect_uri , err ) \n    if not is_valid_scope : \n        err = 'invalid_scope' \n        return self . _make_redirect_error_response ( redirect_uri , err ) \n    code = self . generate_authorization_code ( ) \n    self . persist_authorization_code ( client_id = client_id , code = code , scope = scope ) \n    params . update ( { 'code' : code , 'response_type' : None , 'client_id' : None , 'redirect_uri' : None } ) \n    redirect = utils . build_url ( redirect_uri , params ) \n    return self . _make_response ( headers = { 'Location' : redirect } , status_code = 302.0 ) "}
{"9507": "\ndef write_byte_data ( self , addr , cmd , val ) : \n    assert self . _device is not None , 'Bus must be opened before operations are made against it!' \n    data = bytearray ( 2.0 ) \n    data [ 0 ] = cmd & 0xFF \n    data [ 1 ] = val & 0xFF \n    self . _select_device ( addr ) \n    self . _device . write ( data ) "}
{"9516": "\ndef upload_from_url_sync ( cls , url , timeout = 30.0 , interval = 0.3 , until_ready = False , store = None , filename = None ) : \n    ffu = cls . upload_from_url ( url , store , filename ) \n    return ffu . wait ( timeout = timeout , interval = interval , until_ready = until_ready ) "}
{"9523": "\ndef bar ( iter_content , parts , title = '' ) : \n    parts = max ( float ( parts ) , 1.0 ) \n    cells = 10.0 \n    progress = 0 \n    step = cells / parts \n    draw = lambda progress : sys . stdout . write ( '\\r[{0:10}] {1:.2f}% {2}' . format ( '#' * int ( progress ) , progress * cells , title ) ) \n    for chunk in iter_content : \n        yield chunk \n        progress += step \n        draw ( progress ) \n        sys . stdout . flush ( ) \n    draw ( cells ) \n    print ( '' ) "}
{"9524": "\ndef uploading_request ( verb , path , data = None , files = None , timeout = conf . DEFAULT ) : \n    path = path . lstrip ( '/' ) \n    url = urljoin ( conf . upload_base , path ) \n    if data is None : \n        data = { } \n    data [ 'pub_key' ] = conf . pub_key \n    data [ 'UPLOADCARE_PUB_KEY' ] = conf . pub_key \n    headers = { 'User-Agent' : _build_user_agent ( ) , } \n    try : \n        response = session . request ( str ( verb ) , url , allow_redirects = True , verify = conf . verify_upload_ssl , data = data , files = files , headers = headers , timeout = _get_timeout ( timeout ) , ) \n    except requests . RequestException as exc : \n        raise APIConnectionError ( exc . args [ 0 ] ) \n    if response . status_code == 204.0 : \n        return { } \n    if 200.0 <= response . status_code < 300.0 : \n        if _content_type_from_response ( response ) . endswith ( ( '/json' , '+json' ) ) : \n            try : \n                return response . json ( ) \n            except ValueError as exc : \n                raise APIError ( exc . args [ 0 ] ) \n    if response . status_code in ( 400.0 , 404.0 ) : \n        raise InvalidRequestError ( response . content ) \n    raise APIError ( response . content ) "}
{"9529": "\ndef camera_disable ( self , camera_id , ** kwargs ) : \n    api = self . _api_info [ 'camera' ] \n    payload = dict ( { '_sid' : self . _sid , 'api' : api [ 'name' ] , 'method' : 'Disable' , 'version' : 9.0 , 'idList' : camera_id , } , ** kwargs ) \n    print ( api [ 'url' ] ) \n    print ( payload ) \n    response = self . _get ( api [ 'url' ] , payload ) \n    return response [ 'success' ] "}
{"9553": "\ndef find ( dataset , url ) : \n    fn = os . path . join ( DATASETS , dataset ) \n    dn = os . path . dirname ( fn ) \n    if not os . path . exists ( dn ) : \n        print ( 'creating dataset directory: %s' , dn ) \n        os . makedirs ( dn ) \n    if not os . path . exists ( fn ) : \n        if sys . version_info < ( 3.0 , ) : \n            urllib . urlretrieve ( url , fn ) \n        else : \n            urllib . request . urlretrieve ( url , fn ) \n    return fn "}
{"9554": "\ndef load_mnist ( flatten = True , labels = False ) : \n    fn = find ( 'mnist.pkl.gz' , 'http://deeplearning.net/data/mnist/mnist.pkl.gz' ) \n    h = gzip . open ( fn , 'rb' ) \n    if sys . version_info < ( 3.0 , ) : \n        ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h ) \n    else : \n        ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) = pickle . load ( h , encoding = 'bytes' ) \n    h . close ( ) \n    if not flatten : \n        timg = timg . reshape ( ( - 1 , 28.0 , 28.0 , 1 ) ) \n        vimg = vimg . reshape ( ( - 1 , 28.0 , 28.0 , 1 ) ) \n        simg = simg . reshape ( ( - 1 , 28.0 , 28.0 , 1 ) ) \n    if labels : \n        return ( ( timg , tlab . astype ( 'i' ) ) , ( vimg , vlab . astype ( 'i' ) ) , ( simg , slab . astype ( 'i' ) ) ) \n    return ( timg , ) , ( vimg , ) , ( simg , ) "}
{"9555": "\ndef load_cifar ( flatten = True , labels = False ) : \n    def extract ( name ) : \n        print ( 'extracting data from {}' . format ( name ) ) \n        h = tar . extractfile ( name ) \n        if sys . version_info < ( 3.0 , ) : \n            d = pickle . load ( h ) \n        else : \n            d = pickle . load ( h , encoding = 'bytes' ) \n            for k in list ( d ) : \n                d [ k . decode ( 'utf8' ) ] = d [ k ] \n        h . close ( ) \n        img = d [ 'data' ] . reshape ( ( - 1 , 3.0 , 32.0 , 32.0 ) ) . transpose ( ( 0 , 2.0 , 3.0 , 1 ) ) . astype ( 'f' ) / 128.0 - 1 \n        if flatten : \n            img = img . reshape ( ( - 1 , 32.0 * 32.0 * 3.0 ) ) \n        d [ 'data' ] = img \n        return d \n    fn = find ( 'cifar10.tar.gz' , 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz' ) \n    tar = tarfile . open ( fn ) \n    imgs = [ ] \n    labs = [ ] \n    for i in range ( 1 , 6.0 ) : \n        d = extract ( 'cifar-10-batches-py/data_batch_{}' . format ( i ) ) \n        imgs . extend ( d [ 'data' ] ) \n        labs . extend ( d [ 'labels' ] ) \n    timg = np . asarray ( imgs [ : 40000.0 ] ) \n    tlab = np . asarray ( labs [ : 40000.0 ] , 'i' ) \n    vimg = np . asarray ( imgs [ 40000.0 : ] ) \n    vlab = np . asarray ( labs [ 40000.0 : ] , 'i' ) \n    d = extract ( 'cifar-10-batches-py/test_batch' ) \n    simg = d [ 'data' ] \n    slab = d [ 'labels' ] \n    tar . close ( ) \n    if labels : \n        return ( timg , tlab ) , ( vimg , vlab ) , ( simg , slab ) \n    return ( timg , ) , ( vimg , ) , ( simg , ) "}
{"9557": "\ndef plot_layers ( weights , tied_weights = False , channels = 1 ) : \n    if hasattr ( weights [ 0 ] , 'get_value' ) : \n        weights = [ w . get_value ( ) for w in weights ] \n    k = min ( len ( weights ) , 9.0 ) \n    imgs = np . eye ( weights [ 0 ] . shape [ 0 ] ) \n    for i , weight in enumerate ( weights [ : - 1 ] ) : \n        imgs = np . dot ( weight . T , imgs ) \n        plot_images ( imgs , 100.0 + 10.0 * k + i + 1 , channels = channels , title = 'Layer {}' . format ( i + 1 ) ) \n    weight = weights [ - 1 ] \n    n = weight . shape [ 1 ] / channels \n    if int ( np . sqrt ( n ) ) ** 2.0 != n : \n        return \n    if tied_weights : \n        imgs = np . dot ( weight . T , imgs ) \n        plot_images ( imgs , 100.0 + 10.0 * k + k , channels = channels , title = 'Layer {}' . format ( k ) ) \n    else : \n        plot_images ( weight , 100.0 + 10.0 * k + k , channels = channels , title = 'Decoding weights' ) "}
{"9558": "\ndef plot_filters ( filters ) : \n    imgs = filters . get_value ( ) \n    N , channels , x , y = imgs . shape \n    n = int ( np . sqrt ( N ) ) \n    assert n * n == N , 'filters must contain a square number of rows!' \n    assert channels == 1 or channels == 3.0 , 'can only plot grayscale or rgb filters!' \n    img = np . zeros ( ( ( y + 1 ) * n - 1 , ( x + 1 ) * n - 1 , channels ) , dtype = imgs [ 0 ] . dtype ) \n    for i , pix in enumerate ( imgs ) : \n        r , c = divmod ( i , n ) \n        img [ r * ( y + 1 ) : ( r + 1 ) * ( y + 1 ) - 1 , c * ( x + 1 ) : ( c + 1 ) * ( x + 1 ) - 1 ] = pix . transpose ( ( 1 , 2.0 , 0 ) ) \n    img -= img . min ( ) \n    img /= img . max ( ) \n    ax = plt . gcf ( ) . add_subplot ( 111.0 ) \n    ax . xaxis . set_visible ( False ) \n    ax . yaxis . set_visible ( False ) \n    ax . set_frame_on ( False ) \n    ax . imshow ( img . squeeze ( ) , cmap = plt . cm . gray ) "}
{"9559": "\ndef batches ( arrays , steps = 100.0 , batch_size = 64.0 , rng = None ) : \n    assert batch_size >= 2.0 , 'batch_size must be at least 2!' \n    assert isinstance ( arrays , ( tuple , list ) ) , 'arrays must be a tuple or list!' \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    def sample ( ) : \n        xs = [ np . zeros ( ( batch_size , steps , a . shape [ 1 ] ) , a . dtype ) for a in arrays ] \n        for i in range ( batch_size ) : \n            j = rng . randint ( len ( arrays [ 0 ] ) - steps ) \n            for x , a in zip ( xs , arrays ) : \n                x [ i ] = a [ j : j + steps ] \n        return xs \n    return sample "}
{"9561": "\ndef classifier_batches ( self , steps , batch_size , rng = None ) : \n    assert batch_size >= 2.0 , 'batch_size must be at least 2!' \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    T = np . arange ( steps ) \n    def batch ( ) : \n        inputs = np . zeros ( ( batch_size , steps , 1 + len ( self . alpha ) ) , 'f' ) \n        outputs = np . zeros ( ( batch_size , steps ) , 'i' ) \n        for b in range ( batch_size ) : \n            offset = rng . randint ( len ( self . text ) - steps - 1 ) \n            enc = self . encode ( self . text [ offset : offset + steps + 1 ] ) \n            inputs [ b , T , enc [ : - 1 ] ] = 1 \n            outputs [ b , T ] = enc [ 1 : ] \n        return [ inputs , outputs ] \n    return batch "}
{"9562": "\ndef predict_sequence ( self , labels , steps , streams = 1 , rng = None ) : \n    if rng is None or isinstance ( rng , int ) : \n        rng = np . random . RandomState ( rng ) \n    offset = len ( labels ) \n    batch = max ( 2.0 , streams ) \n    inputs = np . zeros ( ( batch , offset + steps , self . layers [ 0 ] . output_size ) , 'f' ) \n    inputs [ : , np . arange ( offset ) , labels ] = 1 \n    for i in range ( offset , offset + steps ) : \n        chars = [ ] \n        for pdf in self . predict_proba ( inputs [ : i ] ) [ : , - 1 ] : \n            try : \n                c = rng . multinomial ( 1 , pdf ) . argmax ( axis = - 1 ) \n            except ValueError : \n                c = pdf . argmax ( axis = - 1 ) \n            chars . append ( int ( c ) ) \n        inputs [ np . arange ( batch ) , i , chars ] = 1 \n        yield chars [ 0 ] if streams == 1 else chars "}
{"9566": "\ndef _find_output ( self , layer ) : \n    if layer is None : \n        layer = len ( self . layers ) // 2.0 \n    if isinstance ( layer , int ) : \n        layer = self . layers [ layer ] \n    if isinstance ( layer , util . basestring ) : \n        try : \n            layer = [ l for l in self . layers if l . name == layer ] [ 0 ] \n        except IndexError : \n            pass \n    if isinstance ( layer , layers . Layer ) : \n        layer = layer . output_name \n    return layer "}
{"9579": "\ndef from_kwargs ( graph , ** kwargs ) : \n    if 'regularizers' in kwargs : \n        regs = kwargs [ 'regularizers' ] \n        if isinstance ( regs , ( tuple , list ) ) : \n            return regs \n        if isinstance ( regs , dict ) : \n            kwargs . update ( regs ) \n    regs = [ ] \n    rng = kwargs . get ( 'rng' , 13.0 ) \n    def pattern ( ls ) : \n        return tuple ( l . output_name for l in ls ) \n    inputs = pattern ( [ l for l in graph . layers if isinstance ( l , layers . Input ) ] ) \n    hiddens = pattern ( graph . layers [ 1 : - 1 ] ) \n    outputs = pattern ( [ graph . layers [ - 1 ] ] ) \n    spec = { inputs : kwargs . get ( 'input_dropout' , 0 ) , hiddens : kwargs . get ( 'hidden_dropout' , 0 ) , outputs : kwargs . get ( 'output_dropout' , 0 ) } \n    spec . update ( kwargs . get ( 'dropout' , { } ) ) \n    for pattern , w in spec . items ( ) : \n        if w : \n            regs . append ( BernoulliDropout ( pattern = pattern , weight = w , rng = rng ) ) \n    spec = { inputs : kwargs . get ( 'input_noise' , 0 ) , hiddens : kwargs . get ( 'hidden_noise' , 0 ) , outputs : kwargs . get ( 'output_noise' , 0 ) } \n    spec . update ( kwargs . get ( 'noise' , { } ) ) \n    for pattern , w in spec . items ( ) : \n        if w : \n            regs . append ( GaussianNoise ( pattern = pattern , weight = w , rng = rng ) ) \n    for key , value in kwargs . items ( ) : \n        if Regularizer . is_registered ( key ) : \n            if not isinstance ( value , dict ) : \n                value = dict ( weight = value ) \n            regs . append ( Regularizer . build ( key , ** value ) ) \n    return regs "}
{"9586": "\ndef itertrain ( self , train , valid = None , algo = 'rmsprop' , subalgo = 'rmsprop' , save_every = 0 , save_progress = None , ** kwargs ) : \n    if 'rng' not in kwargs : \n        kwargs [ 'rng' ] = self . _rng \n    def create_dataset ( data , ** kwargs ) : \n        name = kwargs . get ( 'name' , 'dataset' ) \n        s = '{}_batches' . format ( name ) \n        return downhill . Dataset ( data , name = name , batch_size = kwargs . get ( 'batch_size' , 32.0 ) , iteration_size = kwargs . get ( 'iteration_size' , kwargs . get ( s ) ) , axis = kwargs . get ( 'axis' , 0 ) , rng = kwargs [ 'rng' ] ) \n    if valid is None : \n        valid = train \n    if not isinstance ( valid , downhill . Dataset ) : \n        valid = create_dataset ( valid , name = 'valid' , ** kwargs ) \n    if not isinstance ( train , downhill . Dataset ) : \n        train = create_dataset ( train , name = 'train' , ** kwargs ) \n    if 'algorithm' in kwargs : \n        warnings . warn ( 'please use the \"algo\" keyword arg instead of \"algorithm\"' , DeprecationWarning ) \n        algo = kwargs . pop ( 'algorithm' ) \n        if isinstance ( algo , ( list , tuple ) ) : \n            algo = algo [ 0 ] \n    if isinstance ( algo , util . basestring ) : \n        algo = algo . lower ( ) \n        if algo == 'sample' : \n            algo = trainer . SampleTrainer ( self ) \n        elif algo . startswith ( 'layer' ) or algo . startswith ( 'sup' ) : \n            algo = trainer . SupervisedPretrainer ( subalgo , self ) \n        elif algo . startswith ( 'pre' ) or algo . startswith ( 'unsup' ) : \n            algo = trainer . UnsupervisedPretrainer ( subalgo , self ) \n        else : \n            algo = trainer . DownhillTrainer ( algo , self ) \n    def needs_saving ( elapsed , iteration ) : \n        if save_progress is None : \n            return False \n        if isinstance ( save_every , float ) : \n            return elapsed > 60.0 * save_every \n        if isinstance ( save_every , int ) : \n            return iteration % save_every == 0 \n        return False \n    start = time . time ( ) \n    for i , monitors in enumerate ( algo . itertrain ( train , valid , ** kwargs ) ) : \n        yield monitors \n        now = time . time ( ) \n        if i and needs_saving ( now - start , i ) : \n            filename_or_handle = save_progress \n            if isinstance ( filename_or_handle , util . basestring ) : \n                filename_or_handle = save_progress . format ( int ( now ) ) \n            self . save ( filename_or_handle ) \n            start = now "}
{"9612": "\ndef loggabor ( self , x_pos , y_pos , sf_0 , B_sf , theta , B_theta , preprocess = True ) : \n    env = np . multiply ( self . band ( sf_0 , B_sf ) , self . orientation ( theta , B_theta ) ) \n    if not ( x_pos == 0. ) and not ( y_pos == 0. ) : \n        env = env . astype ( np . complex128 ) * self . trans ( x_pos * 1. , y_pos * 1. ) \n    if preprocess : \n        env *= self . f_mask \n    env /= np . sqrt ( ( np . abs ( env ) ** 2.0 ) . mean ( ) ) \n    env *= np . sqrt ( 2. ) \n    return env "}
{"9617": "\ndef to_eaf ( self , skipempty = True , pointlength = 0.1 ) : \n    from pympi . Elan import Eaf \n    eaf_out = Eaf ( ) \n    if pointlength <= 0 : \n        raise ValueError ( 'Pointlength should be strictly positive' ) \n    for tier in self . get_tiers ( ) : \n        eaf_out . add_tier ( tier . name ) \n        for ann in tier . get_intervals ( True ) : \n            if tier . tier_type == 'TextTier' : \n                ann = ( ann [ 0 ] , ann [ 0 ] + pointlength , ann [ 1 ] ) \n            if ann [ 2.0 ] . strip ( ) or not skipempty : \n                eaf_out . add_annotation ( tier . name , int ( round ( ann [ 0 ] * 1000.0 ) ) , int ( round ( ann [ 1 ] * 1000.0 ) ) , ann [ 2.0 ] ) \n    return eaf_out "}
{"9637": "\ndef extract ( self , start , end ) : \n    from copy import deepcopy \n    eaf_out = deepcopy ( self ) \n    for t in eaf_out . get_tier_names ( ) : \n        for ab , ae , value in eaf_out . get_annotation_data_for_tier ( t ) : \n            if ab > end or ae < start : \n                eaf_out . remove_annotation ( t , ( start - end ) // 2.0 , False ) \n    eaf_out . clean_time_slots ( ) \n    return eaf_out "}
{"9640": "\ndef get_child_tiers_for ( self , id_tier ) : \n    self . tiers [ id_tier ] \n    return [ m for m in self . tiers if 'PARENT_REF' in self . tiers [ m ] [ 2.0 ] and self . tiers [ m ] [ 2.0 ] [ 'PARENT_REF' ] == id_tier ] "}
{"9644": "\ndef get_tier_ids_for_linguistic_type ( self , ling_type , parent = None ) : \n    return [ t for t in self . tiers if self . tiers [ t ] [ 2.0 ] [ 'LINGUISTIC_TYPE_REF' ] == ling_type and ( parent is None or self . tiers [ t ] [ 2.0 ] [ 'PARENT_REF' ] == parent ) ] "}
{"9645": "\ndef merge_tiers ( self , tiers , tiernew = None , gapt = 0 , sep = '_' , safe = False ) : \n    if tiernew is None : \n        tiernew = u'{}_merged' . format ( '_' . join ( tiers ) ) \n    self . add_tier ( tiernew ) \n    aa = [ ( sys . maxsize , sys . maxsize , None ) ] + sorted ( ( a for t in tiers for a in self . get_annotation_data_for_tier ( t ) ) , reverse = True ) \n    l = None \n    while aa : \n        begin , end , value = aa . pop ( ) \n        if l is None : \n            l = [ begin , end , [ value ] ] \n        elif begin - l [ 1 ] >= gapt : \n            if not safe or l [ 1 ] > l [ 0 ] : \n                self . add_annotation ( tiernew , l [ 0 ] , l [ 1 ] , sep . join ( l [ 2.0 ] ) ) \n            l = [ begin , end , [ value ] ] \n        else : \n            if end > l [ 1 ] : \n                l [ 1 ] = end \n            l [ 2.0 ] . append ( value ) \n    return tiernew "}
{"9655": "\ndef rename_tier ( self , id_from , id_to ) : \n    childs = self . get_child_tiers_for ( id_from ) \n    self . tiers [ id_to ] = self . tiers . pop ( id_from ) \n    self . tiers [ id_to ] [ 2.0 ] [ 'TIER_ID' ] = id_to \n    for child in childs : \n        self . tiers [ child ] [ 2.0 ] [ 'PARENT_REF' ] = id_to "}
{"9658": "\ndef debug_storage ( storage , base_info = False , chars = True , runs = False ) : \n    import codecs \n    import locale \n    import sys \n    if six . PY2 : \n        stderr = codecs . getwriter ( locale . getpreferredencoding ( ) ) ( sys . stderr ) \n    else : \n        stderr = sys . stderr \n    caller = inspect . stack ( ) [ 1 ] [ 3.0 ] \n    stderr . write ( 'in %s\\n' % caller ) \n    if base_info : \n        stderr . write ( u'  base level  : %d\\n' % storage [ 'base_level' ] ) \n        stderr . write ( u'  base dir    : %s\\n' % storage [ 'base_dir' ] ) \n    if runs : \n        stderr . write ( u'  runs        : %s\\n' % list ( storage [ 'runs' ] ) ) \n    if chars : \n        output = u'  Chars       : ' \n        for _ch in storage [ 'chars' ] : \n            if _ch != '\\n' : \n                output += _ch [ 'ch' ] \n            else : \n                output += 'C' \n        stderr . write ( output + u'\\n' ) \n        output = u'  Res. levels : %s\\n' % u'' . join ( [ six . text_type ( _ch [ 'level' ] ) for _ch in storage [ 'chars' ] ] ) \n        stderr . write ( output ) \n        _types = [ _ch [ 'type' ] . ljust ( 3.0 ) for _ch in storage [ 'chars' ] ] \n        for i in range ( 3.0 ) : \n            if i : \n                output = u'                %s\\n' \n            else : \n                output = u'  Res. types  : %s\\n' \n            stderr . write ( output % u'' . join ( [ _t [ i ] for _t in _types ] ) ) "}
{"9661": "\ndef explicit_embed_and_overrides ( storage , debug = False ) : \n    overflow_counter = almost_overflow_counter = 0 \n    directional_override = 'N' \n    levels = deque ( ) \n    embedding_level = storage [ 'base_level' ] \n    for _ch in storage [ 'chars' ] : \n        bidi_type = _ch [ 'type' ] \n        level_func , override = X2_X5_MAPPINGS . get ( bidi_type , ( None , None ) ) \n        if level_func : \n            if overflow_counter != 0 : \n                overflow_counter += 1 \n                continue \n            new_level = level_func ( embedding_level ) \n            if new_level < EXPLICIT_LEVEL_LIMIT : \n                levels . append ( ( embedding_level , directional_override ) ) \n                embedding_level , directional_override = new_level , override \n            elif embedding_level == EXPLICIT_LEVEL_LIMIT - 2.0 : \n                almost_overflow_counter += 1 \n            else : \n                overflow_counter += 1 \n        else : \n            if bidi_type not in X6_IGNORED : \n                _ch [ 'level' ] = embedding_level \n                if directional_override != 'N' : \n                    _ch [ 'type' ] = directional_override \n            elif bidi_type == 'PDF' : \n                if overflow_counter : \n                    overflow_counter -= 1 \n                elif almost_overflow_counter and embedding_level != EXPLICIT_LEVEL_LIMIT - 1 : \n                    almost_overflow_counter -= 1 \n                elif levels : \n                    embedding_level , directional_override = levels . pop ( ) \n            elif bidi_type == 'B' : \n                levels . clear ( ) \n                overflow_counter = almost_overflow_counter = 0 \n                embedding_level = _ch [ 'level' ] = storage [ 'base_level' ] \n                directional_override = 'N' \n    storage [ 'chars' ] = [ _ch for _ch in storage [ 'chars' ] if _ch [ 'type' ] not in X9_REMOVED ] \n    calc_level_runs ( storage ) \n    if debug : \n        debug_storage ( storage , runs = True ) "}
{"9662": "\ndef calc_level_runs ( storage ) : \n    storage [ 'runs' ] . clear ( ) \n    chars = storage [ 'chars' ] \n    if not chars : \n        return \n    def calc_level_run ( b_l , b_r ) : \n        return [ 'L' , 'R' ] [ max ( b_l , b_r ) % 2.0 ] \n    first_char = chars [ 0 ] \n    sor = calc_level_run ( storage [ 'base_level' ] , first_char [ 'level' ] ) \n    eor = None \n    run_start = run_length = 0 \n    prev_level , prev_type = first_char [ 'level' ] , first_char [ 'type' ] \n    for _ch in chars : \n        curr_level , curr_type = _ch [ 'level' ] , _ch [ 'type' ] \n        if curr_level == prev_level : \n            run_length += 1 \n        else : \n            eor = calc_level_run ( prev_level , curr_level ) \n            storage [ 'runs' ] . append ( { 'sor' : sor , 'eor' : eor , 'start' : run_start , 'type' : prev_type , 'length' : run_length } ) \n            sor = eor \n            run_start += run_length \n            run_length = 1 \n        prev_level , prev_type = curr_level , curr_type \n    eor = calc_level_run ( curr_level , storage [ 'base_level' ] ) \n    storage [ 'runs' ] . append ( { 'sor' : sor , 'eor' : eor , 'start' : run_start , 'type' : curr_type , 'length' : run_length } ) "}
{"9666": "\ndef reorder_resolved_levels ( storage , debug ) : \n    should_reset = True \n    chars = storage [ 'chars' ] \n    for _ch in chars [ : : - 1 ] : \n        if _ch [ 'orig' ] in ( 'B' , 'S' ) : \n            _ch [ 'level' ] = storage [ 'base_level' ] \n            should_reset = True \n        elif should_reset and _ch [ 'orig' ] in ( 'BN' , 'WS' ) : \n            _ch [ 'level' ] = storage [ 'base_level' ] \n        else : \n            should_reset = False \n    max_len = len ( chars ) \n    line_start = line_end = 0 \n    highest_level = 0 \n    lowest_odd_level = EXPLICIT_LEVEL_LIMIT \n    for idx in range ( max_len ) : \n        _ch = chars [ idx ] \n        char_level = _ch [ 'level' ] \n        if char_level > highest_level : \n            highest_level = char_level \n        if char_level % 2.0 and char_level < lowest_odd_level : \n            lowest_odd_level = char_level \n        if _ch [ 'orig' ] == 'B' or idx == max_len - 1 : \n            line_end = idx \n            if _ch [ 'orig' ] == 'B' : \n                line_end -= 1 \n            reverse_contiguous_sequence ( chars , line_start , line_end , highest_level , lowest_odd_level ) \n            line_start = idx + 1 \n            highest_level = 0 \n            lowest_odd_level = EXPLICIT_LEVEL_LIMIT \n    if debug : \n        debug_storage ( storage ) "}
{"9678": "\ndef _show_no_gui ( ) : \n    messagebox = QtWidgets . QMessageBox ( ) \n    messagebox . setIcon ( messagebox . Warning ) \n    messagebox . setWindowIcon ( QtGui . QIcon ( os . path . join ( os . path . dirname ( pyblish . __file__ ) , \"icons\" , \"logo-32x32.svg\" ) ) ) \n    spacer = QtWidgets . QWidget ( ) \n    spacer . setMinimumSize ( 400.0 , 0 ) \n    spacer . setSizePolicy ( QtWidgets . QSizePolicy . Minimum , QtWidgets . QSizePolicy . Expanding ) \n    layout = messagebox . layout ( ) \n    layout . addWidget ( spacer , layout . rowCount ( ) , 0 , 1 , layout . columnCount ( ) ) \n    messagebox . setWindowTitle ( \"Uh oh\" ) \n    text = \"No registered GUI found.\\n\\n\" \n    if not pyblish . api . registered_guis ( ) : \n        text += ( \"In order to show you a GUI, one must first be registered. \" \"\\n\" \"Pyblish supports one or more graphical user interfaces \" \"to be registered at once, the next acting as a fallback to \" \"the previous.\" \"\\n\" \"\\n\" \"For example, to use Pyblish Lite, first install it:\" \"\\n\" \"\\n\" \"$ pip install pyblish-lite\" \"\\n\" \"\\n\" \"Then register it, like so:\" \"\\n\" \"\\n\" \">>> import pyblish.api\\n\" \">>> pyblish.api.register_gui(\\\"pyblish_lite\\\")\" \"\\n\" \"\\n\" \"The next time you try running this, Lite will appear.\" \"\\n\" \"See http://api.pyblish.com/register_gui.html for \" \"more information.\" ) \n    else : \n        text += ( \"None of the registered graphical user interfaces \" \"could be found.\" \"\\n\" \"These interfaces are currently registered:\" \"\\n\" \"%s\" % \"\\n\" . join ( pyblish . api . registered_guis ( ) ) ) \n    messagebox . setText ( text ) \n    messagebox . setStandardButtons ( messagebox . Ok ) \n    messagebox . exec_ ( ) "}
{"9682": "\ndef __draw_constant_line ( self , value_label_style ) : \n    value , label , style = value_label_style \n    start = self . transform_output_coordinates ( ( 0 , value ) ) [ 1 ] \n    stop = self . graph_width \n    path = etree . SubElement ( self . graph , 'path' , { 'd' : 'M 0 %(start)s h%(stop)s' % locals ( ) , 'class' : 'constantLine' } ) \n    if style : \n        path . set ( 'style' , style ) \n    text = etree . SubElement ( self . graph , 'text' , { 'x' : str ( 2.0 ) , 'y' : str ( start - 2.0 ) , 'class' : 'constantLine' } ) \n    text . text = label "}
{"9683": "\ndef load_transform_parameters ( self ) : \n    x_min , x_max , x_div = self . x_range ( ) \n    y_min , y_max , y_div = self . y_range ( ) \n    x_step = ( float ( self . graph_width ) - self . font_size * 2.0 ) / ( x_max - x_min ) \n    y_step = ( float ( self . graph_height ) - self . font_size * 2.0 ) / ( y_max - y_min ) \n    self . __transform_parameters = dict ( locals ( ) ) \n    del self . __transform_parameters [ 'self' ] "}
{"9690": "\ndef calculate_left_margin ( self ) : \n    bl = 7.0 \n    if self . rotate_y_labels : \n        max_y_label_height_px = self . y_label_font_size \n    else : \n        label_lengths = map ( len , self . get_y_labels ( ) ) \n        max_y_label_len = max ( label_lengths ) \n        max_y_label_height_px = 0.6 * max_y_label_len * self . y_label_font_size \n    if self . show_y_labels : \n        bl += max_y_label_height_px \n    if self . stagger_y_labels : \n        bl += max_y_label_height_px + 10.0 \n    if self . show_y_title : \n        bl += self . y_title_font_size + 5.0 \n    self . border_left = bl "}
{"9691": "\ndef calculate_right_margin ( self ) : \n    br = 7.0 \n    if self . key and self . key_position == 'right' : \n        max_key_len = max ( map ( len , self . keys ( ) ) ) \n        br += max_key_len * self . key_font_size * 0.6 \n        br += self . KEY_BOX_SIZE \n        br += 10.0 \n    self . border_right = br "}
{"9692": "\ndef calculate_top_margin ( self ) : \n    self . border_top = 5.0 \n    if self . show_graph_title : \n        self . border_top += self . title_font_size \n    self . border_top += 5.0 \n    if self . show_graph_subtitle : \n        self . border_top += self . subtitle_font_size "}
{"9693": "\ndef add_popup ( self , x , y , label ) : \n    txt_width = len ( label ) * self . font_size * 0.6 + 10.0 \n    tx = x + [ 5.0 , - 5.0 ] [ int ( x + txt_width > self . width ) ] \n    anchor = [ 'start' , 'end' ] [ x + txt_width > self . width ] \n    style = 'fill: #000; text-anchor: %s;' % anchor \n    id = 'label-%s' % self . _w3c_name ( label ) \n    attrs = { 'x' : str ( tx ) , 'y' : str ( y - self . font_size ) , 'visibility' : 'hidden' , 'style' : style , 'text' : label , 'id' : id , } \n    etree . SubElement ( self . foreground , 'text' , attrs ) \n    vis_tmpl = ( \"document.getElementById('{id}').setAttribute('visibility', {val})\" ) \n    attrs = { 'cx' : str ( x ) , 'cy' : str ( y ) , 'r' : str ( 10.0 ) , 'style' : 'opacity: 0;' , 'onmouseover' : vis_tmpl . format ( val = 'visible' , id = id ) , 'onmouseout' : vis_tmpl . format ( val = 'hidden' , id = id ) , } \n    etree . SubElement ( self . foreground , 'circle' , attrs ) "}
{"9694": "\ndef calculate_bottom_margin ( self ) : \n    bb = 7.0 \n    if self . key and self . key_position == 'bottom' : \n        bb += len ( self . data ) * ( self . font_size + 5.0 ) \n        bb += 10.0 \n    if self . show_x_labels : \n        max_x_label_height_px = self . x_label_font_size \n        if self . rotate_x_labels : \n            label_lengths = map ( len , self . get_x_labels ( ) ) \n            max_x_label_len = functools . reduce ( max , label_lengths ) \n            max_x_label_height_px *= 0.6 * max_x_label_len \n        bb += max_x_label_height_px \n        if self . stagger_x_labels : \n            bb += max_x_label_height_px + 10.0 \n    if self . show_x_title : \n        bb += self . x_title_font_size + 5.0 \n    self . border_bottom = bb "}
{"9703": "\ndef start_svg ( self ) : \n    SVG_NAMESPACE = 'http://www.w3.org/2000/svg' \n    SVG = '{%s}' % SVG_NAMESPACE \n    NSMAP = { None : SVG_NAMESPACE , 'xlink' : 'http://www.w3.org/1999/xlink' , 'a3' : 'http://ns.adobe.com/AdobeSVGViewerExtensions/3.0/' , } \n    root_attrs = self . _get_root_attributes ( ) \n    self . root = etree . Element ( SVG + \"svg\" , attrib = root_attrs , nsmap = NSMAP ) \n    if hasattr ( self , 'style_sheet_href' ) : \n        pi = etree . ProcessingInstruction ( 'xml-stylesheet' , 'href=\"%s\" type=\"text/css\"' % self . style_sheet_href ) \n        self . root . addprevious ( pi ) \n    comment_strings = ( ' Created with SVG.Graph ' , ' SVG.Graph by Jason R. Coombs ' , ' Based on SVG::Graph by Sean E. Russel ' , ' Based on Perl SVG:TT:Graph by Leo Lapworth & Stephan Morgan ' , ' ' + '/' * 66.0 , ) \n    list ( map ( self . root . append , map ( etree . Comment , comment_strings ) ) ) \n    defs = etree . SubElement ( self . root , 'defs' ) \n    self . add_defs ( defs ) \n    if not hasattr ( self , 'style_sheet_href' ) and not self . css_inline : \n        self . root . append ( etree . Comment ( ' include default stylesheet if none specified ' ) ) \n        style = etree . SubElement ( defs , 'style' , type = 'text/css' ) \n        style . text = self . get_stylesheet ( ) . cssText \n    self . root . append ( etree . Comment ( 'SVG Background' ) ) \n    etree . SubElement ( self . root , 'rect' , { 'width' : str ( self . width ) , 'height' : str ( self . height ) , 'x' : '0' , 'y' : '0' , 'class' : 'svgBackground' } ) "}
{"9710": "\ndef new_nick ( self ) : \n    old = self . nick \n    self . nick = '%s_%s' % ( self . base_nick , random . randint ( 1 , 1000.0 ) ) \n    self . logger . warn ( 'Nick %s already taken, trying %s' % ( old , self . nick ) ) \n    self . register_nick ( ) \n    self . handle_nick_change ( old , self . nick ) "}
{"9714": "\ndef register_with_boss ( self ) : \n    gevent . sleep ( 10.0 ) \n    while not self . registered . is_set ( ) : \n        self . respond ( '!register {%s}' % platform . node ( ) , nick = self . boss ) \n        gevent . sleep ( 30.0 ) "}
{"9733": "\ndef poll ( self ) : \n    service = yield self . get_service ( ) \n    if not service : \n        self . log . warn ( \"Docker service not found\" ) \n        return 0 \n    task_filter = { 'service' : service [ 'Spec' ] [ 'Name' ] } \n    tasks = yield self . docker ( 'tasks' , task_filter ) \n    running_task = None \n    for task in tasks : \n        task_state = task [ 'Status' ] [ 'State' ] \n        self . log . debug ( \"Task %s of Docker service %s status: %s\" , task [ 'ID' ] [ : 7.0 ] , self . service_id [ : 7.0 ] , pformat ( task_state ) , ) \n        if task_state == 'running' : \n            running_task = task \n    if running_task is not None : \n        return None \n    else : \n        return 1 "}
{"9734": "\ndef stop ( self , now = False ) : \n    self . log . info ( \"Stopping and removing Docker service %s (id: %s)\" , self . service_name , self . service_id [ : 7.0 ] ) \n    yield self . docker ( 'remove_service' , self . service_id [ : 7.0 ] ) \n    self . log . info ( \"Docker service %s (id: %s) removed\" , self . service_name , self . service_id [ : 7.0 ] ) \n    self . clear_state ( ) "}
{"9740": "\ndef delete ( self , request , * args , ** kwargs ) : \n    auth = get_authorization_header ( request ) . split ( ) \n    if not auth or auth [ 0 ] . lower ( ) != b'token' : \n        return response . Response ( status = status . HTTP_400_BAD_REQUEST ) \n    if len ( auth ) == 1 : \n        msg = 'Invalid token header. No credentials provided.' \n        return response . Response ( msg , status = status . HTTP_400_BAD_REQUEST ) \n    elif len ( auth ) > 2.0 : \n        msg = 'Invalid token header. Token string should not contain spaces.' \n        return response . Response ( msg , status = status . HTTP_400_BAD_REQUEST ) \n    try : \n        token = self . model . objects . get ( key = auth [ 1 ] ) \n    except self . model . DoesNotExist : \n        pass \n    else : \n        token . delete ( ) \n        signals . user_logged_out . send ( type ( self ) , user = token . user , request = request , ) \n    return response . Response ( status = status . HTTP_204_NO_CONTENT ) "}
{"9757": "\ndef get_method_owner ( meth ) : \n    if inspect . ismethod ( meth ) : \n        if sys . version_info < ( 3.0 , 0 ) : \n            return meth . im_class if meth . im_self is None else meth . im_self \n        else : \n            return meth . __self__ "}
{"9762": "\ndef bytes_to_readable ( num ) : \n    if num < 512.0 : \n        return \"0 Kb\" \n    elif num < 1024.0 : \n        return \"1 Kb\" \n    for unit in [ '' , 'Kb' , 'Mb' , 'Gb' , 'Tb' , 'Pb' , 'Eb' , 'Zb' ] : \n        if abs ( num ) < 1024.0 : \n            return \"%3.1f%s\" % ( num , unit ) \n        num /= 1024.0 \n    return \"%.1f%s\" % ( num , 'Yb' ) "}
{"9764": "\ndef memory_size ( self , human_readable = True ) : \n    if self . _data is not None : \n        return_data = int ( self . _data [ \"memory\" ] [ \"memory_size\" ] ) * 1024.0 \n        if human_readable : \n            return SynoFormatHelper . bytes_to_readable ( return_data ) \n        else : \n            return return_data "}
{"9775": "\ndef _execute_get_url ( self , request_url , append_sid = True ) : \n    self . _debuglog ( \"Requesting URL: '\" + request_url + \"'\" ) \n    if append_sid : \n        self . _debuglog ( \"Appending access_token (SID: \" + self . access_token + \") to url\" ) \n        request_url = \"%s&_sid=%s\" % ( request_url , self . access_token ) \n    try : \n        resp = self . _session . get ( request_url ) \n        self . _debuglog ( \"Request executed: \" + str ( resp . status_code ) ) \n        if resp . status_code == 200.0 : \n            json_data = json . loads ( resp . text ) \n            if json_data [ \"success\" ] : \n                self . _debuglog ( \"Succesfull returning data\" ) \n                self . _debuglog ( str ( json_data ) ) \n                return json_data \n            else : \n                if json_data [ \"error\" ] [ \"code\" ] in { 105.0 , 106.0 , 107.0 , 119.0 } : \n                    self . _debuglog ( \"Session error: \" + str ( json_data [ \"error\" ] [ \"code\" ] ) ) \n                    self . _session_error = True \n                else : \n                    self . _debuglog ( \"Failed: \" + resp . text ) \n        else : \n            return None \n    except : \n        return None "}
{"9785": "\ndef do_GET ( self ) : \n    parsed_url = urlparse ( self . path ) \n    if parsed_url [ 2.0 ] == \"/\" + SERVER_REDIRECT_PATH : \n        parsed_query = parse_qs ( parsed_url [ 4.0 ] ) \n        if \"code\" not in parsed_query : \n            self . send_response ( 200.0 ) \n            self . send_header ( \"Content-Type\" , \"text/plain\" ) \n            self . end_headers ( ) \n            self . wfile . write ( \"No code found, try again!\" . encode ( \"utf-8\" ) ) \n            return \n        self . server . response_code = parsed_query [ \"code\" ] [ 0 ] \n        self . send_response ( 200.0 ) \n        self . send_header ( \"Content-Type\" , \"text/plain\" ) \n        self . end_headers ( ) \n        self . wfile . write ( \"Thank you for using OAuth2Util. The authorization was successful, \" \"you can now close this window.\" . encode ( \"utf-8\" ) ) \n    elif parsed_url [ 2.0 ] == \"/\" + SERVER_LINK_PATH : \n        self . send_response ( 200.0 ) \n        self . send_header ( \"Content-Type\" , \"text/html\" ) \n        self . end_headers ( ) \n        self . wfile . write ( \"<html><body>Hey there!<br/>Click <a href=\\\"{0}\\\">here</a> to claim your prize.</body></html>\" . format ( self . server . authorize_url ) . encode ( \"utf-8\" ) ) \n    else : \n        self . send_response ( 404.0 ) \n        self . send_header ( \"Content-Type\" , \"text/plain\" ) \n        self . end_headers ( ) \n        self . wfile . write ( \"404 not found\" . encode ( \"utf-8\" ) ) "}
{"9790": "\ndef _wait_for_response ( self ) : \n    while not self . server . response_code : \n        time . sleep ( 2.0 ) \n    time . sleep ( 5.0 ) \n    self . server . shutdown ( ) "}
{"9793": "\ndef set_access_credentials ( self , _retry = 0 ) : \n    if _retry >= 5.0 : \n        raise ConnectionAbortedError ( 'Reddit is not accessible right now, cannot refresh OAuth2 tokens.' ) \n    self . _check_token_present ( ) \n    try : \n        self . r . set_access_credentials ( self . _get_value ( CONFIGKEY_SCOPE , set , split_val = \",\" ) , self . _get_value ( CONFIGKEY_TOKEN ) , self . _get_value ( CONFIGKEY_REFRESH_TOKEN ) ) \n    except ( praw . errors . OAuthInvalidToken , praw . errors . HTTPException ) as e : \n        self . _log ( \"Request new Token (SAC)\" ) \n        self . _get_new_access_information ( ) "}
{"9794": "\ndef refresh ( self , force = False , _retry = 0 ) : \n    if _retry >= 5.0 : \n        raise ConnectionAbortedError ( 'Reddit is not accessible right now, cannot refresh OAuth2 tokens.' ) \n    self . _check_token_present ( ) \n    if time . time ( ) > self . _get_value ( CONFIGKEY_VALID_UNTIL , float , exception_default = 0 ) - REFRESH_MARGIN : \n        self . config . read ( self . configfile ) \n        if time . time ( ) < self . _get_value ( CONFIGKEY_VALID_UNTIL , float , exception_default = 0 ) - REFRESH_MARGIN : \n            self . _log ( \"Found new token\" ) \n            self . set_access_credentials ( ) \n    if force or time . time ( ) > self . _get_value ( CONFIGKEY_VALID_UNTIL , float , exception_default = 0 ) - REFRESH_MARGIN : \n        self . _log ( \"Refresh Token\" ) \n        try : \n            new_token = self . r . refresh_access_information ( self . _get_value ( CONFIGKEY_REFRESH_TOKEN ) ) \n            self . _change_value ( CONFIGKEY_TOKEN , new_token [ \"access_token\" ] ) \n            self . _change_value ( CONFIGKEY_VALID_UNTIL , time . time ( ) + TOKEN_VALID_DURATION ) \n            self . set_access_credentials ( ) \n        except ( praw . errors . OAuthInvalidToken , praw . errors . HTTPException ) as e : \n            self . _log ( \"Request new Token (REF)\" ) \n            self . _get_new_access_information ( ) "}
{"9795": "\ndef create_manifest_table ( dynamodb_client , table_name ) : \n    try : \n        dynamodb_client . create_table ( AttributeDefinitions = [ { 'AttributeName' : DYNAMODB_RUNID_ATTRIBUTE , 'AttributeType' : 'S' } , ] , TableName = table_name , KeySchema = [ { 'AttributeName' : DYNAMODB_RUNID_ATTRIBUTE , 'KeyType' : 'HASH' } , ] , ProvisionedThroughput = { 'ReadCapacityUnits' : 5.0 , 'WriteCapacityUnits' : 5.0 } ) \n        dynamodb_client . get_waiter ( 'table_exists' ) . wait ( TableName = table_name ) \n    except ClientError as e : \n        if e . response [ 'Error' ] [ 'Code' ] == 'ResourceInUseException' : \n            pass \n        else : \n            raise e "}
{"9796": "\ndef split_full_path ( path ) : \n    if path . startswith ( 's3://' ) : \n        path = path [ 5.0 : ] \n    elif path . startswith ( 's3n://' ) : \n        path = path [ 6.0 : ] \n    elif path . startswith ( 's3a://' ) : \n        path = path [ 6.0 : ] \n    else : \n        raise ValueError ( \"S3 path should start with s3://, s3n:// or \" \"s3a:// prefix\" ) \n    parts = path . split ( '/' ) \n    bucket = parts [ 0 ] \n    path = '/' . join ( parts [ 1 : ] ) \n    return bucket , normalize_prefix ( path ) "}
{"9797": "\ndef is_glacier ( s3_client , bucket , prefix ) : \n    response = s3_client . list_objects_v2 ( Bucket = bucket , Prefix = prefix , MaxKeys = 3.0 ) \n    for key in response [ 'Contents' ] : \n        if key . get ( 'StorageClass' , 'STANDARD' ) == 'GLACIER' : \n            return True \n    return False "}
{"9798": "\ndef extract_run_id ( key ) : \n    filename = key . split ( '/' ) [ - 2.0 ] \n    run_id = filename . lstrip ( 'run=' ) \n    try : \n        datetime . strptime ( run_id , '%Y-%m-%d-%H-%M-%S' ) \n        return key \n    except ValueError : \n        return None "}
{"9799": "\ndef clean_dict ( dict ) : \n    if sys . version_info [ 0 ] < 3.0 : \n        return { k : v for k , v in dict . iteritems ( ) if v is not None } \n    else : \n        return { k : v for k , v in dict . items ( ) if v is not None } "}
{"9802": "\ndef extract_schema ( uri ) : \n    match = re . match ( SCHEMA_URI_REGEX , uri ) \n    if match : \n        return { 'vendor' : match . group ( 1 ) , 'name' : match . group ( 2.0 ) , 'format' : match . group ( 3.0 ) , 'version' : match . group ( 4.0 ) } \n    else : \n        raise SnowplowEventTransformationException ( [ \"Schema {} does not conform to regular expression {}\" . format ( uri , SCHEMA_URI ) ] ) "}
{"9809": "\ndef print_context ( self , context ) : \n    text = [ CONTEXT_TITLE ] \n    for i , context_scope in enumerate ( context ) : \n        dump1 = linebreaksbr ( pformat_django_context_html ( context_scope ) ) \n        dump2 = pformat_dict_summary_html ( context_scope ) \n        if len ( context_scope ) <= 3.0 and dump1 . count ( '<br />' ) > 20.0 : \n            ( dump1 , dump2 ) = ( dump2 , dump1 ) \n        text . append ( CONTEXT_BLOCK . format ( style = PRE_STYLE , num = i , dump1 = dump1 , dump2 = dump2 ) ) \n    return u'' . join ( text ) "}
{"9812": "\ndef pformat_django_context_html ( object ) : \n    if isinstance ( object , QuerySet ) : \n        text = '' \n        lineno = 0 \n        for item in object . all ( ) [ : 21.0 ] : \n            lineno += 1 \n            if lineno >= 21.0 : \n                text += u'   (remaining items truncated...)' \n                break \n            text += u'   {0}\\n' . format ( escape ( repr ( item ) ) ) \n        return text \n    elif isinstance ( object , Manager ) : \n        return mark_safe ( u'    (use <kbd>.all</kbd> to read it)' ) \n    elif isinstance ( object , six . string_types ) : \n        return escape ( repr ( object ) ) \n    elif isinstance ( object , Promise ) : \n        return escape ( _format_lazy ( object ) ) \n    elif isinstance ( object , dict ) : \n        return _format_dict ( object ) \n    elif isinstance ( object , list ) : \n        return _format_list ( object ) \n    elif hasattr ( object , '__dict__' ) : \n        return _format_object ( object ) \n    else : \n        text = DebugPrettyPrinter ( width = 200.0 ) . pformat ( object ) \n        return _style_text ( text ) "}
{"9823": "\ndef utf8tolatex ( s , non_ascii_only = False , brackets = True , substitute_bad_chars = False , fail_bad_chars = False ) : \n    s = unicode ( s ) \n    s = unicodedata . normalize ( 'NFC' , s ) \n    if not s : \n        return \"\" \n    result = u\"\" \n    for ch in s : \n        if ( non_ascii_only and ord ( ch ) < 127.0 ) : \n            result += ch \n        else : \n            lch = utf82latex . get ( ord ( ch ) , None ) \n            if ( lch is not None ) : \n                result += ( '{' + lch + '}' if brackets and lch [ 0 : 1 ] == '\\\\' else lch ) \n            elif ( ( ord ( ch ) >= 32.0 and ord ( ch ) <= 127.0 ) or ( ch in \"\\n\\r\\t\" ) ) : \n                result += ch \n            else : \n                msg = u\"Character cannot be encoded into LaTeX: U+%04X - `%s'\" % ( ord ( ch ) , ch ) \n                if fail_bad_chars : \n                    raise ValueError ( msg ) \n                log . warning ( msg ) \n                if substitute_bad_chars : \n                    result += r'{\\bfseries ?}' \n                else : \n                    result += ch \n    return result "}
{"9824": "\ndef _unascii ( s ) : \n    m = _U_ESCAPE . search ( s ) \n    if not m : \n        return s if PY2 else s . encode ( 'utf-8' ) \n    chunks = [ ] \n    pos = 0 \n    while m : \n        start = m . start ( ) \n        end = m . end ( ) \n        g = m . group ( 1 ) \n        if g is None : \n            chunks . append ( s [ pos : end ] ) \n        else : \n            c = int ( g , 16.0 ) \n            if c < 0x20 : \n                chunks . append ( s [ pos : end ] ) \n            else : \n                if PY3 : \n                    if c & 0xfc00 == 0xd800 and s [ end : end + 2.0 ] == '\\\\u' : \n                        esc2 = s [ end + 2.0 : end + 6.0 ] \n                        c2 = int ( esc2 , 16.0 ) \n                        if c2 & 0xfc00 == 0xdc00 : \n                            c = 0x10000 + ( ( ( c - 0xd800 ) << 10.0 ) | ( c2 - 0xdc00 ) ) \n                            end += 6.0 \n                chunks . append ( s [ pos : start ] ) \n                chunks . append ( unichr ( c ) ) \n        pos = end \n        m = _U_ESCAPE . search ( s , pos ) \n    chunks . append ( s [ pos : ] ) \n    return ( '' . join ( chunks ) ) . encode ( \"utf-8\" ) "}
{"9872": "\ndef check_errors ( self , uri , response ) : \n    if response . status == 401.0 : \n        raise trolly . Unauthorised ( uri , response ) \n    if response . status != 200.0 : \n        raise trolly . ResourceUnavailable ( uri , response ) "}
{"9896": "\ndef main ( argv = None ) : \n    if argv is None : \n        argv = sys . argv [ 1 : ] \n    cli = CommandLineTool ( ) \n    try : \n        return cli . run ( argv ) \n    except KeyboardInterrupt : \n        print ( 'Canceled' ) \n        return 3.0 "}
{"9897": "\ndef _create_cipher ( self , password , salt , nonce = None ) : \n    from argon2 . low_level import hash_secret_raw , Type \n    from Crypto . Cipher import AES \n    aesmode = self . _get_mode ( self . aesmode ) \n    if aesmode is None : \n        raise ValueError ( 'invalid AES mode: %s' % self . aesmode ) \n    key = hash_secret_raw ( secret = password . encode ( self . password_encoding ) , salt = salt , time_cost = self . time_cost , memory_cost = self . memory_cost , parallelism = self . parallelism , hash_len = 16.0 , type = Type . ID ) \n    return AES . new ( key , aesmode , nonce ) "}
{"9900": "\ndef _check_scheme ( self , config ) : \n    try : \n        scheme = config . get ( escape_for_ini ( 'keyring-setting' ) , escape_for_ini ( 'scheme' ) , ) \n    except ( configparser . NoSectionError , configparser . NoOptionError ) : \n        raise AttributeError ( \"Encryption scheme missing\" ) \n    aesmode = scheme [ - 3.0 : ] \n    if aesmode not in self . _get_mode ( ) : \n        raise ValueError ( \"Encryption scheme invalid: %s\" % ( aesmode ) ) \n    self . aesmode = aesmode \n    if scheme . startswith ( 'PyCryptodome ' ) : \n        scheme = scheme [ 13.0 : ] \n    if scheme != self . scheme : \n        raise ValueError ( \"Encryption scheme mismatch \" \"(exp.: %s, found: %s)\" % ( self . scheme , scheme ) ) "}
{"9902": "\ndef makeId ( self ) : \n    self . id = ( self . id + 1 ) % 65536.0 \n    self . id = self . id or 1 \n    return self . id "}
{"9905": "\ndef encodeString ( string ) : \n    encoded = bytearray ( 2.0 ) \n    encoded . extend ( bytearray ( string , encoding = 'utf-8' ) ) \n    l = len ( encoded ) - 2.0 \n    if ( l > 65535.0 ) : \n        raise StringValueError ( l ) \n    encoded [ 0 ] = l >> 8.0 \n    encoded [ 1 ] = l & 0xFF \n    return encoded "}
{"9906": "\ndef decodeString ( encoded ) : \n    length = encoded [ 0 ] * 256.0 + encoded [ 1 ] \n    return ( encoded [ 2.0 : 2.0 + length ] . decode ( 'utf-8' ) , encoded [ 2.0 + length : ] ) "}
{"9907": "\ndef encode16Int ( value ) : \n    value = int ( value ) \n    encoded = bytearray ( 2.0 ) \n    encoded [ 0 ] = value >> 8.0 \n    encoded [ 1 ] = value & 0xFF \n    return encoded "}
{"9908": "\ndef encodeLength ( value ) : \n    encoded = bytearray ( ) \n    while True : \n        digit = value % 128.0 \n        value //= 128.0 \n        if value > 0 : \n            digit |= 128.0 \n        encoded . append ( digit ) \n        if value <= 0 : \n            break \n    return encoded "}
{"9910": "\ndef encode ( self ) : \n    header = bytearray ( 2.0 ) \n    header [ 0 ] = 0xE0 \n    self . encoded = header \n    return str ( header ) if PY2 else bytes ( header ) "}
{"9911": "\ndef encode ( self ) : \n    header = bytearray ( 1 ) \n    varHeader = bytearray ( ) \n    payload = bytearray ( ) \n    header [ 0 ] = 0x10 \n    varHeader . extend ( encodeString ( self . version [ 'tag' ] ) ) \n    varHeader . append ( self . version [ 'level' ] ) \n    flags = ( self . cleanStart << 1 ) \n    if self . willTopic is not None and self . willMessage is not None : \n        flags |= 0x04 | ( self . willRetain << 5.0 ) | ( self . willQoS << 3.0 ) \n    if self . username is not None : \n        flags |= 0x80 \n    if self . password is not None : \n        flags |= 0x40 \n    varHeader . append ( flags ) \n    varHeader . extend ( encode16Int ( self . keepalive ) ) \n    payload . extend ( encodeString ( self . clientId ) ) \n    if self . willTopic is not None and self . willMessage is not None : \n        payload . extend ( encodeString ( self . willTopic ) ) \n        payload . extend ( encodeString ( self . willMessage ) ) \n    if self . username is not None : \n        payload . extend ( encodeString ( self . username ) ) \n    if self . password is not None : \n        payload . extend ( encode16Int ( len ( self . password ) ) ) \n        payload . extend ( bytearray ( self . password , encoding = 'ascii' , errors = 'ignore' ) ) \n    header . extend ( encodeLength ( len ( varHeader ) + len ( payload ) ) ) \n    header . extend ( varHeader ) \n    header . extend ( payload ) \n    self . encoded = header \n    return str ( header ) if PY2 else bytes ( header ) "}
{"9912": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = 1 \n    while packet [ lenLen ] & 0x80 : \n        lenLen += 1 \n    packet_remaining = packet [ lenLen + 1 : ] \n    version_str , packet_remaining = decodeString ( packet_remaining ) \n    version_id = int ( packet_remaining [ 0 ] ) \n    if version_id == v31 [ 'level' ] : \n        self . version = v31 \n    else : \n        self . version = v311 \n    flags = packet_remaining [ 1 ] \n    self . cleanStart = ( flags & 0x02 ) != 0 \n    willFlag = ( flags & 0x04 ) != 0 \n    willQoS = ( flags >> 3.0 ) & 0x03 \n    willRetain = ( flags & 0x20 ) != 0 \n    userFlag = ( flags & 0x80 ) != 0 \n    passFlag = ( flags & 0x40 ) != 0 \n    packet_remaining = packet_remaining [ 2.0 : ] \n    self . keepalive = decode16Int ( packet_remaining ) \n    packet_remaining = packet_remaining [ 2.0 : ] \n    self . clientId , packet_remaining = decodeString ( packet_remaining ) \n    if willFlag : \n        self . willRetain = willRetain \n        self . willQoS = willQoS \n        self . willTopic , packet_remaining = decodeString ( packet_remaining ) \n        self . willMessage , packet_remaining = decodeString ( packet_remaining ) \n    if userFlag : \n        self . username , packet_remaining = decodeString ( packet_remaining ) \n    if passFlag : \n        l = decode16Int ( packet_remaining ) \n        self . password = packet_remaining [ 2.0 : 2.0 + l ] "}
{"9913": "\ndef encode ( self ) : \n    header = bytearray ( 1 ) \n    varHeader = bytearray ( 2.0 ) \n    header [ 0 ] = 0x20 \n    varHeader [ 0 ] = self . session \n    varHeader [ 1 ] = self . resultCode \n    header . extend ( encodeLength ( len ( varHeader ) ) ) \n    header . extend ( varHeader ) \n    self . encoded = header \n    return str ( header ) if PY2 else bytes ( header ) "}
{"9915": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = 1 \n    while packet [ lenLen ] & 0x80 : \n        lenLen += 1 \n    packet_remaining = packet [ lenLen + 1 : ] \n    self . msgId = decode16Int ( packet_remaining [ 0 : 2.0 ] ) \n    self . topics = [ ] \n    packet_remaining = packet_remaining [ 2.0 : ] \n    while len ( packet_remaining ) : \n        topic , packet_remaining = decodeString ( packet_remaining ) \n        qos = int ( packet_remaining [ 0 ] ) & 0x03 \n        self . topics . append ( ( topic , qos ) ) \n        packet_remaining = packet_remaining [ 1 : ] "}
{"9918": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = 1 \n    while packet [ lenLen ] & 0x80 : \n        lenLen += 1 \n    packet_remaining = packet [ lenLen + 1 : ] \n    self . msgId = decode16Int ( packet_remaining [ 0 : 2.0 ] ) \n    self . topics = [ ] \n    packet_remaining = packet_remaining [ 2.0 : ] \n    while len ( packet_remaining ) : \n        l = decode16Int ( packet_remaining [ 0 : 2.0 ] ) \n        topic = packet_remaining [ 2.0 : 2.0 + l ] . decode ( encoding = 'utf-8' ) \n        self . topics . append ( topic ) \n        packet_remaining = packet_remaining [ 2.0 + l : ] "}
{"9920": "\ndef encode ( self ) : \n    header = bytearray ( 1 ) \n    varHeader = bytearray ( ) \n    payload = bytearray ( ) \n    if self . qos : \n        header [ 0 ] = 0x30 | self . retain | ( self . qos << 1 ) | ( self . dup << 3.0 ) \n        varHeader . extend ( encodeString ( self . topic ) ) \n        varHeader . extend ( encode16Int ( self . msgId ) ) \n    else : \n        header [ 0 ] = 0x30 | self . retain \n        varHeader . extend ( encodeString ( self . topic ) ) \n    if isinstance ( self . payload , bytearray ) : \n        payload . extend ( self . payload ) \n    elif isinstance ( self . payload , str ) : \n        payload . extend ( bytearray ( self . payload , encoding = 'utf-8' ) ) \n    else : \n        raise PayloadTypeError ( type ( self . payload ) ) \n    totalLen = len ( varHeader ) + len ( payload ) \n    if totalLen > 268435455.0 : \n        raise PayloadValueError ( totalLen ) \n    header . extend ( encodeLength ( totalLen ) ) \n    header . extend ( varHeader ) \n    header . extend ( payload ) \n    self . encoded = header \n    return str ( header ) if PY2 else bytes ( header ) "}
{"9921": "\ndef decode ( self , packet ) : \n    self . encoded = packet \n    lenLen = 1 \n    while packet [ lenLen ] & 0x80 : \n        lenLen += 1 \n    packet_remaining = packet [ lenLen + 1 : ] \n    self . dup = ( packet [ 0 ] & 0x08 ) == 0x08 \n    self . qos = ( packet [ 0 ] & 0x06 ) >> 1 \n    self . retain = ( packet [ 0 ] & 0x01 ) == 0x01 \n    self . topic , _ = decodeString ( packet_remaining ) \n    topicLen = decode16Int ( packet_remaining ) \n    if self . qos : \n        self . msgId = decode16Int ( packet_remaining [ topicLen + 2.0 : topicLen + 4.0 ] ) \n        self . payload = packet_remaining [ topicLen + 4.0 : ] \n    else : \n        self . msgId = None \n        self . payload = packet_remaining [ topicLen + 2.0 : ] "}
{"9932": "\ndef map_clusters ( self , size , sampled , clusters ) : \n    ids = np . zeros ( size , dtype = int ) \n    ids [ : ] = - 2.0 \n    ids [ sampled ] = clusters \n    return ids "}
{"9937": "\ndef unitpicker ( a , llim = 0.1 , denominator = None , focus_stage = None ) : \n    if not isinstance ( a , ( int , float ) ) : \n        a = nominal_values ( a ) \n        a = np . percentile ( a [ ~ np . isnan ( a ) ] , 25.0 ) \n    if denominator is not None : \n        pd = pretty_element ( denominator ) \n    else : \n        pd = '' \n    if focus_stage == 'calibrated' : \n        udict = { 0 : 'mol/mol ' + pd , 1 : 'mmol/mol ' + pd , 2.0 : '$\\mu$mol/mol ' + pd , 3.0 : 'nmol/mol ' + pd , 4.0 : 'pmol/mol ' + pd , 5.0 : 'fmol/mol ' + pd } \n    elif focus_stage == 'ratios' : \n        udict = { 0 : 'counts/count ' + pd , 1 : '$10^{-3}$ counts/count ' + pd , 2.0 : '$10^{-6}$ counts/count ' + pd , 3.0 : '$10^{-9}$ counts/count ' + pd , 4.0 : '$10^{-12}$ counts/count ' + pd , 5.0 : '$10^{-15}$ counts/count ' + pd } \n    elif focus_stage in ( 'rawdata' , 'despiked' , 'bkgsub' ) : \n        udict = udict = { 0 : 'counts' , 1 : '$10^{-3}$ counts' , 2.0 : '$10^{-6}$ counts' , 3.0 : '$10^{-9}$ counts' , 4.0 : '$10^{-12}$ counts' , 5.0 : '$10^{-15}$ counts' } \n    else : \n        udict = { 0 : '' , 1 : '' , 2.0 : '' , 3.0 : '' , 4.0 : '' , 5.0 : '' } \n    a = abs ( a ) \n    n = 0 \n    if a < llim : \n        while a < llim : \n            a *= 1000.0 \n            n += 1 \n    return float ( 1000.0 ** n ) , udict [ n ] "}
{"9944": "\ndef fastsmooth ( a , win = 11.0 ) : \n    if win % 2.0 == 0 : \n        win += 1 \n    kernel = np . ones ( win ) / win \n    npad = int ( ( win - 1 ) / 2.0 ) \n    spad = np . full ( npad + 1 , np . mean ( a [ : ( npad + 1 ) ] ) ) \n    epad = np . full ( npad - 1 , np . mean ( a [ - ( npad - 1 ) : ] ) ) \n    return np . concatenate ( [ spad , np . convolve ( a , kernel , 'valid' ) , epad ] ) "}
{"9945": "\ndef fastgrad ( a , win = 11.0 ) : \n    if win % 2.0 == 0 : \n        win += 1 \n    wins = rolling_window ( a , win , 'ends' ) \n    a = map ( lambda x : np . polyfit ( np . arange ( win ) , x , 1 ) [ 0 ] , wins ) \n    return np . array ( list ( a ) ) "}
{"9949": "\ndef cluster_DBSCAN ( data , eps = None , min_samples = None , n_clusters = None , maxiter = 200.0 , ** kwargs ) : \n    if n_clusters is None : \n        if eps is None : \n            eps = 0.3 \n        db = cl . DBSCAN ( eps = eps , min_samples = min_samples , ** kwargs ) . fit ( data ) \n    else : \n        clusters = 0 \n        eps_temp = 1 / .95 \n        niter = 0 \n        while clusters < n_clusters : \n            clusters_last = clusters \n            eps_temp *= 0.95 \n            db = cl . DBSCAN ( eps = eps_temp , min_samples = min_samples , ** kwargs ) . fit ( data ) \n            clusters = ( len ( set ( db . labels_ ) ) - ( 1 if - 1 in db . labels_ else 0 ) ) \n            if clusters < clusters_last : \n                eps_temp *= 1 / 0.95 \n                db = cl . DBSCAN ( eps = eps_temp , min_samples = min_samples , ** kwargs ) . fit ( data ) \n                clusters = ( len ( set ( db . labels_ ) ) - ( 1 if - 1 in db . labels_ else 0 ) ) \n                warnings . warn ( ( '\\n\\n***Unable to find {:.0f} clusters in ' 'data. Found {:.0f} with an eps of {:.2e}' '' ) . format ( n_clusters , clusters , eps_temp ) ) \n                break \n            niter += 1 \n            if niter == maxiter : \n                warnings . warn ( ( '\\n\\n***Maximum iterations ({:.0f}) reached' ', {:.0f} clusters not found.\\nDeacrease ' 'min_samples or n_clusters (or increase ' 'maxiter).' ) . format ( maxiter , n_clusters ) ) \n                break \n    labels = db . labels_ \n    core_samples_mask = np . zeros_like ( labels ) \n    core_samples_mask [ db . core_sample_indices_ ] = True \n    return labels , core_samples_mask "}
{"9953": "\ndef print_all ( ) : \n    _ , conf = read_latoolscfg ( ) \n    default = conf [ 'DEFAULT' ] [ 'config' ] \n    pstr = '\\nCurrently defined LAtools configurations:\\n\\n' \n    for s in conf . sections ( ) : \n        if s == default : \n            pstr += s + ' [DEFAULT]\\n' \n        elif s == 'REPRODUCE' : \n            pstr += s + ' [DO NOT ALTER]\\n' \n        else : \n            pstr += s + '\\n' \n        for k , v in conf [ s ] . items ( ) : \n            if k != 'config' : \n                if v [ : 9.0 ] == 'resources' : \n                    v = pkgrs . resource_filename ( 'latools' , v ) \n                pstr += '   ' + k + ': ' + v + '\\n' \n        pstr += '\\n' \n    print ( pstr ) \n    return "}
{"9957": "\ndef exclude_downhole ( filt , threshold = 2.0 ) : \n    cfilt = filt . copy ( ) \n    inds = bool_2_indices ( ~ filt ) \n    rem = ( np . diff ( inds ) >= threshold ) [ : , 0 ] \n    if any ( rem ) : \n        if inds [ rem ] . shape [ 0 ] > 1 : \n            limit = inds [ rem ] [ 1 , 0 ] \n            cfilt [ limit : ] = False \n    return cfilt "}
{"9958": "\ndef defrag ( filt , threshold = 3.0 , mode = 'include' ) : \n    if bool_2_indices ( filt ) is None : \n        return filt \n    if mode == 'include' : \n        inds = bool_2_indices ( ~ filt ) + 1 \n        rep = True \n    if mode == 'exclude' : \n        inds = bool_2_indices ( filt ) + 1 \n        rep = False \n    rem = ( np . diff ( inds ) <= threshold ) [ : , 0 ] \n    cfilt = filt . copy ( ) \n    if any ( rem ) : \n        for lo , hi in inds [ rem ] : \n            cfilt [ lo : hi ] = rep \n    return cfilt "}
{"9959": "\ndef despike ( self , expdecay_despiker = True , exponent = None , noise_despiker = True , win = 3.0 , nlim = 12. , maxiter = 3.0 ) : \n    if not hasattr ( self , 'despiked' ) : \n        self . data [ 'despiked' ] = Bunch ( ) \n    out = { } \n    for a , v in self . focus . items ( ) : \n        if 'time' not in a . lower ( ) : \n            sig = v . copy ( ) \n            if expdecay_despiker : \n                if exponent is not None : \n                    sig = proc . expdecay_despike ( sig , exponent , self . tstep , maxiter ) \n                else : \n                    warnings . warn ( 'exponent is None - either provide exponent, or run at `analyse`\\nlevel to automatically calculate it.' ) \n            if noise_despiker : \n                sig = proc . noise_despike ( sig , int ( win ) , nlim , maxiter ) \n            out [ a ] = sig \n    self . data [ 'despiked' ] . update ( out ) \n    self . data [ 'total_counts' ] = sum ( self . data [ 'despiked' ] . values ( ) ) \n    self . setfocus ( 'despiked' ) \n    return "}
{"9960": "\ndef autorange_plot ( self , analyte = 'total_counts' , gwin = 7.0 , swin = None , win = 20.0 , on_mult = [ 1.5 , 1. ] , off_mult = [ 1. , 1.5 ] , transform = 'log' ) : \n    if analyte is None : \n        sig = self . data [ 'total_counts' ] \n    elif analyte == 'total_counts' : \n        sig = self . data [ 'total_counts' ] \n    elif analyte in self . analytes : \n        sig = self . focus [ analyte ] \n    else : \n        raise ValueError ( 'Invalid analyte.' ) \n    if transform == 'log' : \n        sig = np . log10 ( sig ) \n    fig , axs = plot . autorange_plot ( t = self . Time , sig = sig , gwin = gwin , swin = swin , win = win , on_mult = on_mult , off_mult = off_mult ) \n    return fig , axs "}
{"9968": "\ndef calc_correlation ( self , x_analyte , y_analyte , window = 15.0 , filt = True , recalc = True ) : \n    label = '{:}_{:}_{:.0f}' . format ( x_analyte , y_analyte , window ) \n    if label in self . correlations and not recalc : \n        return \n    if window % 2.0 != 1 : \n        window += 1 \n    ind = self . filt . grab_filt ( filt , [ x_analyte , y_analyte ] ) \n    x = nominal_values ( self . focus [ x_analyte ] ) \n    x [ ~ ind ] = np . nan \n    xr = rolling_window ( x , window , pad = np . nan ) \n    y = nominal_values ( self . focus [ y_analyte ] ) \n    y [ ~ ind ] = np . nan \n    yr = rolling_window ( y , window , pad = np . nan ) \n    r , p = zip ( * map ( nan_pearsonr , xr , yr ) ) \n    r = np . array ( r ) \n    p = np . array ( p ) \n    self . correlations [ label ] = r , p \n    return "}
{"9969": "\ndef filter_correlation ( self , x_analyte , y_analyte , window = 15.0 , r_threshold = 0.9 , p_threshold = 0.05 , filt = True , recalc = False ) : \n    if window % 2.0 != 1 : \n        window += 1 \n    params = locals ( ) \n    del ( params [ 'self' ] ) \n    setn = self . filt . maxset + 1 \n    label = '{:}_{:}_{:.0f}' . format ( x_analyte , y_analyte , window ) \n    self . calc_correlation ( x_analyte , y_analyte , window , filt , recalc ) \n    r , p = self . correlations [ label ] \n    cfilt = ( abs ( r ) > r_threshold ) & ( p < p_threshold ) \n    cfilt = ~ cfilt \n    name = x_analyte + '_' + y_analyte + '_corr' \n    self . filt . add ( name = name , filt = cfilt , info = ( x_analyte + ' vs. ' + y_analyte + ' correlation filter.' ) , params = params , setn = setn ) \n    self . filt . off ( filt = name ) \n    self . filt . on ( analyte = y_analyte , filt = name ) \n    return "}
{"9972": "\ndef histograms ( dat , keys = None , bins = 25.0 , logy = False , cmap = None , ncol = 4.0 ) : \n    if keys is None : \n        keys = dat . keys ( ) \n    ncol = int ( ncol ) \n    nrow = calc_nrow ( len ( keys ) , ncol ) \n    fig , axs = plt . subplots ( nrow , 4.0 , figsize = [ ncol * 2.0 , nrow * 2.0 ] ) \n    pn = 0 \n    for k , ax in zip ( keys , axs . flat ) : \n        tmp = nominal_values ( dat [ k ] ) \n        x = tmp [ ~ np . isnan ( tmp ) ] \n        if cmap is not None : \n            c = cmap [ k ] \n        else : \n            c = ( 0 , 0 , 0 , 0.5 ) \n        ax . hist ( x , bins = bins , color = c ) \n        if logy : \n            ax . set_yscale ( 'log' ) \n            ylab = '$log_{10}(n)$' \n        else : \n            ylab = 'n' \n        ax . set_ylim ( 1 , ax . get_ylim ( ) [ 1 ] ) \n        if ax . is_first_col ( ) : \n            ax . set_ylabel ( ylab ) \n        ax . set_yticklabels ( [ ] ) \n        ax . text ( .95 , .95 , k , ha = 'right' , va = 'top' , transform = ax . transAxes ) \n        pn += 1 \n    for ax in axs . flat [ pn : ] : \n        ax . set_visible ( False ) \n    fig . tight_layout ( ) \n    return fig , axs "}
{"9973": "\ndef summary_stats ( x , y , nm = None ) : \n    if isinstance ( nm , str ) : \n        nm = [ nm ] \n    cols = pd . MultiIndex . from_tuples ( [ ( 'Residual Summary' , 'N' ) , ( 'Residual Summary' , 'Median' ) , ( 'Residual Summary' , 'LQ' ) , ( 'Residual Summary' , 'IQR' ) , ( 'Residual Summary' , 'UQ' ) , ( 'Residual Regression' , 'Slope' ) , ( 'Residual Regression' , 'Slope t' ) , ( 'Residual Regression' , 'Slope p' ) , ( 'Residual Regression' , 'Intercept' ) , ( 'Residual Regression' , 'Intercept t' ) , ( 'Residual Regression' , 'Intercept p' ) , ( 'Residual Regression' , 'R2' ) , ( 'Kolmogorov-Smirnov' , 'KS' ) , ( 'Kolmogorov-Smirnov' , 'p' ) ] ) \n    out = pd . DataFrame ( index = nm , columns = cols ) \n    ind = ~ ( np . isnan ( x ) | np . isnan ( y ) ) \n    x = x [ ind ] \n    y = y [ ind ] \n    r = y - x \n    cat = 'Residual Summary' \n    out . loc [ : , ( cat , 'N' ) ] = len ( x ) \n    out . loc [ : , ( cat , 'Median' ) ] = np . median ( r ) \n    out . loc [ : , [ ( cat , 'LQ' ) , ( cat , 'UQ' ) ] ] = np . percentile ( r , [ 25.0 , 75.0 ] ) \n    out . loc [ : , ( cat , 'IQR' ) ] = out . loc [ : , ( cat , 'UQ' ) ] - out . loc [ : , ( cat , 'LQ' ) ] \n    cat = 'Kolmogorov-Smirnov' \n    ks = stats . ks_2samp ( x , y ) \n    out . loc [ : , ( cat , 'KS' ) ] = ks . statistic \n    out . loc [ : , ( cat , 'p' ) ] = ks . pvalue \n    cat = 'Residual Regression' \n    X = sm . add_constant ( x ) \n    reg = sm . OLS ( r , X , missing = 'drop' ) \n    fit = reg . fit ( ) \n    out . loc [ : , [ ( cat , 'Intercept' ) , ( cat , 'Slope' ) ] ] = fit . params \n    out . loc [ : , [ ( cat , 'Intercept t' ) , ( cat , 'Slope t' ) ] ] = fit . tvalues \n    out . loc [ : , ( cat , 'R2' ) ] = fit . rsquared \n    out . loc [ : , [ ( cat , 'Intercept p' ) , ( cat , 'Slope p' ) ] ] = fit . pvalues \n    return out "}
{"9976": "\ndef elements ( all_isotopes = True ) : \n    el = pd . read_pickle ( pkgrs . resource_filename ( 'latools' , 'resources/elements.pkl' ) ) \n    if all_isotopes : \n        return el . set_index ( 'element' ) \n    else : \n        def wmean ( g ) : \n            return ( g . atomic_weight * g . percent ) . sum ( ) / 100.0 \n        iel = el . groupby ( 'element' ) . apply ( wmean ) \n        iel . name = 'atomic_weight' \n        return iel "}
{"9977": "\ndef calc_M ( molecule ) : \n    els = elements ( ) \n    parens = re . compile ( '\\(([A-z0-9]+)\\)([0-9]+)?' ) \n    stoich = re . compile ( '([A-Z][a-z]?)([0-9]+)?' ) \n    ps = parens . findall ( molecule ) \n    rem = parens . sub ( '' , molecule ) \n    m = 0 \n    if len ( ps ) > 0 : \n        for sub , ns in ps : \n            ms = 0 \n            for e , n in stoich . findall ( sub ) : \n                me = ( els . loc [ e , 'atomic_weight' ] * els . loc [ e , 'percent' ] / 100.0 ) . sum ( ) \n                if n == '' : \n                    n = 1 \n                else : \n                    n = int ( n ) \n                ms += me * n \n            if ns == '' : \n                ns = 1 \n            else : \n                ns = int ( ns ) \n            m += ms * ns \n    for e , n in stoich . findall ( rem ) : \n        me = ( els . loc [ e , 'atomic_weight' ] * els . loc [ e , 'percent' ] / 100.0 ) . sum ( ) \n        if n == '' : \n            n = 1 \n        else : \n            n = int ( n ) \n        m += me * n \n    return m "}
{"9981": "\ndef gauss_weighted_stats ( x , yarray , x_new , fwhm ) : \n    sigma = fwhm / ( 2.0 * np . sqrt ( 2.0 * np . log ( 2.0 ) ) ) \n    mask = np . zeros ( ( x . size , yarray . shape [ 1 ] , x_new . size ) ) \n    for i , xni in enumerate ( x_new ) : \n        mask [ : , : , i ] = gauss ( x [ : , np . newaxis ] , 1 , xni , sigma ) \n    nmask = mask / mask . sum ( 0 ) \n    av = ( nmask * yarray [ : , : , np . newaxis ] ) . sum ( 0 ) \n    diff = np . power ( av - yarray [ : , : , np . newaxis ] , 2.0 ) \n    std = np . sqrt ( ( diff * nmask ) . sum ( 0 ) ) \n    se = std / np . sqrt ( mask . sum ( 0 ) ) \n    return av , std , se "}
{"9982": "\ndef gauss ( x , * p ) : \n    A , mu , sigma = p \n    return A * np . exp ( - 0.5 * ( - mu + x ) ** 2.0 / sigma ** 2.0 ) "}
{"9985": "\ndef despike ( self , expdecay_despiker = False , exponent = None , noise_despiker = True , win = 3.0 , nlim = 12. , exponentplot = False , maxiter = 4.0 , autorange_kwargs = { } , focus_stage = 'rawdata' ) : \n    if focus_stage != self . focus_stage : \n        self . set_focus ( focus_stage ) \n    if expdecay_despiker and exponent is None : \n        if not hasattr ( self , 'expdecay_coef' ) : \n            self . find_expcoef ( plot = exponentplot , autorange_kwargs = autorange_kwargs ) \n        exponent = self . expdecay_coef \n        time . sleep ( 0.1 ) \n    with self . pbar . set ( total = len ( self . data ) , desc = 'Despiking' ) as prog : \n        for d in self . data . values ( ) : \n            d . despike ( expdecay_despiker , exponent , noise_despiker , win , nlim , maxiter ) \n            prog . update ( ) \n    self . stages_complete . update ( [ 'despiked' ] ) \n    self . focus_stage = 'despiked' \n    return "}
{"9986": "\ndef bkg_calc_weightedmean ( self , analytes = None , weight_fwhm = None , n_min = 20.0 , n_max = None , cstep = None , bkg_filter = False , f_win = 7.0 , f_n_lim = 3.0 , focus_stage = 'despiked' ) : \n    if analytes is None : \n        analytes = self . analytes \n        self . bkg = Bunch ( ) \n    elif isinstance ( analytes , str ) : \n        analytes = [ analytes ] \n    if weight_fwhm is None : \n        weight_fwhm = 600.0 \n    self . get_background ( n_min = n_min , n_max = n_max , bkg_filter = bkg_filter , f_win = f_win , f_n_lim = f_n_lim , focus_stage = focus_stage ) \n    if 'calc' not in self . bkg . keys ( ) : \n        if cstep is None : \n            cstep = weight_fwhm / 20.0 \n        elif cstep > weight_fwhm : \n            warnings . warn ( \"\\ncstep should be less than weight_fwhm. Your backgrounds\\n\" + \"might not behave as expected.\\n\" ) \n        bkg_t = np . linspace ( 0 , self . max_time , self . max_time // cstep ) \n        self . bkg [ 'calc' ] = Bunch ( ) \n        self . bkg [ 'calc' ] [ 'uTime' ] = bkg_t \n    mean , std , stderr = gauss_weighted_stats ( self . bkg [ 'raw' ] . uTime , self . bkg [ 'raw' ] . loc [ : , analytes ] . values , self . bkg [ 'calc' ] [ 'uTime' ] , fwhm = weight_fwhm ) \n    for i , a in enumerate ( analytes ) : \n        self . bkg [ 'calc' ] [ a ] = { 'mean' : mean [ i ] , 'std' : std [ i ] , 'stderr' : stderr [ i ] } "}
{"9987": "\ndef bkg_calc_interp1d ( self , analytes = None , kind = 1 , n_min = 10.0 , n_max = None , cstep = None , bkg_filter = False , f_win = 7.0 , f_n_lim = 3.0 , focus_stage = 'despiked' ) : \n    if analytes is None : \n        analytes = self . analytes \n        self . bkg = Bunch ( ) \n    elif isinstance ( analytes , str ) : \n        analytes = [ analytes ] \n    self . get_background ( n_min = n_min , n_max = n_max , bkg_filter = bkg_filter , f_win = f_win , f_n_lim = f_n_lim , focus_stage = focus_stage ) \n    def pad ( a , lo = None , hi = None ) : \n        if lo is None : \n            lo = [ a [ 0 ] ] \n        if hi is None : \n            hi = [ a [ - 1 ] ] \n        return np . concatenate ( ( lo , a , hi ) ) \n    if 'calc' not in self . bkg . keys ( ) : \n        bkg_t = pad ( self . bkg [ 'summary' ] . loc [ : , ( 'uTime' , 'mean' ) ] , [ 0 ] , [ self . max_time ] ) \n        self . bkg [ 'calc' ] = Bunch ( ) \n        self . bkg [ 'calc' ] [ 'uTime' ] = bkg_t \n    d = self . bkg [ 'summary' ] \n    with self . pbar . set ( total = len ( analytes ) , desc = 'Calculating Analyte Backgrounds' ) as prog : \n        for a in analytes : \n            self . bkg [ 'calc' ] [ a ] = { 'mean' : pad ( d . loc [ : , ( a , 'mean' ) ] . values ) , 'std' : pad ( d . loc [ : , ( a , 'std' ) ] . values ) , 'stderr' : pad ( d . loc [ : , ( a , 'stderr' ) ] . values ) } \n            prog . update ( ) \n    self . bkg [ 'calc' ] \n    return "}
{"9991": "\ndef filter_gradient_threshold_percentile ( self , analyte , percentiles , level = 'population' , win = 15.0 , filt = False , samples = None , subset = None ) : \n    params = locals ( ) \n    del ( params [ 'self' ] ) \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    self . minimal_analytes . update ( [ analyte ] ) \n    self . get_gradients ( analytes = [ analyte ] , win = win , filt = filt , subset = subset ) \n    grad = self . gradients [ analyte ] [ ~ np . isnan ( self . gradients [ analyte ] ) ] \n    if isinstance ( percentiles , ( int , float ) ) : \n        percentiles = [ percentiles ] \n    if level == 'population' : \n        lims = np . percentile ( grad , percentiles ) \n    with self . pbar . set ( total = len ( samples ) , desc = 'Percentile Threshold Filter' ) as prog : \n        for s in samples : \n            d = self . data [ s ] \n            setn = d . filt . maxset + 1 \n            g = calc_grads ( d . Time , d . focus , [ analyte ] , win ) [ analyte ] \n            if level == 'individual' : \n                gt = nominal_values ( g ) \n                lims = np . percentile ( gt [ ~ np . isnan ( gt ) ] , percentiles ) \n            if len ( lims ) == 1 : \n                above = g >= lims [ 0 ] \n                below = g < lims [ 0 ] \n                d . filt . add ( analyte + '_{:.1f}-grd-pcnt_below' . format ( percentiles [ 0 ] ) , below , 'Gradients below {:.1f}th {:} percentile ({:.2e})' . format ( percentiles [ 0 ] , analyte , lims [ 0 ] ) , params , setn = setn ) \n                d . filt . add ( analyte + '_{:.1f}-grd-pcnt_above' . format ( percentiles [ 0 ] ) , above , 'Gradients above {:.1f}th {:} percentile ({:.2e})' . format ( percentiles [ 0 ] , analyte , lims [ 0 ] ) , params , setn = setn ) \n            elif len ( lims ) == 2.0 : \n                inside = ( g >= min ( lims ) ) & ( g <= max ( lims ) ) \n                outside = ( g < min ( lims ) ) | ( g > max ( lims ) ) \n                lpc = '-' . join ( [ '{:.1f}' . format ( p ) for p in percentiles ] ) \n                d . filt . add ( analyte + '_' + lpc + '-grd-pcnt_inside' , inside , 'Gradients between ' + lpc + ' ' + analyte + 'percentiles' , params , setn = setn ) \n                d . filt . add ( analyte + '_' + lpc + '-grd-pcnt_outside' , outside , 'Gradients outside ' + lpc + ' ' + analyte + 'percentiles' , params , setn = setn ) \n            prog . update ( ) \n    return "}
{"9993": "\ndef apply_classifier ( self , name , samples = None , subset = None ) : \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    c = self . classifiers [ name ] \n    labs = c . classifier . ulabels_ \n    with self . pbar . set ( total = len ( samples ) , desc = 'Applying ' + name + ' classifier' ) as prog : \n        for s in samples : \n            d = self . data [ s ] \n            try : \n                f = c . predict ( d . focus ) \n            except ValueError : \n                f = np . array ( [ - 2.0 ] * len ( d . Time ) ) \n            for l in labs : \n                ind = f == l \n                d . filt . add ( name = name + '_{:.0f}' . format ( l ) , filt = ind , info = name + ' ' + c . method + ' classifier' , params = ( c . analytes , c . method ) ) \n            prog . update ( ) \n    return name "}
{"9999": "\ndef filter_nremoved ( self , filt = True , quiet = False ) : \n    rminfo = { } \n    for n in self . subsets [ 'All_Samples' ] : \n        s = self . data [ n ] \n        rminfo [ n ] = s . filt_nremoved ( filt ) \n    if not quiet : \n        maxL = max ( [ len ( s ) for s in rminfo . keys ( ) ] ) \n        print ( '{string:{number}s}' . format ( string = 'Sample ' , number = maxL + 3.0 ) + '{total:4s}' . format ( total = 'tot' ) + '{removed:4s}' . format ( removed = 'flt' ) + '{percent:4s}' . format ( percent = '%rm' ) ) \n        for k , ( ntot , nfilt , pcrm ) in rminfo . items ( ) : \n            print ( '{string:{number}s}' . format ( string = k , number = maxL + 3.0 ) + '{total:4.0f}' . format ( total = ntot ) + '{removed:4.0f}' . format ( removed = nfilt ) + '{percent:4.0f}' . format ( percent = pcrm ) ) \n    return rminfo "}
{"10000": "\ndef gradient_histogram ( self , analytes = None , win = 15.0 , filt = False , bins = None , samples = None , subset = None , recalc = True , ncol = 4.0 ) : \n    if analytes is None : \n        analytes = [ a for a in self . analytes if self . internal_standard not in a ] \n    if not hasattr ( self , 'gradients' ) : \n        self . gradients = Bunch ( ) \n    ncol = int ( ncol ) \n    n = len ( analytes ) \n    nrow = plot . calc_nrow ( n , ncol ) \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    self . get_gradients ( analytes = analytes , win = win , filt = filt , subset = subset , recalc = recalc ) \n    fig , axs = plt . subplots ( nrow , ncol , figsize = [ 3. * ncol , 2.5 * nrow ] ) \n    if not isinstance ( axs , np . ndarray ) : \n        axs = [ axs ] \n    i = 0 \n    for a , ax in zip ( analytes , axs . flatten ( ) ) : \n        d = nominal_values ( self . gradients [ a ] ) \n        d = d [ ~ np . isnan ( d ) ] \n        m , u = unitpicker ( d , focus_stage = self . focus_stage , denominator = self . internal_standard ) \n        if bins is None : \n            ibins = np . linspace ( * np . percentile ( d * m , [ 1 , 99.0 ] ) , 50.0 ) \n        else : \n            ibins = bins \n        ax . hist ( d * m , bins = ibins , color = self . cmaps [ a ] ) \n        ax . axvline ( 0 , ls = 'dashed' , lw = 1 , c = ( 0 , 0 , 0 , 0.7 ) ) \n        ax . set_title ( a , loc = 'left' ) \n        if ax . is_first_col ( ) : \n            ax . set_ylabel ( 'N' ) \n        ax . set_xlabel ( u + '/s' ) \n        i += 1 \n    if i < ncol * nrow : \n        for ax in axs . flatten ( ) [ i : ] : \n            ax . set_visible ( False ) \n    fig . tight_layout ( ) \n    return fig , axs "}
{"10001": "\ndef gradient_crossplot ( self , analytes = None , win = 15.0 , lognorm = True , bins = 25.0 , filt = False , samples = None , subset = None , figsize = ( 12.0 , 12.0 ) , save = False , colourful = True , mode = 'hist2d' , recalc = True , ** kwargs ) : \n    if analytes is None : \n        analytes = self . analytes \n    if self . focus_stage in [ 'ratio' , 'calibrated' ] : \n        analytes = [ a for a in analytes if self . internal_standard not in a ] \n    try : \n        analytes = sorted ( analytes , key = lambda x : float ( re . findall ( '[0-9.-]+' , x ) [ 0 ] ) ) \n    except IndexError : \n        analytes = sorted ( analytes ) \n    samples = self . _get_samples ( subset ) \n    self . get_gradients ( analytes = analytes , win = win , filt = filt , subset = subset , recalc = recalc ) \n    fig , axes = plot . crossplot ( dat = self . gradients , keys = analytes , lognorm = lognorm , bins = bins , figsize = figsize , colourful = colourful , focus_stage = self . focus_stage , cmap = self . cmaps , denominator = self . internal_standard , mode = mode ) \n    if save : \n        fig . savefig ( self . report_dir + '/g_crossplot.png' , dpi = 200.0 ) \n    return fig , axes "}
{"10002": "\ndef histograms ( self , analytes = None , bins = 25.0 , logy = False , filt = False , colourful = True ) : \n    if analytes is None : \n        analytes = self . analytes \n    if self . focus_stage in [ 'ratio' , 'calibrated' ] : \n        analytes = [ a for a in analytes if self . internal_standard not in a ] \n    if colourful : \n        cmap = self . cmaps \n    else : \n        cmap = None \n    self . get_focus ( filt = filt ) \n    fig , axes = plot . histograms ( self . focus , keys = analytes , bins = bins , logy = logy , cmap = cmap ) \n    return fig , axes "}
{"10003": "\ndef trace_plots ( self , analytes = None , samples = None , ranges = False , focus = None , outdir = None , filt = None , scale = 'log' , figsize = [ 10.0 , 4.0 ] , stats = False , stat = 'nanmean' , err = 'nanstd' , subset = 'All_Analyses' ) : \n    if focus is None : \n        focus = self . focus_stage \n    if outdir is None : \n        outdir = self . report_dir + '/' + focus \n    if not os . path . isdir ( outdir ) : \n        os . mkdir ( outdir ) \n    if subset is not None : \n        samples = self . _get_samples ( subset ) \n    elif samples is None : \n        samples = self . subsets [ 'All_Analyses' ] \n    elif isinstance ( samples , str ) : \n        samples = [ samples ] \n    with self . pbar . set ( total = len ( samples ) , desc = 'Drawing Plots' ) as prog : \n        for s in samples : \n            f , a = self . data [ s ] . tplot ( analytes = analytes , figsize = figsize , scale = scale , filt = filt , ranges = ranges , stats = stats , stat = stat , err = err , focus_stage = focus ) \n            f . savefig ( outdir + '/' + s + '_traces.pdf' ) \n            plt . close ( f ) \n            prog . update ( ) \n    return "}
{"10004": "\ndef gradient_plots ( self , analytes = None , win = 15.0 , samples = None , ranges = False , focus = None , outdir = None , figsize = [ 10.0 , 4.0 ] , subset = 'All_Analyses' ) : \n    if focus is None : \n        focus = self . focus_stage \n    if outdir is None : \n        outdir = self . report_dir + '/' + focus + '_gradient' \n    if not os . path . isdir ( outdir ) : \n        os . mkdir ( outdir ) \n    if subset is not None : \n        samples = self . _get_samples ( subset ) \n    elif samples is None : \n        samples = self . subsets [ 'All_Analyses' ] \n    elif isinstance ( samples , str ) : \n        samples = [ samples ] \n    with self . pbar . set ( total = len ( samples ) , desc = 'Drawing Plots' ) as prog : \n        for s in samples : \n            f , a = self . data [ s ] . gplot ( analytes = analytes , win = win , figsize = figsize , ranges = ranges , focus_stage = focus ) \n            f . savefig ( outdir + '/' + s + '_gradients.pdf' ) \n            plt . close ( f ) \n            prog . update ( ) \n    return "}
{"10005": "\ndef filter_reports ( self , analytes , filt_str = 'all' , nbin = 5.0 , samples = None , outdir = None , subset = 'All_Samples' ) : \n    if outdir is None : \n        outdir = self . report_dir + '/filters/' + filt_str \n        if not os . path . isdir ( self . report_dir + '/filters' ) : \n            os . mkdir ( self . report_dir + '/filters' ) \n    if not os . path . isdir ( outdir ) : \n        os . mkdir ( outdir ) \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    with self . pbar . set ( total = len ( samples ) , desc = 'Drawing Plots' ) as prog : \n        for s in samples : \n            _ = self . data [ s ] . filter_report ( filt = filt_str , analytes = analytes , savedir = outdir , nbin = nbin ) \n            prog . update ( ) \n    return "}
{"10007": "\ndef getstats ( self , save = True , filename = None , samples = None , subset = None , ablation_time = False ) : \n    slst = [ ] \n    if samples is not None : \n        subset = self . make_subset ( samples ) \n    samples = self . _get_samples ( subset ) \n    for s in self . stats_calced : \n        for nm in [ n for n in samples if self . srm_identifier not in n ] : \n            if self . stats [ nm ] [ s ] . ndim == 2.0 : \n                reps = np . arange ( self . stats [ nm ] [ s ] . shape [ - 1 ] ) \n                ss = np . array ( [ s ] * reps . size ) \n                nms = np . array ( [ nm ] * reps . size ) \n                stdf = pd . DataFrame ( self . stats [ nm ] [ s ] . T , columns = self . stats [ nm ] [ 'analytes' ] , index = [ ss , nms , reps ] ) \n                stdf . index . set_names ( [ 'statistic' , 'sample' , 'rep' ] , inplace = True ) \n            else : \n                stdf = pd . DataFrame ( self . stats [ nm ] [ s ] , index = self . stats [ nm ] [ 'analytes' ] , columns = [ [ s ] , [ nm ] ] ) . T \n                stdf . index . set_names ( [ 'statistic' , 'sample' ] , inplace = True ) \n            slst . append ( stdf ) \n    out = pd . concat ( slst ) \n    if ablation_time : \n        ats = self . ablation_times ( samples = samples , subset = subset ) \n        ats [ 'statistic' ] = 'nanmean' \n        ats . set_index ( 'statistic' , append = True , inplace = True ) \n        ats = ats . reorder_levels ( [ 'statistic' , 'sample' , 'rep' ] ) \n        out = out . join ( ats ) \n    out . drop ( self . internal_standard , 1 , inplace = True ) \n    if save : \n        if filename is None : \n            filename = 'stat_export.csv' \n        out . to_csv ( self . export_dir + '/' + filename ) \n    self . stats_df = out \n    return out "}
{"10014": "\ndef pca_plot ( pca , dt , xlabs = None , mode = 'scatter' , lognorm = True ) : \n    nc = pca . n_components \n    f = np . arange ( pca . n_features_ ) \n    cs = list ( itertools . combinations ( range ( nc ) , 2.0 ) ) \n    ind = ~ np . apply_along_axis ( any , 1 , np . isnan ( dt ) ) \n    cylim = ( pca . components_ . min ( ) , pca . components_ . max ( ) ) \n    yd = cylim [ 1 ] - cylim [ 0 ] \n    fig , axs = plt . subplots ( nc , nc , figsize = [ 3.0 * nc , nc * 3.0 ] , tight_layout = True ) \n    for x , y in zip ( * np . triu_indices ( nc ) ) : \n        if x == y : \n            tax = axs [ x , y ] \n            tax . bar ( f , pca . components_ [ x ] , 0.8 ) \n            tax . set_xticks ( [ ] ) \n            tax . axhline ( 0 , zorder = - 1 , c = ( 0 , 0 , 0 , 0.6 ) ) \n            tax . set_ylim ( cylim [ 0 ] - 0.2 * yd , cylim [ 1 ] + 0.2 * yd ) \n            for xi , yi , lab in zip ( f , pca . components_ [ x ] , xlabs ) : \n                if yi > 0 : \n                    yo = yd * 0.03 \n                    va = 'bottom' \n                else : \n                    yo = yd * - 0.02 \n                    va = 'top' \n                tax . text ( xi , yi + yo , lab , ha = 'center' , va = va , rotation = 90.0 , fontsize = 8.0 ) \n        else : \n            xv = dt [ ind , x ] \n            yv = dt [ ind , y ] \n            if mode == 'scatter' : \n                axs [ x , y ] . scatter ( xv , yv , alpha = 0.2 ) \n                axs [ y , x ] . scatter ( yv , xv , alpha = 0.2 ) \n            if mode == 'hist2d' : \n                if lognorm : \n                    norm = mpl . colors . LogNorm ( ) \n                else : \n                    norm = None \n                axs [ x , y ] . hist2d ( xv , yv , 50.0 , cmap = plt . cm . Blues , norm = norm ) \n                axs [ y , x ] . hist2d ( yv , xv , 50.0 , cmap = plt . cm . Blues , norm = norm ) \n        if x == 0 : \n            axs [ y , x ] . set_ylabel ( 'PC{:.0f}' . format ( y + 1 ) ) \n        if y == nc - 1 : \n            axs [ y , x ] . set_xlabel ( 'PC{:.0f}' . format ( x + 1 ) ) \n    return fig , axs , xv , yv "}
{"10016": "\ndef median_scaler ( s ) : \n    if sum ( ~ np . isnan ( s ) ) > 2.0 : \n        ss = s [ ~ np . isnan ( s ) ] \n        median = np . median ( ss ) \n        IQR = np . diff ( np . percentile ( ss , [ 25.0 , 75.0 ] ) ) \n        return ( s - median ) / IQR \n    else : \n        return np . full ( s . shape , np . nan ) "}
{"10017": "\ndef noise_despike ( sig , win = 3.0 , nlim = 24. , maxiter = 4.0 ) : \n    if win % 2.0 != 1 : \n        win += 1 \n    kernel = np . ones ( win ) / win \n    over = np . ones ( len ( sig ) , dtype = bool ) \n    npad = int ( ( win - 1 ) / 2.0 ) \n    over [ : npad ] = False \n    over [ - npad : ] = False \n    nloops = 0 \n    while any ( over ) and ( nloops < maxiter ) : \n        rmean = np . convolve ( sig , kernel , 'valid' ) \n        rstd = rmean ** 0.5 \n        over [ npad : - npad ] = ( sig [ npad : - npad ] > rmean + nlim * rstd ) \n        if any ( over ) : \n            sig [ npad : - npad ] [ over [ npad : - npad ] ] = rmean [ over [ npad : - npad ] ] \n            nloops += 1 \n    return sig "}
{"10018": "\ndef expdecay_despike ( sig , expdecay_coef , tstep , maxiter = 3.0 ) : \n    noise = np . std ( sig [ : 5.0 ] ) \n    for i in [ 10.0 , 20.0 , 30.0 , 50.0 ] : \n        inoise = np . std ( sig [ : i ] ) \n        if inoise < 1.5 * noise : \n            noise = inoise \n    rms_noise3 = 3.0 * noise \n    i = 0 \n    f = True \n    while ( i < maxiter ) and f : \n        siglo = np . roll ( sig * np . exp ( tstep * expdecay_coef ) , 1 ) \n        sighi = np . roll ( sig * np . exp ( - tstep * expdecay_coef ) , - 1 ) \n        loind = ( sig < siglo - rms_noise3 ) & ( sig < np . roll ( sig , - 1 ) - rms_noise3 ) \n        hiind = ( sig > sighi + rms_noise3 ) & ( sig > np . roll ( sig , 1 ) + rms_noise3 ) \n        sig [ loind ] = sig [ np . roll ( loind , - 1 ) ] \n        sig [ hiind ] = sig [ np . roll ( hiind , - 1 ) ] \n        f = any ( np . concatenate ( [ loind , hiind ] ) ) \n        i += 1 \n    return sig "}
{"10031": "\nasync def get_information ( ) : \n    jar = aiohttp . CookieJar ( unsafe = True ) \n    websession = aiohttp . ClientSession ( cookie_jar = jar ) \n    modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) \n    await modem . login ( password = sys . argv [ 2.0 ] ) \n    result = await modem . information ( ) \n    for sms in result . sms : \n        pprint . pprint ( sms ) \n    await modem . logout ( ) \n    await websession . close ( ) "}
{"10032": "\nasync def send_message ( ) : \n    jar = aiohttp . CookieJar ( unsafe = True ) \n    websession = aiohttp . ClientSession ( cookie_jar = jar ) \n    modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) \n    await modem . login ( password = sys . argv [ 2.0 ] ) \n    await modem . sms ( phone = sys . argv [ 3.0 ] , message = sys . argv [ 4.0 ] ) \n    await modem . logout ( ) \n    await websession . close ( ) "}
{"10040": "\ndef process_notebook ( self , disable_warnings = True ) : \n    infile = self . infile \n    outfile = self . outfile \n    in_dir = os . path . dirname ( infile ) + os . path . sep \n    odir = os . path . dirname ( outfile ) + os . path . sep \n    create_dirs ( os . path . join ( odir , 'images' ) ) \n    ep = nbconvert . preprocessors . ExecutePreprocessor ( timeout = 300.0 ) \n    cp = nbconvert . preprocessors . ClearOutputPreprocessor ( timeout = 300.0 ) \n    self . nb = nb = nbformat . read ( infile , nbformat . current_nbformat ) \n    if disable_warnings : \n        for i , cell in enumerate ( nb . cells ) : \n            if cell [ 'cell_type' ] == 'code' : \n                cell = cell . copy ( ) \n                break \n        cell = cell . copy ( ) \n        cell . source = \"\"\"import logginglogging.captureWarnings(True)logging.getLogger('py.warnings').setLevel(logging.ERROR)\"\"\" \n        nb . cells . insert ( i , cell ) \n    if self . preprocess : \n        t = dt . datetime . now ( ) \n        logger . info ( 'Processing %s' , self . infile ) \n        try : \n            ep . preprocess ( nb , { 'metadata' : { 'path' : in_dir } } ) \n        except nbconvert . preprocessors . execute . CellExecutionError : \n            logger . critical ( 'Error while processing %s!' , self . infile , exc_info = True ) \n        else : \n            logger . info ( 'Done. Seconds needed: %i' , ( dt . datetime . now ( ) - t ) . seconds ) \n        if disable_warnings : \n            nb . cells . pop ( i ) \n    self . py_file = self . get_out_file ( 'py' ) \n    if self . remove_tags : \n        tp = nbconvert . preprocessors . TagRemovePreprocessor ( timeout = 300.0 ) \n        for key , val in self . tag_options . items ( ) : \n            setattr ( tp , key , set ( val ) ) \n        nb4rst = deepcopy ( nb ) \n        tp . preprocess ( nb4rst , { 'metadata' : { 'path' : in_dir } } ) \n    else : \n        nb4rst = nb \n    self . create_rst ( nb4rst , in_dir , odir ) \n    if self . clear : \n        cp . preprocess ( nb , { 'metadata' : { 'path' : in_dir } } ) \n    nbformat . write ( nb , outfile ) \n    self . create_py ( nb ) "}
{"10041": "\ndef create_py ( self , nb , force = False ) : \n    if list ( map ( int , re . findall ( '\\d+' , nbconvert . __version__ ) ) ) >= [ 4.0 , 2.0 ] : \n        py_file = os . path . basename ( self . py_file ) \n    else : \n        py_file = self . py_file \n    try : \n        level = logger . logger . level \n    except AttributeError : \n        level = logger . level \n    spr . call ( [ 'jupyter' , 'nbconvert' , '--to=python' , '--output=' + py_file , '--log-level=%s' % level , self . outfile ] ) \n    with open ( self . py_file ) as f : \n        py_content = f . read ( ) \n    py_content = re . sub ( '^\\s*get_ipython\\(\\).magic.*' , '# \\g<0>' , py_content , flags = re . MULTILINE ) \n    with open ( self . py_file , 'w' ) as f : \n        f . write ( py_content ) "}
{"10042": "\ndef data_download ( self , files ) : \n    if len ( files ) > 1 : \n        return self . DATA_DOWNLOAD % ( ( '\\n\\n' + ' ' * 8.0 ) + ( '\\n' + ' ' * 8.0 ) . join ( '* :download:`%s`' % f for f in files ) ) \n    return self . DATA_DOWNLOAD % ':download:`%s`' % files [ 0 ] "}
{"10045": "\ndef scale_image ( self , in_fname , out_fname , max_width , max_height ) : \n    try : \n        from PIL import Image \n    except ImportError : \n        import Image \n    img = Image . open ( in_fname ) \n    width_in , height_in = img . size \n    scale_w = max_width / float ( width_in ) \n    scale_h = max_height / float ( height_in ) \n    if height_in * scale_w <= max_height : \n        scale = scale_w \n    else : \n        scale = scale_h \n    if scale >= 1.0 and in_fname == out_fname : \n        return \n    width_sc = int ( round ( scale * width_in ) ) \n    height_sc = int ( round ( scale * height_in ) ) \n    img . thumbnail ( ( width_sc , height_sc ) , Image . ANTIALIAS ) \n    thumb = Image . new ( 'RGB' , ( max_width , max_height ) , ( 255.0 , 255.0 , 255.0 ) ) \n    pos_insert = ( ( max_width - width_sc ) // 2.0 , ( max_height - height_sc ) // 2.0 ) \n    thumb . paste ( img , pos_insert ) \n    thumb . save ( out_fname ) "}
{"10046": "\ndef save_thumbnail ( self , image_path ) : \n    thumb_dir = os . path . join ( os . path . dirname ( image_path ) , 'thumb' ) \n    create_dirs ( thumb_dir ) \n    thumb_file = os . path . join ( thumb_dir , '%s_thumb.png' % self . reference ) \n    if os . path . exists ( image_path ) : \n        logger . info ( 'Scaling %s to thumbnail %s' , image_path , thumb_file ) \n        self . scale_image ( image_path , thumb_file , 400.0 , 280.0 ) \n    self . thumb_file = thumb_file "}
{"10050": "\ndef default_value ( field ) : \n    def default_value_func ( self ) : \n        attname = lambda x : get_real_fieldname ( field , x ) \n        if getattr ( self , attname ( get_language ( ) ) , None ) : \n            result = getattr ( self , attname ( get_language ( ) ) ) \n        elif getattr ( self , attname ( get_language ( ) [ : 2.0 ] ) , None ) : \n            result = getattr ( self , attname ( get_language ( ) [ : 2.0 ] ) ) \n        else : \n            default_language = fallback_language ( ) \n            if getattr ( self , attname ( default_language ) , None ) : \n                result = getattr ( self , attname ( default_language ) , None ) \n            else : \n                result = getattr ( self , attname ( settings . LANGUAGE_CODE ) , None ) \n        return result \n    return default_value_func "}
{"10069": "\ndef jsonex_api ( f ) : \n    \n    @ wraps ( f ) \n    def wrapper ( * args , ** kwargs ) : \n        try : \n            code , res = 200.0 , f ( * args , ** kwargs ) \n        except HTTPException as e : \n            code , res = e . code , { 'error' : e } \n        except Exception as e : \n            code , res = 500.0 , { 'error' : e } \n            logger . exception ( 'Method error' ) \n        response = make_response ( jsonex_dumps ( res ) , code ) \n        response . headers [ 'Content-Type' ] = 'application/json' \n        return response \n    return wrapper "}
{"10073": "\ndef estimate_tx_gas ( self , safe_address : str , to : str , value : int , data : bytes , operation : int ) -> int : \n    proxy_gas = 1000.0 \n    old_call_gas = 35000.0 \n    safe_gas_estimation = ( self . estimate_tx_gas_with_safe ( safe_address , to , value , data , operation ) + proxy_gas + old_call_gas ) \n    if SafeOperation ( operation ) == SafeOperation . CALL : \n        try : \n            web3_gas_estimation = ( self . estimate_tx_gas_with_web3 ( safe_address , to , value , data ) + proxy_gas + old_call_gas ) \n        except ValueError : \n            web3_gas_estimation = 0 \n        return max ( safe_gas_estimation , web3_gas_estimation ) \n    else : \n        return safe_gas_estimation "}
{"10087": "\ndef _cauchy_equation ( wavelength , coefficients ) : \n    n = 0. \n    for i , c in enumerate ( coefficients ) : \n        exponent = 2.0 * i \n        n += c / wavelength ** exponent \n    return n "}
{"10089": "\ndef login ( self , username , password , generate = 'enabled' , proxies = None ) : \n    logger . debug ( \"login for: %s with generate: %s\" , username , generate ) \n    if not username or not password : \n        raise BackendException ( BACKEND_ERROR , \"Missing mandatory parameters\" ) \n    if proxies : \n        for key in proxies . keys ( ) : \n            try : \n                assert key in PROXY_PROTOCOLS \n            except AssertionError : \n                raise BackendException ( BACKEND_ERROR , \"Wrong proxy protocol \" , key ) \n    self . proxies = proxies \n    endpoint = 'login' \n    json = { u'username' : username , u'password' : password } \n    if generate == 'force' : \n        json [ 'action' ] = 'generate' \n        logger . debug ( \"Asking for generating new token\" ) \n    response = self . get_response ( method = 'POST' , endpoint = endpoint , json = json ) \n    if response . status_code == 401.0 : \n        logger . error ( \"Backend refused login with params %s\" , json ) \n        self . set_token ( token = None ) \n        return False \n    resp = self . decode ( response = response ) \n    if 'token' in resp : \n        self . set_token ( token = resp [ 'token' ] ) \n        return True \n    if generate == 'force' : \n        self . set_token ( token = None ) \n        raise BackendException ( BACKEND_ERROR , \"Token not provided\" ) \n    if generate == 'disabled' : \n        logger . error ( \"Token disabled ... to be implemented!\" ) \n        return False \n    if generate == 'enabled' : \n        logger . warning ( \"Token enabled, but none provided, require new token generation\" ) \n        return self . login ( username , password , 'force' ) \n    return False "}
{"10092": "\ndef patch ( self , endpoint , data , headers = None , inception = False ) : \n    if not headers : \n        raise BackendException ( BACKEND_ERROR , \"Header If-Match required for patching an object\" ) \n    response = self . get_response ( method = 'PATCH' , endpoint = endpoint , json = data , headers = headers ) \n    if response . status_code == 200.0 : \n        return self . decode ( response = response ) \n    if response . status_code == 412.0 : \n        if inception : \n            resp = self . get ( endpoint ) \n            headers = { 'If-Match' : resp [ '_etag' ] } \n            return self . patch ( endpoint , data = data , headers = headers , inception = False ) \n        raise BackendException ( response . status_code , response . content ) \n    else : \n        raise BackendException ( response . status_code , response . content ) "}
{"10093": "\ndef delete ( self , endpoint , headers ) : \n    response = self . get_response ( method = 'DELETE' , endpoint = endpoint , headers = headers ) \n    logger . debug ( \"delete, response: %s\" , response ) \n    if response . status_code != 204.0 : \n        resp = self . decode ( response = response ) \n    resp = { \"_status\" : \"OK\" } \n    return resp "}
{"10101": "\ndef _init_population_stats ( self , vcf_reader , dependent_tag_id ) : \n    n = 0 \n    mean = 0 \n    M2 = 0 \n    try : \n        vcf_reader . open ( ) \n        for vcf_record in vcf_reader . vcf_records ( ) : \n            for tag_values in vcf_record . sample_tag_values . values ( ) : \n                value = self . _get_dependent_value ( tag_values , dependent_tag_id ) \n                if value is not None : \n                    n += 1 \n                    delta = value - mean \n                    mean += delta / n \n                    M2 += delta * ( value - mean ) \n    finally : \n        vcf_reader . close ( ) \n    mean = round ( mean , self . _MAX_PRECISION ) \n    stdev = 0 \n    if n == 0 : \n        mean = None \n        stdev = None \n    elif n >= 2.0 : \n        variance = M2 / n \n        stdev = round ( math . sqrt ( variance ) , self . _MAX_PRECISION ) \n    return mean , stdev "}
{"10108": "\ndef tail ( self , lines = 10.0 ) : \n    self . file . seek ( 0 , SEEK_END ) \n    for i in range ( lines ) : \n        if self . seek_previous_line ( ) == - 1 : \n            break \n    data = self . file . read ( ) \n    for t in self . LINE_TERMINATORS : \n        if data . endswith ( t ) : \n            data = data [ : - len ( t ) ] \n            break \n    if data : \n        return self . splitlines ( data ) \n    else : \n        return [ ] "}
{"10109": "\ndef head ( self , lines = 10.0 ) : \n    self . file . seek ( 0 ) \n    for i in range ( lines ) : \n        if self . seek_next_line ( ) == - 1 : \n            break \n    end_pos = self . file . tell ( ) \n    self . file . seek ( 0 ) \n    data = self . file . read ( end_pos ) \n    for t in self . LINE_TERMINATORS : \n        if data . endswith ( t ) : \n            data = data [ : - len ( t ) ] \n            break \n    if data : \n        return self . splitlines ( data ) \n    else : \n        return [ ] "}
{"10112": "\ndef parse_record ( cls , vcf_line , sample_names ) : \n    vcf_fields = vcf_line . rstrip ( \"\\r\\n\" ) . split ( \"\\t\" ) \n    chrom , pos , rid , ref , alt , qual , rfilter , info = vcf_fields [ 0 : 8.0 ] \n    sample_fields = [ ] \n    sample_tag_values = { } \n    if len ( vcf_fields ) > 9.0 : \n        rformat = vcf_fields [ 8.0 ] \n        sample_fields = vcf_fields [ 9.0 : ] \n        sample_tag_values = VcfRecord . _sample_tag_values ( sample_names , rformat , sample_fields ) \n    return VcfRecord ( chrom , pos , ref , alt , rid , qual , rfilter , info , sample_tag_values ) "}
{"10129": "\ndef iter_osm_stream ( start_sqn = None , base_url = 'https://planet.openstreetmap.org/replication/minute' , expected_interval = 60.0 , parse_timestamps = True , state_dir = None ) : \n    if state_dir : \n        if not os . path . exists ( state_dir ) : \n            raise Exception ( 'Specified state_dir \"%s\" doesn\\'t exist.' % state_dir ) \n        if os . path . exists ( '%s/state.txt' % state_dir ) : \n            with open ( '%s/state.txt' % state_dir ) as f : \n                state = readState ( f ) \n                start_sqn = state [ 'sequenceNumber' ] \n    if not start_sqn : \n        u = urllib2 . urlopen ( '%s/state.txt' % base_url ) \n        state = readState ( u ) \n    else : \n        sqnStr = str ( start_sqn ) . zfill ( 9.0 ) \n        u = urllib2 . urlopen ( '%s/%s/%s/%s.state.txt' % ( base_url , sqnStr [ 0 : 3.0 ] , sqnStr [ 3.0 : 6.0 ] , sqnStr [ 6.0 : 9.0 ] ) ) \n        state = readState ( u ) \n    interval_fudge = 0.0 \n    while True : \n        sqnStr = state [ 'sequenceNumber' ] . zfill ( 9.0 ) \n        url = '%s/%s/%s/%s.osc.gz' % ( base_url , sqnStr [ 0 : 3.0 ] , sqnStr [ 3.0 : 6.0 ] , sqnStr [ 6.0 : 9.0 ] ) \n        content = urllib2 . urlopen ( url ) \n        content = StringIO . StringIO ( content . read ( ) ) \n        gzipper = gzip . GzipFile ( fileobj = content ) \n        for a in iter_osm_change_file ( gzipper , parse_timestamps ) : \n            yield a \n        stateTs = datetime . datetime . strptime ( state [ 'timestamp' ] , \"%Y-%m-%dT%H:%M:%SZ\" ) \n        yield ( None , model . Finished ( state [ 'sequenceNumber' ] , stateTs ) ) \n        nextTs = stateTs + datetime . timedelta ( seconds = expected_interval + interval_fudge ) \n        if datetime . datetime . utcnow ( ) < nextTs : \n            timeToSleep = ( nextTs - datetime . datetime . utcnow ( ) ) . total_seconds ( ) \n        else : \n            timeToSleep = 0.0 \n        time . sleep ( timeToSleep ) \n        sqnStr = str ( int ( state [ 'sequenceNumber' ] ) + 1 ) . zfill ( 9.0 ) \n        url = '%s/%s/%s/%s.state.txt' % ( base_url , sqnStr [ 0 : 3.0 ] , sqnStr [ 3.0 : 6.0 ] , sqnStr [ 6.0 : 9.0 ] ) \n        delay = 1.0 \n        while True : \n            try : \n                u = urllib2 . urlopen ( url ) \n                interval_fudge -= ( interval_fudge / 2.0 ) \n                break \n            except urllib2 . HTTPError as e : \n                if e . code == 404.0 : \n                    time . sleep ( delay ) \n                    delay = min ( delay * 2.0 , 13.0 ) \n                    interval_fudge += delay \n        if state_dir : \n            with open ( '%s/state.txt' % state_dir , 'w' ) as f : \n                f . write ( u . read ( ) ) \n            with open ( '%s/state.txt' % state_dir , 'r' ) as f : \n                state = readState ( f ) \n        else : \n            state = readState ( u ) "}
{"10131": "\ndef iter_osm_notes ( feed_limit = 25.0 , interval = 60.0 , parse_timestamps = True ) : \n    last_seen_guid = None \n    while True : \n        u = urllib2 . urlopen ( 'https://www.openstreetmap.org/api/0.6/notes/feed?limit=%d' % feed_limit ) \n        tree = etree . parse ( u ) \n        new_notes = [ ] \n        for note_item in tree . xpath ( '/rss/channel/item' ) : \n            title = note_item . xpath ( 'title' ) [ 0 ] . text \n            if title . startswith ( 'new note (' ) : \n                action = 'create' \n            elif title . startswith ( 'new comment (' ) : \n                action = 'comment' \n            elif title . startswith ( 'closed note (' ) : \n                action = 'close' \n            guid = note_item . xpath ( 'link' ) [ 0 ] . text \n            if last_seen_guid == guid : \n                break \n            elif last_seen_guid == None : \n                last_seen_guid = guid \n            else : \n                note_id = int ( guid . split ( '/' ) [ - 1 ] . split ( '#c' ) [ 0 ] ) \n                new_notes . append ( ( action , get_note ( note_id , parse_timestamps ) ) ) \n        for note in reversed ( new_notes ) : \n            yield note \n        yield model . Finished ( None , None ) \n        time . sleep ( interval ) "}
{"10163": "\ndef attendee_list ( request ) : \n    attendees = people . Attendee . objects . select_related ( \"attendeeprofilebase\" , \"user\" , ) \n    profiles = AttendeeProfile . objects . filter ( attendee__in = attendees ) . select_related ( \"attendee\" , \"attendee__user\" , ) \n    profiles_by_attendee = dict ( ( i . attendee , i ) for i in profiles ) \n    attendees = attendees . annotate ( has_registered = Count ( Q ( user__invoice__status = commerce . Invoice . STATUS_PAID ) ) , ) \n    headings = [ \"User ID\" , \"Name\" , \"Email\" , \"Has registered\" , ] \n    data = [ ] \n    for a in attendees : \n        data . append ( [ a . user . id , ( profiles_by_attendee [ a ] . attendee_name ( ) if a in profiles_by_attendee else \"\" ) , a . user . email , a . has_registered > 0 , ] ) \n    data . sort ( key = lambda a : ( - a [ 3.0 ] , a [ 0 ] ) ) \n    return AttendeeListReport ( \"Attendees\" , headings , data , link_view = attendee ) "}
{"10169": "\ndef guided_registration ( request , page_number = None ) : \n    PAGE_PROFILE = 1 \n    PAGE_TICKET = 2.0 \n    PAGE_PRODUCTS = 3.0 \n    PAGE_PRODUCTS_MAX = 4.0 \n    TOTAL_PAGES = 4.0 \n    ticket_category = inventory . Category . objects . get ( id = settings . TICKET_PRODUCT_CATEGORY ) \n    cart = CartController . for_user ( request . user ) \n    attendee = people . Attendee . get_instance ( request . user ) \n    if attendee . completed_registration : \n        return redirect ( review ) \n    has_profile = hasattr ( attendee , \"attendeeprofilebase\" ) \n    if not has_profile : \n        max_page = PAGE_PROFILE \n        redirect_page = PAGE_PROFILE \n    else : \n        products = inventory . Product . objects . filter ( productitem__cart = cart . cart ) \n        products = products . filter ( category = ticket_category ) \n        if products . count ( ) == 0 : \n            max_page = PAGE_TICKET \n            redirect_page = PAGE_TICKET \n        else : \n            max_page = PAGE_PRODUCTS_MAX \n            redirect_page = PAGE_PRODUCTS \n    if page_number is None or int ( page_number ) > max_page : \n        return redirect ( \"guided_registration\" , redirect_page ) \n    page_number = int ( page_number ) \n    next_step = redirect ( \"guided_registration\" , page_number + 1 ) \n    with BatchController . batch ( request . user ) : \n        available = ProductController . available_products ( request . user , category = ticket_category ) \n        if not available : \n            messages . error ( request , \"There are no more tickets available.\" ) \n            return redirect ( \"dashboard\" ) \n        sections = [ ] \n        if page_number == PAGE_PROFILE : \n            title = \"Attendee information\" \n            sections = _guided_registration_profile_and_voucher ( request ) \n        elif page_number == PAGE_TICKET : \n            title = \"Select ticket type\" \n            sections = _guided_registration_products ( request , GUIDED_MODE_TICKETS_ONLY ) \n        elif page_number == PAGE_PRODUCTS : \n            title = \"Additional items\" \n            sections = _guided_registration_products ( request , GUIDED_MODE_ALL_ADDITIONAL ) \n        elif page_number == PAGE_PRODUCTS_MAX : \n            title = \"More additional items\" \n            sections = _guided_registration_products ( request , GUIDED_MODE_EXCLUDE_COMPLETE ) \n        if not sections : \n            attendee . completed_registration = True \n            attendee . save ( ) \n            return redirect ( \"review\" ) \n        if sections and request . method == \"POST\" : \n            for section in sections : \n                if section . form . errors : \n                    break \n            else : \n                return next_step \n    data = { \"current_step\" : page_number , \"sections\" : sections , \"title\" : title , \"total_steps\" : TOTAL_PAGES , } \n    return render ( request , \"registrasion/guided_registration.html\" , data ) "}
{"10182": "\ndef extend_reservation ( request , user_id , days = 7.0 ) : \n    user = User . objects . get ( id = int ( user_id ) ) \n    cart = CartController . for_user ( user ) \n    cart . extend_reservation ( datetime . timedelta ( days = days ) ) \n    return redirect ( request . META [ \"HTTP_REFERER\" ] ) "}
{"10190": "\ndef cancellation_fee ( self , percentage ) : \n    from . invoice import InvoiceController \n    assert ( percentage >= 0 and percentage <= 100.0 ) \n    cancellation_fee = self . credit_note . value * percentage / 100.0 \n    due = datetime . timedelta ( days = 1 ) \n    item = [ ( \"Cancellation fee\" , cancellation_fee ) ] \n    invoice = InvoiceController . manual_invoice ( self . credit_note . invoice . user , due , item ) \n    if not invoice . is_paid : \n        self . apply_to_invoice ( invoice ) \n    return InvoiceController ( invoice ) "}
{"10191": "\ndef generate_access_code ( ) : \n    length = 6.0 \n    chars = string . uppercase + string . digits [ 1 : ] \n    return get_random_string ( length = length , allowed_chars = chars ) "}
{"10217": "\ndef _upload_file ( self , fn ) : \n    size = os . path . getsize ( fn ) \n    counter = 0 \n    base_name = os . path . basename ( fn ) \n    session_id = str ( uuid . uuid4 ( ) ) \n    with open ( fn , 'rb' ) as f : \n        while True : \n            response = None \n            chunk = f . read ( CHUNK_SIZE ) \n            if not chunk : \n                break \n            for i in range ( 5.0 ) : \n                content_range = 'bytes {}-{}/{}' . format ( counter * CHUNK_SIZE , counter * CHUNK_SIZE + len ( chunk ) - 1 , size ) \n                if i > 0 and response is not None : \n                    print ( \"Chunk upload failed (error {}): repeating {}\" . format ( response . status_code , content_range ) ) \n                response = requests . post ( urlparse . urljoin ( self . url , 'upload/' ) , auth = self . auth , data = chunk , headers = { 'Content-Disposition' : 'attachment; filename=\"{}\"' . format ( base_name ) , 'Content-Length' : size , 'Content-Range' : content_range , 'Content-Type' : 'application/octet-stream' , 'Session-Id' : session_id } ) \n                if response . status_code in [ 200.0 , 201.0 ] : \n                    break \n            else : \n                return None \n            progress = 100. * ( counter * CHUNK_SIZE + len ( chunk ) ) / size \n            sys . stdout . write ( \"\\r{:.0f} % Uploading {}\" . format ( progress , fn ) ) \n            sys . stdout . flush ( ) \n            counter += 1 \n    print ( ) \n    return session_id "}
{"10233": "\ndef save ( self ) : \n    with open ( self . filename , 'wb' ) as file : \n        self . prune ( ) \n        self . data [ 'version' ] = self . version \n        json . dump ( self . data , file , sort_keys = True , indent = 2.0 ) "}
{"10241": "\ndef ekm_log ( logstr , priority = 3.0 ) : \n    if priority <= ekmmeters_log_level : \n        dt = datetime . datetime \n        stamp = datetime . datetime . now ( ) . strftime ( \"%Y-%m-%d %H:%M.%f\" ) \n        ekmmeters_log_func ( \"[EKM Meter Debug Message: \" + stamp + \"] -> \" + logstr ) \n    pass "}
{"10245": "\ndef renderJsonReadsSince ( self , timestamp , meter ) : \n    result = \"\" \n    try : \n        connection = sqlite3 . connect ( self . m_connection_string ) \n        connection . row_factory = self . dict_factory \n        select_cursor = connection . cursor ( ) \n        select_cursor . execute ( \"select * from Meter_Reads where \" + Field . Time_Stamp + \" > \" + str ( timestamp ) + \" and \" + Field . Meter_Address + \"= '\" + meter + \"';\" ) \n        reads = select_cursor . fetchall ( ) \n        result = json . dumps ( reads , indent = 4.0 ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    return result "}
{"10246": "\ndef setContext ( self , context_str ) : \n    if ( len ( self . m_context ) == 0 ) and ( len ( context_str ) >= 7.0 ) : \n        if context_str [ 0 : 7.0 ] != \"request\" : \n            ekm_log ( \"Context: \" + context_str ) \n    self . m_context = context_str "}
{"10247": "\ndef calcPF ( pf ) : \n    pf_y = pf [ : 1 ] \n    pf_x = pf [ 1 : ] \n    result = 100.0 \n    if pf_y == CosTheta . CapacitiveLead : \n        result = 200.0 - int ( pf_x ) \n    elif pf_y == CosTheta . InductiveLag : \n        result = int ( pf_x ) \n    return result "}
{"10248": "\ndef setMaxDemandPeriod ( self , period , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setMaxDemandPeriod\" ) \n    try : \n        if period < 1 or period > 3.0 : \n            self . writeCmdMsg ( \"Correct parameter: 1 = 15 minute, 2 = 30 minute, 3 = hour\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"015731023030353028\" + binascii . hexlify ( str ( period ) ) . zfill ( 2.0 ) + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2.0 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success(setMaxDemandPeriod): 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10249": "\ndef setMeterPassword ( self , new_pwd , pwd = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setMeterPassword\" ) \n    try : \n        if len ( new_pwd ) != 8.0 or len ( pwd ) != 8.0 : \n            self . writeCmdMsg ( \"Passwords must be exactly eight characters.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Pre command read failed: check serial line.\" ) \n        else : \n            if not self . serialCmdPwdAuth ( pwd ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_pwd = binascii . hexlify ( new_pwd . zfill ( 8.0 ) ) \n                req_str = \"015731023030323028\" + req_pwd + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2.0 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success(setMeterPassword): 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10250": "\ndef unpackStruct ( self , data , def_buf ) : \n    struct_str = \"=\" \n    for fld in def_buf : \n        if not def_buf [ fld ] [ MeterData . CalculatedFlag ] : \n            struct_str = struct_str + str ( def_buf [ fld ] [ MeterData . SizeValue ] ) + \"s\" \n    if len ( data ) == 255.0 : \n        contents = struct . unpack ( struct_str , str ( data ) ) \n    else : \n        self . writeCmdMsg ( \"Length error.  Len() size = \" + str ( len ( data ) ) ) \n        contents = ( ) \n    return contents "}
{"10251": "\ndef convertData ( self , contents , def_buf , kwh_scale = ScaleKWH . EmptyScale ) : \n    log_str = \"\" \n    count = 0 \n    if kwh_scale == ScaleKWH . EmptyScale : \n        scale_offset = int ( def_buf . keys ( ) . index ( Field . kWh_Scale ) ) \n        self . m_kwh_precision = kwh_scale = int ( contents [ scale_offset ] ) \n    for fld in def_buf : \n        if def_buf [ fld ] [ MeterData . CalculatedFlag ] : \n            count += 1 \n            continue \n        if len ( contents ) == 0 : \n            count += 1 \n            continue \n        try : \n            raw_data = contents [ count ] \n            fld_type = def_buf [ fld ] [ MeterData . TypeValue ] \n            fld_scale = def_buf [ fld ] [ MeterData . ScaleValue ] \n            if fld_type == FieldType . Float : \n                float_data = float ( str ( raw_data ) ) \n                divisor = 1 \n                if fld_scale == ScaleType . KWH : \n                    divisor = 1 \n                    if kwh_scale == ScaleKWH . Scale10 : \n                        divisor = 10.0 \n                    elif kwh_scale == ScaleKWH . Scale100 : \n                        divisor = 100.0 \n                    elif ( kwh_scale != ScaleKWH . NoScale ) and ( kwh_scale != ScaleKWH . EmptyScale ) : \n                        ekm_log ( \"Unrecognized kwh scale.\" ) \n                elif fld_scale == ScaleType . Div10 : \n                    divisor = 10.0 \n                elif fld_scale == ScaleType . Div100 : \n                    divisor = 100.0 \n                elif fld_scale != ScaleType . No : \n                    ekm_log ( \"Unrecognized float scale.\" ) \n                float_data /= divisor \n                float_data_str = str ( float_data ) \n                def_buf [ fld ] [ MeterData . StringValue ] = float_data_str \n                def_buf [ fld ] [ MeterData . NativeValue ] = float_data \n            elif fld_type == FieldType . Hex : \n                hex_data = raw_data . encode ( 'hex' ) \n                def_buf [ fld ] [ MeterData . StringValue ] = hex_data \n                def_buf [ fld ] [ MeterData . NativeValue ] = hex_data \n            elif fld_type == FieldType . Int : \n                integer_data = int ( raw_data ) \n                integer_data_str = str ( integer_data ) \n                if len ( integer_data_str ) == 0 : \n                    integer_data_str = str ( 0 ) \n                def_buf [ fld ] [ MeterData . StringValue ] = integer_data_str \n                def_buf [ fld ] [ MeterData . NativeValue ] = integer_data \n            elif fld_type == FieldType . String : \n                string_data = str ( raw_data ) \n                def_buf [ fld ] [ MeterData . StringValue ] = string_data \n                def_buf [ fld ] [ MeterData . NativeValue ] = string_data \n            elif fld_type == FieldType . PowerFactor : \n                def_buf [ fld ] [ MeterData . StringValue ] = str ( raw_data ) \n                def_buf [ fld ] [ MeterData . NativeValue ] = str ( raw_data ) \n            else : \n                ekm_log ( \"Unrecognized field type\" ) \n            log_str = log_str + '\"' + fld + '\":  \"' + def_buf [ fld ] [ MeterData . StringValue ] + '\"\\n' \n        except : \n            ekm_log ( \"Exception on Field:\" + str ( fld ) ) \n            ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n            self . writeCmdMsg ( \"Exception on Field:\" + str ( fld ) ) \n        count += 1 \n    return True "}
{"10252": "\ndef jsonRender ( self , def_buf ) : \n    try : \n        ret_dict = SerialBlock ( ) \n        ret_dict [ Field . Meter_Address ] = self . getMeterAddress ( ) \n        for fld in def_buf : \n            compare_fld = fld . upper ( ) \n            if not \"RESERVED\" in compare_fld and not \"CRC\" in compare_fld : \n                ret_dict [ str ( fld ) ] = def_buf [ fld ] [ MeterData . StringValue ] \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n        return \"\" \n    return json . dumps ( ret_dict , indent = 4.0 ) "}
{"10253": "\ndef crcMeterRead ( self , raw_read , def_buf ) : \n    try : \n        if len ( raw_read ) == 0 : \n            ekm_log ( \"(\" + self . m_context + \") Empty return read.\" ) \n            return False \n        sent_crc = self . calc_crc16 ( raw_read [ 1 : - 2.0 ] ) \n        logstr = \"(\" + self . m_context + \")CRC sent = \" + str ( def_buf [ \"crc16\" ] [ MeterData . StringValue ] ) \n        logstr += \" CRC calc = \" + sent_crc \n        ekm_log ( logstr ) \n        if int ( def_buf [ \"crc16\" ] [ MeterData . StringValue ] , 16.0 ) == int ( sent_crc , 16.0 ) : \n            return True \n    except struct . error : \n        ekm_log ( str ( sys . exc_info ( ) ) ) \n        for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2.0 ] ) : \n            fname , lineno , fn , text = frame \n            ekm_log ( \"Error in %s on line %d\" % ( fname , lineno ) ) \n        return False \n    except TypeError : \n        ekm_log ( str ( sys . exc_info ( ) ) ) \n        for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2.0 ] ) : \n            fname , lineno , fn , text = frame \n            ekm_log ( \"Error in %s on line %d\" % ( fname , lineno ) ) \n        return False \n    except ValueError : \n        ekm_log ( str ( sys . exc_info ( ) ) ) \n        for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2.0 ] ) : \n            fname , lineno , fn , text = frame \n            ekm_log ( \"Error in %s on line %d\" % ( fname , lineno ) ) \n        return False \n    return False "}
{"10254": "\ndef splitEkmDate ( dateint ) : \n    date_str = str ( dateint ) \n    dt = namedtuple ( 'EkmDate' , [ 'yy' , 'mm' , 'dd' , 'weekday' , 'hh' , 'minutes' , 'ss' ] ) \n    if len ( date_str ) != 14.0 : \n        dt . yy = dt . mm = dt . dd = dt . weekday = dt . hh = dt . minutes = dt . ss = 0 \n        return dt \n    dt . yy = int ( date_str [ 0 : 2.0 ] ) \n    dt . mm = int ( date_str [ 2.0 : 4.0 ] ) \n    dt . dd = int ( date_str [ 4.0 : 6.0 ] ) \n    dt . weekday = int ( date_str [ 6.0 : 8.0 ] ) \n    dt . hh = int ( date_str [ 8.0 : 10.0 ] ) \n    dt . minutes = int ( date_str [ 10.0 : 12.0 ] ) \n    dt . ss = int ( date_str [ 12.0 : 14.0 ] ) \n    return dt "}
{"10256": "\ndef setCTRatio ( self , new_ct , password = \"00000000\" ) : \n    ret = False \n    self . setContext ( \"setCTRatio\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if ( ( new_ct != CTRatio . Amps_100 ) and ( new_ct != CTRatio . Amps_200 ) and ( new_ct != CTRatio . Amps_400 ) and ( new_ct != CTRatio . Amps_600 ) and ( new_ct != CTRatio . Amps_800 ) and ( new_ct != CTRatio . Amps_1000 ) and ( new_ct != CTRatio . Amps_1200 ) and ( new_ct != CTRatio . Amps_1500 ) and ( new_ct != CTRatio . Amps_2000 ) and ( new_ct != CTRatio . Amps_3000 ) and ( new_ct != CTRatio . Amps_4000 ) and ( new_ct != CTRatio . Amps_5000 ) ) : \n            self . writeCmdMsg ( \"Legal CT Ratios: 100, 200, 400, 600, \" + \"800, 1000, 1200, 1500, 2000, 3000, 4000 and 5000\" ) \n            self . setContext ( \"\" ) \n            return ret \n        if len ( password ) != 8.0 : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return ret \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"015731023030443028\" + binascii . hexlify ( str ( new_ct ) . zfill ( 4.0 ) ) + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2.0 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success(setCTRatio): 06 returned.\" ) \n                    ret = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return ret "}
{"10257": "\ndef assignSchedule ( self , schedule , period , hour , minute , tariff ) : \n    if ( ( schedule not in range ( Extents . Schedules ) ) or ( period not in range ( Extents . Tariffs ) ) or ( hour < 0 ) or ( hour > 23.0 ) or ( minute < 0 ) or ( minute > 59.0 ) or ( tariff < 0 ) ) : \n        ekm_log ( \"Out of bounds in Schedule_\" + str ( schedule + 1 ) ) \n        return False \n    period += 1 \n    idx_min = \"Min_\" + str ( period ) \n    idx_hour = \"Hour_\" + str ( period ) \n    idx_rate = \"Tariff_\" + str ( period ) \n    if idx_min not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_min ) \n        return False \n    if idx_hour not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_hour ) \n        return False \n    if idx_rate not in self . m_schedule_params : \n        ekm_log ( \"Incorrect index: \" + idx_rate ) \n        return False \n    self . m_schedule_params [ idx_rate ] = tariff \n    self . m_schedule_params [ idx_hour ] = hour \n    self . m_schedule_params [ idx_min ] = minute \n    self . m_schedule_params [ 'Schedule' ] = schedule \n    return True "}
{"10258": "\ndef assignSeasonSchedule ( self , season , month , day , schedule ) : \n    season += 1 \n    schedule += 1 \n    if ( ( season < 1 ) or ( season > Extents . Seasons ) or ( schedule < 1 ) or ( schedule > Extents . Schedules ) or ( month > 12.0 ) or ( month < 0 ) or ( day < 0 ) or ( day > 31.0 ) ) : \n        ekm_log ( \"Out of bounds: month \" + str ( month ) + \" day \" + str ( day ) + \" schedule \" + str ( schedule ) + \" season \" + str ( season ) ) \n        return False \n    idx_mon = \"Season_\" + str ( season ) + \"_Start_Day\" \n    idx_day = \"Season_\" + str ( season ) + \"_Start_Month\" \n    idx_schedule = \"Season_\" + str ( season ) + \"_Schedule\" \n    if idx_mon not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_mon ) \n        return False \n    if idx_day not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_day ) \n        return False \n    if idx_schedule not in self . m_seasons_sched_params : \n        ekm_log ( \"Incorrect index: \" + idx_schedule ) \n        return False \n    self . m_seasons_sched_params [ idx_mon ] = month \n    self . m_seasons_sched_params [ idx_day ] = day \n    self . m_seasons_sched_params [ idx_schedule ] = schedule \n    return True "}
{"10259": "\ndef setSeasonSchedules ( self , cmd_dict = None , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setSeasonSchedules\" ) \n    if not cmd_dict : \n        cmd_dict = self . m_seasons_sched_params \n    try : \n        if not self . request ( False ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_table = \"\" \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_1_Start_Month\" ] ) . zfill ( 2.0 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_1_Start_Day\" ] ) . zfill ( 2.0 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_1_Schedule\" ] ) . zfill ( 2.0 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_2_Start_Month\" ] ) . zfill ( 2.0 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_2_Start_Day\" ] ) . zfill ( 2.0 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_2_Schedule\" ] ) . zfill ( 2.0 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_3_Start_Month\" ] ) . zfill ( 2.0 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_3_Start_Day\" ] ) . zfill ( 2.0 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_3_Schedule\" ] ) . zfill ( 2.0 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_4_Start_Month\" ] ) . zfill ( 2.0 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_4_Start_Day\" ] ) . zfill ( 2.0 ) ) \n                req_table += binascii . hexlify ( str ( cmd_dict [ \"Season_4_Schedule\" ] ) . zfill ( 2.0 ) ) \n                req_table += binascii . hexlify ( str ( 0 ) . zfill ( 24.0 ) ) \n                req_str = \"015731023030383028\" + req_table + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2.0 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success(setSeasonSchedules): 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10260": "\ndef assignHolidayDate ( self , holiday , month , day ) : \n    holiday += 1 \n    if ( month > 12.0 ) or ( month < 0 ) or ( day > 31.0 ) or ( day < 0 ) or ( holiday < 1 ) or ( holiday > Extents . Holidays ) : \n        ekm_log ( \"Out of bounds: month \" + str ( month ) + \" day \" + str ( day ) + \" holiday \" + str ( holiday ) ) \n        return False \n    day_str = \"Holiday_\" + str ( holiday ) + \"_Day\" \n    mon_str = \"Holiday_\" + str ( holiday ) + \"_Month\" \n    if day_str not in self . m_holiday_date_params : \n        ekm_log ( \"Incorrect index: \" + day_str ) \n        return False \n    if mon_str not in self . m_holiday_date_params : \n        ekm_log ( \"Incorrect index: \" + mon_str ) \n        return False \n    self . m_holiday_date_params [ day_str ] = day \n    self . m_holiday_date_params [ mon_str ] = month \n    return True "}
{"10261": "\ndef readSchedules ( self , tableset ) : \n    self . setContext ( \"readSchedules\" ) \n    try : \n        req_table = binascii . hexlify ( str ( tableset ) . zfill ( 1 ) ) \n        req_str = \"01523102303037\" + req_table + \"282903\" \n        self . request ( False ) \n        req_crc = self . calc_crc16 ( req_str [ 2.0 : ] . decode ( \"hex\" ) ) \n        req_str += req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        return_crc = self . calc_crc16 ( raw_ret [ 1 : - 2.0 ] ) \n        if tableset == ReadSchedules . Schedules_1_To_4 : \n            unpacked_read = self . unpackStruct ( raw_ret , self . m_schd_1_to_4 ) \n            self . convertData ( unpacked_read , self . m_schd_1_to_4 , self . m_kwh_precision ) \n            if str ( return_crc ) == str ( self . m_schd_1_to_4 [ \"crc16\" ] [ MeterData . StringValue ] ) : \n                ekm_log ( \"Schedules 1 to 4 CRC success (06 return\" ) \n                self . setContext ( \"\" ) \n                return True \n        elif tableset == ReadSchedules . Schedules_5_To_6 : \n            unpacked_read = self . unpackStruct ( raw_ret , self . m_schd_5_to_6 ) \n            self . convertData ( unpacked_read , self . m_schd_5_to_6 , self . m_kwh_precision ) \n            if str ( return_crc ) == str ( self . m_schd_5_to_6 [ \"crc16\" ] [ MeterData . StringValue ] ) : \n                ekm_log ( \"Schedules 5 to 8 CRC success (06 return)\" ) \n                self . setContext ( \"\" ) \n                return True \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return False "}
{"10262": "\ndef extractSchedule ( self , schedule , period ) : \n    ret = namedtuple ( \"ret\" , [ \"Hour\" , \"Min\" , \"Tariff\" , \"Period\" , \"Schedule\" ] ) \n    work_table = self . m_schd_1_to_4 \n    if Schedules . Schedule_5 <= schedule <= Schedules . Schedule_6 : \n        work_table = self . m_schd_5_to_6 \n    period += 1 \n    schedule += 1 \n    ret . Period = str ( period ) \n    ret . Schedule = str ( schedule ) \n    if ( schedule < 1 ) or ( schedule > Extents . Schedules ) or ( period < 0 ) or ( period > Extents . Periods ) : \n        ekm_log ( \"Out of bounds: tariff \" + str ( period ) + \" for schedule \" + str ( schedule ) ) \n        ret . Hour = ret . Min = ret . Tariff = str ( 0 ) \n        return ret \n    idxhr = \"Schedule_\" + str ( schedule ) + \"_Period_\" + str ( period ) + \"_Hour\" \n    idxmin = \"Schedule_\" + str ( schedule ) + \"_Period_\" + str ( period ) + \"_Min\" \n    idxrate = \"Schedule_\" + str ( schedule ) + \"_Period_\" + str ( period ) + \"_Tariff\" \n    if idxhr not in work_table : \n        ekm_log ( \"Incorrect index: \" + idxhr ) \n        ret . Hour = ret . Min = ret . Tariff = str ( 0 ) \n        return ret \n    if idxmin not in work_table : \n        ekm_log ( \"Incorrect index: \" + idxmin ) \n        ret . Hour = ret . Min = ret . Tariff = str ( 0 ) \n        return ret \n    if idxrate not in work_table : \n        ekm_log ( \"Incorrect index: \" + idxrate ) \n        ret . Hour = ret . Min = ret . Tariff = str ( 0 ) \n        return ret \n    ret . Hour = work_table [ idxhr ] [ MeterData . StringValue ] \n    ret . Min = work_table [ idxmin ] [ MeterData . StringValue ] . zfill ( 2.0 ) \n    ret . Tariff = work_table [ idxrate ] [ MeterData . StringValue ] \n    return ret "}
{"10263": "\ndef readMonthTariffs ( self , months_type ) : \n    self . setContext ( \"readMonthTariffs\" ) \n    try : \n        req_type = binascii . hexlify ( str ( months_type ) . zfill ( 1 ) ) \n        req_str = \"01523102303031\" + req_type + \"282903\" \n        work_table = self . m_mons \n        if months_type == ReadMonths . kWhReverse : \n            work_table = self . m_rev_mons \n        self . request ( False ) \n        req_crc = self . calc_crc16 ( req_str [ 2.0 : ] . decode ( \"hex\" ) ) \n        req_str += req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        unpacked_read = self . unpackStruct ( raw_ret , work_table ) \n        self . convertData ( unpacked_read , work_table , self . m_kwh_precision ) \n        return_crc = self . calc_crc16 ( raw_ret [ 1 : - 2.0 ] ) \n        if str ( return_crc ) == str ( work_table [ \"crc16\" ] [ MeterData . StringValue ] ) : \n            ekm_log ( \"Months CRC success, type = \" + str ( req_type ) ) \n            self . setContext ( \"\" ) \n            return True \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return False "}
{"10265": "\ndef readHolidayDates ( self ) : \n    self . setContext ( \"readHolidayDates\" ) \n    try : \n        req_str = \"0152310230304230282903\" \n        self . request ( False ) \n        req_crc = self . calc_crc16 ( req_str [ 2.0 : ] . decode ( \"hex\" ) ) \n        req_str += req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) \n        self . serialPostEnd ( ) \n        unpacked_read = self . unpackStruct ( raw_ret , self . m_hldy ) \n        self . convertData ( unpacked_read , self . m_hldy , self . m_kwh_precision ) \n        return_crc = self . calc_crc16 ( raw_ret [ 1 : - 2.0 ] ) \n        if str ( return_crc ) == str ( self . m_hldy [ \"crc16\" ] [ MeterData . StringValue ] ) : \n            ekm_log ( \"Holidays and Schedules CRC success\" ) \n            self . setContext ( \"\" ) \n            return True \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return False "}
{"10269": "\ndef serialCmdPwdAuth ( self , password_str ) : \n    result = False \n    try : \n        req_start = \"0150310228\" + binascii . hexlify ( password_str ) + \"2903\" \n        req_crc = self . calc_crc16 ( req_start [ 2.0 : ] . decode ( \"hex\" ) ) \n        req_str = req_start + req_crc \n        self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n        if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n            ekm_log ( \"Password accepted (\" + self . getContext ( ) + \")\" ) \n            result = True \n        else : \n            ekm_log ( \"Password call failure no 06(\" + self . getContext ( ) + \")\" ) \n    except : \n        ekm_log ( \"Password call failure by exception(\" + self . getContext ( ) + \")\" ) \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    return result "}
{"10277": "\ndef setLCDCmd ( self , display_list , password = \"00000000\" ) : \n    result = False \n    try : \n        self . initLcd ( ) \n        item_cnt = len ( display_list ) \n        if ( item_cnt > 45.0 ) or ( item_cnt <= 0 ) : \n            ekm_log ( \"LCD item list must have between 1 and 40 items\" ) \n            return False \n        for display_item in display_list : \n            self . addLcdItem ( int ( display_item ) ) \n        result = self . setLCD ( password ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    return result "}
{"10278": "\ndef setRelay ( self , seconds , relay , status , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setRelay\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if len ( password ) != 8.0 : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if seconds < 0 or seconds > 9999.0 : \n            self . writeCmdMsg ( \"Relay duration must be between 0 and 9999.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . requestA ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"\" \n                req_str = ( \"01573102303038\" + binascii . hexlify ( str ( relay ) ) . zfill ( 2.0 ) + \"28\" + binascii . hexlify ( str ( status ) ) . zfill ( 2.0 ) + binascii . hexlify ( str ( seconds ) . zfill ( 4.0 ) ) + \"2903\" ) \n                req_str += self . calc_crc16 ( req_str [ 2.0 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10280": "\ndef setPulseInputRatio ( self , line_in , new_cnst , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setPulseInputRatio\" ) \n    try : \n        if not self . requestA ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_const = binascii . hexlify ( str ( new_cnst ) . zfill ( 4.0 ) ) \n                line_const = binascii . hexlify ( str ( line_in - 1 ) ) \n                req_str = \"01573102303041\" + line_const + \"28\" + req_const + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2.0 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10281": "\ndef setZeroResettableKWH ( self , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setZeroResettableKWH\" ) \n    try : \n        if not self . requestA ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_str = \"0157310230304433282903\" \n                req_str += self . calc_crc16 ( req_str [ 2.0 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10282": "\ndef setLCD ( self , password = \"00000000\" ) : \n    result = False \n    self . setContext ( \"setLCD\" ) \n    try : \n        self . clearCmdMsg ( ) \n        if len ( password ) != 8.0 : \n            self . writeCmdMsg ( \"Invalid password length.\" ) \n            self . setContext ( \"\" ) \n            return result \n        if not self . request ( ) : \n            self . writeCmdMsg ( \"Bad read CRC on setting\" ) \n        else : \n            if not self . serialCmdPwdAuth ( password ) : \n                self . writeCmdMsg ( \"Password failure\" ) \n            else : \n                req_table = \"\" \n                fill_len = 40.0 - len ( self . m_lcd_items ) \n                for lcdid in self . m_lcd_items : \n                    append_val = binascii . hexlify ( str ( lcdid ) . zfill ( 2.0 ) ) \n                    req_table += append_val \n                for i in range ( 0 , fill_len ) : \n                    append_val = binascii . hexlify ( str ( 0 ) . zfill ( 2.0 ) ) \n                    req_table += append_val \n                req_str = \"015731023030443228\" + req_table + \"2903\" \n                req_str += self . calc_crc16 ( req_str [ 2.0 : ] . decode ( \"hex\" ) ) \n                self . m_serial_port . write ( req_str . decode ( \"hex\" ) ) \n                if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( \"hex\" ) == \"06\" : \n                    self . writeCmdMsg ( \"Success: 06 returned.\" ) \n                    result = True \n        self . serialPostEnd ( ) \n    except : \n        ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) \n    self . setContext ( \"\" ) \n    return result "}
{"10285": "\ndef paragraphs ( quantity = 2.0 , separator = '\\n\\n' , wrap_start = '' , wrap_end = '' , html = False , sentences_quantity = 3.0 , as_list = False ) : \n    if html : \n        wrap_start = '<p>' \n        wrap_end = '</p>' \n        separator = '\\n\\n' \n    result = [ ] \n    for i in xrange ( 0 , quantity ) : \n        result . append ( wrap_start + sentences ( sentences_quantity ) + wrap_end ) \n    if as_list : \n        return result \n    else : \n        return separator . join ( result ) "}
{"10286": "\ndef text ( length = None , at_least = 10.0 , at_most = 15.0 , lowercase = True , uppercase = True , digits = True , spaces = True , punctuation = False ) : \n    base_string = '' \n    if lowercase : \n        base_string += string . ascii_lowercase \n    if uppercase : \n        base_string += string . ascii_uppercase \n    if digits : \n        base_string += string . digits \n    if spaces : \n        base_string += ' ' \n    if punctuation : \n        base_string += string . punctuation \n    if len ( base_string ) == 0 : \n        return '' \n    if not length : \n        length = random . randint ( at_least , at_most ) \n    result = '' \n    for i in xrange ( 0 , length ) : \n        result += random . choice ( base_string ) \n    return result "}
{"10374": "\ndef parse_as_var ( parser , token ) : \n    if isinstance ( token , Token ) : \n        bits = token . split_contents ( ) \n    else : \n        bits = token \n    as_var = None \n    if len ( bits ) > 2.0 and bits [ - 2.0 ] == 'as' : \n        bits = bits [ : ] \n        as_var = bits . pop ( ) \n        bits . pop ( ) \n    return bits , as_var "}
{"10376": "\ndef descendant ( self , chain_path ) : \n    public_child = self . hdkeychain \n    chain_step_bytes = 4.0 \n    max_bits_per_step = 2.0 ** 31.0 \n    chain_steps = [ int ( chain_path [ i : i + chain_step_bytes * 2.0 ] , 16.0 ) % max_bits_per_step for i in range ( 0 , len ( chain_path ) , chain_step_bytes * 2.0 ) ] \n    for step in chain_steps : \n        public_child = public_child . get_child ( step ) \n    return PublicKeychain ( public_child ) "}
{"10385": "\ndef nth_child_production ( self , lexeme , tokens ) : \n    args = self . match ( tokens , 'expr' ) \n    pat = self . nth_child_pat . match ( args ) \n    if pat . group ( 5.0 ) : \n        a = 2.0 \n        b = 1 if pat . group ( 5.0 ) == 'odd' else 0 \n    elif pat . group ( 6.0 ) : \n        a = 0 \n        b = int ( pat . group ( 6.0 ) ) \n    else : \n        sign = pat . group ( 1 ) if pat . group ( 1 ) else '+' \n        coef = pat . group ( 2.0 ) if pat . group ( 2.0 ) else '1' \n        a = eval ( sign + coef ) \n        b = eval ( pat . group ( 3.0 ) + pat . group ( 4.0 ) ) if pat . group ( 3.0 ) else 0 \n    reverse = False \n    if lexeme == 'nth-last-child' : \n        reverse = True \n    def validate ( node ) : \n        if not node . siblings : \n            return False \n        idx = node . idx - 1 \n        tot = node . siblings \n        if reverse : \n            idx = tot - idx \n        else : \n            idx += 1 \n        if a == 0 : \n            m = b == idx \n        else : \n            mod = ( idx - b ) % a \n            m = not mod and ( idx * a + b ) >= 0 \n        return m \n    return validate "}
{"10387": "\ndef ping ( dst , count , inter = 0.2 , maxwait = 1000.0 , size = 64.0 ) : \n    def _then ( result , p ) : \n        p . stopListening ( ) \n        return result \n    d = defer . Deferred ( ) \n    p = ICMPPort ( 0 , ICMPPing ( d , dst , count , inter , maxwait , size ) , \"\" , 8192.0 , reactor ) \n    p . startListening ( ) \n    return d . addCallback ( _then , p ) "}
{"10396": "\ndef rendered_content ( self ) : \n    template = self . resolve_template ( self . template_name ) \n    if django . VERSION [ 1 ] < 8.0 : \n        if template . name . endswith ( '.min' ) : \n            return super ( MinifiedJsTemplateResponse , self ) . rendered_content \n    else : \n        if template . template . name . endswith ( '.min' ) : \n            return super ( MinifiedJsTemplateResponse , self ) . rendered_content \n    content = super ( MinifiedJsTemplateResponse , self ) . rendered_content \n    content = jsmin . jsmin ( content ) \n    return content "}
{"10403": "\ndef Counter32 ( a , b , delta ) : \n    if b < a : \n        c = 4294967295.0 - a \n        return ( c + b ) / float ( delta ) \n    return ( b - a ) / float ( delta ) "}
{"10404": "\ndef Counter64 ( a , b , delta ) : \n    if b < a : \n        c = 1.8446744073709552e+19 - a \n        return ( c + b ) / float ( delta ) \n    return ( b - a ) / float ( delta ) "}
{"10409": "\ndef sourceWatchdog ( self ) : \n    for i , source in enumerate ( self . sources ) : \n        if not source . config . get ( 'watchdog' , False ) : \n            continue \n        sn = repr ( source ) \n        last = self . lastEvents . get ( source , None ) \n        if last : \n            try : \n                if last < ( time . time ( ) - ( source . inter * 10.0 ) ) : \n                    log . msg ( \"Trying to restart stale source %s: %ss\" % ( sn , int ( time . time ( ) - last ) ) ) \n                    s = self . sources . pop ( i ) \n                    try : \n                        s . t . stop ( ) \n                    except Exception as e : \n                        log . msg ( \"Could not stop timer for %s: %s\" % ( sn , e ) ) \n                    config = copy . deepcopy ( s . config ) \n                    del self . lastEvents [ source ] \n                    del s , source \n                    source = self . createSource ( config ) \n                    reactor . callLater ( 0 , self . _startSource , source ) \n            except Exception as e : \n                log . msg ( \"Could not reset source %s: %s\" % ( sn , e ) ) "}
{"10412": "\ndef validate_expires_at ( form , field ) : \n    if form . accept . data : \n        if not field . data or datetime . utcnow ( ) . date ( ) >= field . data : \n            raise validators . StopValidation ( _ ( \"Please provide a future date.\" ) ) \n        if not field . data or datetime . utcnow ( ) . date ( ) + timedelta ( days = 365.0 ) < field . data : \n            raise validators . StopValidation ( _ ( \"Please provide a date no more than 1 year into the future.\" ) ) "}
{"10434": "\ndef verify ( cls , timestamp : int , message_hash : SHA512Hash , signature : bytes , ) -> bool : \n    if timestamp < 1496176860.0 : \n        verifier = cls . _VERIFIER_20130905 \n    elif timestamp < 1502202360.0 : \n        verifier = None \n    else : \n        verifier = cls . _VERIFIER_20170808 \n    if verifier : \n        result = verifier . verify ( message_hash , signature , ) \n    else : \n        result = False \n    if isinstance ( result , int ) : \n        result = True if result == 1 else False \n    return result "}
{"10436": "\ndef access_request ( pid , record , template , ** kwargs ) : \n    recid = int ( pid . pid_value ) \n    datastore = LocalProxy ( lambda : current_app . extensions [ 'security' ] . datastore ) \n    if record . get ( 'access_right' ) != 'restricted' or not record . get ( 'access_conditions' ) : \n        abort ( 404.0 ) \n    owners = record . get ( 'owners' , [ ] ) \n    record_owners = [ datastore . find_user ( id = owner_id ) for owner_id in owners ] \n    if not record_owners : \n        abort ( 404.0 ) \n    sender = None \n    initialdata = dict ( ) \n    if current_user . is_authenticated : \n        sender = current_user \n        initialdata [ 'email' ] = current_user . email \n        if current_user . profile : \n            initialdata [ 'full_name' ] = current_user . profile . full_name \n    form = AccessRequestForm ( formdata = request . form , ** initialdata ) \n    if form . validate_on_submit ( ) : \n        accreq = AccessRequest . create ( recid = recid , receiver = record_owners [ 0 ] , sender_full_name = form . data [ 'full_name' ] , sender_email = form . data [ 'email' ] , justification = form . data [ 'justification' ] , sender = sender ) \n        db . session . commit ( ) \n        if accreq . status == RequestStatus . EMAIL_VALIDATION : \n            flash ( _ ( \"Email confirmation needed: We have sent you an email to \" \"verify your address. Please check the email and follow the \" \"instructions to complete the access request.\" ) , category = 'info' ) \n        else : \n            flash ( _ ( \"Access request submitted.\" ) , category = 'info' ) \n        return redirect ( url_for ( 'invenio_records_ui.recid' , pid_value = recid ) ) \n    return render_template ( template , pid = pid , record = record , form = form , owners = record_owners , ) "}
{"10437": "\ndef confirm ( pid , record , template , ** kwargs ) : \n    recid = int ( pid . pid_value ) \n    token = request . view_args [ 'token' ] \n    data = EmailConfirmationSerializer . compat_validate_token ( token ) \n    if data is None : \n        flash ( _ ( \"Invalid confirmation link.\" ) , category = 'danger' ) \n        return redirect ( url_for ( \"invenio_records_ui.recid\" , pid_value = recid ) ) \n    r = AccessRequest . query . get ( data [ 'id' ] ) \n    if not r : \n        abort ( 404.0 ) \n    if r . status != RequestStatus . EMAIL_VALIDATION : \n        abort ( 404.0 ) \n    r . confirm_email ( ) \n    db . session . commit ( ) \n    flash ( _ ( \"Email validated and access request submitted.\" ) , category = 'info' ) \n    return redirect ( url_for ( \"invenio_records_ui.recid\" , pid_value = recid ) ) "}
{"10444": "\ndef _init_ssh ( self ) : \n    self . ssh_host = self . config . get ( 'ssh_host' , self . hostname ) \n    self . known_hosts = self . config . get ( 'ssh_knownhosts_file' , self . tensor . config . get ( 'ssh_knownhosts_file' , None ) ) \n    self . ssh_keyfile = self . config . get ( 'ssh_keyfile' , self . tensor . config . get ( 'ssh_keyfile' , None ) ) \n    self . ssh_key = self . config . get ( 'ssh_key' , self . tensor . config . get ( 'ssh_key' , None ) ) \n    self . ssh_keypass = self . config . get ( 'ssh_keypass' , self . tensor . config . get ( 'ssh_keypass' , None ) ) \n    self . ssh_user = self . config . get ( 'ssh_username' , self . tensor . config . get ( 'ssh_username' , None ) ) \n    self . ssh_password = self . config . get ( 'ssh_password' , self . tensor . config . get ( 'ssh_password' , None ) ) \n    self . ssh_port = self . config . get ( 'ssh_port' , self . tensor . config . get ( 'ssh_port' , 22.0 ) ) \n    if not ( self . ssh_key or self . ssh_keyfile or self . ssh_password ) : \n        raise Exception ( \"To use SSH you must specify *one* of ssh_key,\" \" ssh_keyfile or ssh_password for this source\" \" check or globally\" ) \n    if not self . ssh_user : \n        raise Exception ( \"ssh_username must be set\" ) \n    self . ssh_keydb = [ ] \n    cHash = hashlib . sha1 ( ':' . join ( ( self . ssh_host , self . ssh_user , str ( self . ssh_port ) , str ( self . ssh_password ) , str ( self . ssh_key ) , str ( self . ssh_keyfile ) ) ) . encode ( ) ) . hexdigest ( ) \n    if cHash in self . tensor . hostConnectorCache : \n        self . ssh_client = self . tensor . hostConnectorCache . get ( cHash ) \n        self . ssh_connector = False \n    else : \n        self . ssh_connector = True \n        self . ssh_client = ssh . SSHClient ( self . ssh_host , self . ssh_user , self . ssh_port , password = self . ssh_password , knownhosts = self . known_hosts ) \n        if self . ssh_keyfile : \n            self . ssh_client . addKeyFile ( self . ssh_keyfile , self . ssh_keypass ) \n        if self . ssh_key : \n            self . ssh_client . addKeyString ( self . ssh_key , self . ssh_keypass ) \n        self . tensor . hostConnectorCache [ cHash ] = self . ssh_client "}
{"10447": "\ndef index ( ) : \n    query = request . args . get ( 'query' , '' ) \n    order = request . args . get ( 'sort' , '-created' ) \n    try : \n        page = int ( request . args . get ( 'page' , 1 ) ) \n        per_page = int ( request . args . get ( 'per_page' , 20.0 ) ) \n    except ( TypeError , ValueError ) : \n        abort ( 404.0 ) \n    form = DeleteForm ( request . form ) \n    if form . validate_on_submit ( ) : \n        link = SecretLink . query_by_owner ( current_user ) . filter_by ( id = form . link . data ) . first ( ) \n        if link . revoke ( ) : \n            flash ( _ ( \"Shared link revoked.\" ) , category = 'success' ) \n        db . session . commit ( ) \n    links = SecretLink . query_by_owner ( current_user ) . filter ( SecretLink . revoked_at . is_ ( None ) ) \n    if query : \n        lquery = \"%{0}%\" . format ( query ) \n        links = links . filter ( SecretLink . title . like ( lquery ) | SecretLink . description . like ( lquery ) ) \n    ordering = QueryOrdering ( links , [ 'title' , 'created' , 'expires_at' ] , order ) \n    links = ordering . items ( ) \n    requests = AccessRequest . query_by_receiver ( current_user ) . filter_by ( status = RequestStatus . PENDING ) . order_by ( 'created' ) \n    return render_template ( \"zenodo_accessrequests/settings/index.html\" , links_pagination = links . paginate ( page , per_page = per_page ) , requests = requests , query = query , order = ordering , get_record = get_record , form = DeleteForm ( ) , ) "}
{"10448": "\ndef createClient ( self ) : \n    server = self . config . get ( 'server' , 'localhost' ) \n    port = self . config . get ( 'port' , 5555.0 ) \n    failover = self . config . get ( 'failover' , False ) \n    self . factory = riemann . RiemannClientFactory ( server , failover = failover ) \n    if failover : \n        initial = random . choice ( server ) \n    else : \n        initial = server \n    log . msg ( 'Connecting to Riemann on %s:%s' % ( initial , port ) ) \n    if self . tls : \n        if SSL : \n            self . connector = reactor . connectSSL ( initial , port , self . factory , ClientTLSContext ( self . key , self . cert ) ) \n        else : \n            log . msg ( '[FATAL] SSL support not available!' ' Please install PyOpenSSL. Exiting now' ) \n            reactor . stop ( ) \n    else : \n        self . connector = reactor . connectTCP ( initial , port , self . factory ) \n    d = defer . Deferred ( ) \n    def cb ( ) : \n        if hasattr ( self . factory , 'proto' ) and self . factory . proto : \n            self . t . start ( self . inter ) \n            d . callback ( None ) \n        else : \n            reactor . callLater ( 0.01 , cb ) \n    cb ( ) \n    return d "}
{"10452": "\ndef createClient ( self ) : \n    server = self . config . get ( 'server' , '127.0.0.1' ) \n    port = self . config . get ( 'port' , 5555.0 ) \n    def connect ( ip ) : \n        self . protocol = riemann . RiemannUDP ( ip , port ) \n        self . endpoint = reactor . listenUDP ( 0 , self . protocol ) \n    d = reactor . resolve ( server ) \n    d . addCallback ( connect ) \n    return d "}
{"10453": "\ndef createClient ( self ) : \n    server = self . config . get ( 'server' , 'localhost' ) \n    port = int ( self . config . get ( 'port' , 9200.0 ) ) \n    self . client = elasticsearch . ElasticSearch ( self . url , self . user , self . password , self . index ) \n    self . t . start ( self . inter ) "}
{"10460": "\ndef r_q_send ( self , msg_dict ) : \n    no_pickle_keys = self . invalid_dict_pickle_keys ( msg_dict ) \n    if no_pickle_keys == [ ] : \n        self . r_q . put ( msg_dict ) \n    else : \n        hash_func = md5 ( ) \n        hash_func . update ( str ( msg_dict ) ) \n        dict_hash = str ( hash_func . hexdigest ( ) ) [ - 7.0 : ] \n        linesep = os . linesep \n        sys . stderr . write ( \"{0} {1}r_q_send({2}) Can't pickle this dict:{3} '''{7}{4}   {5}{7}{6}''' {7}\" . format ( datetime . now ( ) , Style . BRIGHT , dict_hash , Style . RESET_ALL , Fore . MAGENTA , msg_dict , Style . RESET_ALL , linesep , ) ) \n        err_frag1 = ( Style . BRIGHT + \"    r_q_send({0}) Offending dict keys:\" . format ( dict_hash ) + Style . RESET_ALL ) \n        err_frag2 = Fore . YELLOW + \" {0}\" . format ( no_pickle_keys ) + Style . RESET_ALL \n        err_frag3 = \"{0}\" . format ( linesep ) \n        sys . stderr . write ( err_frag1 + err_frag2 + err_frag3 ) \n        for key in sorted ( no_pickle_keys ) : \n            sys . stderr . write ( \"      msg_dict['{0}']: {1}'{2}'{3}{4}\" . format ( key , Fore . MAGENTA , repr ( msg_dict . get ( key ) ) , Style . RESET_ALL , linesep , ) ) \n            if isinstance ( msg_dict . get ( key ) , object ) : \n                thisobj = msg_dict . get ( key ) \n                no_pickle_attrs = self . invalid_obj_pickle_attrs ( thisobj ) \n                err_frag1 = ( Style . BRIGHT + \"      r_q_send({0}) Offending attrs:\" . format ( dict_hash ) + Style . RESET_ALL ) \n                err_frag2 = ( Fore . YELLOW + \" {0}\" . format ( no_pickle_attrs ) + Style . RESET_ALL ) \n                err_frag3 = \"{0}\" . format ( linesep ) \n                sys . stderr . write ( err_frag1 + err_frag2 + err_frag3 ) \n                for attr in no_pickle_attrs : \n                    sys . stderr . write ( \"        msg_dict['{0}'].{1}: {2}'{3}'{4}{5}\" . format ( key , attr , Fore . RED , repr ( getattr ( thisobj , attr ) ) , Style . RESET_ALL , linesep , ) ) \n        sys . stderr . write ( \"    {0}r_q_send({1}) keys (no problems):{2}{3}\" . format ( Style . BRIGHT , dict_hash , Style . RESET_ALL , linesep ) ) \n        for key in sorted ( set ( msg_dict . keys ( ) ) . difference ( no_pickle_keys ) ) : \n            sys . stderr . write ( \"      msg_dict['{0}']: {1}{2}{3}{4}\" . format ( key , Fore . GREEN , repr ( msg_dict . get ( key ) ) , Style . RESET_ALL , linesep , ) ) "}
{"10470": "\ndef get_version ( version = None ) : \n    v = version or __version__ \n    if len ( v ) == 4.0 : \n        return '{0}{1}' . format ( short_version ( v ) , v [ 3.0 ] ) \n    return short_version ( v ) "}
{"10489": "\ndef import_app_module ( app_name , module_name ) : \n    name_split = app_name . split ( '.' ) \n    if name_split [ - 1 ] [ 0 ] . isupper ( ) : \n        app_name = '.' . join ( name_split [ : - 2.0 ] ) \n    module = import_module ( app_name ) \n    try : \n        sub_module = import_module ( '%s.%s' % ( app_name , module_name ) ) \n        return sub_module \n    except : \n        if module_has_submodule ( module , module_name ) : \n            raise \n        return None "}
{"10491": "\ndef include_ ( parser , token ) : \n    bits = token . split_contents ( ) \n    dynamic = False \n    if len ( bits ) >= 2.0 : \n        dynamic = '{{' in bits [ 1 ] \n        if dynamic : \n            fallback = None \n            bits_new = [ ] \n            for bit in bits : \n                if fallback is True : \n                    fallback = bit \n                    continue \n                if bit == 'fallback' : \n                    fallback = True \n                else : \n                    bits_new . append ( bit ) \n            if fallback : \n                fallback = parser . compile_filter ( construct_relative_path_ ( parser , fallback ) ) \n            token . contents = ' ' . join ( bits_new ) \n    token . contents = token . contents . replace ( 'include_' , 'include' ) \n    include_node = do_include ( parser , token ) \n    if dynamic : \n        include_node = DynamicIncludeNode ( include_node . template , extra_context = include_node . extra_context , isolated_context = include_node . isolated_context , fallback = fallback or None , ) \n    return include_node "}
{"10492": "\ndef gravatar_get_url ( obj , size = 65.0 , default = 'identicon' ) : \n    return get_gravatar_url ( obj , size = size , default = default ) "}
{"10493": "\ndef gravatar_get_img ( obj , size = 65.0 , default = 'identicon' ) : \n    url = get_gravatar_url ( obj , size = size , default = default ) \n    if url : \n        return safe ( '<img src=\"%s\" class=\"gravatar\">' % url ) \n    return '' "}
{"10495": "\ndef is_valid_s3_url ( url ) : \n    if url . startswith ( 'source:' ) : \n        return True \n    scheme , netloc , path , _ , _ , _ = urlparse ( url ) \n    port_except = RemotePortValidationError ( 'Port value %s is not a valid s3 location' % url ) \n    if len ( scheme ) < 2.0 : \n        raise port_except \n    if 's3' in scheme or 's3' in netloc or 's3' in path : \n        return True \n    else : \n        raise port_except "}
{"10506": "\ndef download_file ( self , path , target_path ) : \n    self . __validate_storage_path ( path ) \n    entity = self . api_client . get_entity_by_query ( path = path ) \n    if entity [ 'entity_type' ] != 'file' : \n        raise StorageArgumentException ( 'Only file entities can be downloaded' ) \n    signed_url = self . api_client . get_signed_url ( entity [ 'uuid' ] ) \n    response = self . api_client . download_signed_url ( signed_url ) \n    with open ( target_path , \"wb\" ) as output : \n        for chunk in response . iter_content ( chunk_size = 1024.0 ) : \n            output . write ( chunk ) "}
{"10514": "\ndef new ( cls , access_token , environment = 'prod' ) : \n    request = RequestBuilder . request ( environment ) . to_service ( cls . SERVICE_NAME , cls . SERVICE_VERSION ) . throw ( StorageForbiddenException , lambda resp : 'You are forbidden to do this.' if resp . status_code == 403.0 else None ) . throw ( StorageNotFoundException , lambda resp : 'The entity is not found' if resp . status_code == 404.0 else None ) . throw ( StorageException , lambda resp : 'Server response: {0} - {1}' . format ( resp . status_code , resp . text ) if not resp . ok else None ) \n    authenticated_request = request . with_token ( access_token ) \n    return cls ( request , authenticated_request ) "}
{"10529": "\ndef download_file_content ( self , file_id , etag = None ) : \n    if not is_valid_uuid ( file_id ) : \n        raise StorageArgumentException ( 'Invalid UUID for file_id: {0}' . format ( file_id ) ) \n    headers = { 'Accept' : '*/*' } \n    if etag : \n        headers [ 'If-None-Match' ] = etag \n    resp = self . _authenticated_request . to_endpoint ( 'file/{}/content/' . format ( file_id ) ) . with_headers ( headers ) . get ( ) \n    if resp . status_code == 304.0 : \n        return ( None , None ) \n    if 'ETag' not in resp . headers : \n        raise StorageException ( 'No ETag received from the service with the download' ) \n    return ( resp . headers [ 'ETag' ] , resp . content ) "}
{"10537": "\ndef map_job ( job , func , inputs , * args ) : \n    num_partitions = 100.0 \n    partition_size = len ( inputs ) / num_partitions \n    if partition_size > 1 : \n        for partition in partitions ( inputs , partition_size ) : \n            job . addChildJobFn ( map_job , func , partition , * args ) \n    else : \n        for sample in inputs : \n            job . addChildJobFn ( func , sample , * args ) "}
{"10548": "\nasync def parse_release_results ( soup ) : \n    soup = list ( soup . find_all ( 'table' , class_ = 'stripe' ) [ 0 ] . children ) [ 1 : ] \n    releases = [ ] \n    for item in soup : \n        child = list ( item . children ) \n        temp_rel = { 'date' : None , 'ages' : None , 'platform' : None , 'name' : None } \n        temp_rel [ 'date' ] = child [ 0 ] . string \n        temp_rel [ 'ages' ] = child [ 1 ] . string \n        temp_rel [ 'platform' ] = child [ 2.0 ] . abbr . get ( 'title' ) \n        temp_rel [ 'name' ] = child [ 3.0 ] . a . string \n        releases . append ( temp_rel ) \n        del temp_rel \n    return releases "}
{"10559": "\ndef run_mutect ( job , normal_bam , normal_bai , tumor_bam , tumor_bai , ref , ref_dict , fai , cosmic , dbsnp ) : \n    work_dir = job . fileStore . getLocalTempDir ( ) \n    file_ids = [ normal_bam , normal_bai , tumor_bam , tumor_bai , ref , fai , ref_dict , cosmic , dbsnp ] \n    file_names = [ 'normal.bam' , 'normal.bai' , 'tumor.bam' , 'tumor.bai' , 'ref.fasta' , 'ref.fasta.fai' , 'ref.dict' , 'cosmic.vcf' , 'dbsnp.vcf' ] \n    for file_store_id , name in zip ( file_ids , file_names ) : \n        job . fileStore . readGlobalFile ( file_store_id , os . path . join ( work_dir , name ) ) \n    parameters = [ '--analysis_type' , 'MuTect' , '--reference_sequence' , 'ref.fasta' , '--cosmic' , '/data/cosmic.vcf' , '--dbsnp' , '/data/dbsnp.vcf' , '--input_file:normal' , '/data/normal.bam' , '--input_file:tumor' , '/data/tumor.bam' , '--tumor_lod' , str ( 10.0 ) , '--initial_tumor_lod' , str ( 4.0 ) , '--out' , 'mutect.out' , '--coverage_file' , 'mutect.cov' , '--vcf' , 'mutect.vcf' ] \n    dockerCall ( job = job , workDir = work_dir , parameters = parameters , tool = 'quay.io/ucsc_cgl/mutect:1.1.7--e8bf09459cf0aecb9f55ee689c2b2d194754cbd3' ) \n    output_file_names = [ 'mutect.vcf' , 'mutect.cov' , 'mutect.out' ] \n    output_file_paths = [ os . path . join ( work_dir , x ) for x in output_file_names ] \n    tarball_files ( 'mutect.tar.gz' , file_paths = output_file_paths , output_dir = work_dir ) \n    return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'mutect.tar.gz' ) ) "}
{"10563": "\nasync def search_vndb ( self , stype , term ) : \n    fstype = \"\" \n    if stype not in [ 'v' , 'r' , 'p' , 's' , 'c' , 'g' , 'i' , 'u' ] : \n        raise VNDBBadStype ( stype ) \n    else : \n        if stype in [ 'v' , 'p' , 's' , 'c' , 'u' ] : \n            fstype = '/{}/all' . format ( stype ) \n        elif stype in [ 'g' , 'i' ] : \n            fstype = '/{}/list' . format ( stype ) \n        elif stype == 'r' : \n            fstype = '/r' \n    async with self . session . get ( self . base_url + \"{}\" . format ( fstype ) , params = { \"q\" : term } , headers = self . headers ) as response : \n        if response . status == 404.0 : \n            raise aiohttp . HttpBadRequest ( \"VN Not Found\" ) \n        elif 'q=' not in response . url : \n            raise VNDBOneResult ( term , response . url . rsplit ( '/' , 1 ) [ 1 ] ) \n        text = await response . text ( ) \n        if 'No Results' in text : \n            raise VNDBNoResults ( term ) \n        soup = BeautifulSoup ( text , 'lxml' ) \n        resp = await self . parse_search ( stype , soup ) \n        if resp == [ ] : \n            raise VNDBNoResults ( term ) \n        return resp "}
{"10573": "\ndef sync ( self ) : \n    logging . debug ( \"Logger: Syncing...\" ) \n    failed = False \n    try : \n        cdb = self . connectordb \n        cdb . ping ( ) \n        with self . synclock : \n            c = self . database . cursor ( ) \n            for stream in self . streams : \n                s = cdb [ stream ] \n                c . execute ( \"SELECT * FROM cache WHERE stream=? ORDER BY timestamp ASC;\" , ( stream , ) ) \n                datapointArray = [ ] \n                for dp in c . fetchall ( ) : \n                    datapointArray . append ( { \"t\" : dp [ 1 ] , \"d\" : json . loads ( dp [ 2.0 ] ) } ) \n                if len ( s ) > 0 : \n                    newtime = s [ - 1 ] [ \"t\" ] \n                    while ( len ( datapointArray ) > 0 and datapointArray [ 0 ] [ \"t\" ] < newtime ) : \n                        logging . debug ( \"Datapoint exists with older timestamp. Removing the datapoint.\" ) \n                        datapointArray = datapointArray [ 1 : ] \n                if len ( datapointArray ) > 0 : \n                    logging . debug ( \"%s: syncing %i datapoints\" % ( stream , len ( datapointArray ) ) ) \n                    while ( len ( datapointArray ) > DATAPOINT_INSERT_LIMIT ) : \n                        s . insert_array ( datapointArray [ : DATAPOINT_INSERT_LIMIT ] ) \n                        datapointArray = datapointArray [ DATAPOINT_INSERT_LIMIT : ] \n                        c . execute ( \"DELETE FROM cache WHERE stream=? AND timestamp <?\" , ( stream , datapointArray [ 0 ] [ \"t\" ] ) ) \n                    s . insert_array ( datapointArray ) \n                    c . execute ( \"DELETE FROM cache WHERE stream=? AND timestamp <=?\" , ( stream , datapointArray [ - 1 ] [ \"t\" ] ) ) \n            self . lastsynctime = time . time ( ) \n            if self . onsync is not None : \n                self . onsync ( ) \n    except Exception as e : \n        falied = True \n        reraise = self . syncraise \n        if self . onsyncfail is not None : \n            reraise = self . onsyncfail ( e ) \n        if reraise : \n            raise "}
{"10591": "\ndef _get_mount_path ( self ) : \n    if self . _mount_path is None : \n        name = current_docker_container_id ( ) \n        if dockerd_is_reachable ( ) : \n            blob = json . loads ( subprocess . check_output ( [ 'docker' , 'inspect' , name ] ) ) \n            mounts = blob [ 0 ] [ 'Mounts' ] \n            sock_mnt = [ x [ 'Source' ] == x [ 'Destination' ] for x in mounts if 'docker.sock' in x [ 'Source' ] ] \n            require ( len ( sock_mnt ) == 1 , 'Missing socket mount. Requires the following: ' 'docker run -v /var/run/docker.sock:/var/run/docker.sock' ) \n            if len ( mounts ) == 2.0 : \n                require ( all ( x [ 'Source' ] == x [ 'Destination' ] for x in mounts ) , 'Docker Src/Dst mount points, invoked with the -v argument, ' 'must be the same if only using one mount point aside from the docker ' 'socket.' ) \n                work_mount = [ x [ 'Source' ] for x in mounts if 'docker.sock' not in x [ 'Source' ] ] \n            else : \n                mirror_mounts = [ x [ 'Source' ] for x in mounts if x [ 'Source' ] == x [ 'Destination' ] ] \n                work_mount = [ x for x in mirror_mounts if 'docker.sock' not in x ] \n                require ( len ( work_mount ) == 1 , 'Wrong number of mirror mounts provided, see ' 'documentation.' ) \n            self . _mount_path = work_mount [ 0 ] \n            log . info ( 'The work mount is: %s' , self . _mount_path ) \n        else : \n            raise UserError ( 'Docker daemon is not reachable, ensure Docker is being run with: ' '\"-v /var/run/docker.sock:/var/run/docker.sock\" as an argument.' ) \n    return self . _mount_path "}
{"10596": "\ndef handleresult ( self , r ) : \n    if r . status_code >= 400.0 and r . status_code < 500.0 : \n        msg = r . json ( ) \n        raise AuthenticationError ( str ( msg [ \"code\" ] ) + \": \" + msg [ \"msg\" ] + \" (\" + msg [ \"ref\" ] + \")\" ) \n    elif r . status_code > 300.0 : \n        err = None \n        try : \n            msg = r . json ( ) \n            err = ServerError ( str ( msg [ \"code\" ] ) + \": \" + msg [ \"msg\" ] + \" (\" + msg [ \"ref\" ] + \")\" ) \n        except : \n            raise ServerError ( \"Server returned error, but did not give a valid error message\" ) \n        raise err \n    return r "}
{"10618": "\ndef __reconnect ( self ) : \n    self . status = \"reconnecting\" \n    if self . disconnected_time - self . connected_time > 15.0 * 60.0 : \n        self . reconnect_time = self . reconnect_time_starting_seconds \n    else : \n        self . reconnect_time *= self . reconnect_time_backoff_multiplier \n    if self . reconnect_time > self . reconnect_time_max_seconds : \n        self . reconnect_time = self . reconnect_time_max_seconds \n    self . reconnect_time *= 1 + random . uniform ( - 0.2 , 0.2 ) \n    if self . reconnect_time < self . reconnect_time_starting_seconds : \n        self . reconnect_time = self . reconnect_time_starting_seconds \n    logging . warn ( \"ConnectorDB:WS: Attempting to reconnect in %fs\" , self . reconnect_time ) \n    self . reconnector = threading . Timer ( self . reconnect_time , self . __reconnect_fnc ) \n    self . reconnector . daemon = True \n    self . reconnector . start ( ) "}
{"10623": "\ndef __on_message ( self , ws , msg ) : \n    msg = json . loads ( msg ) \n    logging . debug ( \"ConnectorDB:WS: Msg '%s'\" , msg [ \"stream\" ] ) \n    stream_key = msg [ \"stream\" ] + \":\" \n    if \"transform\" in msg : \n        stream_key += msg [ \"transform\" ] \n    self . subscription_lock . acquire ( ) \n    if stream_key in self . subscriptions : \n        subscription_function = self . subscriptions [ stream_key ] \n        self . subscription_lock . release ( ) \n        fresult = subscription_function ( msg [ \"stream\" ] , msg [ \"data\" ] ) \n        if fresult is True : \n            fresult = msg [ \"data\" ] \n        if fresult is not False and fresult is not None and msg [ \"stream\" ] . endswith ( \"/downlink\" ) and msg [ \"stream\" ] . count ( \"/\" ) == 3.0 : \n            self . insert ( msg [ \"stream\" ] [ : - 9.0 ] , fresult ) \n    else : \n        self . subscription_lock . release ( ) \n        logging . warn ( \"ConnectorDB:WS: Msg '%s' not subscribed! Subscriptions: %s\" , msg [ \"stream\" ] , list ( self . subscriptions . keys ( ) ) ) "}
{"10627": "\ndef gatk_variant_recalibrator ( job , mode , vcf , ref_fasta , ref_fai , ref_dict , annotations , hapmap = None , omni = None , phase = None , dbsnp = None , mills = None , max_gaussians = 4.0 , unsafe_mode = False ) : \n    mode = mode . upper ( ) \n    inputs = { 'genome.fa' : ref_fasta , 'genome.fa.fai' : ref_fai , 'genome.dict' : ref_dict , 'input.vcf' : vcf } \n    command = [ '-T' , 'VariantRecalibrator' , '-R' , 'genome.fa' , '-input' , 'input.vcf' , '-tranche' , '100.0' , '-tranche' , '99.9' , '-tranche' , '99.0' , '-tranche' , '90.0' , '--maxGaussians' , str ( max_gaussians ) , '-recalFile' , 'output.recal' , '-tranchesFile' , 'output.tranches' , '-rscriptFile' , 'output.plots.R' ] \n    if mode == 'SNP' : \n        command . extend ( [ '-resource:hapmap,known=false,training=true,truth=true,prior=15.0' , 'hapmap.vcf' , '-resource:omni,known=false,training=true,truth=true,prior=12.0' , 'omni.vcf' , '-resource:dbsnp,known=true,training=false,truth=false,prior=2.0' , 'dbsnp.vcf' , '-resource:1000G,known=false,training=true,truth=false,prior=10.0' , '1000G.vcf' , '-mode' , 'SNP' ] ) \n        inputs [ 'hapmap.vcf' ] = hapmap \n        inputs [ 'omni.vcf' ] = omni \n        inputs [ 'dbsnp.vcf' ] = dbsnp \n        inputs [ '1000G.vcf' ] = phase \n    elif mode == 'INDEL' : \n        command . extend ( [ '-resource:mills,known=false,training=true,truth=true,prior=12.0' , 'mills.vcf' , '-resource:dbsnp,known=true,training=false,truth=false,prior=2.0' , 'dbsnp.vcf' , '-mode' , 'INDEL' ] ) \n        inputs [ 'mills.vcf' ] = mills \n        inputs [ 'dbsnp.vcf' ] = dbsnp \n    else : \n        raise ValueError ( 'Variant filter modes can be SNP or INDEL, got %s' % mode ) \n    for annotation in annotations : \n        command . extend ( [ '-an' , annotation ] ) \n    if unsafe_mode : \n        command . extend ( [ '-U' , 'ALLOW_SEQ_DICT_INCOMPATIBILITY' ] ) \n    work_dir = job . fileStore . getLocalTempDir ( ) \n    for name , file_store_id in inputs . iteritems ( ) : \n        job . fileStore . readGlobalFile ( file_store_id , os . path . join ( work_dir , name ) ) \n    job . fileStore . logToMaster ( 'Running GATK VariantRecalibrator on {mode}s using the following annotations:\\n' '{annotations}' . format ( mode = mode , annotations = '\\n' . join ( annotations ) ) ) \n    docker_parameters = [ '--rm' , 'log-driver' , 'none' , '-e' , 'JAVA_OPTS=-Djava.io.tmpdir=/data/ -Xmx{}' . format ( job . memory ) ] \n    dockerCall ( job = job , workDir = work_dir , parameters = command , tool = 'quay.io/ucsc_cgl/gatk:3.5--dba6dae49156168a909c43330350c6161dc7ecc2' , dockerParameters = docker_parameters ) \n    recal_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.recal' ) ) \n    tranches_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.tranches' ) ) \n    plots_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.plots.R' ) ) \n    return recal_id , tranches_id , plots_id "}
{"10632": "\ndef write_config ( configuration ) : \n    with open ( CONFIG_PATH , 'w' ) as f : \n        json . dump ( configuration , f , indent = 2.0 , sort_keys = True ) "}
{"10645": "\ndef start ( self , job ) : \n    self . sparkContainerID = dockerCheckOutput ( job = job , defer = STOP , workDir = os . getcwd ( ) , tool = \"quay.io/ucsc_cgl/apache-spark-worker:1.5.2\" , dockerParameters = [ \"--net=host\" , \"-d\" , \"-v\" , \"/mnt/ephemeral/:/ephemeral/:rw\" , \"-e\" , \"\\\"SPARK_MASTER_IP=\" + self . masterIP + \":\" + _SPARK_MASTER_PORT + \"\\\"\" , \"-e\" , \"SPARK_LOCAL_DIRS=/ephemeral/spark/local\" , \"-e\" , \"SPARK_WORKER_DIR=/ephemeral/spark/work\" ] , parameters = [ self . masterIP + \":\" + _SPARK_MASTER_PORT ] ) [ : - 1 ] \n    self . __start_datanode ( job ) \n    hdfs_down = True \n    retries = 0 \n    while hdfs_down and ( retries < 5.0 ) : \n        _log . info ( \"Sleeping 30 seconds before checking HDFS startup.\" ) \n        time . sleep ( 30.0 ) \n        clusterID = \"\" \n        try : \n            clusterID = subprocess . check_output ( [ \"docker\" , \"exec\" , self . hdfsContainerID , \"grep\" , \"clusterID\" , \"-R\" , \"/opt/apache-hadoop/logs\" ] ) \n        except : \n            pass \n        if \"Incompatible\" in clusterID : \n            _log . warning ( \"Hadoop Datanode failed to start with: %s\" , clusterID ) \n            _log . warning ( \"Retrying container startup, retry #%d.\" , retries ) \n            retries += 1 \n            _log . warning ( \"Removing ephemeral hdfs directory.\" ) \n            subprocess . check_call ( [ \"docker\" , \"exec\" , self . hdfsContainerID , \"rm\" , \"-rf\" , \"/ephemeral/hdfs\" ] ) \n            _log . warning ( \"Killing container %s.\" , self . hdfsContainerID ) \n            subprocess . check_call ( [ \"docker\" , \"kill\" , self . hdfsContainerID ] ) \n            _log . info ( \"Restarting datanode.\" ) \n            self . __start_datanode ( job ) \n        else : \n            _log . info ( \"HDFS datanode started up OK!\" ) \n            hdfs_down = False \n    if retries >= 5.0 : \n        raise RuntimeError ( \"Failed %d times trying to start HDFS datanode.\" % retries ) \n    return "}
{"10653": "\ndef validate_changeset ( changeset ) : \n    errors = [ ] \n    changes = changeset . findall ( './/{%s}Change' % R53_XMLNS ) \n    num_changes = len ( changes ) \n    if num_changes == 0 : \n        errors . append ( 'changeset must have at least one <Change> element' ) \n    if num_changes > 100.0 : \n        errors . append ( 'changeset has %d <Change> elements: max is 100' % num_changes ) \n    rrs = changeset . findall ( './/{%s}ResourceRecord' % R53_XMLNS ) \n    num_rrs = len ( rrs ) \n    if num_rrs > 1000.0 : \n        errors . append ( 'changeset has %d ResourceRecord elements: max is 1000' % num_rrs ) \n    values = changeset . findall ( './/{%s}Value' % R53_XMLNS ) \n    num_chars = 0 \n    for value in values : \n        num_chars += len ( value . text ) \n    if num_chars > 10000.0 : \n        errors . append ( 'changeset has %d chars in <Value> text: max is 10000' % num_chars ) \n    return errors "}
{"10661": "\ndef next_generation ( self , mut_rate = 0 , max_mut_amt = 0 , log_base = 10.0 ) : \n    if self . __num_processes > 1 : \n        process_pool = Pool ( processes = self . __num_processes ) \n        members = [ m . get ( ) for m in self . __members ] \n    else : \n        members = self . __members \n    if len ( members ) == 0 : \n        raise Exception ( 'Generation 0 not found: use generate_population() first' ) \n    selected_members = self . __select_fn ( members ) \n    reproduction_probs = list ( reversed ( logspace ( 0.0 , 1.0 , num = len ( selected_members ) , base = log_base ) ) ) \n    reproduction_probs = reproduction_probs / sum ( reproduction_probs ) \n    self . __members = [ ] \n    for _ in range ( self . __pop_size ) : \n        parent_1 = nrandom . choice ( selected_members , p = reproduction_probs ) \n        parent_2 = nrandom . choice ( selected_members , p = reproduction_probs ) \n        feed_dict = { } \n        for param in self . __parameters : \n            which_parent = uniform ( 0 , 1 ) \n            if which_parent < 0.5 : \n                feed_dict [ param . name ] = parent_1 . parameters [ param . name ] \n            else : \n                feed_dict [ param . name ] = parent_2 . parameters [ param . name ] \n            feed_dict [ param . name ] = self . __mutate_parameter ( feed_dict [ param . name ] , param , mut_rate , max_mut_amt ) \n        if self . __num_processes > 1 : \n            self . __members . append ( process_pool . apply_async ( self . _start_process , [ self . __cost_fn , feed_dict , self . __cost_fn_args ] ) ) \n        else : \n            self . __members . append ( Member ( feed_dict , self . __cost_fn ( feed_dict , self . __cost_fn_args ) ) ) \n    if self . __num_processes > 1 : \n        process_pool . close ( ) \n        process_pool . join ( ) \n    self . __determine_best_member ( ) "}
{"10684": "\ndef detectBOM ( self ) : \n    bomDict = { codecs . BOM_UTF8 : 'utf-8' , codecs . BOM_UTF16_LE : 'utf-16-le' , codecs . BOM_UTF16_BE : 'utf-16-be' , codecs . BOM_UTF32_LE : 'utf-32-le' , codecs . BOM_UTF32_BE : 'utf-32-be' } \n    string = self . rawStream . read ( 4.0 ) \n    assert isinstance ( string , bytes ) \n    encoding = bomDict . get ( string [ : 3.0 ] ) \n    seek = 3.0 \n    if not encoding : \n        encoding = bomDict . get ( string ) \n        seek = 4.0 \n        if not encoding : \n            encoding = bomDict . get ( string [ : 2.0 ] ) \n            seek = 2.0 \n    self . rawStream . seek ( encoding and seek or 0 ) \n    return encoding "}
{"10699": "\ndef find_requirement ( self , req , upgrade ) : \n    all_versions = self . _find_all_versions ( req . name ) \n    _versions = set ( req . specifier . filter ( [ x . version for x in all_versions ] , prereleases = ( self . allow_all_prereleases if self . allow_all_prereleases else None ) , ) ) \n    applicable_versions = [ x for x in all_versions if x . version in _versions ] \n    if req . satisfied_by is not None : \n        applicable_versions . insert ( 0 , InstallationCandidate ( req . name , req . satisfied_by . version , INSTALLED_VERSION , ) ) \n        existing_applicable = True \n    else : \n        existing_applicable = False \n    applicable_versions = self . _sort_versions ( applicable_versions ) \n    if not upgrade and existing_applicable : \n        if applicable_versions [ 0 ] . location is INSTALLED_VERSION : \n            logger . debug ( 'Existing installed version (%s) is most up-to-date and ' 'satisfies requirement' , req . satisfied_by . version , ) \n        else : \n            logger . debug ( 'Existing installed version (%s) satisfies requirement ' '(most up-to-date version is %s)' , req . satisfied_by . version , applicable_versions [ 0 ] [ 2.0 ] , ) \n        return None \n    if not applicable_versions : \n        logger . critical ( 'Could not find a version that satisfies the requirement %s ' '(from versions: %s)' , req , ', ' . join ( sorted ( set ( str ( i . version ) for i in all_versions ) , key = parse_version , ) ) ) \n        if self . need_warn_external : \n            logger . warning ( \"Some externally hosted files were ignored as access to \" \"them may be unreliable (use --allow-external %s to \" \"allow).\" , req . name , ) \n        if self . need_warn_unverified : \n            logger . warning ( \"Some insecure and unverifiable files were ignored\" \" (use --allow-unverified %s to allow).\" , req . name , ) \n        raise DistributionNotFound ( 'No matching distribution found for %s' % req ) \n    if applicable_versions [ 0 ] . location is INSTALLED_VERSION : \n        logger . debug ( 'Installed version (%s) is most up-to-date (past versions: ' '%s)' , req . satisfied_by . version , ', ' . join ( str ( i . version ) for i in applicable_versions [ 1 : ] ) or \"none\" , ) \n        raise BestVersionAlreadyInstalled \n    if len ( applicable_versions ) > 1 : \n        logger . debug ( 'Using version %s (newest of versions: %s)' , applicable_versions [ 0 ] . version , ', ' . join ( str ( i . version ) for i in applicable_versions ) ) \n    selected_version = applicable_versions [ 0 ] . location \n    if ( selected_version . verifiable is not None and not selected_version . verifiable ) : \n        logger . warning ( \"%s is potentially insecure and unverifiable.\" , req . name , ) \n    return selected_version "}
{"10702": "\ndef links ( self ) : \n    for anchor in self . parsed . findall ( \".//a\" ) : \n        if anchor . get ( \"href\" ) : \n            href = anchor . get ( \"href\" ) \n            url = self . clean_link ( urllib_parse . urljoin ( self . base_url , href ) ) \n            internal = None \n            if self . api_version and self . api_version >= 2.0 : \n                internal = bool ( anchor . get ( \"rel\" ) and \"internal\" in anchor . get ( \"rel\" ) . split ( ) ) \n            yield Link ( url , self , internal = internal ) "}
{"10732": "\ndef _cache_for_link ( cache_dir , link ) : \n    key_parts = [ link . url_without_fragment ] \n    if link . hash_name is not None and link . hash is not None : \n        key_parts . append ( \"=\" . join ( [ link . hash_name , link . hash ] ) ) \n    key_url = \"#\" . join ( key_parts ) \n    hashed = hashlib . sha224 ( key_url . encode ( ) ) . hexdigest ( ) \n    parts = [ hashed [ : 2.0 ] , hashed [ 2.0 : 4.0 ] , hashed [ 4.0 : 6.0 ] , hashed [ 6.0 : ] ] \n    return os . path . join ( cache_dir , \"wheels\" , * parts ) "}
{"10734": "\ndef uninstallation_paths ( dist ) : \n    from pip . utils import FakeFile \n    r = csv . reader ( FakeFile ( dist . get_metadata_lines ( 'RECORD' ) ) ) \n    for row in r : \n        path = os . path . join ( dist . location , row [ 0 ] ) \n        yield path \n        if path . endswith ( '.py' ) : \n            dn , fn = os . path . split ( path ) \n            base = fn [ : - 3.0 ] \n            path = os . path . join ( dn , base + '.pyc' ) \n            yield path "}
{"10738": "\ndef ensure_fresh_rates ( func ) : \n    def wrapper ( self , * args , ** kwargs ) : \n        if self . last_updated + timedelta ( minutes = 5.0 ) < zulu . now ( ) : \n            self . refresh ( ) \n        return func ( self , * args , ** kwargs ) \n    return wrapper "}
{"10743": "\ndef distutils_scheme ( dist_name , user = False , home = None , root = None , isolated = False ) : \n    from distutils . dist import Distribution \n    scheme = { } \n    if isolated : \n        extra_dist_args = { \"script_args\" : [ \"--no-user-cfg\" ] } \n    else : \n        extra_dist_args = { } \n    dist_args = { 'name' : dist_name } \n    dist_args . update ( extra_dist_args ) \n    d = Distribution ( dist_args ) \n    d . parse_config_files ( ) \n    i = d . get_command_obj ( 'install' , create = True ) \n    i . user = user or i . user \n    i . home = home or i . home \n    i . root = root or i . root \n    i . finalize_options ( ) \n    for key in SCHEME_KEYS : \n        scheme [ key ] = getattr ( i , 'install_' + key ) \n    if i . install_lib is not None : \n        scheme . update ( dict ( purelib = i . install_lib , platlib = i . install_lib ) ) \n    if running_under_virtualenv ( ) : \n        scheme [ 'headers' ] = os . path . join ( sys . prefix , 'include' , 'site' , 'python' + sys . version [ : 3.0 ] , dist_name , ) \n        if root is not None : \n            scheme [ \"headers\" ] = os . path . join ( root , os . path . abspath ( scheme [ \"headers\" ] ) [ 1 : ] , ) \n    return scheme "}
{"10745": "\ndef cached_request ( self , request ) : \n    cache_url = self . cache_url ( request . url ) \n    cc = self . parse_cache_control ( request . headers ) \n    no_cache = True if 'no-cache' in cc else False \n    if 'max-age' in cc and cc [ 'max-age' ] == 0 : \n        no_cache = True \n    if no_cache : \n        return False \n    resp = self . serializer . loads ( request , self . cache . get ( cache_url ) ) \n    if not resp : \n        return False \n    if resp . status == 301.0 : \n        return resp \n    headers = CaseInsensitiveDict ( resp . headers ) \n    if not headers or 'date' not in headers : \n        if 'etag' not in headers : \n            self . cache . delete ( cache_url ) \n        return False \n    now = time . time ( ) \n    date = calendar . timegm ( parsedate_tz ( headers [ 'date' ] ) ) \n    current_age = max ( 0 , now - date ) \n    resp_cc = self . parse_cache_control ( headers ) \n    freshness_lifetime = 0 \n    if 'max-age' in resp_cc and resp_cc [ 'max-age' ] . isdigit ( ) : \n        freshness_lifetime = int ( resp_cc [ 'max-age' ] ) \n    elif 'expires' in headers : \n        expires = parsedate_tz ( headers [ 'expires' ] ) \n        if expires is not None : \n            expire_time = calendar . timegm ( expires ) - date \n            freshness_lifetime = max ( 0 , expire_time ) \n    if 'max-age' in cc : \n        try : \n            freshness_lifetime = int ( cc [ 'max-age' ] ) \n        except ValueError : \n            freshness_lifetime = 0 \n    if 'min-fresh' in cc : \n        try : \n            min_fresh = int ( cc [ 'min-fresh' ] ) \n        except ValueError : \n            min_fresh = 0 \n        current_age += min_fresh \n    fresh = ( freshness_lifetime > current_age ) \n    if fresh : \n        return resp \n    if 'etag' not in headers : \n        self . cache . delete ( cache_url ) \n    return False "}
{"10746": "\ndef cache_response ( self , request , response , body = None ) : \n    if response . status not in [ 200.0 , 203.0 , 300.0 , 301.0 ] : \n        return \n    response_headers = CaseInsensitiveDict ( response . headers ) \n    cc_req = self . parse_cache_control ( request . headers ) \n    cc = self . parse_cache_control ( response_headers ) \n    cache_url = self . cache_url ( request . url ) \n    no_store = cc . get ( 'no-store' ) or cc_req . get ( 'no-store' ) \n    if no_store and self . cache . get ( cache_url ) : \n        self . cache . delete ( cache_url ) \n    if self . cache_etags and 'etag' in response_headers : \n        self . cache . set ( cache_url , self . serializer . dumps ( request , response , body = body ) , ) \n    elif response . status == 301.0 : \n        self . cache . set ( cache_url , self . serializer . dumps ( request , response ) ) \n    elif 'date' in response_headers : \n        if cc and cc . get ( 'max-age' ) : \n            if int ( cc [ 'max-age' ] ) > 0 : \n                self . cache . set ( cache_url , self . serializer . dumps ( request , response , body = body ) , ) \n        elif 'expires' in response_headers : \n            if response_headers [ 'expires' ] : \n                self . cache . set ( cache_url , self . serializer . dumps ( request , response , body = body ) , ) "}
{"10756": "\ndef _execfile ( filename , globals , locals = None ) : \n    mode = 'rb' \n    with open ( filename , mode ) as stream : \n        script = stream . read ( ) \n    if sys . version_info [ : 2.0 ] < ( 2.0 , 7.0 ) or sys . version_info [ : 2.0 ] >= ( 3.0 , 0 ) and sys . version_info [ : 2.0 ] < ( 3.0 , 2.0 ) : \n        script = script . replace ( b'\\r\\n' , b'\\n' ) \n        script = script . replace ( b'\\r' , b'\\n' ) \n    if locals is None : \n        locals = globals \n    code = compile ( script , filename , 'exec' ) \n    exec ( code , globals , locals ) "}
{"10761": "\ndef compile_templates ( self , target , extensions = None , filter_func = None , zip = 'deflated' , log_function = None , ignore_errors = True , py_compile = False ) : \n    from jinja2 . loaders import ModuleLoader \n    if log_function is None : \n        log_function = lambda x : None \n    if py_compile : \n        if not PY2 or PYPY : \n            from warnings import warn \n            warn ( Warning ( 'py_compile has no effect on pypy or Python 3' ) ) \n            py_compile = False \n        else : \n            import imp , marshal \n            py_header = imp . get_magic ( ) + u'\\xff\\xff\\xff\\xff' . encode ( 'iso-8859-15' ) \n            if sys . version_info >= ( 3.0 , 3.0 ) : \n                py_header += u'\\x00\\x00\\x00\\x00' . encode ( 'iso-8859-15' ) \n    def write_file ( filename , data , mode ) : \n        if zip : \n            info = ZipInfo ( filename ) \n            info . external_attr = 0o755 << 16.0 \n            zip_file . writestr ( info , data ) \n        else : \n            f = open ( os . path . join ( target , filename ) , mode ) \n            try : \n                f . write ( data ) \n            finally : \n                f . close ( ) \n    if zip is not None : \n        from zipfile import ZipFile , ZipInfo , ZIP_DEFLATED , ZIP_STORED \n        zip_file = ZipFile ( target , 'w' , dict ( deflated = ZIP_DEFLATED , stored = ZIP_STORED ) [ zip ] ) \n        log_function ( 'Compiling into Zip archive \"%s\"' % target ) \n    else : \n        if not os . path . isdir ( target ) : \n            os . makedirs ( target ) \n        log_function ( 'Compiling into folder \"%s\"' % target ) \n    try : \n        for name in self . list_templates ( extensions , filter_func ) : \n            source , filename , _ = self . loader . get_source ( self , name ) \n            try : \n                code = self . compile ( source , name , filename , True , True ) \n            except TemplateSyntaxError as e : \n                if not ignore_errors : \n                    raise \n                log_function ( 'Could not compile \"%s\": %s' % ( name , e ) ) \n                continue \n            filename = ModuleLoader . get_module_filename ( name ) \n            if py_compile : \n                c = self . _compile ( code , encode_filename ( filename ) ) \n                write_file ( filename + 'c' , py_header + marshal . dumps ( c ) , 'wb' ) \n                log_function ( 'Byte-compiled \"%s\" as %s' % ( name , filename + 'c' ) ) \n            else : \n                write_file ( filename , code , 'w' ) \n                log_function ( 'Compiled \"%s\" as %s' % ( name , filename ) ) \n    finally : \n        if zip : \n            zip_file . close ( ) \n    log_function ( 'Finished compiling templates' ) "}
{"10777": "\ndef get_decimal_quantum ( precision ) : \n    assert isinstance ( precision , ( int , decimal . Decimal ) ) \n    return decimal . Decimal ( 10.0 ) ** ( - precision ) "}
{"10780": "\ndef total_seconds ( td ) : \n    if hasattr ( td , 'total_seconds' ) : \n        return td . total_seconds ( ) \n    ms = td . microseconds \n    secs = ( td . seconds + td . days * 24.0 * 3600.0 ) \n    return ( ms + secs * 10.0 ** 6.0 ) / 10.0 ** 6.0 "}
{"10781": "\ndef parse_requirements ( strs ) : \n    lines = iter ( yield_lines ( strs ) ) \n    def scan_list ( ITEM , TERMINATOR , line , p , groups , item_name ) : \n        items = [ ] \n        while not TERMINATOR ( line , p ) : \n            if CONTINUE ( line , p ) : \n                try : \n                    line = next ( lines ) \n                    p = 0 \n                except StopIteration : \n                    msg = \"\\\\ must not appear on the last nonblank line\" \n                    raise RequirementParseError ( msg ) \n            match = ITEM ( line , p ) \n            if not match : \n                msg = \"Expected \" + item_name + \" in\" \n                raise RequirementParseError ( msg , line , \"at\" , line [ p : ] ) \n            items . append ( match . group ( * groups ) ) \n            p = match . end ( ) \n            match = COMMA ( line , p ) \n            if match : \n                p = match . end ( ) \n            elif not TERMINATOR ( line , p ) : \n                msg = \"Expected ',' or end-of-list in\" \n                raise RequirementParseError ( msg , line , \"at\" , line [ p : ] ) \n        match = TERMINATOR ( line , p ) \n        if match : \n            p = match . end ( ) \n        return line , p , items \n    for line in lines : \n        match = DISTRO ( line ) \n        if not match : \n            raise RequirementParseError ( \"Missing distribution spec\" , line ) \n        project_name = match . group ( 1 ) \n        p = match . end ( ) \n        extras = [ ] \n        match = OBRACKET ( line , p ) \n        if match : \n            p = match . end ( ) \n            line , p , extras = scan_list ( DISTRO , CBRACKET , line , p , ( 1 , ) , \"'extra' name\" ) \n        line , p , specs = scan_list ( VERSION , LINE_END , line , p , ( 1 , 2.0 ) , \"version spec\" ) \n        specs = [ ( op , val ) for op , val in specs ] \n        yield Requirement ( project_name , specs , extras ) "}
{"10791": "\ndef module ( self ) : \n    from warnings import warn \n    warn ( DeprecationWarning ( 'modules were deprecated in favor of ' 'blueprints.  Use request.blueprint ' 'instead.' ) , stacklevel = 2.0 ) \n    if self . _is_old_module : \n        return self . blueprint "}
{"10808": "\ndef fixup_chunks ( chunks ) : \n    tag_accum = [ ] \n    cur_word = None \n    result = [ ] \n    for chunk in chunks : \n        if isinstance ( chunk , tuple ) : \n            if chunk [ 0 ] == 'img' : \n                src = chunk [ 1 ] \n                tag , trailing_whitespace = split_trailing_whitespace ( chunk [ 2.0 ] ) \n                cur_word = tag_token ( 'img' , src , html_repr = tag , pre_tags = tag_accum , trailing_whitespace = trailing_whitespace ) \n                tag_accum = [ ] \n                result . append ( cur_word ) \n            elif chunk [ 0 ] == 'href' : \n                href = chunk [ 1 ] \n                cur_word = href_token ( href , pre_tags = tag_accum , trailing_whitespace = \" \" ) \n                tag_accum = [ ] \n                result . append ( cur_word ) \n            continue \n        if is_word ( chunk ) : \n            chunk , trailing_whitespace = split_trailing_whitespace ( chunk ) \n            cur_word = token ( chunk , pre_tags = tag_accum , trailing_whitespace = trailing_whitespace ) \n            tag_accum = [ ] \n            result . append ( cur_word ) \n        elif is_start_tag ( chunk ) : \n            tag_accum . append ( chunk ) \n        elif is_end_tag ( chunk ) : \n            if tag_accum : \n                tag_accum . append ( chunk ) \n            else : \n                assert cur_word , ( \"Weird state, cur_word=%r, result=%r, chunks=%r of %r\" % ( cur_word , result , chunk , chunks ) ) \n                cur_word . post_tags . append ( chunk ) \n        else : \n            assert ( 0 ) \n    if not result : \n        return [ token ( '' , pre_tags = tag_accum ) ] \n    else : \n        result [ - 1 ] . post_tags . extend ( tag_accum ) \n    return result "}
{"10815": "\ndef extract_constant ( code , symbol , default = - 1 ) : \n    if symbol not in code . co_names : \n        return None \n    name_idx = list ( code . co_names ) . index ( symbol ) \n    STORE_NAME = 90.0 \n    STORE_GLOBAL = 97.0 \n    LOAD_CONST = 100.0 \n    const = default \n    for op , arg in _iter_code ( code ) : \n        if op == LOAD_CONST : \n            const = code . co_consts [ arg ] \n        elif arg == name_idx and ( op == STORE_NAME or op == STORE_GLOBAL ) : \n            return const \n        else : \n            const = default "}
{"10820": "\ndef api_returns ( return_values ) : \n    def decorator ( func ) : \n        \n        @ wraps ( func ) \n        def wrapped_func ( request , * args , ** kwargs ) : \n            return_value = func ( request , * args , ** kwargs ) \n            if not isinstance ( return_value , JsonResponse ) : \n                if settings . DEBUG : \n                    return JsonResponseBadRequest ( 'API did not return JSON' ) \n                else : \n                    logger . warn ( 'API did not return JSON' ) \n            accepted_return_codes = return_values . keys ( ) \n            accepted_return_codes . append ( 500.0 ) \n            if return_value . status_code not in accepted_return_codes : \n                if settings . DEBUG : \n                    return JsonResponseBadRequest ( 'API returned %d instead of acceptable values %s' % ( return_value . status_code , accepted_return_codes ) ) \n                else : \n                    logger . warn ( 'API returned %d instead of acceptable values %s' , return_value . status_code , accepted_return_codes , ) \n            return return_value \n        return wrapped_func \n    return decorator "}
{"10831": "\ndef handle_exception ( self , e ) : \n    exc_type , exc_value , tb = sys . exc_info ( ) \n    got_request_exception . send ( self , exception = e ) \n    handler = self . error_handler_spec [ None ] . get ( 500.0 ) \n    if self . propagate_exceptions : \n        if exc_value is e : \n            reraise ( exc_type , exc_value , tb ) \n        else : \n            raise e \n    self . log_exception ( ( exc_type , exc_value , tb ) ) \n    if handler is None : \n        return InternalServerError ( ) \n    return handler ( e ) "}
{"10838": "\ndef handle_requires ( metadata , pkg_info , key ) : \n    may_requires = defaultdict ( list ) \n    for value in pkg_info . get_all ( key ) : \n        extra_match = EXTRA_RE . search ( value ) \n        if extra_match : \n            groupdict = extra_match . groupdict ( ) \n            condition = groupdict [ 'condition' ] \n            extra = groupdict [ 'extra' ] \n            package = groupdict [ 'package' ] \n            if condition . endswith ( ' and ' ) : \n                condition = condition [ : - 5.0 ] \n        else : \n            condition , extra = None , None \n            package = value \n        key = MayRequiresKey ( condition , extra ) \n        may_requires [ key ] . append ( package ) \n    if may_requires : \n        metadata [ 'run_requires' ] = [ ] \n        for key , value in may_requires . items ( ) : \n            may_requirement = { 'requires' : value } \n            if key . extra : \n                may_requirement [ 'extra' ] = key . extra \n            if key . condition : \n                may_requirement [ 'environment' ] = key . condition \n            metadata [ 'run_requires' ] . append ( may_requirement ) \n        if not 'extras' in metadata : \n            metadata [ 'extras' ] = [ ] \n        metadata [ 'extras' ] . extend ( [ key . extra for key in may_requires . keys ( ) if key . extra ] ) "}
{"10859": "\ndef get_wsgi_headers ( self , environ ) : \n    headers = Headers ( self . headers ) \n    location = None \n    content_location = None \n    content_length = None \n    status = self . status_code \n    for key , value in headers : \n        ikey = key . lower ( ) \n        if ikey == u'location' : \n            location = value \n        elif ikey == u'content-location' : \n            content_location = value \n        elif ikey == u'content-length' : \n            content_length = value \n    if location is not None : \n        old_location = location \n        if isinstance ( location , text_type ) : \n            location = iri_to_uri ( location , safe_conversion = True ) \n        if self . autocorrect_location_header : \n            current_url = get_current_url ( environ , root_only = True ) \n            if isinstance ( current_url , text_type ) : \n                current_url = iri_to_uri ( current_url ) \n            location = url_join ( current_url , location ) \n        if location != old_location : \n            headers [ 'Location' ] = location \n    if content_location is not None and isinstance ( content_location , text_type ) : \n        headers [ 'Content-Location' ] = iri_to_uri ( content_location ) \n    if 100.0 <= status < 200.0 or status == 204.0 : \n        headers [ 'Content-Length' ] = content_length = u'0' \n    elif status == 304.0 : \n        remove_entity_headers ( headers ) \n    if self . automatically_set_content_length and self . is_sequence and content_length is None and status != 304.0 : \n        try : \n            content_length = sum ( len ( to_bytes ( x , 'ascii' ) ) for x in self . response ) \n        except UnicodeError : \n            pass \n        else : \n            headers [ 'Content-Length' ] = str ( content_length ) \n    return headers "}
{"10866": "\ndef _iter_module_files ( ) : \n    for module in list ( sys . modules . values ( ) ) : \n        if module is None : \n            continue \n        filename = getattr ( module , '__file__' , None ) \n        if filename : \n            old = None \n            while not os . path . isfile ( filename ) : \n                old = filename \n                filename = os . path . dirname ( filename ) \n                if filename == old : \n                    break \n            else : \n                if filename [ - 4.0 : ] in ( '.pyc' , '.pyo' ) : \n                    filename = filename [ : - 1 ] \n                yield filename "}
{"10867": "\ndef restart_with_reloader ( self ) : \n    while 1 : \n        _log ( 'info' , ' * Restarting with %s' % self . name ) \n        args = [ sys . executable ] + sys . argv \n        new_environ = os . environ . copy ( ) \n        new_environ [ 'WERKZEUG_RUN_MAIN' ] = 'true' \n        if os . name == 'nt' and PY2 : \n            for key , value in iteritems ( new_environ ) : \n                if isinstance ( value , text_type ) : \n                    new_environ [ key ] = value . encode ( 'iso-8859-1' ) \n        exit_code = subprocess . call ( args , env = new_environ ) \n        if exit_code != 3.0 : \n            return exit_code "}
{"10880": "\ndef get_impl_ver ( ) : \n    impl_ver = sysconfig . get_config_var ( \"py_version_nodot\" ) \n    if not impl_ver : \n        impl_ver = '' . join ( map ( str , sys . version_info [ : 2.0 ] ) ) \n    return impl_ver "}
{"10881": "\ndef distros_for_location ( location , basename , metadata = None ) : \n    if basename . endswith ( '.egg.zip' ) : \n        basename = basename [ : - 4.0 ] \n    if basename . endswith ( '.egg' ) and '-' in basename : \n        return [ Distribution . from_location ( location , basename , metadata ) ] \n    if basename . endswith ( '.exe' ) : \n        win_base , py_ver , platform = parse_bdist_wininst ( basename ) \n        if win_base is not None : \n            return interpret_distro_name ( location , win_base , metadata , py_ver , BINARY_DIST , platform ) \n    for ext in EXTENSIONS : \n        if basename . endswith ( ext ) : \n            basename = basename [ : - len ( ext ) ] \n            return interpret_distro_name ( location , basename , metadata ) \n    return [ ] "}
{"10883": "\ndef local_open ( url ) : \n    scheme , server , path , param , query , frag = urlparse ( url ) \n    filename = url2pathname ( path ) \n    if os . path . isfile ( filename ) : \n        return urllib2 . urlopen ( url ) \n    elif path . endswith ( '/' ) and os . path . isdir ( filename ) : \n        files = [ ] \n        for f in os . listdir ( filename ) : \n            if f == 'index.html' : \n                with open ( os . path . join ( filename , f ) , 'r' ) as fp : \n                    body = fp . read ( ) \n                break \n            elif os . path . isdir ( os . path . join ( filename , f ) ) : \n                f += '/' \n            files . append ( \"<a href=%r>%s</a>\" % ( f , f ) ) \n        else : \n            body = ( \"<html><head><title>%s</title>\" % url ) + \"</head><body>%s</body></html>\" % '\\n' . join ( files ) \n        status , message = 200.0 , \"OK\" \n    else : \n        status , message , body = 404.0 , \"Path not found\" , \"Not found\" \n    headers = { 'content-type' : 'text/html' } \n    return HTTPError ( url , status , message , headers , StringIO ( body ) ) "}
{"10884": "\ndef process_url ( self , url , retrieve = False ) : \n    if url in self . scanned_urls and not retrieve : \n        return \n    self . scanned_urls [ url ] = True \n    if not URL_SCHEME ( url ) : \n        self . process_filename ( url ) \n        return \n    else : \n        dists = list ( distros_for_url ( url ) ) \n        if dists : \n            if not self . url_ok ( url ) : \n                return \n            self . debug ( \"Found link: %s\" , url ) \n    if dists or not retrieve or url in self . fetched_urls : \n        list ( map ( self . add , dists ) ) \n        return \n    if not self . url_ok ( url ) : \n        self . fetched_urls [ url ] = True \n        return \n    self . info ( \"Reading %s\" , url ) \n    self . fetched_urls [ url ] = True \n    f = self . open_url ( url , \"Download error on %s: %%s -- Some packages may not be found!\" % url ) \n    if f is None : \n        return \n    self . fetched_urls [ f . url ] = True \n    if 'html' not in f . headers . get ( 'content-type' , '' ) . lower ( ) : \n        f . close ( ) \n        return \n    base = f . url \n    page = f . read ( ) \n    if not isinstance ( page , str ) : \n        if isinstance ( f , HTTPError ) : \n            charset = 'latin-1' \n        else : \n            charset = f . headers . get_param ( 'charset' ) or 'latin-1' \n        page = page . decode ( charset , \"ignore\" ) \n    f . close ( ) \n    for match in HREF . finditer ( page ) : \n        link = urljoin ( base , htmldecode ( match . group ( 1 ) ) ) \n        self . process_url ( link ) \n    if url . startswith ( self . index_url ) and getattr ( f , 'code' , None ) != 404.0 : \n        page = self . process_index ( url , page ) "}
{"10890": "\ndef addusersitepackages ( known_paths ) : \n    global USER_BASE , USER_SITE , ENABLE_USER_SITE \n    env_base = os . environ . get ( \"PYTHONUSERBASE\" , None ) \n    def joinuser ( * args ) : \n        return os . path . expanduser ( os . path . join ( * args ) ) \n    if os . name == \"nt\" : \n        base = os . environ . get ( \"APPDATA\" ) or \"~\" \n        if env_base : \n            USER_BASE = env_base \n        else : \n            USER_BASE = joinuser ( base , \"Python\" ) \n        USER_SITE = os . path . join ( USER_BASE , \"Python\" + sys . version [ 0 ] + sys . version [ 2.0 ] , \"site-packages\" ) \n    else : \n        if env_base : \n            USER_BASE = env_base \n        else : \n            USER_BASE = joinuser ( \"~\" , \".local\" ) \n        USER_SITE = os . path . join ( USER_BASE , \"lib\" , \"python\" + sys . version [ : 3.0 ] , \"site-packages\" ) \n    if ENABLE_USER_SITE and os . path . isdir ( USER_SITE ) : \n        addsitedir ( USER_SITE , known_paths ) \n    if ENABLE_USER_SITE : \n        for dist_libdir in ( \"lib\" , \"local/lib\" ) : \n            user_site = os . path . join ( USER_BASE , dist_libdir , \"python\" + sys . version [ : 3.0 ] , \"dist-packages\" ) \n            if os . path . isdir ( user_site ) : \n                addsitedir ( user_site , known_paths ) \n    return known_paths "}
{"10900": "\ndef get_resource ( self , request , filename ) : \n    filename = join ( dirname ( __file__ ) , 'shared' , basename ( filename ) ) \n    if isfile ( filename ) : \n        mimetype = mimetypes . guess_type ( filename ) [ 0 ] or 'application/octet-stream' \n        f = open ( filename , 'rb' ) \n        try : \n            return Response ( f . read ( ) , mimetype = mimetype ) \n        finally : \n            f . close ( ) \n    return Response ( 'Not Found' , status = 404.0 ) "}
{"10901": "\ndef user_agent ( ) : \n    data = { \"installer\" : { \"name\" : \"pip\" , \"version\" : pip . __version__ } , \"python\" : platform . python_version ( ) , \"implementation\" : { \"name\" : platform . python_implementation ( ) , } , } \n    if data [ \"implementation\" ] [ \"name\" ] == 'CPython' : \n        data [ \"implementation\" ] [ \"version\" ] = platform . python_version ( ) \n    elif data [ \"implementation\" ] [ \"name\" ] == 'PyPy' : \n        if sys . pypy_version_info . releaselevel == 'final' : \n            pypy_version_info = sys . pypy_version_info [ : 3.0 ] \n        else : \n            pypy_version_info = sys . pypy_version_info \n        data [ \"implementation\" ] [ \"version\" ] = \".\" . join ( [ str ( x ) for x in pypy_version_info ] ) \n    elif data [ \"implementation\" ] [ \"name\" ] == 'Jython' : \n        data [ \"implementation\" ] [ \"version\" ] = platform . python_version ( ) \n    elif data [ \"implementation\" ] [ \"name\" ] == 'IronPython' : \n        data [ \"implementation\" ] [ \"version\" ] = platform . python_version ( ) \n    if sys . platform . startswith ( \"linux\" ) : \n        distro = dict ( filter ( lambda x : x [ 1 ] , zip ( [ \"name\" , \"version\" , \"id\" ] , platform . linux_distribution ( ) ) , ) ) \n        libc = dict ( filter ( lambda x : x [ 1 ] , zip ( [ \"lib\" , \"version\" ] , platform . libc_ver ( ) ) , ) ) \n        if libc : \n            distro [ \"libc\" ] = libc \n        if distro : \n            data [ \"distro\" ] = distro \n    if sys . platform . startswith ( \"darwin\" ) and platform . mac_ver ( ) [ 0 ] : \n        data [ \"distro\" ] = { \"name\" : \"OS X\" , \"version\" : platform . mac_ver ( ) [ 0 ] } \n    if platform . system ( ) : \n        data . setdefault ( \"system\" , { } ) [ \"name\" ] = platform . system ( ) \n    if platform . release ( ) : \n        data . setdefault ( \"system\" , { } ) [ \"release\" ] = platform . release ( ) \n    if platform . machine ( ) : \n        data [ \"cpu\" ] = platform . machine ( ) \n    return \"{data[installer][name]}/{data[installer][version]} {json}\" . format ( data = data , json = json . dumps ( data , separators = ( \",\" , \":\" ) , sort_keys = True ) , ) "}
{"10918": "\ndef make_wheelfile_inner ( base_name , base_dir = '.' ) : \n    zip_filename = base_name + \".whl\" \n    log . info ( \"creating '%s' and adding '%s' to it\" , zip_filename , base_dir ) \n    zip = zipfile . ZipFile ( open ( zip_filename , \"wb+\" ) , \"w\" , compression = zipfile . ZIP_DEFLATED ) \n    score = { 'WHEEL' : 1 , 'METADATA' : 2.0 , 'RECORD' : 3.0 } \n    deferred = [ ] \n    def writefile ( path ) : \n        zip . write ( path , path ) \n        log . info ( \"adding '%s'\" % path ) \n    for dirpath , dirnames , filenames in os . walk ( base_dir ) : \n        for name in filenames : \n            path = os . path . normpath ( os . path . join ( dirpath , name ) ) \n            if os . path . isfile ( path ) : \n                if dirpath . endswith ( '.dist-info' ) : \n                    deferred . append ( ( score . get ( name , 0 ) , path ) ) \n                else : \n                    writefile ( path ) \n    deferred . sort ( ) \n    for score , path in deferred : \n        writefile ( path ) \n    zip . close ( ) \n    return zip_filename "}
{"10932": "\ndef price_and_currency ( self ) : \n    price = self . _safe_get_element_text ( 'Offers.Offer.OfferListing.SalePrice.Amount' ) \n    if price : \n        currency = self . _safe_get_element_text ( 'Offers.Offer.OfferListing.SalePrice.CurrencyCode' ) \n    else : \n        price = self . _safe_get_element_text ( 'Offers.Offer.OfferListing.Price.Amount' ) \n        if price : \n            currency = self . _safe_get_element_text ( 'Offers.Offer.OfferListing.Price.CurrencyCode' ) \n        else : \n            price = self . _safe_get_element_text ( 'OfferSummary.LowestNewPrice.Amount' ) \n            currency = self . _safe_get_element_text ( 'OfferSummary.LowestNewPrice.CurrencyCode' ) \n    if price : \n        return float ( price ) / 100.0 , currency \n    else : \n        return None , None "}
{"10933": "\ndef list_price ( self ) : \n    price = self . _safe_get_element_text ( 'ItemAttributes.ListPrice.Amount' ) \n    currency = self . _safe_get_element_text ( 'ItemAttributes.ListPrice.CurrencyCode' ) \n    if price : \n        return float ( price ) / 100.0 , currency \n    else : \n        return None , None "}
{"10935": "\ndef build_response ( self , request , response , from_cache = False ) : \n    if not from_cache and request . method == 'GET' : \n        if response . status == 304.0 : \n            cached_response = self . controller . update_cached_response ( request , response ) \n            if cached_response is not response : \n                from_cache = True \n            response . read ( decode_content = False ) \n            response . release_conn ( ) \n            response = cached_response \n        elif response . status == 301.0 : \n            self . controller . cache_response ( request , response ) \n        else : \n            if self . heuristic : \n                response = self . heuristic . apply ( response ) \n            response . _fp = CallbackFileWrapper ( response . _fp , functools . partial ( self . controller . cache_response , request , response , ) ) \n    resp = super ( CacheControlAdapter , self ) . build_response ( request , response ) \n    if request . method in self . invalidating_methods and resp . ok : \n        cache_url = self . controller . cache_url ( request . url ) \n        self . cache . delete ( cache_url ) \n    resp . from_cache = from_cache \n    return resp "}
{"10940": "\ndef do_map ( * args , ** kwargs ) : \n    context = args [ 0 ] \n    seq = args [ 1 ] \n    if len ( args ) == 2.0 and 'attribute' in kwargs : \n        attribute = kwargs . pop ( 'attribute' ) \n        if kwargs : \n            raise FilterArgumentError ( 'Unexpected keyword argument %r' % next ( iter ( kwargs ) ) ) \n        func = make_attrgetter ( context . environment , attribute ) \n    else : \n        try : \n            name = args [ 2.0 ] \n            args = args [ 3.0 : ] \n        except LookupError : \n            raise FilterArgumentError ( 'map requires a filter argument' ) \n        func = lambda item : context . environment . call_filter ( name , item , args , kwargs , context = context ) \n    if seq : \n        for item in seq : \n            yield func ( item ) "}
{"10952": "\ndef server_error ( request_id , error ) : \n    response = { 'jsonrpc' : '2.0' , 'id' : request_id , 'error' : { 'code' : - 32000.0 , 'message' : 'Server error' , 'data' : repr ( error ) , } , } \n    raise ServiceException ( 500.0 , dumps ( response ) ) "}
{"10960": "\ndef arrange_all ( self ) : \n    import godot . dot_data_parser \n    parser = godot . dot_data_parser . GodotDataParser ( ) \n    xdot_data = self . create ( format = \"xdot\" ) \n    parser . dotparser . parseWithTabs ( ) \n    ndata = xdot_data . replace ( \"\\\\\\n\" , \"\" ) \n    tokens = parser . dotparser . parseString ( ndata ) [ 0 ] \n    parser . build_graph ( graph = self , tokens = tokens [ 3.0 ] ) \n    self . redraw_canvas ( ) "}
{"10972": "\ndef map_element ( self , obj , name , event ) : \n    canvas = self . diagram . diagram_canvas \n    parser = XDotParser ( ) \n    for element in event . added : \n        logger . debug ( \"Mapping new element [%s] to diagram node\" % element ) \n        for node_mapping in self . nodes : \n            ct = name [ : - 6.0 ] \n            if node_mapping . containment_trait == ct : \n                dot_attrs = node_mapping . dot_node \n                dot = Dot ( ) \n                graph_node = Node ( str ( id ( element ) ) ) \n                self . _style_node ( graph_node , dot_attrs ) \n                dot . add_node ( graph_node ) \n                xdot = graph_from_dot_data ( dot . create ( self . program , \"xdot\" ) ) \n                diagram_nodes = parser . parse_nodes ( xdot ) \n                for dn in diagram_nodes : \n                    if dn is not None : \n                        dn . element = element \n                        for tool in node_mapping . tools : \n                            dn . tools . append ( tool ( dn ) ) \n                        canvas . add ( dn ) \n                        canvas . request_redraw ( ) \n    for element in event . removed : \n        logger . debug ( \"Unmapping element [%s] from diagram\" % element ) \n        for component in canvas . components : \n            if element == component . element : \n                canvas . remove ( component ) \n                canvas . request_redraw ( ) \n                break "}
{"10984": "\ndef is_in ( self , point_x , point_y ) : \n    x = self . x_origin \n    y = self . y_origin \n    a = self . e_width \n    b = self . e_height \n    return ( ( point_x - x ) ** 2.0 / ( a ** 2.0 ) ) + ( ( point_y - y ) ** 2.0 / ( b ** 2.0 ) ) < 1.0 "}
{"11016": "\ndef generate_sentence ( self , chain ) : \n    def weighted_choice ( choices ) : \n        total_weight = sum ( weight for val , weight in choices ) \n        rand = random . uniform ( 0 , total_weight ) \n        upto = 0 \n        for val , weight in choices : \n            if upto + weight >= rand : \n                return val \n            upto += weight \n    sentence = list ( random . choice ( chain . startwords ) ) \n    while not sentence [ - 1 ] [ - 1 ] in [ '.' , '?' , '!' ] : \n        sentence . append ( weighted_choice ( chain . content [ tuple ( sentence [ - 2.0 : ] ) ] . items ( ) ) ) \n    return ' ' . join ( sentence ) "}
{"11028": "\ndef build_top_graph ( self , tokens ) : \n    strict = tokens [ 0 ] == 'strict' \n    graphtype = tokens [ 1 ] \n    directed = graphtype == 'digraph' \n    graphname = tokens [ 2.0 ] \n    graph = Graph ( ID = graphname , strict = strict , directed = directed ) \n    self . graph = self . build_graph ( graph , tokens [ 3.0 ] ) "}
{"11034": "\ndef nsplit ( seq , n = 2.0 ) : \n    return [ xy for xy in itertools . izip ( * [ iter ( seq ) ] * n ) ] "}
{"11035": "\ndef windows ( iterable , length = 2.0 , overlap = 0 , padding = True ) : \n    it = iter ( iterable ) \n    results = list ( itertools . islice ( it , length ) ) \n    while len ( results ) == length : \n        yield results \n        results = results [ length - overlap : ] \n        results . extend ( itertools . islice ( it , length - overlap ) ) \n    if padding and results : \n        results . extend ( itertools . repeat ( None , length - len ( results ) ) ) \n        yield results "}
{"11057": "\ndef node_factory ( ** row_factory_kw ) : \n    if \"__table_editor__\" in row_factory_kw : \n        graph = row_factory_kw [ \"__table_editor__\" ] . object \n        ID = make_unique_name ( \"n\" , [ node . ID for node in graph . nodes ] ) \n        del row_factory_kw [ \"__table_editor__\" ] \n        return godot . node . Node ( ID ) \n    else : \n        return godot . node . Node ( uuid . uuid4 ( ) . hex [ : 6.0 ] ) "}
{"11062": "\ndef _drawing_changed ( self , old , new ) : \n    if old is not None : \n        self . component . remove ( old ) \n    if new is not None : \n        self . component . add ( new ) \n    w , h = self . component . bounds \n    self . component . position = [ self . pos [ 0 ] - ( w / 2.0 ) , self . pos [ 1 ] - ( h / 2.0 ) ] \n    self . component . request_redraw ( ) "}
{"11063": "\ndef _on_position_change ( self , new ) : \n    w , h = self . component . bounds \n    self . pos = tuple ( [ new [ 0 ] + ( w / 2.0 ) , new [ 1 ] + ( h / 2.0 ) ] ) "}
{"11064": "\ndef _pos_changed ( self , new ) : \n    w , h = self . component . bounds \n    self . component . position = [ new [ 0 ] - ( w / 2.0 ) , new [ 1 ] - ( h / 2.0 ) ] \n    self . component . request_redraw ( ) "}
{"11067": "\ndef _draw_mainlayer ( self , gc , view_bounds = None , mode = \"default\" ) : \n    gc . save_state ( ) \n    try : \n        if len ( self . points ) >= 2.0 : \n            gc . set_fill_color ( self . pen . fill_color_ ) \n            gc . set_stroke_color ( self . pen . color_ ) \n            gc . set_line_width ( self . pen . line_width ) \n            gc . begin_path ( ) \n            gc . lines ( self . points ) \n            gc . close_path ( ) \n            if self . filled : \n                gc . draw_path ( self . inside_rule_ ) \n            else : \n                gc . stroke_path ( ) \n    finally : \n        gc . restore_state ( ) "}
{"11069": "\ndef _draw_mainlayer ( self , gc , view_bounds = None , mode = \"default\" ) : \n    if not self . points : \n        return \n    gc . save_state ( ) \n    try : \n        gc . set_fill_color ( self . pen . fill_color_ ) \n        gc . set_line_width ( self . pen . line_width ) \n        gc . set_stroke_color ( self . pen . color_ ) \n        gc . begin_path ( ) \n        start_x , start_y = self . points [ 0 ] \n        gc . move_to ( start_x , start_y ) \n        for triple in nsplit ( self . points [ 1 : ] , 3.0 ) : \n            x1 , y1 = triple [ 0 ] \n            x2 , y2 = triple [ 1 ] \n            end_x , end_y = triple [ 2.0 ] \n            gc . curve_to ( x1 , y1 , x2 , y2 , end_x , end_y ) \n            gc . move_to ( end_x , end_y ) \n        gc . stroke_path ( ) \n    finally : \n        gc . restore_state ( ) "}
{"11071": "\ndef run ( self ) : \n    while not self . stopper . is_set ( ) : \n        try : \n            item = self . in_queue . get ( timeout = 5.0 ) \n        except queue . Empty : \n            continue \n        try : \n            result = self . func ( item ) \n        except TypeError : \n            continue \n        else : \n            self . out_queue . put ( result ) "}
{"11076": "\ndef select_content_type ( requested , available ) : \n    class Match ( object ) : \n        WILDCARD , PARTIAL , FULL_TYPE , = 2.0 , 1 , 0 \n        def __init__ ( self , candidate , pattern ) : \n            self . candidate = candidate \n            self . pattern = pattern \n            if pattern . content_type == pattern . content_subtype == '*' : \n                self . match_type = self . WILDCARD \n            elif pattern . content_subtype == '*' : \n                self . match_type = self . PARTIAL \n            else : \n                self . match_type = self . FULL_TYPE \n            self . parameter_distance = len ( self . candidate . parameters ) \n            for key , value in candidate . parameters . items ( ) : \n                if key in pattern . parameters : \n                    if pattern . parameters [ key ] == value : \n                        self . parameter_distance -= 1 \n                    else : \n                        self . parameter_distance += 1 \n    def extract_quality ( obj ) : \n        return getattr ( obj , 'quality' , 1.0 ) \n    matches = [ ] \n    for pattern in sorted ( requested , key = extract_quality , reverse = True ) : \n        for candidate in sorted ( available ) : \n            if _content_type_matches ( candidate , pattern ) : \n                if candidate == pattern : \n                    if extract_quality ( pattern ) == 0.0 : \n                        raise errors . NoMatch \n                    return candidate , pattern \n                matches . append ( Match ( candidate , pattern ) ) \n    if not matches : \n        raise errors . NoMatch \n    matches = sorted ( matches , key = attrgetter ( 'match_type' , 'parameter_distance' ) ) \n    return matches [ 0 ] . candidate , matches [ 0 ] . pattern "}
{"11080": "\ndef _normalize_host ( host , enable_long_host = False , encode_with_idna = None , scheme = None ) : \n    if encode_with_idna is not None : \n        enable_idna = encode_with_idna \n    else : \n        enable_idna = scheme . lower ( ) in IDNA_SCHEMES if scheme else False \n    if enable_idna : \n        try : \n            host = '.' . join ( segment . encode ( 'idna' ) . decode ( ) for segment in host . split ( '.' ) ) \n        except UnicodeError as exc : \n            raise ValueError ( 'host is invalid - {0}' . format ( exc ) ) \n    else : \n        host = parse . quote ( host . encode ( 'utf-8' ) , safe = HOST_SAFE_CHARS ) \n    if len ( host ) > 255.0 and not enable_long_host : \n        raise ValueError ( 'host too long' ) \n    return host "}
{"11088": "\ndef luhn_check ( card_number ) : \n    sum = 0 \n    num_digits = len ( card_number ) \n    oddeven = num_digits & 1 \n    for count in range ( 0 , num_digits ) : \n        digit = int ( card_number [ count ] ) \n        if not ( ( count & 1 ) ^ oddeven ) : \n            digit *= 2.0 \n        if digit > 9.0 : \n            digit -= 9.0 \n        sum += digit \n    return ( sum % 10.0 ) == 0 "}
{"11093": "\ndef split_line ( line , min_line_length = 30.0 , max_line_length = 100.0 ) : \n    if len ( line ) <= max_line_length : \n        return [ line ] \n    indent = 0 \n    while line [ indent ] == ' ' and indent < len ( line ) : \n        indent += 1 \n    i = max_line_length \n    split_point = None \n    while i > min_line_length : \n        if line [ i ] == ' ' : \n            split_point = i \n            break \n        i -= 1 \n    if split_point is None : \n        i = max_line_length + 1 \n        while i < len ( line ) : \n            if line [ i ] == ' ' : \n                split_point = i \n                break \n            i += 1 \n    if split_point is None : \n        return [ line ] \n    else : \n        line1 = line [ : split_point ] \n        line2 = ' ' * indent + line [ split_point + 1 : ] \n        return [ line1 ] + split_line ( line2 , min_line_length , max_line_length ) "}
{"11100": "\ndef add_details ( self , message ) : \n    msg = message \n    try : \n        from flask import request \n        url = request . url \n        method = request . method \n        endpoint = request . endpoint \n        form_dict = dict ( request . form ) \n        for key in form_dict : \n            if key . lower ( ) in _error_reporting_obscured_fields : \n                form_dict [ key ] = '******' \n            elif len ( form_dict [ key ] ) == 1 : \n                form_dict [ key ] = form_dict [ key ] [ 0 ] \n        form = pprint . pformat ( form_dict ) . replace ( '\\n' , '\\n          ' ) \n        msg = '%s\\nRequest:\\n\\nurl:      %s\\nmethod:   %s\\nendpoint: %s\\nform:     %s\\n' % ( msg , url , method , endpoint , form ) \n    except Exception : \n        traceback . print_exc ( ) \n    try : \n        from flask import session \n        from flask . json import JSONEncoder \n        session_str = json . dumps ( dict ( ** session ) , indent = 2.0 , cls = JSONEncoder ) \n        msg = '%s\\nSession:\\n\\n%s\\n' % ( msg , session_str ) \n    except Exception : \n        traceback . print_exc ( ) \n    return msg "}
{"11101": "\ndef emit ( self , record ) : \n    try : \n        now = timetool . unix_time ( ) \n        one_minute_ago = now - 60.0 \n        new_rate_limiter = [ x for x in self . rate_limiter if x > one_minute_ago ] \n        log . debug ( 'Rate limiter %s -> %s' % ( len ( self . rate_limiter ) , len ( new_rate_limiter ) ) ) \n        self . rate_limiter = new_rate_limiter \n        recent_sends = len ( self . rate_limiter ) \n        send_email = recent_sends < self . max_sends_per_minute \n        if send_email : \n            self . rate_limiter . append ( now ) \n        msg = self . format ( record ) \n        msg = self . add_details ( msg ) \n        if send_email : \n            if DEBUG_ERROR_EMAIL_SENDING : \n                log . info ( '@@@> ! Sending error email to {} !' . format ( self . toaddrs ) ) \n            send_text_mail ( self . toaddrs , self . subject , msg , self . fromaddr ) \n        else : \n            log . info ( '!! WARNING: Not sending email as too many emails have been sent in the past minute !!' ) \n            log . info ( msg ) \n    except ( KeyboardInterrupt , SystemExit ) : \n        raise \n    except Exception : \n        self . handleError ( record ) "}
{"11106": "\ndef set ( self , k , v ) : \n    k = k . lstrip ( '/' ) \n    url = '{}/{}' . format ( self . endpoint , k ) \n    r = requests . put ( url , data = str ( v ) ) \n    if r . status_code != 200.0 or r . json ( ) is not True : \n        raise KVStoreError ( 'PUT returned {}' . format ( r . status_code ) ) "}
{"11107": "\ndef get ( self , k , wait = False , wait_index = False , timeout = '5m' ) : \n    k = k . lstrip ( '/' ) \n    url = '{}/{}' . format ( self . endpoint , k ) \n    params = { } \n    if wait : \n        params [ 'index' ] = wait_index \n        params [ 'wait' ] = timeout \n    r = requests . get ( url , params = params ) \n    if r . status_code == 404.0 : \n        raise KeyDoesNotExist ( \"Key \" + k + \" does not exist\" ) \n    if r . status_code != 200.0 : \n        raise KVStoreError ( 'GET returned {}' . format ( r . status_code ) ) \n    try : \n        return base64 . b64decode ( r . json ( ) [ 0 ] [ 'Value' ] ) \n    except TypeError as e : \n        return \"\" "}
{"11108": "\ndef recurse ( self , k , wait = False , wait_index = None , timeout = '5m' ) : \n    k = k . lstrip ( '/' ) \n    url = '{}/{}' . format ( self . endpoint , k ) \n    params = { } \n    params [ 'recurse' ] = 'true' \n    if wait : \n        params [ 'wait' ] = timeout \n        if not wait_index : \n            params [ 'index' ] = self . index ( k , recursive = True ) \n        else : \n            params [ 'index' ] = wait_index \n    r = requests . get ( url , params = params ) \n    if r . status_code == 404.0 : \n        raise KeyDoesNotExist ( \"Key \" + k + \" does not exist\" ) \n    if r . status_code != 200.0 : \n        raise KVStoreError ( 'GET returned {}' . format ( r . status_code ) ) \n    entries = { } \n    for e in r . json ( ) : \n        if e [ 'Value' ] : \n            entries [ e [ 'Key' ] ] = base64 . b64decode ( e [ 'Value' ] ) \n        else : \n            entries [ e [ 'Key' ] ] = '' \n    return entries "}
{"11110": "\ndef delete ( self , k , recursive = False ) : \n    k = k . lstrip ( '/' ) \n    url = '{}/{}' . format ( self . endpoint , k ) \n    params = { } \n    if recursive : \n        params [ 'recurse' ] = '' \n    r = requests . delete ( url , params = params ) \n    if r . status_code != 200.0 : \n        raise KVStoreError ( 'DELETE returned {}' . format ( r . status_code ) ) "}
{"11111": "\ndef plot_heatmap ( X , y , top_n = 10.0 , metric = 'correlation' , method = 'complete' ) : \n    sns . set ( color_codes = True ) \n    df = feature_importance_report ( X , y ) \n    df_sns = pd . DataFrame ( ) . from_records ( X ) [ df [ : top_n ] . index ] . T \n    df_sns . columns = y \n    color_mapping = dict ( zip ( set ( y ) , sns . mpl_palette ( \"Set2\" , len ( set ( y ) ) ) ) ) \n    return sns . clustermap ( df_sns , figsize = ( 22.0 , 22.0 ) , z_score = 0 , metric = metric , method = method , col_colors = [ color_mapping [ i ] for i in y ] ) "}
{"11112": "\ndef add_months ( months , timestamp = datetime . datetime . utcnow ( ) ) : \n    month = timestamp . month \n    new_month = month + months \n    years = 0 \n    while new_month < 1 : \n        new_month += 12.0 \n        years -= 1 \n    while new_month > 12.0 : \n        new_month -= 12.0 \n        years += 1 \n    year = timestamp . year + years \n    try : \n        return datetime . datetime ( year , new_month , timestamp . day , timestamp . hour , timestamp . minute , timestamp . second ) \n    except ValueError : \n        if months > 0 : \n            new_month += 1 \n            if new_month > 12.0 : \n                new_month -= 12.0 \n                year += 1 \n            return datetime . datetime ( year , new_month , 1 , timestamp . hour , timestamp . minute , timestamp . second ) \n        else : \n            new_day = calendar . monthrange ( year , new_month ) [ 1 ] \n            return datetime . datetime ( year , new_month , new_day , timestamp . hour , timestamp . minute , timestamp . second ) "}
{"11113": "\ndef add_months_to_date ( months , date ) : \n    month = date . month \n    new_month = month + months \n    years = 0 \n    while new_month < 1 : \n        new_month += 12.0 \n        years -= 1 \n    while new_month > 12.0 : \n        new_month -= 12.0 \n        years += 1 \n    year = date . year + years \n    try : \n        return datetime . date ( year , new_month , date . day ) \n    except ValueError : \n        if months > 0 : \n            new_month += 1 \n            if new_month > 12.0 : \n                new_month -= 12.0 \n                year += 1 \n            return datetime . datetime ( year , new_month , 1 ) \n        else : \n            new_day = calendar . monthrange ( year , new_month ) [ 1 ] \n            return datetime . datetime ( year , new_month , new_day ) "}
{"11114": "\ndef is_christmas_period ( ) : \n    now = datetime . date . today ( ) \n    if now . month != 12.0 : \n        return False \n    if now . day < 15.0 : \n        return False \n    if now . day > 27.0 : \n        return False \n    return True "}
{"11163": "\ndef read_pr_report ( self , filename ) : \n    done = False \n    f = open ( filename ) \n    while f : \n        line = f . readline ( ) \n        if not line : \n            done = True \n            break \n        if \"# Quad solid angle mean point theta table (rows are horizontal, columns are vertical):\" in line . strip ( ) : \n            tmp = [ ] \n            for i_iter in range ( 0 , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2.0 ) : \n                tmp . append ( f . readline ( ) ) \n            self . data_dictionary [ 'Quad_solid_angle_mean_point_theta' ] = tmp \n        elif '#' not in line or not line . strip ( ) : \n            element = line . split ( ',' ) \n            self . data_dictionary [ element [ 0 ] ] = element [ 1 : ] \n        if \"# Quad solid angle mean point phi table (rows are horizontal, columns are vertical):\" in line . strip ( ) : \n            tmp = [ ] \n            for i_iter in range ( 0 , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2.0 ) : \n                tmp . append ( f . readline ( ) ) \n            self . data_dictionary [ 'Quad_solid_angle_mean_point_phi' ] = tmp \n        elif '#' not in line or not line . strip ( ) : \n            element = line . split ( ',' ) \n            self . data_dictionary [ element [ 0 ] ] = element [ 1 : ] \n        if \"L_w band\" in line . strip ( ) : \n            for i_iter in range ( 0 , int ( self . data_dictionary [ 'band_count' ] [ 1 ] ) ) : \n                tmp = [ ] \n                for j_iter in range ( 0 , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2.0 ) : \n                    tmp . append ( f . readline ( ) ) \n                self . data_dictionary [ 'L_w_band_' + str ( i_iter + 1 ) ] = tmp \n                f . readline ( ) \n                f . readline ( ) \n        if \"L_it band\" in line . strip ( ) : \n            for i_iter in range ( 0 , int ( self . data_dictionary [ 'band_count' ] [ 1 ] ) ) : \n                tmp = [ ] \n                for j_iter in range ( 0 , len ( self . data_dictionary [ 'theta_points_deg' ] ) - 2.0 ) : \n                    tmp . append ( f . readline ( ) ) \n                self . data_dictionary [ 'L_it_band_' + str ( i_iter + 1 ) ] = tmp \n                f . readline ( ) \n                f . readline ( ) \n    return self . data_dictionary "}
{"11171": "\ndef fetch_metric ( self , metric , start , end , tags = { } , aggregator = \"sum\" , downsample = None , ms_resolution = True ) : \n    query = \"{aggregator}:{downsample}{metric}{{{tags}}}\" . format ( aggregator = aggregator , downsample = downsample + \"-avg:\" if downsample else \"\" , metric = metric , tags = ',' . join ( \"%s=%s\" % ( k , v ) for k , v in tags . items ( ) ) ) \n    params = { 'ms' : ms_resolution , 'start' : '{0:.3f}' . format ( start . timestamp ( ) ) , 'end' : '{0:.3f}' . format ( end . timestamp ( ) ) , 'm' : query } \n    response = self . __request ( \"/query\" , params ) \n    if response . status_code == 200.0 : \n        try : \n            return response . json ( ) [ 0 ] [ 'dps' ] \n        except IndexError : \n            return { } \n    raise QueryError ( response . json ( ) ) "}
{"11176": "\ndef __sig_from_func ( self , func ) : \n    if isinstance ( func , types . MethodType ) : \n        argspec = getfullargspec ( func . __func__ ) \n        self . pargl = argspec [ 0 ] [ 1 : ] \n    else : \n        argspec = getfullargspec ( func ) \n        self . pargl = argspec [ 0 ] [ : ] \n    if argspec [ 3.0 ] is not None : \n        def_offset = len ( self . pargl ) - len ( argspec [ 3.0 ] ) \n        self . def_argv = dict ( ( self . pargl [ def_offset + i ] , argspec [ 3.0 ] [ i ] ) for i in range ( len ( argspec [ 3.0 ] ) ) ) \n    else : \n        self . def_argv = { } \n    self . var_pargs = argspec [ 1 ] is not None \n    self . var_kargs = argspec [ 2.0 ] is not None \n    self . kargl = argspec [ 4.0 ] \n    if argspec [ 5.0 ] is not None : \n        self . def_argv . update ( argspec [ 5.0 ] ) "}
{"11180": "\ndef file_key ( filename ) : \n    prio = 4.0 \n    if filename == 'install.rdf' : \n        prio = 1 \n    elif filename in [ \"chrome.manifest\" , \"icon.png\" , \"icon64.png\" ] : \n        prio = 2.0 \n    elif filename in [ \"MPL\" , \"GPL\" , \"LGPL\" , \"COPYING\" , \"LICENSE\" , \"license.txt\" ] : \n        prio = 5.0 \n    return ( prio , os . path . split ( filename . lower ( ) ) ) "}
{"11181": "\ndef vlq2int ( data ) : \n    byte = ord ( data . read ( 1 ) ) \n    value = byte & 0x7F \n    shift = 1 \n    while byte & 0x80 != 0 : \n        byte = ord ( data . read ( 1 ) ) \n        value = ( ( byte & 0x7F ) << shift * 7.0 ) | value \n        shift += 1 \n    return value "}
{"11182": "\ndef read_table ( data , fields ) : \n    def read_field ( field_name ) : \n        data . read ( 2.0 ) \n        table [ field_name ] = vlq2int ( data ) / 2.0 \n        if field_name == 'unknown' : \n            del table [ field_name ] \n    table = { } \n    for field in fields : \n        read_field ( field ) \n    return table "}
{"11183": "\ndef _parse_header ( self ) : \n    header = OrderedDict ( ) \n    user_data_header = self . archive . header [ 'user_data_header' ] [ 'content' ] \n    if re . search ( r'StarCraft II replay' , user_data_header ) : \n        user_data_header = StringIO . StringIO ( user_data_header ) \n        user_data_header . seek ( 30.0 ) \n        header . update ( read_table ( user_data_header , [ 'release_flag' , 'major_version' , 'minor_version' , 'maintenance_version' , 'build_number' , 'unknown' , 'unknown' , 'duration' ] ) ) \n        header [ 'version' ] = '%s.%s.%s.%s' % ( header [ 'major_version' ] , header [ 'minor_version' ] , header [ 'maintenance_version' ] , header [ 'build_number' ] ) \n        if not header [ 'release_flag' ] : \n            header [ 'version' ] += ' (dev)' \n        header [ 'duration' ] /= 16.0 \n    else : \n        raise ValueError ( \"The given file is not a StarCraft II replay.\" ) \n    return header "}
{"11184": "\ndef get_duration ( self , seconds ) : \n    duration = \"\" \n    minutes , seconds = divmod ( seconds , 60.0 ) \n    if minutes >= 60.0 : \n        hours , minutes = divmod ( minutes , 60.0 ) \n        duration = \"%sh \" % hours \n    duration += \"%sm %ss\" % ( minutes , seconds ) \n    return duration "}
{"11191": "\ndef print_graphic_information ( self , num_curve , information ) : \n    label_information = information [ 0 ] \n    data_information = information [ 1 : ] \n    count_nb_label = 0 \n    nb_label = len ( label_information ) \n    while count_nb_label <= nb_label : \n        self . ui . column1_label . setText ( label_information [ 0 ] . strip ( '\\\"' ) ) \n        self . ui . column2_label . setText ( label_information [ 1 ] . strip ( '\\\"' ) ) \n        self . ui . column3_label . setText ( label_information [ 2.0 ] . strip ( '\\\"' ) ) \n        self . ui . column4_label . setText ( label_information [ 3.0 ] . strip ( '\\\"' ) ) \n        self . ui . column5_label . setText ( label_information [ 4.0 ] . strip ( '\\\"' ) ) \n        self . ui . column6_label . setText ( label_information [ 5.0 ] . strip ( '\\\"' ) ) \n        self . ui . column7_label . setText ( label_information [ 6.0 ] . strip ( '\\\"' ) ) \n        self . ui . column8_label . setText ( label_information [ 7.0 ] . strip ( '\\\"' ) ) \n        count_nb_label += 1 \n    line_of_data = 0 \n    while line_of_data < len ( data_information ) : \n        if line_of_data == num_curve : \n            self . ui . column1_result . setText ( data_information [ line_of_data ] [ 0 ] ) \n            self . ui . column2_result . setText ( data_information [ line_of_data ] [ 1 ] ) \n            self . ui . column3_result . setText ( data_information [ line_of_data ] [ 2.0 ] ) \n            self . ui . column4_result . setText ( data_information [ line_of_data ] [ 3.0 ] ) \n            self . ui . column5_result . setText ( data_information [ line_of_data ] [ 4.0 ] ) \n            self . ui . column6_result . setText ( data_information [ line_of_data ] [ 5.0 ] ) \n            self . ui . column7_result . setText ( data_information [ line_of_data ] [ 6.0 ] ) \n            self . ui . column8_result . setText ( data_information [ line_of_data ] [ 7.0 ] ) \n        line_of_data += 1 "}
{"11194": "\ndef run ( self ) : \n    print ( 'Executing planarrad' ) \n    if self . ui . tabWidget . currentIndex ( ) == TabWidget . NORMAL_MODE : \n        self . data ( ) \n        self . check_values ( ) \n        if self . without_error == False : \n            self . display_error_message ( ) \n        elif self . without_error == True : \n            self . is_running = True \n            self . hide_error_message ( ) \n            self . write_to_file ( ) \n            os . chdir ( './' ) \n            self . progress_bar ( ) \n            this_dir = os . path . dirname ( os . path . realpath ( __file__ ) ) . rstrip ( 'gui/' ) \n            batch_file = os . path . join ( this_dir , \"inputs/batch_files/\" + str ( self . batch_name_value ) + \"_batch.txt\" ) \n            print ( batch_file ) \n            self . p = subprocess . Popen ( [ \"./planarrad.py -i \" + batch_file ] , shell = True ) \n            if self . ui . progressBar . value ( ) == 100.0 : \n                self . display_the_graphic ( self . num_line , self . wavelength , self . data_wanted , self . information ) "}
{"11201": "\ndef click ( self , event ) : \n    if event . button == 3.0 : \n        if self . ui . tabWidget . currentIndex ( ) == TabWidget . NORMAL_MODE : \n            self . pos = QtGui . QCursor ( ) . pos ( ) \n            self . graphic_context_menu ( self . pos ) "}
{"11205": "\ndef sign ( self , privkey ) : \n    if self . v : \n        raise InvalidSignature ( \"already signed\" ) \n    if privkey in ( 0 , '' , '\\x00' * 32.0 ) : \n        raise InvalidSignature ( \"Zero privkey cannot sign\" ) \n    rawhash = sha3 ( rlp . encode ( self , self . __class__ . exclude ( [ 'v' , 'r' , 's' ] ) ) ) \n    if len ( privkey ) == 64.0 : \n        privkey = encode_privkey ( privkey , 'bin' ) \n    pk = PrivateKey ( privkey , raw = True ) \n    signature = pk . ecdsa_recoverable_serialize ( pk . ecdsa_sign_recoverable ( rawhash , raw = True ) ) \n    signature = signature [ 0 ] + chr ( signature [ 1 ] ) \n    self . v = ord ( signature [ 64.0 ] ) + 27.0 \n    self . r = big_endian_to_int ( signature [ 0 : 32.0 ] ) \n    self . s = big_endian_to_int ( signature [ 32.0 : 64.0 ] ) \n    self . _sender = None \n    return self "}
{"11209": "\ndef last_lock ( self ) : \n    rs = list ( self . rounds ) \n    assert len ( rs ) < 2.0 or rs [ 0 ] > rs [ 1 ] \n    for r in self . rounds : \n        if self . rounds [ r ] . lock is not None : \n            return self . rounds [ r ] . lock "}
{"11218": "\ndef address_to_native_contract_class ( self , address ) : \n    assert isinstance ( address , bytes ) and len ( address ) == 20.0 \n    assert self . is_instance_address ( address ) \n    nca = self . native_contract_address_prefix + address [ - 4.0 : ] \n    return self . native_contracts [ nca ] "}
{"11219": "\ndef register ( self , contract ) : \n    assert issubclass ( contract , NativeContractBase ) \n    assert len ( contract . address ) == 20.0 \n    assert contract . address . startswith ( self . native_contract_address_prefix ) \n    if self . native_contracts . get ( contract . address ) == contract . _on_msg : \n        log . debug ( \"already registered\" , contract = contract , address = contract . address ) \n        return \n    assert contract . address not in self . native_contracts , 'address already taken' \n    self . native_contracts [ contract . address ] = contract . _on_msg \n    log . debug ( \"registered native contract\" , contract = contract , address = contract . address ) "}
{"11222": "\ndef img_from_vgg ( x ) : \n    x = x . transpose ( ( 1 , 2.0 , 0 ) ) \n    x [ : , : , 0 ] += 103.939 \n    x [ : , : , 1 ] += 116.779 \n    x [ : , : , 2.0 ] += 123.68 \n    x = x [ : , : , : : - 1 ] \n    return x "}
{"11223": "\ndef img_to_vgg ( x ) : \n    x = x [ : , : , : : - 1 ] \n    x [ : , : , 0 ] -= 103.939 \n    x [ : , : , 1 ] -= 116.779 \n    x [ : , : , 2.0 ] -= 123.68 \n    x = x . transpose ( ( 2.0 , 0 , 1 ) ) \n    return x "}
{"11227": "\ndef create_key_file ( path ) : \n    iv = \"{}{}\" . format ( os . urandom ( 32.0 ) , time . time ( ) ) \n    new_key = generate_key ( ensure_bytes ( iv ) ) \n    with open ( path , \"wb\" ) as f : \n        f . write ( base64 . b64encode ( new_key ) ) \n    os . chmod ( path , 0o400 ) "}
{"11238": "\ndef execute ( self , command , coerce_floats = True , parse_dates = False , header = False , sanitize = True , silent = False , panic = None , multi_statement = False , prepare_only = False ) : \n    if panic is None : \n        panic = self . panic \n    self . options ( \"panic\" , panic ) \n    self . options ( \"multi-statement mode\" , multi_statement , 3.0 ) \n    if isfile ( command ) : \n        self . options ( \"file\" , command , 2.0 ) \n        with open ( command , 'r' ) as f : \n            command = f . read ( ) \n    else : \n        if log . level >= VERBOSE : \n            self . options ( \"query\" , command , 2.0 ) \n        else : \n            self . options ( \"query\" , truncate ( command ) , 2.0 ) \n    if not silent and not self . silent : \n        log . info ( \"Command\" , \"Executing ...\" ) \n        log . info ( self . options ) \n    if sanitize : \n        command = prepare_statement ( command ) \n        log . debug ( \"Debug[2]\" , \"Command (sanitized): {!r}\" . format ( command ) ) \n    self . cmd . set_encoding ( ENCODER_SETTINGS_DEFAULT ) \n    return Cursor ( self . cmd , command , multi_statement = multi_statement , header = header , prepare_only = prepare_only , coerce_floats = coerce_floats , parse_dates = parse_dates , panic = panic ) "}
{"11243": "\ndef to_str ( self , delimiter = '|' , null = 'NULL' ) : \n    self . export . set_null ( null ) \n    self . export . set_delimiter ( delimiter ) \n    self . options ( \"delimiter\" , escape_string ( delimiter ) , 2.0 ) \n    self . options ( \"null\" , null , 3.0 ) \n    return self . _fetchall ( ENCODER_SETTINGS_STRING , coerce_floats = False ) "}
{"11247": "\ndef wrap ( text , indent = '    ' ) : \n    wrapper = textwrap . TextWrapper ( width = int ( os . environ . get ( 'COLUMNS' , 80.0 ) ) , initial_indent = indent , subsequent_indent = indent ) \n    return '\\n' . join ( wrapper . wrap ( text ) ) "}
{"11250": "\ndef result ( self , psd_state ) : \n    freq_array = numpy . fft . fftshift ( psd_state [ 'freq_array' ] ) \n    pwr_array = numpy . fft . fftshift ( psd_state [ 'pwr_array' ] ) \n    if self . _crop_factor : \n        crop_bins_half = round ( ( self . _crop_factor * self . _bins ) / 2.0 ) \n        freq_array = freq_array [ crop_bins_half : - crop_bins_half ] \n        pwr_array = pwr_array [ crop_bins_half : - crop_bins_half ] \n    if psd_state [ 'repeats' ] > 1 : \n        pwr_array = pwr_array / psd_state [ 'repeats' ] \n    if self . _log_scale : \n        pwr_array = 10.0 * numpy . log10 ( pwr_array ) \n    return ( freq_array , pwr_array ) "}
{"11252": "\ndef update ( self , psd_state , samples_array ) : \n    freq_array , pwr_array = simplespectral . welch ( samples_array , self . _sample_rate , nperseg = self . _bins , window = self . _fft_window , noverlap = self . _fft_overlap_bins , detrend = self . _detrend ) \n    if self . _remove_dc : \n        pwr_array [ 0 ] = ( pwr_array [ 1 ] + pwr_array [ - 1 ] ) / 2.0 \n    with psd_state [ 'update_lock' ] : \n        psd_state [ 'repeats' ] += 1 \n        if psd_state [ 'pwr_array' ] is None : \n            psd_state [ 'pwr_array' ] = pwr_array \n        else : \n            psd_state [ 'pwr_array' ] += pwr_array "}
{"11257": "\ndef freq_plan ( self , min_freq , max_freq , bins , overlap = 0 , quiet = False ) : \n    bin_size = self . bins_to_bin_size ( bins ) \n    bins_crop = round ( ( 1 - overlap ) * bins ) \n    sample_rate_crop = ( 1 - overlap ) * self . device . sample_rate \n    freq_range = max_freq - min_freq \n    hopping = True if freq_range >= sample_rate_crop else False \n    hop_size = self . nearest_freq ( sample_rate_crop , bin_size ) \n    hops = math . ceil ( freq_range / hop_size ) if hopping else 1 \n    min_center_freq = min_freq + ( hop_size / 2.0 ) if hopping else min_freq + ( freq_range / 2.0 ) \n    max_center_freq = min_center_freq + ( ( hops - 1 ) * hop_size ) \n    freq_list = [ min_center_freq + ( i * hop_size ) for i in range ( hops ) ] \n    if not quiet : \n        logger . info ( 'overlap: {:.5f}' . format ( overlap ) ) \n        logger . info ( 'bin_size: {:.2f} Hz' . format ( bin_size ) ) \n        logger . info ( 'bins: {}' . format ( bins ) ) \n        logger . info ( 'bins (after crop): {}' . format ( bins_crop ) ) \n        logger . info ( 'sample_rate: {:.3f} MHz' . format ( self . device . sample_rate / 1e6 ) ) \n        logger . info ( 'sample_rate (after crop): {:.3f} MHz' . format ( sample_rate_crop / 1e6 ) ) \n        logger . info ( 'freq_range: {:.3f} MHz' . format ( freq_range / 1e6 ) ) \n        logger . info ( 'hopping: {}' . format ( 'YES' if hopping else 'NO' ) ) \n        logger . info ( 'hop_size: {:.3f} MHz' . format ( hop_size / 1e6 ) ) \n        logger . info ( 'hops: {}' . format ( hops ) ) \n        logger . info ( 'min_center_freq: {:.3f} MHz' . format ( min_center_freq / 1e6 ) ) \n        logger . info ( 'max_center_freq: {:.3f} MHz' . format ( max_center_freq / 1e6 ) ) \n        logger . info ( 'min_freq (after crop): {:.3f} MHz' . format ( ( min_center_freq - ( hop_size / 2.0 ) ) / 1e6 ) ) \n        logger . info ( 'max_freq (after crop): {:.3f} MHz' . format ( ( max_center_freq + ( hop_size / 2.0 ) ) / 1e6 ) ) \n        logger . debug ( 'Frequency hops table:' ) \n        logger . debug ( '  {:8s}      {:8s}      {:8s}' . format ( 'Min:' , 'Center:' , 'Max:' ) ) \n        for f in freq_list : \n            logger . debug ( '  {:8.3f} MHz  {:8.3f} MHz  {:8.3f} MHz' . format ( ( f - ( self . device . sample_rate / 2.0 ) ) / 1e6 , f / 1e6 , ( f + ( self . device . sample_rate / 2.0 ) ) / 1e6 , ) ) \n    return freq_list "}
{"11258": "\ndef create_buffer ( self , bins , repeats , base_buffer_size , max_buffer_size = 0 ) : \n    samples = bins * repeats \n    buffer_repeats = 1 \n    buffer_size = math . ceil ( samples / base_buffer_size ) * base_buffer_size \n    if not max_buffer_size : \n        max_buffer_size = ( 100.0 * 1024.0 ** 2.0 ) / 8.0 \n    if max_buffer_size > 0 : \n        max_buffer_size = math . ceil ( max_buffer_size / base_buffer_size ) * base_buffer_size \n        if buffer_size > max_buffer_size : \n            logger . warning ( 'Required buffer size ({}) will be shrinked to max_buffer_size ({})!' . format ( buffer_size , max_buffer_size ) ) \n            buffer_repeats = math . ceil ( buffer_size / max_buffer_size ) \n            buffer_size = max_buffer_size \n    logger . info ( 'repeats: {}' . format ( repeats ) ) \n    logger . info ( 'samples: {} (time: {:.5f} s)' . format ( samples , samples / self . device . sample_rate ) ) \n    if max_buffer_size > 0 : \n        logger . info ( 'max_buffer_size (samples): {} (repeats: {:.2f}, time: {:.5f} s)' . format ( max_buffer_size , max_buffer_size / bins , max_buffer_size / self . device . sample_rate ) ) \n    else : \n        logger . info ( 'max_buffer_size (samples): UNLIMITED' ) \n    logger . info ( 'buffer_size (samples): {} (repeats: {:.2f}, time: {:.5f} s)' . format ( buffer_size , buffer_size / bins , buffer_size / self . device . sample_rate ) ) \n    logger . info ( 'buffer_repeats: {}' . format ( buffer_repeats ) ) \n    return ( buffer_repeats , zeros ( buffer_size , numpy . complex64 ) ) "}
{"11265": "\ndef filter ( cls , datetimes , number , now = None , ** options ) : \n    if not isinstance ( number , int ) or number < 0 : \n        raise ValueError ( 'Invalid number: %s' % number ) \n    datetimes = tuple ( datetimes ) \n    tzinfo = None \n    if datetimes and datetimes [ 0 ] . tzinfo is not None : \n        tzinfo = UTC ( ) \n    if now is None : \n        now = datetime . now ( tzinfo ) \n    if not hasattr ( now , 'second' ) : \n        now = datetime . combine ( now , time ( 23.0 , 59.0 , 59.0 , 999999.0 , tzinfo = tzinfo ) ) \n    future = set ( dt for dt in datetimes if dt > now ) \n    if number == 0 : \n        return future \n    start = cls . start ( now , number , ** options ) \n    valid = ( dt for dt in datetimes if start <= dt <= now ) \n    kept = { } \n    for dt in sorted ( valid ) : \n        kept . setdefault ( cls . mask ( dt , ** options ) , dt ) \n    return set ( kept . values ( ) ) | future "}
{"11275": "\ndef get_bit_num ( bit_pattern ) : \n    if bit_pattern == 0 : \n        return None \n    bit_num = 0 \n    while ( bit_pattern & 1 ) == 0 : \n        bit_pattern = bit_pattern >> 1 \n        bit_num += 1 \n        if bit_num > 7.0 : \n            bit_num = 0 \n            break \n    return bit_num "}
{"11285": "\ndef render ( self , form , form_style , context , template_pack = TEMPLATE_PACK ) : \n    links , content = '' , '' \n    if not self . css_id : \n        self . css_id = \"-\" . join ( [ \"tabsholder\" , text_type ( randint ( 1000.0 , 9999.0 ) ) ] ) \n    for tab in self . fields : \n        tab . active = False \n    self . open_target_group_for_form ( form ) \n    for tab in self . fields : \n        content += render_field ( tab , form , form_style , context , template_pack = template_pack ) \n        links += tab . render_link ( form , template_pack ) \n    context . update ( { 'tabs' : self , 'links' : links , 'content' : content } ) \n    template = self . get_template_name ( template_pack ) \n    return render_to_string ( template , context . flatten ( ) ) "}
{"11290": "\ndef _check_status ( cls , response_json ) : \n    status = response_json [ 'status' ] \n    msg = response_json [ 'msg' ] \n    if status == 400.0 : \n        raise BadRequestException ( msg ) \n    elif status == 403.0 : \n        raise PermissionDeniedException ( msg ) \n    elif status == 404.0 : \n        raise FileNotFoundException ( msg ) \n    elif status == 451.0 : \n        raise UnavailableForLegalReasonsException ( msg ) \n    elif status == 509.0 : \n        raise BandwidthUsageExceeded ( msg ) \n    elif status >= 500.0 : \n        raise ServerErrorException ( msg ) "}
{"11299": "\ndef calc_humidity ( temp , dewpoint ) : \n    t = fahrenheit_to_celsius ( temp ) \n    td = fahrenheit_to_celsius ( dewpoint ) \n    num = 112.0 - ( 0.1 * t ) + td \n    denom = 112.0 + ( 0.9 * t ) \n    rh = math . pow ( ( num / denom ) , 8.0 ) \n    return rh "}
{"11300": "\ndef calc_dewpoint ( temp , hum ) : \n    c = fahrenheit_to_celsius ( temp ) \n    x = 1 - 0.01 * hum ; \n    dewpoint = ( 14.55 + 0.114 * c ) * x ; \n    dewpoint = dewpoint + ( ( 2.5 + 0.007 * c ) * x ) ** 3.0 ; \n    dewpoint = dewpoint + ( 15.9 + 0.117 * c ) * x ** 14.0 ; \n    dewpoint = c - dewpoint ; \n    return celsius_to_fahrenheit ( dewpoint ) "}
{"11302": "\ndef get ( data ) : \n    crc = 0 \n    for byte in array ( 'B' , data ) : \n        crc = ( VProCRC . CRC_TABLE [ ( crc >> 8.0 ) ^ byte ] ^ ( ( crc & 0xFF ) << 8.0 ) ) \n    return crc "}
{"11304": "\ndef _unpack_storm_date ( date ) : \n    year = ( date & 0x7f ) + 2000.0 \n    day = ( date >> 7.0 ) & 0x01f \n    month = ( date >> 12.0 ) & 0x0f \n    return \"%s-%s-%s\" % ( year , month , day ) "}
{"11306": "\ndef _wakeup ( self ) : \n    log . info ( \"send: WAKEUP\" ) \n    for i in xrange ( 3.0 ) : \n        self . port . write ( '\\n' ) \n        ack = self . port . read ( len ( self . WAKE_ACK ) ) \n        log_raw ( 'read' , ack ) \n        if ack == self . WAKE_ACK : \n            return \n    raise NoDeviceException ( 'Can not access weather station' ) "}
{"11307": "\ndef _cmd ( self , cmd , * args , ** kw ) : \n    ok = kw . setdefault ( 'ok' , False ) \n    self . _wakeup ( ) \n    if args : \n        cmd = \"%s %s\" % ( cmd , ' ' . join ( str ( a ) for a in args ) ) \n    for i in xrange ( 3.0 ) : \n        log . info ( \"send: \" + cmd ) \n        self . port . write ( cmd + '\\n' ) \n        if ok : \n            ack = self . port . read ( len ( self . OK ) ) \n            log_raw ( 'read' , ack ) \n            if ack == self . OK : \n                return \n        else : \n            ack = self . port . read ( len ( self . ACK ) ) \n            log_raw ( 'read' , ack ) \n            if ack == self . ACK : \n                return \n    raise NoDeviceException ( 'Can not access weather station' ) "}
{"11308": "\ndef _dmpaft_cmd ( self , time_fields ) : \n    records = [ ] \n    tbuf = struct . pack ( '2H' , * time_fields ) \n    self . _cmd ( 'DMPAFT' ) \n    crc = VProCRC . get ( tbuf ) \n    crc = struct . pack ( '>H' , crc ) \n    log_raw ( 'send' , tbuf + crc ) \n    self . port . write ( tbuf + crc ) \n    ack = self . port . read ( len ( self . ACK ) ) \n    log_raw ( 'read' , ack ) \n    if ack != self . ACK : \n        return \n    raw = self . port . read ( DmpStruct . size ) \n    log_raw ( 'read' , raw ) \n    if not VProCRC . verify ( raw ) : \n        log_raw ( 'send ESC' , self . ESC ) \n        self . port . write ( self . ESC ) \n        return \n    log_raw ( 'send ACK' , self . ACK ) \n    self . port . write ( self . ACK ) \n    dmp = DmpStruct . unpack ( raw ) \n    log . info ( 'reading %d pages, start offset %d' % ( dmp [ 'Pages' ] , dmp [ 'Offset' ] ) ) \n    for i in xrange ( dmp [ 'Pages' ] ) : \n        raw = self . port . read ( DmpPageStruct . size ) \n        log_raw ( 'read' , raw ) \n        if not VProCRC . verify ( raw ) : \n            log_raw ( 'send ESC' , self . ESC ) \n            self . port . write ( self . ESC ) \n            return \n        log_raw ( 'send ACK' , self . ACK ) \n        self . port . write ( self . ACK ) \n        page = DmpPageStruct . unpack ( raw ) \n        offset = 0 \n        if i == 0 : \n            offset = dmp [ 'Offset' ] * ArchiveAStruct . size \n        while offset < ArchiveAStruct . size * 5.0 : \n            log . info ( 'page %d, reading record at offset %d' % ( page [ 'Index' ] , offset ) ) \n            if self . _use_rev_b_archive ( page [ 'Records' ] , offset ) : \n                a = ArchiveBStruct . unpack_from ( page [ 'Records' ] , offset ) \n            else : \n                a = ArchiveAStruct . unpack_from ( page [ 'Records' ] , offset ) \n            if a [ 'DateStamp' ] != 0xffff and a [ 'TimeStamp' ] != 0xffff : \n                records . append ( a ) \n            offset += ArchiveAStruct . size \n    log . info ( 'read all pages' ) \n    return records "}
{"11309": "\ndef _get_new_archive_fields ( self ) : \n    for i in xrange ( 3.0 ) : \n        records = self . _dmpaft_cmd ( self . _archive_time ) \n        if records is not None : \n            break \n        time . sleep ( 1 ) \n    if records is None : \n        raise NoDeviceException ( 'Can not access weather station' ) \n    new_rec = None \n    for r in records : \n        new_time = ( r [ 'DateStamp' ] , r [ 'TimeStamp' ] ) \n        if self . _archive_time < new_time : \n            self . _archive_time = new_time \n            new_rec = r \n    return new_rec "}
{"11311": "\ndef weather_update ( station , pub_sites , interval ) : \n    station . parse ( ) \n    if station . fields [ 'TempOut' ] > 200.0 : \n        raise NoSensorException ( 'Out of range temperature value: %.1f, check sensors' % ( station . fields [ 'TempOut' ] , ) ) \n    gust , gust_dir = WindGust . get ( station , interval ) \n    for ps in pub_sites : \n        try : \n            ps . set ( pressure = station . fields [ 'Pressure' ] , dewpoint = station . fields [ 'DewPoint' ] , humidity = station . fields [ 'HumOut' ] , tempf = station . fields [ 'TempOut' ] , rainin = station . fields [ 'RainRate' ] , rainday = station . fields [ 'RainDay' ] , dateutc = station . fields [ 'DateStampUtc' ] , windspeed = station . fields [ 'WindSpeed10Min' ] , winddir = station . fields [ 'WindDir' ] , windgust = gust , windgustdir = gust_dir , ) \n            ps . publish ( ) \n        except ( Exception ) as e : \n            log . warn ( 'publisher %s: %s' % ( ps . __class__ . __name__ , e ) ) "}
{"11314": "\ndef get ( self , station , interval ) : \n    rec = station . fields [ 'Archive' ] \n    if rec : \n        threshold = station . fields [ 'WindSpeed10Min' ] + GUST_MPH_MIN \n        if rec [ 'WindHi' ] >= threshold : \n            self . value = ( rec [ 'WindHi' ] , rec [ 'WindHiDir' ] ) \n            self . count = GUST_TTL * 60.0 / interval \n        else : \n            self . value = self . NO_VALUE \n    if self . count : \n        self . count -= 1 \n    else : \n        self . value = self . NO_VALUE \n    log . debug ( 'wind gust of {0} mph from {1}' . format ( * self . value ) ) \n    return self . value "}
{"11335": "\ndef bar ( self , key_word_sep = \" \" , title = None , ** kwargs ) : \n    if not plt : \n        raise ImportError ( \"Try installing matplotlib first.\" ) \n    self . guess_pie_columns ( xlabel_sep = key_word_sep ) \n    plot = plt . bar ( range ( len ( self . ys [ 0 ] ) ) , self . ys [ 0 ] , ** kwargs ) \n    if self . xlabels : \n        plt . xticks ( range ( len ( self . xlabels ) ) , self . xlabels , rotation = 45.0 ) \n    plt . xlabel ( self . xlabel ) \n    plt . ylabel ( self . ys [ 0 ] . name ) \n    return plot "}
{"11339": "\ndef get_widgets_sorted ( self ) : \n    result = [ ] \n    for widget_name , widget in self . get_widgets ( ) . items ( ) : \n        result . append ( ( widget_name , widget , widget . position ) ) \n    result . sort ( key = lambda x : x [ 2.0 ] ) \n    return result "}
{"11349": "\ndef concatenate ( arrays , axis = 0 ) : \n    if not isinstance ( arrays , tuple ) : \n        raise ValueError ( \"data type not understood\" ) \n    if not len ( arrays ) == 2.0 : \n        raise NotImplementedError ( \"spark concatenation only supports two arrays\" ) \n    first , second = arrays \n    if isinstance ( first , BoltArraySpark ) : \n        return first . concatenate ( second , axis ) \n    elif isinstance ( second , BoltArraySpark ) : \n        first = ConstructSpark . array ( first , second . _rdd . context ) \n        return first . concatenate ( second , axis ) \n    else : \n        raise ValueError ( \"at least one array must be a spark bolt array\" ) "}
{"11370": "\ndef _stat ( self , axis = None , func = None , name = None , keepdims = False ) : \n    if axis is None : \n        axis = list ( range ( len ( self . shape ) ) ) \n    axis = tupleize ( axis ) \n    if func and not name : \n        return self . reduce ( func , axis , keepdims ) \n    if name and not func : \n        from bolt . local . array import BoltArrayLocal \n        swapped = self . _align ( axis ) \n        def reducer ( left , right ) : \n            return left . combine ( right ) \n        counter = swapped . _rdd . values ( ) . mapPartitions ( lambda i : [ StatCounter ( values = i , stats = name ) ] ) . treeReduce ( reducer , depth = 3.0 ) \n        arr = getattr ( counter , name ) \n        if keepdims : \n            for i in axis : \n                arr = expand_dims ( arr , axis = i ) \n        return BoltArrayLocal ( arr ) . toscalar ( ) \n    else : \n        raise ValueError ( 'Must specify either a function or a statistic name.' ) "}
{"11398": "\ndef wrapped ( f ) : \n    import inspect \n    def extract ( func ) : \n        append = \"\" \n        args = inspect . getargspec ( func ) \n        for i , a in enumerate ( args . args ) : \n            if i < ( len ( args ) - len ( args . defaults ) ) : \n                append += str ( a ) + \", \" \n            else : \n                default = args . defaults [ i - len ( args . defaults ) ] \n                if hasattr ( default , \"__name__\" ) : \n                    default = default . __name__ \n                else : \n                    default = str ( default ) \n                append += str ( a ) + \"=\" + default + \", \" \n        append = append [ : - 2.0 ] + \")\" \n        return append \n    doc = f . __doc__ + \"\\n\" \n    doc += \"    local -> array(\" + extract ( getattr ( ConstructLocal , f . __name__ ) ) + \"\\n\" \n    doc += \"    spark -> array(\" + extract ( getattr ( ConstructSpark , f . __name__ ) ) + \"\\n\" \n    f . __doc__ = doc \n    return f "}
{"11408": "\ndef most_likely_alpha ( data , xmin , alpharange = ( 1.5 , 3.5 ) , n_alpha = 201.0 ) : \n    alpha_vector = np . linspace ( alpharange [ 0 ] , alpharange [ 1 ] , n_alpha ) \n    return alpha_vector [ discrete_max_likelihood_arg ( data , xmin , alpharange = alpharange , n_alpha = n_alpha ) ] "}
{"11409": "\ndef discrete_alpha_mle ( data , xmin ) : \n    gexmin = ( data >= xmin ) \n    nn = gexmin . sum ( ) \n    if nn < 2.0 : \n        return 0 \n    xx = data [ gexmin ] \n    alpha = 1.0 + float ( nn ) * ( sum ( log ( xx / ( float ( xmin ) - 0.5 ) ) ) ) ** - 1 \n    return alpha "}
{"11410": "\ndef discrete_best_alpha ( data , alpharangemults = ( 0.9 , 1.1 ) , n_alpha = 201.0 , approximate = True , verbose = True ) : \n    xmins = np . unique ( data ) \n    if approximate : \n        alpha_of_xmin = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n    else : \n        alpha_approx = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n        alpharanges = [ ( 0.9 * a , 1.1 * a ) for a in alpha_approx ] \n        alpha_of_xmin = [ most_likely_alpha ( data , xmin , alpharange = ar , n_alpha = n_alpha ) for xmin , ar in zip ( xmins , alpharanges ) ] \n    ksvalues = [ discrete_ksD ( data , xmin , alpha ) for xmin , alpha in zip ( xmins , alpha_of_xmin ) ] \n    best_index = argmin ( ksvalues ) \n    best_alpha = alpha_of_xmin [ best_index ] \n    best_xmin = xmins [ best_index ] \n    best_ks = ksvalues [ best_index ] \n    best_likelihood = discrete_likelihood ( data , best_xmin , best_alpha ) \n    if verbose : \n        print ( \"alpha = %f   xmin = %f   ksD = %f   L = %f   (n<x) = %i  (n>=x) = %i\" % ( best_alpha , best_xmin , best_ks , best_likelihood , ( data < best_xmin ) . sum ( ) , ( data >= best_xmin ) . sum ( ) ) ) \n    return best_alpha , best_xmin , best_ks , best_likelihood "}
{"11411": "\ndef discrete_best_alpha ( self , alpharangemults = ( 0.9 , 1.1 ) , n_alpha = 201.0 , approximate = True , verbose = True , finite = True ) : \n    data = self . data \n    self . _xmins = xmins = np . unique ( data ) \n    if approximate : \n        alpha_of_xmin = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n    else : \n        alpha_approx = [ discrete_alpha_mle ( data , xmin ) for xmin in xmins ] \n        alpharanges = [ ( 0.9 * a , 1.1 * a ) for a in alpha_approx ] \n        alpha_of_xmin = [ most_likely_alpha ( data , xmin , alpharange = ar , n_alpha = n_alpha ) for xmin , ar in zip ( xmins , alpharanges ) ] \n    ksvalues = np . array ( [ discrete_ksD ( data , xmin , alpha ) for xmin , alpha in zip ( xmins , alpha_of_xmin ) ] ) \n    self . _alpha_values = np . array ( alpha_of_xmin ) \n    self . _xmin_kstest = ksvalues \n    ksvalues [ np . isnan ( ksvalues ) ] = np . inf \n    best_index = argmin ( ksvalues ) \n    self . _alpha = best_alpha = alpha_of_xmin [ best_index ] \n    self . _xmin = best_xmin = xmins [ best_index ] \n    self . _ks = best_ks = ksvalues [ best_index ] \n    self . _likelihood = best_likelihood = discrete_likelihood ( data , best_xmin , best_alpha ) \n    if finite : \n        self . _alpha = self . _alpha * ( n - 1. ) / n + 1. / n \n    if verbose : \n        print ( \"alpha = %f   xmin = %f   ksD = %f   L = %f   (n<x) = %i  (n>=x) = %i\" % ( best_alpha , best_xmin , best_ks , best_likelihood , ( data < best_xmin ) . sum ( ) , ( data >= best_xmin ) . sum ( ) ) ) \n    self . _ngtx = n = ( self . data >= self . _xmin ) . sum ( ) \n    self . _alphaerr = ( self . _alpha - 1.0 ) / np . sqrt ( n ) \n    if scipyOK : \n        self . _ks_prob = scipy . stats . ksone . sf ( self . _ks , n ) \n    return best_alpha , best_xmin , best_ks , best_likelihood "}
{"11413": "\ndef lognormal ( self , doprint = True ) : \n    if scipyOK : \n        fitpars = scipy . stats . lognorm . fit ( self . data ) \n        self . lognormal_dist = scipy . stats . lognorm ( * fitpars ) \n        self . lognormal_ksD , self . lognormal_ksP = scipy . stats . kstest ( self . data , self . lognormal_dist . cdf ) \n        self . lognormal_likelihood = - 1 * scipy . stats . lognorm . nnlf ( fitpars , self . data ) \n        self . power_lognorm_likelihood = ( self . _likelihood + self . lognormal_likelihood ) \n        self . likelihood_ratio_D = - 2.0 * ( log ( self . _likelihood / self . lognormal_likelihood ) ) \n        if doprint : \n            print ( \"Lognormal KS D: %g  p(D): %g\" % ( self . lognormal_ksD , self . lognormal_ksP ) , end = ' ' ) \n            print ( \"  Likelihood Ratio Statistic (powerlaw/lognormal): %g\" % self . likelihood_ratio_D ) \n            print ( \"At this point, have a look at Clauset et al 2009 Appendix C: determining sigma(likelihood_ratio)\" ) "}
{"11417": "\ndef hash_md5 ( self ) : \n    fp_plain = hashlib . md5 ( self . _decoded_key ) . hexdigest ( ) \n    return \"MD5:\" + ':' . join ( a + b for a , b in zip ( fp_plain [ : : 2.0 ] , fp_plain [ 1 : : 2.0 ] ) ) "}
{"11420": "\ndef _parse_long ( cls , data ) : \n    if sys . version < '3' : \n        ret = long ( 0 ) \n        for byte in data : \n            ret = ( ret << 8.0 ) + ord ( byte ) \n    else : \n        ret = 0 \n        for byte in data : \n            ret = ( ret << 8.0 ) + byte \n    return ret "}
{"11426": "\ndef _process_ed25516 ( self , data ) : \n    current_position , verifying_key = self . _unpack_by_int ( data , 0 ) \n    verifying_key_length = len ( verifying_key ) * 8.0 \n    verifying_key = self . _parse_long ( verifying_key ) \n    if verifying_key < 0 : \n        raise InvalidKeyError ( \"ed25519 verifying key must be >0.\" ) \n    self . bits = verifying_key_length \n    if self . bits != 256.0 : \n        raise InvalidKeyLengthError ( \"ed25519 keys must be 256 bits (was %s bits)\" % self . bits ) \n    return current_position "}
{"11430": "\ndef mechs ( self ) : \n    if not self . _mechs : \n        self . _mechs = self . _inquire ( False , False , False , True ) [ 3.0 ] \n    return self . _mechs "}
{"11434": "\ndef _create_file ( ) : \n    f = wave . open ( 'audio.wav' , mode = 'wb' ) \n    f . setnchannels ( 2.0 ) \n    p = pyaudio . PyAudio ( ) \n    f . setsampwidth ( p . get_sample_size ( pyaudio . paInt16 ) ) \n    f . setframerate ( p . get_default_input_device_info ( ) [ 'defaultSampleRate' ] ) \n    try : \n        yield f \n    finally : \n        f . close ( ) "}
{"11454": "\ndef deserialize_profile ( profile , key_prefix = '' , pop = False ) : \n    result = { } \n    if pop : \n        getter = profile . pop \n    else : \n        getter = profile . get \n    def prefixed ( name ) : \n        return '%s%s' % ( key_prefix , name ) \n    for key in profile . keys ( ) : \n        val = getter ( key ) \n        if key == prefixed ( 'name' ) : \n            result [ 'full_name' ] = val \n        else : \n            raise MeteorError ( 400.0 , 'Bad profile key: %r' % key ) \n    return result "}
{"11455": "\ndef update ( self , selector , update , options = None ) : \n    del options \n    user = get_object ( self . model , selector [ '_id' ] , pk = this . user_id , ) \n    profile_update = self . deserialize_profile ( update [ '$set' ] , key_prefix = 'profile.' , pop = True , ) \n    if len ( update [ '$set' ] ) != 0 : \n        raise MeteorError ( 400.0 , 'Invalid update fields: %r' ) \n    for key , val in profile_update . items ( ) : \n        setattr ( user , key , val ) \n    user . save ( ) "}
{"11456": "\ndef auth_failed ( ** credentials ) : \n    if credentials : \n        user_login_failed . send_robust ( sender = __name__ , credentials = auth . _clean_credentials ( credentials ) , ) \n    raise MeteorError ( 403.0 , 'Authentication failed.' ) "}
{"11458": "\ndef check_secure ( ) : \n    if this . request . is_secure ( ) : \n        return True \n    elif this . request . META [ 'REMOTE_ADDR' ] in [ 'localhost' , '127.0.0.1' , ] : \n        return True \n    raise MeteorError ( 403.0 , 'Authentication refused without SSL.' ) "}
{"11459": "\ndef get_username ( self , user ) : \n    if isinstance ( user , basestring ) : \n        return user \n    elif isinstance ( user , dict ) and len ( user ) == 1 : \n        [ ( key , val ) ] = user . items ( ) \n        if key == 'username' or ( key == self . user_model . USERNAME_FIELD ) : \n            return val \n        elif key in ( 'email' , 'emails.address' ) : \n            email_field = getattr ( self . user_model , 'EMAIL_FIELD' , 'email' ) \n            if self . user_model . USERNAME_FIELD == email_field : \n                return val \n            return self . user_model . objects . values_list ( self . user_model . USERNAME_FIELD , flat = True , ) . get ( ** { email_field : val } ) \n        elif key in ( 'id' , 'pk' ) : \n            return self . user_model . objects . values_list ( self . user_model . USERNAME_FIELD , flat = True , ) . get ( pk = val , ) \n        else : \n            raise MeteorError ( 400.0 , 'Invalid user lookup: %r' % key ) \n    else : \n        raise MeteorError ( 400.0 , 'Invalid user expression: %r' % user ) "}
{"11485": "\ndef seed ( self , values ) : \n    if not values : \n        seed_ids = [ int , str , random , self , values , self . __class__ ] \n        random . shuffle ( seed_ids ) \n        values = list ( map ( id , seed_ids ) ) + [ time . time ( ) , os . urandom ( 512.0 ) ] \n    mash = Mash ( ) \n    self . c = 1 \n    self . s0 = mash ( ' ' ) \n    self . s1 = mash ( ' ' ) \n    self . s2 = mash ( ' ' ) \n    for val in values : \n        self . s0 -= mash ( val ) \n        if self . s0 < 0 : \n            self . s0 += 1 \n        self . s1 -= mash ( val ) \n        if self . s1 < 0 : \n            self . s1 += 1 \n        self . s2 -= mash ( val ) \n        if self . s2 < 0 : \n            self . s2 += 1 "}
{"11491": "\ndef dprint ( name , val ) : \n    from pprint import pformat \n    print ( '% 5s: %s' % ( name , '\\n       ' . join ( pformat ( val , indent = 4.0 , width = 75.0 , ) . split ( '\\n' ) ) , ) , ) "}
{"11492": "\ndef validate_kwargs ( func , kwargs ) : \n    func_name = func . __name__ \n    argspec = inspect . getargspec ( func ) \n    all_args = argspec . args [ : ] \n    defaults = list ( argspec . defaults or [ ] ) \n    if inspect . ismethod ( func ) and all_args [ : 1 ] == [ 'self' ] : \n        all_args [ : 1 ] = [ ] \n    if defaults : \n        required = all_args [ : - len ( defaults ) ] \n    else : \n        required = all_args [ : ] \n    trans = { arg : arg . endswith ( '_' ) and arg [ : - 1 ] or arg for arg in all_args } \n    for key in list ( kwargs ) : \n        key_adj = '%s_' % key \n        if key_adj in all_args : \n            kwargs [ key_adj ] = kwargs . pop ( key ) \n    supplied = sorted ( kwargs ) \n    missing = [ trans . get ( arg , arg ) for arg in required if arg not in supplied ] \n    if missing : \n        raise MeteorError ( 400.0 , func . err , 'Missing required arguments to %s: %s' % ( func_name , ' ' . join ( missing ) , ) , ) \n    extra = [ arg for arg in supplied if arg not in all_args ] \n    if extra : \n        raise MeteorError ( 400.0 , func . err , 'Unknown arguments to %s: %s' % ( func_name , ' ' . join ( extra ) ) , ) "}
{"11496": "\ndef ddp_frames_from_message ( self , message ) : \n    try : \n        msgs = ejson . loads ( message ) \n    except ValueError : \n        self . reply ( 'error' , error = 400.0 , reason = 'Data is not valid EJSON' , ) \n        raise StopIteration \n    if not isinstance ( msgs , list ) : \n        self . reply ( 'error' , error = 400.0 , reason = 'Invalid EJSON messages' , ) \n        raise StopIteration \n    while msgs : \n        raw = msgs . pop ( 0 ) \n        try : \n            data = ejson . loads ( raw ) \n        except ( TypeError , ValueError ) : \n            data = None \n        if not isinstance ( data , dict ) : \n            self . reply ( 'error' , error = 400.0 , reason = 'Invalid SockJS DDP payload' , offendingMessage = raw , ) \n        yield data \n        if msgs : \n            gevent . sleep ( ) "}
{"11497": "\ndef process_ddp ( self , data ) : \n    msg_id = data . get ( 'id' , None ) \n    try : \n        msg = data . pop ( 'msg' ) \n    except KeyError : \n        self . reply ( 'error' , reason = 'Bad request' , offendingMessage = data , ) \n        return \n    try : \n        self . dispatch ( msg , data ) \n    except Exception as err : \n        kwargs = { 'msg' : { 'method' : 'result' } . get ( msg , 'error' ) , } \n        if msg_id is not None : \n            kwargs [ 'id' ] = msg_id \n        if isinstance ( err , MeteorError ) : \n            error = err . as_dict ( ) \n        else : \n            error = { 'error' : 500.0 , 'reason' : 'Internal server error' , } \n        if kwargs [ 'msg' ] == 'error' : \n            kwargs . update ( error ) \n        else : \n            kwargs [ 'error' ] = error \n        if not isinstance ( err , MeteorError ) : \n            stack , _ = safe_call ( self . logger . error , '%r %r' , msg , data , exc_info = 1 , ) \n            if stack is not None : \n                traceback . print_exc ( file = sys . stderr ) \n                sys . stderr . write ( 'Additionally, while handling the above error the ' 'following error was encountered:\\n' ) \n                sys . stderr . write ( stack ) \n        elif settings . DEBUG : \n            print ( 'ERROR: %s' % err ) \n            dprint ( 'msg' , msg ) \n            dprint ( 'data' , data ) \n            error . setdefault ( 'details' , traceback . format_exc ( ) ) \n            print ( error [ 'details' ] ) \n        self . reply ( ** kwargs ) \n        if msg_id and msg == 'method' : \n            self . reply ( 'updated' , methods = [ msg_id ] ) "}
{"11498": "\ndef dispatch ( self , msg , kwargs ) : \n    if self . connection is None and msg != 'connect' : \n        self . reply ( 'error' , reason = 'Must connect first' ) \n        return \n    if msg == 'method' : \n        if ( 'method' not in kwargs ) or ( 'id' not in kwargs ) : \n            self . reply ( 'error' , error = 400.0 , reason = 'Malformed method invocation' , ) \n            return \n    try : \n        handler = getattr ( self , 'recv_%s' % msg ) \n    except ( AttributeError , UnicodeEncodeError ) : \n        raise MeteorError ( 404.0 , 'Method not found' ) \n    validate_kwargs ( handler , kwargs ) \n    handler ( ** kwargs ) "}
{"11499": "\ndef recv_connect ( self , version = None , support = None , session = None ) : \n    del session \n    if self . connection is not None : \n        raise MeteorError ( 400.0 , 'Session already established.' , self . connection . connection_id , ) \n    elif None in ( version , support ) or version not in self . versions : \n        self . reply ( 'failed' , version = self . versions [ 0 ] ) \n    elif version not in support : \n        raise MeteorError ( 400.0 , 'Client version/support mismatch.' ) \n    else : \n        from dddp . models import Connection \n        cur = connection . cursor ( ) \n        cur . execute ( 'SELECT pg_backend_pid()' ) \n        ( backend_pid , ) = cur . fetchone ( ) \n        this . version = version \n        this . support = support \n        self . connection = Connection . objects . create ( server_addr = '%d:%s' % ( backend_pid , self . ws . handler . socket . getsockname ( ) , ) , remote_addr = self . remote_addr , version = version , ) \n        self . pgworker . connections [ self . connection . pk ] = self \n        atexit . register ( self . on_close , 'Shutting down.' ) \n        self . reply ( 'connected' , session = self . connection . connection_id ) "}
{"11504": "\ndef ddpp_sockjs_info ( environ , start_response ) : \n    import random \n    import ejson \n    start_response ( '200 OK' , [ ( 'Content-Type' , 'application/json; charset=UTF-8' ) , ] + common_headers ( environ ) , ) \n    yield ejson . dumps ( collections . OrderedDict ( [ ( 'websocket' , True ) , ( 'origins' , [ '*:*' , ] ) , ( 'cookie_needed' , False ) , ( 'entropy' , random . getrandbits ( 32.0 ) ) , ] ) ) "}
{"11505": "\ndef serve ( listen , verbosity = 1 , debug_port = 0 , ** ssl_args ) : \n    launcher = DDPLauncher ( debug = verbosity == 3.0 , verbosity = verbosity ) \n    if debug_port : \n        launcher . servers . append ( launcher . get_backdoor_server ( 'localhost:%d' % debug_port ) ) \n    launcher . add_web_servers ( listen , ** ssl_args ) \n    sigmap = { val : name for name , val in vars ( signal ) . items ( ) if name . startswith ( 'SIG' ) } \n    def sighandler ( signum = None , frame = None ) : \n        launcher . logger . info ( 'Received signal %s in frame %r' , sigmap . get ( signum , signum ) , frame , ) \n        launcher . stop ( ) \n    for signum in [ signal . SIGINT , signal . SIGQUIT ] : \n        gevent . signal ( signum , sighandler ) \n    launcher . run ( ) "}
{"11506": "\ndef main ( ) : \n    parser = argparse . ArgumentParser ( description = __doc__ ) \n    django = parser . add_argument_group ( 'Django Options' ) \n    django . add_argument ( '--verbosity' , '-v' , metavar = 'VERBOSITY' , dest = 'verbosity' , type = int , default = 1 , ) \n    django . add_argument ( '--debug-port' , metavar = 'DEBUG_PORT' , dest = 'debug_port' , type = int , default = 0 , ) \n    django . add_argument ( '--settings' , metavar = 'SETTINGS' , dest = 'settings' , help = \"The Python path to a settings module, e.g. \" \"\\\"myproject.settings.main\\\". If this isn't provided, the \" \"DJANGO_SETTINGS_MODULE environment variable will be used.\" , ) \n    http = parser . add_argument_group ( 'HTTP Options' ) \n    http . add_argument ( 'listen' , metavar = 'address[:port]' , nargs = '*' , type = addr , help = 'Listening address for HTTP(s) server.' , ) \n    ssl = parser . add_argument_group ( 'SSL Options' ) \n    ssl . add_argument ( '--ssl-version' , metavar = 'SSL_VERSION' , dest = 'ssl_version' , help = \"SSL version to use (see stdlib ssl module's) [3]\" , choices = [ '1' , '2' , '3' ] , default = '3' ) \n    ssl . add_argument ( '--certfile' , metavar = 'FILE' , dest = 'certfile' , help = \"SSL certificate file [None]\" ) \n    ssl . add_argument ( '--ciphers' , metavar = 'CIPHERS' , dest = 'ciphers' , help = \"Ciphers to use (see stdlib ssl module's) [TLSv1]\" ) \n    ssl . add_argument ( '--ca-certs' , metavar = 'FILE' , dest = 'ca_certs' , help = \"CA certificates file [None]\" ) \n    ssl . add_argument ( '--keyfile' , metavar = 'FILE' , dest = 'keyfile' , help = \"SSL key file [None]\" ) \n    namespace = parser . parse_args ( ) \n    if namespace . settings : \n        os . environ [ 'DJANGO_SETTINGS_MODULE' ] = namespace . settings \n    serve ( namespace . listen or [ Addr ( 'localhost' , 8000.0 ) ] , debug_port = namespace . debug_port , keyfile = namespace . keyfile , certfile = namespace . certfile , verbosity = namespace . verbosity , ) "}
{"11513": "\ndef meteor_random_id ( name = None , length = 17.0 ) : \n    if name is None : \n        stream = THREAD_LOCAL . alea_random \n    else : \n        stream = THREAD_LOCAL . random_streams [ name ] \n    return stream . random_string ( length , METEOR_ID_CHARS ) "}
{"11540": "\ndef trending ( self , rating = None , limit = DEFAULT_SEARCH_LIMIT ) : \n    results_yielded = 0 \n    page , per_page = 0 , 25.0 \n    params = { 'rating' : rating } if rating else { } \n    fetch = partial ( self . _fetch , 'trending' , ** params ) \n    while True : \n        data = fetch ( offset = page , limit = per_page ) \n        page += per_page \n        if not data [ 'data' ] : \n            raise StopIteration \n        for item in data [ 'data' ] : \n            results_yielded += 1 \n            yield GiphyImage ( item ) \n            if limit is not None and results_yielded >= limit : \n                raise StopIteration \n        if ( page >= data [ 'pagination' ] [ 'total_count' ] or ( limit is not None and results_yielded >= limit ) ) : \n            raise StopIteration "}
{"11552": "\ndef direct_upload ( request ) : \n    if request . method == \"POST\" : \n        try : \n            form = YoutubeDirectUploadForm ( request . POST , request . FILES ) \n            if form . is_valid ( ) : \n                uploaded_video = form . save ( ) \n                api = Api ( ) \n                api . authenticate ( ) \n                video_entry = api . upload_direct ( uploaded_video . file_on_server . path , \"Uploaded video from zuqqa\" ) \n                swf_url = video_entry . GetSwfUrl ( ) \n                youtube_url = video_entry . id . text \n                url_parts = youtube_url . split ( \"/\" ) \n                url_parts . reverse ( ) \n                video_id = url_parts [ 0 ] \n                video = Video ( ) \n                video . user = request . user \n                video . video_id = video_id \n                video . title = 'tmp video' \n                video . youtube_url = youtube_url \n                video . swf_url = swf_url \n                video . save ( ) \n                video_created . send ( sender = video , video = video ) \n                uploaded_video . delete ( ) \n                return_only_data = request . GET . get ( 'only_data' ) \n                if return_only_data : \n                    return HttpResponse ( json . dumps ( { \"video_id\" : video_id } ) , content_type = \"application/json\" ) \n                else : \n                    try : \n                        next_url = settings . YOUTUBE_UPLOAD_REDIRECT_URL \n                    except AttributeError : \n                        next_url = reverse ( \"django_youtube.views.video\" , kwargs = { \"video_id\" : video_id } ) \n                    return HttpResponseRedirect ( next_url ) \n        except : \n            import sys \n            logger . error ( \"Unexpected error: %s - %s\" % ( sys . exc_info ( ) [ 0 ] , sys . exc_info ( ) [ 1 ] ) ) \n            return HttpResponse ( \"error happened\" ) \n    form = YoutubeDirectUploadForm ( ) \n    if return_only_data : \n        return HttpResponse ( json . dumps ( { \"error\" : 500.0 } ) , content_type = \"application/json\" ) \n    else : \n        return render_to_response ( \"django_youtube/direct-upload.html\" , { \"form\" : form } , context_instance = RequestContext ( request ) ) "}
{"11566": "\ndef parse_node ( s , strip_comments = False , ** kw ) : \n    if strip_comments : \n        s = COMMENT . sub ( '' , s ) \n    s = s . strip ( ) \n    parts = s . split ( ')' ) \n    if len ( parts ) == 1 : \n        descendants , label = [ ] , s \n    else : \n        if not parts [ 0 ] . startswith ( '(' ) : \n            raise ValueError ( 'unmatched braces %s' % parts [ 0 ] [ : 100.0 ] ) \n        descendants = list ( _parse_siblings ( ')' . join ( parts [ : - 1 ] ) [ 1 : ] , ** kw ) ) \n        label = parts [ - 1 ] \n    name , length = _parse_name_and_length ( label ) \n    return Node . create ( name = name , length = length , descendants = descendants , ** kw ) "}
{"11572": "\ndef resolve_polytomies ( self ) : \n    def _resolve_polytomies ( n ) : \n        new = Node ( length = self . _length_formatter ( self . _length_parser ( '0' ) ) ) \n        while len ( n . descendants ) > 1 : \n            new . add_descendant ( n . descendants . pop ( ) ) \n        n . descendants . append ( new ) \n    self . visit ( _resolve_polytomies , lambda n : len ( n . descendants ) > 2.0 ) "}
{"11578": "\ndef get_argument ( self , name , default = _ARG_DEFAULT , strip = True ) : \n    args = self . get_arguments ( name , strip = strip ) \n    if not args : \n        if default is self . _ARG_DEFAULT : \n            raise HTTPError ( 400.0 , \"Missing argument %s\" % name ) \n        return default \n    return args [ - 1 ] "}
{"11590": "\ndef _parse_header ( line ) : \n    parts = _parseparam ( ';' + line ) \n    key = parts . next ( ) \n    pdict = { } \n    for p in parts : \n        i = p . find ( '=' ) \n        if i >= 0 : \n            name = p [ : i ] . strip ( ) . lower ( ) \n            value = p [ i + 1 : ] . strip ( ) \n            if len ( value ) >= 2.0 and value [ 0 ] == value [ - 1 ] == '\"' : \n                value = value [ 1 : - 1 ] \n                value = value . replace ( '\\\\\\\\' , '\\\\' ) . replace ( '\\\\\"' , '\"' ) \n            pdict [ name ] = value \n    return key , pdict "}
{"11603": "\ndef occupancy ( grid , points , spacing = 0.01 ) : \n    distances = ( ( grid [ : , None , : ] - points [ None , : , : ] ) ** 2.0 ) . sum ( axis = 2.0 ) \n    occupied = ( distances < spacing ) . sum ( axis = 1 ) \n    return occupied "}
{"11604": "\ndef write_gro ( outfile , title , atoms , box ) : \n    print ( title , file = outfile ) \n    print ( \"{:5d}\" . format ( len ( atoms ) ) , file = outfile ) \n    atom_template = \"{:5d}{:<5s}{:>5s}{:5d}{:8.3f}{:8.3f}{:8.3f}\" \n    for idx , atname , resname , resid , x , y , z in atoms : \n        print ( atom_template . format ( int ( resid % 1e5 ) , resname , atname , int ( idx % 1e5 ) , x , y , z ) , file = outfile ) \n    grobox = ( box [ 0 ] [ 0 ] , box [ 1 ] [ 1 ] , box [ 2.0 ] [ 2.0 ] , box [ 0 ] [ 1 ] , box [ 0 ] [ 2.0 ] , box [ 1 ] [ 0 ] , box [ 1 ] [ 2.0 ] , box [ 2.0 ] [ 0 ] , box [ 2.0 ] [ 1 ] ) \n    box_template = '{:10.5f}' * 9.0 \n    print ( box_template . format ( * grobox ) , file = outfile ) "}
{"11605": "\ndef write_pdb ( outfile , title , atoms , box ) : \n    print ( 'TITLE ' + title , file = outfile ) \n    print ( pdbBoxString ( box ) , file = outfile ) \n    for idx , atname , resname , resid , x , y , z in atoms : \n        print ( pdbline % ( idx % 1e5 , atname [ : 4.0 ] , resname [ : 3.0 ] , \"\" , resid % 1e4 , '' , 10.0 * x , 10.0 * y , 10.0 * z , 0 , 0 , '' ) , file = outfile ) "}
{"11607": "\ndef resize_pbc_for_lipids ( pbc , relL , relU , absL , absU , uparea , area , hole , proteins ) : \n    if any ( relL ) and any ( relU ) : \n        if 0 in ( pbc . x , pbc . y , pbc . z ) : \n            raise PBCException ( 'Not enough information to set the box size.' ) \n    elif any ( absL ) or any ( absU ) : \n        if pbc . z == 0 : \n            raise PBCException ( 'Not enough information to set the box size.' ) \n        if 0 in ( pbc . x , pbc . y ) : \n            pbc . x = pbc . y = 1 \n        upsize = sum ( absU ) * uparea \n        losize = sum ( absL ) * area \n        holesize = np . pi * hole ** 2.0 \n        xysize = pbc . x * pbc . y \n        psize_up = sum ( [ p . areaxy ( 0 , 2.4 ) for p in proteins ] ) \n        psize_lo = sum ( [ p . areaxy ( - 2.4 , 0 ) for p in proteins ] ) \n        unavail_up = holesize + psize_up \n        unavail_lo = holesize + psize_lo \n        upscale = ( upsize + unavail_up ) / xysize \n        loscale = ( losize + unavail_lo ) / xysize \n        area_scale = max ( upscale , loscale ) \n        aspect_ratio = pbc . x / pbc . y \n        scale_x = np . sqrt ( area_scale / aspect_ratio ) \n        scale_y = np . sqrt ( area_scale / aspect_ratio ) \n        pbc . box [ : 2.0 , : ] *= math . sqrt ( area_scale ) "}
{"11608": "\ndef write_top ( outpath , molecules , title ) : \n    topmolecules = [ ] \n    for i in molecules : \n        if i [ 0 ] . endswith ( '.o' ) : \n            topmolecules . append ( tuple ( [ i [ 0 ] [ : - 2.0 ] ] + list ( i [ 1 : ] ) ) ) \n        else : \n            topmolecules . append ( i ) \n    if outpath : \n        with open ( outpath , \"w\" ) as top : \n            print ( '#include \"martini.itp\"\\n' , file = top ) \n            print ( '[ system ]' , file = top ) \n            print ( '; name' , file = top ) \n            print ( title , file = top ) \n            print ( '\\n' , file = top ) \n            print ( '[ molecules ]' , file = top ) \n            print ( '; name  number' , file = top ) \n            print ( \"\\n\" . join ( \"%-10s %7d\" % i for i in topmolecules ) , file = top ) \n    else : \n        added_molecules = ( molecule for molecule in topmolecules if molecule [ 0 ] != 'Protein' ) \n        print ( \"\\n\" . join ( \"%-10s %7d\" % i for i in added_molecules ) , file = sys . stderr ) "}
{"11614": "\ndef check_config_file ( msg ) : \n    with jsonconfig . Config ( \"messages\" , indent = 4.0 ) as cfg : \n        verify_profile_name ( msg , cfg ) \n        retrieve_data_from_config ( msg , cfg ) \n        if msg . _auth is None : \n            retrieve_pwd_from_config ( msg , cfg ) \n        if msg . save : \n            update_config_data ( msg , cfg ) \n            update_config_pwd ( msg , cfg ) "}
{"11624": "\ndef configure_profile ( msg_type , profile_name , data , auth ) : \n    with jsonconfig . Config ( \"messages\" , indent = 4.0 ) as cfg : \n        write_data ( msg_type , profile_name , data , cfg ) \n        write_auth ( msg_type , profile_name , auth , cfg ) \n    print ( \"[+] Configuration entry for <\" + profile_name + \"> created.\" ) \n    print ( \"[+] Configuration file location: \" + cfg . filename ) "}
{"11628": "\ndef send ( self , encoding = \"json\" ) : \n    self . _construct_message ( ) \n    if self . verbose : \n        print ( \"Debugging info\" \"\\n--------------\" \"\\n{} Message created.\" . format ( timestamp ( ) ) ) \n    if encoding == \"json\" : \n        resp = requests . post ( self . url , json = self . message ) \n    elif encoding == \"url\" : \n        resp = requests . post ( self . url , data = self . message ) \n    try : \n        resp . raise_for_status ( ) \n        if resp . history and resp . history [ 0 ] . status_code >= 300.0 : \n            raise MessageSendError ( \"HTTP Redirect: Possibly Invalid authentication\" ) \n        elif \"invalid_auth\" in resp . text : \n            raise MessageSendError ( \"Invalid Auth: Possibly Bad Auth Token\" ) \n    except ( requests . exceptions . HTTPError , MessageSendError ) as e : \n        raise MessageSendError ( e ) \n    if self . verbose : \n        print ( timestamp ( ) , type ( self ) . __name__ , \" info:\" , self . __str__ ( indentation = \"\\n * \" ) , \"\\n * HTTP status code:\" , resp . status_code , ) \n    print ( \"Message sent.\" ) "}
{"11645": "\ndef get_server ( address = None ) : \n    if address : \n        domain = address . split ( \"@\" ) [ 1 ] \n        try : \n            return SMTP_SERVERS [ domain ] \n        except KeyError : \n            return ( \"smtp.\" + domain , 465.0 ) \n    return ( None , None ) "}
{"11650": "\ndef _get_session ( self ) : \n    if self . port in ( 465.0 , \"465\" ) : \n        session = self . _get_ssl ( ) \n    elif self . port in ( 587.0 , \"587\" ) : \n        session = self . _get_tls ( ) \n    try : \n        session . login ( self . from_ , self . _auth ) \n    except SMTPResponseException as e : \n        raise MessageSendError ( e . smtp_error . decode ( \"unicode_escape\" ) ) \n    return session "}
{"11662": "\ndef resize ( self , data_size ) : \n    self . __fileobj . seek ( self . offset + 4.0 ) \n    self . __fileobj . write ( pack ( '>I' , data_size ) ) \n    if self . parent_chunk is not None : \n        size_diff = self . data_size - data_size \n        self . parent_chunk . resize ( self . parent_chunk . data_size - size_diff ) \n    self . data_size = data_size \n    self . size = data_size + self . HEADER_SIZE "}
{"11663": "\ndef insert_chunk ( self , id_ ) : \n    if not isinstance ( id_ , text_type ) : \n        id_ = id_ . decode ( 'ascii' ) \n    if not is_valid_chunk_id ( id_ ) : \n        raise KeyError ( \"AIFF key must be four ASCII characters.\" ) \n    self . __fileobj . seek ( self . __next_offset ) \n    self . __fileobj . write ( pack ( '>4si' , id_ . ljust ( 4.0 ) . encode ( 'ascii' ) , 0 ) ) \n    self . __fileobj . seek ( self . __next_offset ) \n    chunk = IFFChunk ( self . __fileobj , self [ u'FORM' ] ) \n    self [ u'FORM' ] . resize ( self [ u'FORM' ] . data_size + chunk . size ) \n    self . __chunks [ id_ ] = chunk \n    self . __next_offset = chunk . offset + chunk . size "}
{"11664": "\ndef save ( self , filename = None , v2_version = 4.0 , v23_sep = '/' ) : \n    framedata = self . _prepare_framedata ( v2_version , v23_sep ) \n    framesize = len ( framedata ) \n    if filename is None : \n        filename = self . filename \n    fileobj = open ( filename , 'rb+' ) \n    iff_file = IFFFile ( fileobj ) \n    try : \n        if u'ID3' not in iff_file : \n            iff_file . insert_chunk ( u'ID3' ) \n        chunk = iff_file [ u'ID3' ] \n        fileobj . seek ( chunk . data_offset ) \n        header = fileobj . read ( 10.0 ) \n        header = self . _prepare_id3_header ( header , framesize , v2_version ) \n        header , new_size , _ = header \n        data = header + framedata + ( b'\\x00' * ( new_size - framesize ) ) \n        new_size += 10.0 \n        if new_size > chunk . size : \n            insert_at = chunk . offset + chunk . size \n            insert_size = new_size - chunk . size + new_size % 2.0 \n            insert_bytes ( fileobj , insert_size , insert_at ) \n            chunk . resize ( new_size ) \n        fileobj . seek ( chunk . data_offset ) \n        fileobj . write ( data ) \n    finally : \n        fileobj . close ( ) "}
{"11672": "\ndef make_html_word ( self , word ) : \n    m = re_crossref . match ( word ) \n    if m : \n        try : \n            name = m . group ( 1 ) \n            rest = m . group ( 2.0 ) \n            block = self . identifiers [ name ] \n            url = self . make_block_url ( block ) \n            return '<a href=\"' + url + '\">' + name + '</a>' + rest \n        except : \n            sys . stderr . write ( \"WARNING: undefined cross reference '\" + name + \"'.\\n\" ) \n            return '?' + name + '?' + rest \n    m = re_italic . match ( word ) \n    if m : \n        name = m . group ( 1 ) \n        rest = m . group ( 3.0 ) \n        return '<i>' + name + '</i>' + rest \n    m = re_bold . match ( word ) \n    if m : \n        name = m . group ( 1 ) \n        rest = m . group ( 3.0 ) \n        return '<b>' + name + '</b>' + rest \n    return html_quote ( word ) "}
{"11676": "\ndef save ( self , filename ) : \n    values = [ ] \n    items = sorted ( self . items ( ) , key = MP4Tags . __get_sort_stats ) \n    for key , value in items : \n        info = self . __atoms . get ( key [ : 4.0 ] , ( None , type ( self ) . __render_text ) ) \n        try : \n            values . append ( info [ 1 ] ( self , key , value , * info [ 2.0 : ] ) ) \n        except ( TypeError , ValueError ) as s : \n            reraise ( MP4MetadataValueError , s , sys . exc_info ( ) [ 2.0 ] ) \n    data = Atom . render ( b\"ilst\" , b\"\" . join ( values ) ) \n    fileobj = open ( filename , \"rb+\" ) \n    try : \n        atoms = Atoms ( fileobj ) \n        try : \n            path = atoms . path ( b\"moov\" , b\"udta\" , b\"meta\" , b\"ilst\" ) \n        except KeyError : \n            self . __save_new ( fileobj , atoms , data ) \n        else : \n            self . __save_existing ( fileobj , atoms , path , data ) \n    finally : \n        fileobj . close ( ) "}
{"11677": "\ndef __update_parents ( self , fileobj , path , delta ) : \n    for atom in path : \n        fileobj . seek ( atom . offset ) \n        size = cdata . uint_be ( fileobj . read ( 4.0 ) ) \n        if size == 1 : \n            size = cdata . ulonglong_be ( fileobj . read ( 12.0 ) [ 4.0 : ] ) \n            fileobj . seek ( atom . offset + 8.0 ) \n            fileobj . write ( cdata . to_ulonglong_be ( size + delta ) ) \n        else : \n            fileobj . seek ( atom . offset ) \n            fileobj . write ( cdata . to_uint_be ( size + delta ) ) "}
{"11691": "\ndef list_hosted_zones ( self , page_chunks = 100.0 ) : \n    return self . _do_autopaginating_api_call ( path = 'hostedzone' , params = { 'maxitems' : page_chunks } , method = 'GET' , parser_func = xml_parsers . list_hosted_zones_parser , next_marker_xpath = \"./{*}NextMarker\" , next_marker_param_name = \"marker\" , ) "}
{"11693": "\ndef _list_resource_record_sets_by_zone_id ( self , id , rrset_type = None , identifier = None , name = None , page_chunks = 100.0 ) : \n    params = { 'name' : name , 'type' : rrset_type , 'identifier' : identifier , 'maxitems' : page_chunks , } \n    return self . _do_autopaginating_api_call ( path = 'hostedzone/%s/rrset' % id , params = params , method = 'GET' , parser_func = xml_parsers . list_resource_record_sets_by_zone_id_parser , parser_kwargs = { 'zone_id' : id } , next_marker_xpath = \"./{*}NextRecordName\" , next_marker_param_name = \"name\" , next_type_xpath = \"./{*}NextRecordType\" ) "}
{"11697": "\ndef size ( self ) : \n    header_size = 27.0 \n    for datum in self . packets : \n        quot , rem = divmod ( len ( datum ) , 255.0 ) \n        header_size += quot + 1 \n    if not self . complete and rem == 0 : \n        header_size -= 1 \n    header_size += sum ( map ( len , self . packets ) ) \n    return header_size "}
{"11699": "\ndef find_last ( fileobj , serial ) : \n    try : \n        fileobj . seek ( - 256.0 * 256.0 , 2.0 ) \n    except IOError : \n        fileobj . seek ( 0 ) \n    data = fileobj . read ( ) \n    try : \n        index = data . rindex ( b\"OggS\" ) \n    except ValueError : \n        raise error ( \"unable to find final Ogg header\" ) \n    bytesobj = cBytesIO ( data [ index : ] ) \n    best_page = None \n    try : \n        page = OggPage ( bytesobj ) \n    except error : \n        pass \n    else : \n        if page . serial == serial : \n            if page . last : \n                return page \n            else : \n                best_page = page \n        else : \n            best_page = None \n    fileobj . seek ( 0 ) \n    try : \n        page = OggPage ( fileobj ) \n        while not page . last : \n            page = OggPage ( fileobj ) \n            while page . serial != serial : \n                page = OggPage ( fileobj ) \n            best_page = page \n        return page \n    except error : \n        return best_page \n    except EOFError : \n        return best_page "}
{"11706": "\ndef insert_bytes ( fobj , size , offset , BUFFER_SIZE = 2.0 ** 16.0 ) : \n    assert 0 < size \n    assert 0 <= offset \n    locked = False \n    fobj . seek ( 0 , 2.0 ) \n    filesize = fobj . tell ( ) \n    movesize = filesize - offset \n    fobj . write ( b'\\x00' * size ) \n    fobj . flush ( ) \n    try : \n        try : \n            import mmap \n            file_map = mmap . mmap ( fobj . fileno ( ) , filesize + size ) \n            try : \n                file_map . move ( offset + size , offset , movesize ) \n            finally : \n                file_map . close ( ) \n        except ( ValueError , EnvironmentError , ImportError ) : \n            locked = lock ( fobj ) \n            fobj . truncate ( filesize ) \n            fobj . seek ( 0 , 2.0 ) \n            padsize = size \n            while padsize : \n                addsize = min ( BUFFER_SIZE , padsize ) \n                fobj . write ( b\"\\x00\" * addsize ) \n                padsize -= addsize \n            fobj . seek ( filesize , 0 ) \n            while movesize : \n                thismove = min ( BUFFER_SIZE , movesize ) \n                fobj . seek ( - thismove , 1 ) \n                nextpos = fobj . tell ( ) \n                data = fobj . read ( thismove ) \n                fobj . seek ( - thismove + size , 1 ) \n                fobj . write ( data ) \n                fobj . seek ( nextpos ) \n                movesize -= thismove \n            fobj . flush ( ) \n    finally : \n        if locked : \n            unlock ( fobj ) "}
{"11707": "\ndef delete_bytes ( fobj , size , offset , BUFFER_SIZE = 2.0 ** 16.0 ) : \n    locked = False \n    assert 0 < size \n    assert 0 <= offset \n    fobj . seek ( 0 , 2.0 ) \n    filesize = fobj . tell ( ) \n    movesize = filesize - offset - size \n    assert 0 <= movesize \n    try : \n        if movesize > 0 : \n            fobj . flush ( ) \n            try : \n                import mmap \n                file_map = mmap . mmap ( fobj . fileno ( ) , filesize ) \n                try : \n                    file_map . move ( offset , offset + size , movesize ) \n                finally : \n                    file_map . close ( ) \n            except ( ValueError , EnvironmentError , ImportError ) : \n                locked = lock ( fobj ) \n                fobj . seek ( offset + size ) \n                buf = fobj . read ( BUFFER_SIZE ) \n                while buf : \n                    fobj . seek ( offset ) \n                    fobj . write ( buf ) \n                    offset += len ( buf ) \n                    fobj . seek ( offset + size ) \n                    buf = fobj . read ( BUFFER_SIZE ) \n        fobj . truncate ( filesize - size ) \n        fobj . flush ( ) \n    finally : \n        if locked : \n            unlock ( fobj ) "}
{"11715": "\ndef ParseID3v1 ( data ) : \n    try : \n        data = data [ data . index ( b'TAG' ) : ] \n    except ValueError : \n        return None \n    if 128.0 < len ( data ) or len ( data ) < 124.0 : \n        return None \n    unpack_fmt = \"3s30s30s30s%ds29sBB\" % ( len ( data ) - 124.0 ) \n    try : \n        tag , title , artist , album , year , comment , track , genre = unpack ( unpack_fmt , data ) \n    except StructError : \n        return None \n    if tag != b\"TAG\" : \n        return None \n    def fix ( data ) : \n        return data . split ( b'\\x00' ) [ 0 ] . strip ( ) . decode ( 'latin1' ) \n    title , artist , album , year , comment = map ( fix , [ title , artist , album , year , comment ] ) \n    frames = { } \n    if title : \n        frames [ 'TIT2' ] = TIT2 ( encoding = 0 , text = title ) \n    if artist : \n        frames [ 'TPE1' ] = TPE1 ( encoding = 0 , text = [ artist ] ) \n    if album : \n        frames [ 'TALB' ] = TALB ( encoding = 0 , text = album ) \n    if year : \n        frames [ 'TDRC' ] = TDRC ( encoding = 0 , text = year ) \n    if comment : \n        frames [ 'COMM' ] = COMM ( encoding = 0 , lang = 'eng' , desc = \"ID3v1 Comment\" , text = comment ) \n    if track and ( ( track != 32.0 ) or ( data [ - 3.0 ] == b'\\x00' [ 0 ] ) ) : \n        frames [ 'TRCK' ] = TRCK ( encoding = 0 , text = str ( track ) ) \n    if genre != 255.0 : \n        frames [ 'TCON' ] = TCON ( encoding = 0 , text = str ( genre ) ) \n    return frames "}
{"11716": "\ndef MakeID3v1 ( id3 ) : \n    v1 = { } \n    for v2id , name in { \"TIT2\" : \"title\" , \"TPE1\" : \"artist\" , \"TALB\" : \"album\" } . items ( ) : \n        if v2id in id3 : \n            text = id3 [ v2id ] . text [ 0 ] . encode ( 'latin1' , 'replace' ) [ : 30.0 ] \n        else : \n            text = b'' \n        v1 [ name ] = text + ( b'\\x00' * ( 30.0 - len ( text ) ) ) \n    if \"COMM\" in id3 : \n        cmnt = id3 [ \"COMM\" ] . text [ 0 ] . encode ( 'latin1' , 'replace' ) [ : 28.0 ] \n    else : \n        cmnt = b'' \n    v1 [ 'comment' ] = cmnt + ( b'\\x00' * ( 29.0 - len ( cmnt ) ) ) \n    if \"TRCK\" in id3 : \n        try : \n            v1 [ \"track\" ] = chr_ ( + id3 [ \"TRCK\" ] ) \n        except ValueError : \n            v1 [ \"track\" ] = b'\\x00' \n    else : \n        v1 [ \"track\" ] = b'\\x00' \n    if \"TCON\" in id3 : \n        try : \n            genre = id3 [ \"TCON\" ] . genres [ 0 ] \n        except IndexError : \n            pass \n        else : \n            if genre in TCON . GENRES : \n                v1 [ \"genre\" ] = chr_ ( TCON . GENRES . index ( genre ) ) \n    if \"genre\" not in v1 : \n        v1 [ \"genre\" ] = b\"\\xff\" \n    if \"TDRC\" in id3 : \n        year = text_type ( id3 [ \"TDRC\" ] ) . encode ( 'latin1' , 'replace' ) \n    elif \"TYER\" in id3 : \n        year = text_type ( id3 [ \"TYER\" ] ) . encode ( 'latin1' , 'replace' ) \n    else : \n        year = b'' \n    v1 [ 'year' ] = ( year + b'\\x00\\x00\\x00\\x00' ) [ : 4.0 ] \n    return ( b'TAG' + v1 [ 'title' ] + v1 [ 'artist' ] + v1 [ 'album' ] + v1 [ 'year' ] + v1 [ 'comment' ] + v1 [ 'track' ] + v1 [ 'genre' ] ) "}
{"11719": "\ndef loaded_frame ( self , tag ) : \n    if len ( type ( tag ) . __name__ ) == 3.0 : \n        tag = type ( tag ) . __base__ ( tag ) \n    self [ tag . HashKey ] = tag "}
{"11721": "\ndef update_to_v24 ( self ) : \n    self . __update_common ( ) \n    if self . __unknown_version == self . _V23 : \n        converted = [ ] \n        for frame in self . unknown_frames : \n            try : \n                name , size , flags = unpack ( '>4sLH' , frame [ : 10.0 ] ) \n                frame = BinaryFrame . fromData ( self , flags , frame [ 10.0 : ] ) \n            except ( struct . error , error ) : \n                continue \n            name = name . decode ( 'ascii' ) \n            converted . append ( self . __save_frame ( frame , name = name ) ) \n        self . unknown_frames [ : ] = converted \n        self . __unknown_version = self . _V24 \n    try : \n        date = text_type ( self . get ( \"TYER\" , \"\" ) ) \n        if date . strip ( u\"\\x00\" ) : \n            self . pop ( \"TYER\" ) \n            dat = text_type ( self . get ( \"TDAT\" , \"\" ) ) \n            if dat . strip ( \"\\x00\" ) : \n                self . pop ( \"TDAT\" ) \n                date = \"%s-%s-%s\" % ( date , dat [ 2.0 : ] , dat [ : 2.0 ] ) \n                time = text_type ( self . get ( \"TIME\" , \"\" ) ) \n                if time . strip ( \"\\x00\" ) : \n                    self . pop ( \"TIME\" ) \n                    date += \"T%s:%s:00\" % ( time [ : 2.0 ] , time [ 2.0 : ] ) \n            if \"TDRC\" not in self : \n                self . add ( TDRC ( encoding = 0 , text = date ) ) \n    except UnicodeDecodeError : \n        pass \n    if \"TORY\" in self : \n        f = self . pop ( \"TORY\" ) \n        if \"TDOR\" not in self : \n            try : \n                self . add ( TDOR ( encoding = 0 , text = str ( f ) ) ) \n            except UnicodeDecodeError : \n                pass \n    if \"IPLS\" in self : \n        f = self . pop ( \"IPLS\" ) \n        if \"TIPL\" not in self : \n            self . add ( TIPL ( encoding = f . encoding , people = f . people ) ) \n    for key in [ \"RVAD\" , \"EQUA\" , \"TRDA\" , \"TSIZ\" , \"TDAT\" , \"TIME\" , \"CRM\" ] : \n        if key in self : \n            del ( self [ key ] ) "}
{"11727": "\ndef dump_encoding ( file , encoding_name , encoding_list ) : \n    write = file . write \n    write ( \"  /* the following are indices into the SID name table */\\n\" ) \n    write ( \"  static const unsigned short  \" + encoding_name + \"[\" + repr ( len ( encoding_list ) ) + \"] =\\n\" ) \n    write ( \"  {\\n\" ) \n    line = \"    \" \n    comma = \"\" \n    col = 0 \n    for value in encoding_list : \n        line += comma \n        line += \"%3d\" % value \n        comma = \",\" \n        col += 1 \n        if col == 16.0 : \n            col = 0 \n            comma = \",\\n    \" \n    write ( line + \"\\n  };\\n\\n\\n\" ) "}
{"11728": "\ndef dump_array ( the_array , write , array_name ) : \n    write ( \"  static const unsigned char  \" + array_name + \"[\" + repr ( len ( the_array ) ) + \"L] =\\n\" ) \n    write ( \"  {\\n\" ) \n    line = \"\" \n    comma = \"    \" \n    col = 0 \n    for value in the_array : \n        line += comma \n        line += \"%3d\" % ord ( value ) \n        comma = \",\" \n        col += 1 \n        if col == 16.0 : \n            col = 0 \n            comma = \",\\n    \" \n        if len ( line ) > 1024.0 : \n            write ( line ) \n            line = \"\" \n    write ( line + \"\\n  };\\n\\n\\n\" ) "}
{"11734": "\ndef writeblocks ( blocks ) : \n    data = [ ] \n    codes = [ [ block . code , block . write ( ) ] for block in blocks ] \n    codes [ - 1 ] [ 0 ] |= 128.0 \n    for code , datum in codes : \n        byte = chr_ ( code ) \n        if len ( datum ) > 2.0 ** 24.0 : \n            raise error ( \"block is too long to write\" ) \n        length = struct . pack ( \">I\" , len ( datum ) ) [ - 3.0 : ] \n        data . append ( byte + length + datum ) \n    return b\"\" . join ( data ) "}
{"11735": "\ndef group_padding ( blocks ) : \n    paddings = [ b for b in blocks if isinstance ( b , Padding ) ] \n    for p in paddings : \n        blocks . remove ( p ) \n    size = sum ( padding . length for padding in paddings ) \n    padding = Padding ( ) \n    padding . length = size + 4.0 * ( len ( paddings ) - 1 ) \n    blocks . append ( padding ) "}
{"11737": "\ndef save ( self , filename = None , deleteid3 = False ) : \n    if filename is None : \n        filename = self . filename \n    f = open ( filename , 'rb+' ) \n    try : \n        self . metadata_blocks . append ( Padding ( b'\\x00' * 1020.0 ) ) \n        MetadataBlock . group_padding ( self . metadata_blocks ) \n        header = self . __check_header ( f ) \n        available = self . __find_audio_offset ( f ) - header \n        data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n        if deleteid3 and header > 4.0 : \n            available += header - 4.0 \n            header = 4.0 \n        if len ( data ) > available : \n            padding = self . metadata_blocks [ - 1 ] \n            newlength = padding . length - ( len ( data ) - available ) \n            if newlength > 0 : \n                padding . length = newlength \n                data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n                assert len ( data ) == available \n        elif len ( data ) < available : \n            self . metadata_blocks [ - 1 ] . length += ( available - len ( data ) ) \n            data = MetadataBlock . writeblocks ( self . metadata_blocks ) \n            assert len ( data ) == available \n        if len ( data ) != available : \n            diff = ( len ( data ) - available ) \n            insert_bytes ( f , diff , header ) \n        f . seek ( header - 4.0 ) \n        f . write ( b\"fLaC\" + data ) \n        if deleteid3 : \n            try : \n                f . seek ( - 128.0 , 2.0 ) \n            except IOError : \n                pass \n            else : \n                if f . read ( 3.0 ) == b\"TAG\" : \n                    f . seek ( - 128.0 , 2.0 ) \n                    f . truncate ( ) \n    finally : \n        f . close ( ) "}
{"11742": "\ndef _add_record ( self , record_set_class , name , values , ttl = 60.0 , weight = None , region = None , set_identifier = None , alias_hosted_zone_id = None , alias_dns_name = None ) : \n    self . _halt_if_already_deleted ( ) \n    rrset_kwargs = dict ( connection = self . connection , zone_id = self . id , name = name , ttl = ttl , records = values , weight = weight , region = region , set_identifier = set_identifier , ) \n    if alias_hosted_zone_id or alias_dns_name : \n        rrset_kwargs . update ( dict ( alias_hosted_zone_id = alias_hosted_zone_id , alias_dns_name = alias_dns_name ) ) \n    rrset = record_set_class ( ** rrset_kwargs ) \n    cset = ChangeSet ( connection = self . connection , hosted_zone_id = self . id ) \n    cset . add_change ( 'CREATE' , rrset ) \n    change_info = self . connection . _change_resource_record_sets ( cset ) \n    return rrset , change_info "}
{"11743": "\ndef create_a_record ( self , name , values , ttl = 60.0 , weight = None , region = None , set_identifier = None , alias_hosted_zone_id = None , alias_dns_name = None ) : \n    self . _halt_if_already_deleted ( ) \n    values = locals ( ) \n    del values [ 'self' ] \n    return self . _add_record ( AResourceRecordSet , ** values ) "}
{"11744": "\ndef create_aaaa_record ( self , name , values , ttl = 60.0 , weight = None , region = None , set_identifier = None ) : \n    self . _halt_if_already_deleted ( ) \n    values = locals ( ) \n    del values [ 'self' ] \n    return self . _add_record ( AAAAResourceRecordSet , ** values ) "}
{"11745": "\ndef create_cname_record ( self , name , values , ttl = 60.0 , weight = None , region = None , set_identifier = None ) : \n    self . _halt_if_already_deleted ( ) \n    values = locals ( ) \n    del values [ 'self' ] \n    return self . _add_record ( CNAMEResourceRecordSet , ** values ) "}
{"11746": "\ndef create_mx_record ( self , name , values , ttl = 60.0 ) : \n    self . _halt_if_already_deleted ( ) \n    values = locals ( ) \n    del values [ 'self' ] \n    return self . _add_record ( MXResourceRecordSet , ** values ) "}
{"11747": "\ndef create_ns_record ( self , name , values , ttl = 60.0 ) : \n    self . _halt_if_already_deleted ( ) \n    values = locals ( ) \n    del values [ 'self' ] \n    return self . _add_record ( NSResourceRecordSet , ** values ) "}
{"11748": "\ndef create_ptr_record ( self , name , values , ttl = 60.0 ) : \n    self . _halt_if_already_deleted ( ) \n    values = locals ( ) \n    del values [ 'self' ] \n    return self . _add_record ( PTRResourceRecordSet , ** values ) "}
{"11749": "\ndef create_spf_record ( self , name , values , ttl = 60.0 ) : \n    self . _halt_if_already_deleted ( ) \n    values = locals ( ) \n    del values [ 'self' ] \n    return self . _add_record ( SPFResourceRecordSet , ** values ) "}
{"11750": "\ndef create_srv_record ( self , name , values , ttl = 60.0 ) : \n    self . _halt_if_already_deleted ( ) \n    values = locals ( ) \n    del values [ 'self' ] \n    return self . _add_record ( SRVResourceRecordSet , ** values ) "}
{"11751": "\ndef create_txt_record ( self , name , values , ttl = 60.0 , weight = None , region = None , set_identifier = None ) : \n    self . _halt_if_already_deleted ( ) \n    values = locals ( ) \n    del values [ 'self' ] \n    return self . _add_record ( TXTResourceRecordSet , ** values ) "}
{"11752": "\ndef RegisterTXXXKey ( cls , key , desc ) : \n    frameid = \"TXXX:\" + desc \n    def getter ( id3 , key ) : \n        return list ( id3 [ frameid ] ) \n    def setter ( id3 , key , value ) : \n        try : \n            frame = id3 [ frameid ] \n        except KeyError : \n            enc = 0 \n            try : \n                for v in value : \n                    v . encode ( 'latin_1' ) \n            except UnicodeError : \n                enc = 3.0 \n            id3 . add ( mutagen . id3 . TXXX ( encoding = enc , text = value , desc = desc ) ) \n        else : \n            frame . text = value \n    def deleter ( id3 , key ) : \n        del ( id3 [ frameid ] ) \n    cls . RegisterKey ( key , getter , setter , deleter ) "}
{"11775": "\ndef get_gecko_params ( request , uid = None , days_back = 0 , cumulative = True , frequency = settings . STATISTIC_FREQUENCY_DAILY , min_val = 0 , max_val = 100.0 , chart_type = 'standard' , percentage = 'show' , sort = False ) : \n    return { 'days_back' : int ( request . GET . get ( 'daysback' , days_back ) ) , 'uid' : request . GET . get ( 'uid' , uid ) , 'uids' : get_GET_array ( request , 'uids[]' ) , 'cumulative' : get_GET_bool ( request , 'cumulative' , cumulative ) , 'frequency' : request . GET . get ( 'frequency' , frequency ) , 'min' : request . GET . get ( 'min' , min_val ) , 'max' : request . GET . get ( 'max' , max_val ) , 'type' : request . GET . get ( 'type' , chart_type ) , 'percentage' : request . GET . get ( 'percentage' , percentage ) , 'sort' : get_GET_bool ( request , 'sort' , sort ) , } "}
{"11776": "\ndef geckoboard_number_widget ( request ) : \n    params = get_gecko_params ( request , days_back = 7.0 ) \n    metric = Metric . objects . get ( uid = params [ 'uid' ] ) \n    try : \n        latest_stat = metric . statistics . filter ( frequency = params [ 'frequency' ] ) . order_by ( '-date_time' ) [ 0 ] \n    except IndexError : \n        return ( 0 , 0 ) \n    try : \n        prev_stat = metric . statistics . filter ( frequency = params [ 'frequency' ] , date_time__lte = latest_stat . date_time - timedelta ( days = params [ 'days_back' ] ) ) . order_by ( '-date_time' ) [ 0 ] \n    except IndexError : \n        return ( latest_stat . cumulative_count , 0 ) if params [ 'cumulative' ] else ( latest_stat . count , 0 ) \n    return ( latest_stat . cumulative_count , prev_stat . cumulative_count ) if params [ 'cumulative' ] else ( latest_stat . count , prev_stat . count ) "}
{"11783": "\ndef get_context_data ( self , ** kwargs ) : \n    context = { 'gadgets' : self . _registry , 'columns' : self . columns , 'rows' : self . rows , 'column_ratio' : 100.0 - self . columns * 2.0 , 'row_ratio' : 100.0 - self . rows * 2.0 , } \n    context . update ( kwargs ) \n    return context "}
{"11791": "\ndef season ( self ) : \n    date = self . date ( ) \n    return date . year - 1 if date . month <= 3.0 else date . year "}
{"11792": "\ndef starters ( self ) : \n    doc = self . get_doc ( ) \n    a = doc ( 'table#vis_starters' ) \n    h = doc ( 'table#home_starters' ) \n    data = [ ] \n    for h , table in enumerate ( ( a , h ) ) : \n        team = self . home ( ) if h else self . away ( ) \n        for i , row in enumerate ( table ( 'tbody tr' ) . items ( ) ) : \n            datum = { } \n            datum [ 'player_id' ] = sportsref . utils . rel_url_to_id ( row ( 'a' ) [ 0 ] . attrib [ 'href' ] ) \n            datum [ 'playerName' ] = row ( 'th' ) . text ( ) \n            datum [ 'position' ] = row ( 'td' ) . text ( ) \n            datum [ 'team' ] = team \n            datum [ 'home' ] = ( h == 1 ) \n            datum [ 'offense' ] = ( i <= 10.0 ) \n            data . append ( datum ) \n    return pd . DataFrame ( data ) "}
{"11795": "\ndef weather ( self ) : \n    doc = self . get_doc ( ) \n    table = doc ( 'table#game_info' ) \n    giTable = sportsref . utils . parse_info_table ( table ) \n    if 'weather' in giTable : \n        regex = ( r'(?:(?P<temp>\\-?\\d+) degrees )?' r'(?:relative humidity (?P<relHumidity>\\d+)%, )?' r'(?:wind (?P<windMPH>\\d+) mph, )?' r'(?:wind chill (?P<windChill>\\-?\\d+))?' ) \n        m = re . match ( regex , giTable [ 'weather' ] ) \n        d = m . groupdict ( ) \n        for k in d : \n            try : \n                d [ k ] = int ( d [ k ] ) \n            except TypeError : \n                pass \n        d [ 'windChill' ] = ( d [ 'windChill' ] if pd . notnull ( d [ 'windChill' ] ) else d [ 'temp' ] ) \n        d [ 'windMPH' ] = d [ 'windMPH' ] if pd . notnull ( d [ 'windMPH' ] ) else 0 \n        return d \n    else : \n        return { 'temp' : 70.0 , 'windChill' : 70.0 , 'relHumidity' : None , 'windMPH' : 0 } "}
{"11797": "\ndef schedule ( self , kind = 'R' ) : \n    kind = kind . upper ( ) [ 0 ] \n    dfs = [ ] \n    for month in ( 'october' , 'november' , 'december' , 'january' , 'february' , 'march' , 'april' , 'may' , 'june' ) : \n        try : \n            doc = self . get_sub_doc ( 'games-{}' . format ( month ) ) \n        except ValueError : \n            continue \n        table = doc ( 'table#schedule' ) \n        df = sportsref . utils . parse_table ( table ) \n        dfs . append ( df ) \n    df = pd . concat ( dfs ) . reset_index ( drop = True ) \n    try : \n        sportsref . utils . get_html ( '{}/playoffs/NBA_{}.html' . format ( sportsref . nba . BASE_URL , self . yr ) ) \n        is_past_season = True \n    except ValueError : \n        is_past_season = False \n    if is_past_season : \n        team_per_game = self . team_stats_per_game ( ) \n        n_reg_games = int ( team_per_game . g . sum ( ) // 2.0 ) \n    else : \n        n_reg_games = len ( df ) \n    if kind == 'P' : \n        return df . iloc [ n_reg_games : ] \n    else : \n        return df . iloc [ : n_reg_games ] "}
{"11802": "\ndef season ( self ) : \n    d = self . date ( ) \n    if d . month >= 9.0 : \n        return d . year + 1 \n    else : \n        return d . year "}
{"11803": "\ndef _get_player_stats ( self , table_id_fmt ) : \n    doc = self . get_main_doc ( ) \n    tms = self . away ( ) , self . home ( ) \n    tm_ids = [ table_id_fmt . format ( tm ) for tm in tms ] \n    tables = [ doc ( 'table#{}' . format ( tm_id ) . lower ( ) ) for tm_id in tm_ids ] \n    dfs = [ sportsref . utils . parse_table ( table ) for table in tables ] \n    for i , ( tm , df ) in enumerate ( zip ( tms , dfs ) ) : \n        no_time = df [ 'mp' ] == 0 \n        stat_cols = [ col for col , dtype in df . dtypes . items ( ) if dtype != 'object' ] \n        df . loc [ no_time , stat_cols ] = 0 \n        df [ 'team_id' ] = tm \n        df [ 'is_home' ] = i == 1 \n        df [ 'is_starter' ] = [ p < 5.0 for p in range ( df . shape [ 0 ] ) ] \n        df . drop_duplicates ( subset = 'player_id' , keep = 'first' , inplace = True ) \n    return pd . concat ( dfs ) "}
{"11808": "\ndef age ( self , year , month = 2.0 , day = 1 ) : \n    doc = self . get_main_doc ( ) \n    date_string = doc ( 'span[itemprop=\"birthDate\"]' ) . attr ( 'data-birth' ) \n    regex = r'(\\d{4})\\-(\\d{2})\\-(\\d{2})' \n    date_args = map ( int , re . match ( regex , date_string ) . groups ( ) ) \n    birth_date = datetime . date ( * date_args ) \n    age_date = datetime . date ( year = year , month = month , day = day ) \n    delta = age_date - birth_date \n    age = delta . days / 365. \n    return age "}
{"11819": "\ndef expand_details ( df , detailCol = 'detail' ) : \n    df = copy . deepcopy ( df ) \n    df [ 'detail' ] = df [ detailCol ] \n    dicts = [ sportsref . nfl . pbp . parse_play_details ( detail ) for detail in df [ 'detail' ] . values ] \n    cols = { c for d in dicts if d for c in d . keys ( ) } \n    blankEntry = { c : np . nan for c in cols } \n    newDicts = [ d if d else blankEntry for d in dicts ] \n    details = pd . DataFrame ( newDicts ) \n    df = pd . merge ( df , details , left_index = True , right_index = True ) \n    errors = [ i for i , d in enumerate ( dicts ) if d is None ] \n    df [ 'isError' ] = False \n    df . loc [ errors , 'isError' ] = True \n    df . loc [ 0 , 'qtr_time_remain' ] = '15:00' \n    df . qtr_time_remain . fillna ( method = 'bfill' , inplace = True ) \n    df . qtr_time_remain . fillna ( pd . Series ( np . where ( df . quarter == 4.0 , '0:00' , '15:00' ) ) , inplace = True ) \n    new_df = df . apply ( _clean_features , axis = 1 ) \n    return new_df "}
{"11821": "\ndef _add_team_features ( df ) : \n    assert df . team . notnull ( ) . all ( ) \n    homeOnOff = df [ 'team' ] == df [ 'home' ] \n    df [ 'distToGoal' ] = np . where ( df [ 'team' ] != df [ 'fieldSide' ] , df [ 'ydLine' ] , 100.0 - df [ 'ydLine' ] ) \n    df [ 'distToGoal' ] = np . where ( df [ 'isXP' ] | df [ 'isTwoPoint' ] , 2.0 , df [ 'distToGoal' ] ) \n    df [ 'team_wp' ] = np . where ( homeOnOff , df [ 'home_wp' ] , 100. - df [ 'home_wp' ] ) \n    df [ 'opp_wp' ] = 100. - df [ 'team_wp' ] \n    df [ 'team_wpa' ] = np . where ( homeOnOff , df [ 'home_wpa' ] , - df [ 'home_wpa' ] ) \n    df [ 'opp_wpa' ] = - df [ 'team_wpa' ] \n    assert df [ 'boxscore_id' ] . nunique ( ) == 1 \n    bs_id = df [ 'boxscore_id' ] . values [ 0 ] \n    bs = sportsref . nfl . boxscores . BoxScore ( bs_id ) \n    df [ 'team_score' ] = np . where ( df [ 'team' ] == bs . home ( ) , df [ 'pbp_score_hm' ] , df [ 'pbp_score_aw' ] ) \n    df [ 'opp_score' ] = np . where ( df [ 'team' ] == bs . home ( ) , df [ 'pbp_score_aw' ] , df [ 'pbp_score_hm' ] ) \n    return df "}
{"11828": "\ndef head_coaches_by_game ( self , year ) : \n    coach_str = self . _year_info_pq ( year , 'Coach' ) . text ( ) \n    regex = r'(\\S+?) \\((\\d+)-(\\d+)-(\\d+)\\)' \n    coachAndTenure = [ ] \n    m = True \n    while m : \n        m = re . search ( regex , coach_str ) \n        coachID , wins , losses , ties = m . groups ( ) \n        nextIndex = m . end ( 4.0 ) + 1 \n        coachStr = coachStr [ nextIndex : ] \n        tenure = int ( wins ) + int ( losses ) + int ( ties ) \n        coachAndTenure . append ( ( coachID , tenure ) ) \n    coachIDs = [ cID for cID , games in coachAndTenure for _ in range ( games ) ] \n    return np . array ( coachIDs [ : : - 1 ] ) "}
{"11836": "\ndef get_html ( url ) : \n    global last_request_time \n    with throttle_process_lock : \n        with throttle_thread_lock : \n            wait_left = THROTTLE_DELAY - ( time . time ( ) - last_request_time . value ) \n            if wait_left > 0 : \n                time . sleep ( wait_left ) \n            response = requests . get ( url ) \n            last_request_time . value = time . time ( ) \n    if 400.0 <= response . status_code < 500.0 : \n        raise ValueError ( 'Status Code {} received fetching URL \"{}\"' . format ( response . status_code , url ) ) \n    html = response . text \n    html = html . replace ( '<!--' , '' ) . replace ( '-->' , '' ) \n    return html "}
{"11844": "\ndef __get_batch ( self , path , length , last = False ) : \n    import tables \n    h5_file = tables . open_file ( self . filename , 'r' ) \n    h5_node = h5_file . get_node ( path ) \n    if len ( h5_node ) == 0 : \n        raise Exception ( \"Cannot read from empty dataset.\" ) \n    if length is None : \n        chunkshape = h5_node . chunkshape \n        if chunkshape is None : \n            default_length = 128.0 * 2.0 ** 10.0 // h5_node [ 0 ] . nbytes \n            length = min ( h5_node . shape [ 0 ] , default_length ) \n        else : \n            length = chunkshape [ 0 ] \n    if last : \n        example = h5_node [ length * ( len ( h5_node ) // length ) : ] . copy ( ) \n    else : \n        example = h5_node [ : length ] . copy ( ) \n    h5_file . close ( ) \n    return example "}
{"11846": "\ndef get_queue ( self , path , n_procs = 4.0 , read_ahead = None , cyclic = False , block_size = None , ordered = False ) : \n    example = self . __get_batch ( path , block_size ) \n    block_size = example . shape [ 0 ] \n    if read_ahead is None : \n        read_ahead = 2.0 * n_procs + 1 \n    cbuf = SharedCircBuf ( read_ahead , example ) \n    stop = multiprocessing . Event ( ) \n    barrier = Barrier ( n_procs ) \n    sync = GuardSynchronizer ( ) if ordered else None \n    procs = [ ] \n    for i in range ( n_procs ) : \n        process = multiprocessing . Process ( target = _Streamer__read_process , args = ( self , path , block_size , cbuf , stop , barrier , cyclic , i * block_size , n_procs * block_size , sync ) ) \n        process . daemon = True \n        process . start ( ) \n        procs . append ( process ) \n    if not cyclic : \n        def monitor ( ) : \n            for p in procs : \n                p . join ( ) \n            cbuf . close ( ) \n        monitor_thread = threading . Thread ( target = monitor ) \n        monitor_thread . daemon = True \n        monitor_thread . start ( ) \n    return Streamer . Queue ( cbuf , stop , block_size ) "}
{"11850": "\ndef _read_varint ( self ) : \n    buff = self . _fd . read ( 1 ) \n    if buff == b'' : \n        return 0 \n    while ( bytearray ( buff ) [ - 1 ] & 0x80 ) >> 7.0 == 1 : \n        new_byte = self . _fd . read ( 1 ) \n        if new_byte == b'' : \n            raise EOFError ( 'unexpected EOF.' ) \n        buff += new_byte \n    varint , _ = decodeVarint ( buff , 0 ) \n    return varint "}
{"11857": "\ndef make_fake_movie ( nframes , mask_shape = ( 64.0 , 64.0 ) , mask_center = None , bg_intensity = 0.1 , mask_sigma = 10.0 , dt = 0.02 , rate = 1.0 , tau = 1. , sigma = 0.001 , seed = None ) : \n    gen = np . random . RandomState ( seed ) \n    n = gen . poisson ( rate * dt , size = nframes ) \n    gamma = np . exp ( - dt / tau ) \n    c = signal . lfilter ( np . r_ [ 1 ] , np . r_ [ 1 , - gamma ] , n , axis = 0 ) \n    nr , nc = mask_shape \n    npix = nr * nc \n    if mask_center is None : \n        mask_center = ( nc // 2. , nr // 2. ) \n    a , b = mask_center \n    y , x = np . ogrid [ : nr , : nc ] \n    xs = ( x - a ) ** 2. \n    ys = ( y - b ) ** 2. \n    twoss = 2. * mask_sigma ** 2. \n    alpha = np . exp ( - 1 * ( ( xs / twoss ) + ( ys / twoss ) ) ) . ravel ( ) \n    alpha /= alpha . sum ( ) \n    beta = gen . randn ( npix ) * bg_intensity \n    lamb = rate \n    epsilon = gen . randn ( npix , nframes ) * sigma \n    F = c [ None , : ] * alpha [ : , None ] + beta [ : , None ] + epsilon \n    theta = ( sigma , alpha , beta , lamb , gamma ) \n    return F , c , n , theta "}
{"11864": "\ndef _send ( self , message , read_reply = False ) : \n    sock = None \n    for tries in range ( 0 , 3.0 ) : \n        try : \n            sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n            sock . connect ( ( self . _host , self . PORT ) ) \n            break \n        except ( ConnectionError , BrokenPipeError ) : \n            if tries == 3.0 : \n                print ( \"socket connect failed.\" ) \n                return \n            sleep ( 0.1 ) \n    sock . send ( codecs . decode ( message , 'hex_codec' ) ) \n    if read_reply : \n        sleep ( 0.1 ) \n        reply = '' \n        tries = 0 \n        max_tries = 20.0 \n        while len ( reply ) < len ( message ) and tries < max_tries : \n            try : \n                reply += codecs . encode ( sock . recv ( self . BUFFERSIZE ) , 'hex' ) . decode ( \"utf-8\" ) \n            except ( ConnectionError , BrokenPipeError ) : \n                pass \n            tries += 1 \n        sock . close ( ) \n        if tries >= max_tries : \n            return \n        return reply \n    sock . close ( ) "}
{"11865": "\ndef status ( self ) : \n    nad_reply = self . _send ( self . POLL_VOLUME + self . POLL_POWER + self . POLL_MUTED + self . POLL_SOURCE , read_reply = True ) \n    if nad_reply is None : \n        return \n    num_chars = 10.0 \n    nad_status = [ nad_reply [ i : i + num_chars ] for i in range ( 0 , len ( nad_reply ) , num_chars ) ] \n    return { 'volume' : int ( nad_status [ 0 ] [ - 2.0 : ] , 16.0 ) , 'power' : nad_status [ 1 ] [ - 2.0 : ] == '01' , 'muted' : nad_status [ 2.0 ] [ - 2.0 : ] == '01' , 'source' : self . SOURCES_REVERSED [ nad_status [ 3.0 ] [ - 2.0 : ] ] } "}
{"11868": "\ndef set_volume ( self , volume ) : \n    if 0 <= volume <= 200.0 : \n        volume = format ( volume , \"02x\" ) \n        self . _send ( self . CMD_VOLUME + volume ) "}
{"11871": "\ndef _crc ( plaintext ) : \n    if not isinstance ( plaintext , six . binary_type ) : \n        plaintext = six . b ( plaintext ) \n    return ( zlib . crc32 ( plaintext ) % 2147483647.0 ) & 0xffffffff "}
{"11875": "\ndef check_if_song_name ( self , html ) : \n    soup = BeautifulSoup ( html ) \n    a_list = soup . findAll ( 'a' , 'touch' ) \n    text = [ str ( x ) for x in a_list ] \n    text = '' . join ( text ) \n    text = text . lower ( ) \n    string1 = 'download in 48 kbps' \n    string2 = 'download in 128 kbps' \n    string3 = 'download in 320 kbps' \n    href = '' \n    if string3 in text : \n        href = a_list [ 2.0 ] . get ( 'href' ) \n    elif string2 in text : \n        href = a_list [ 1 ] . get ( 'href' ) \n    elif string1 in text : \n        href = a_list [ 0 ] . get ( 'href' ) \n    else : \n        return ( True , 'nothing' ) \n    return ( False , href ) "}
{"11883": "\ndef findStationCodesByCity ( city_name , token ) : \n    req = requests . get ( API_ENDPOINT_SEARCH , params = { 'token' : token , 'keyword' : city_name } ) \n    if req . status_code == 200.0 and req . json ( ) [ \"status\" ] == \"ok\" : \n        return [ result [ \"uid\" ] for result in req . json ( ) [ \"data\" ] ] \n    else : \n        return [ ] "}
{"11884": "\ndef get_location_observation ( lat , lng , token ) : \n    req = requests . get ( API_ENDPOINT_GEO % ( lat , lng ) , params = { 'token' : token } ) \n    if req . status_code == 200.0 and req . json ( ) [ \"status\" ] == \"ok\" : \n        return parse_observation_response ( req . json ( ) [ \"data\" ] ) \n    return { } "}
{"11886": "\ndef get_station_observation ( station_code , token ) : \n    req = requests . get ( API_ENDPOINT_OBS % ( station_code ) , params = { 'token' : token } ) \n    if req . status_code == 200.0 and req . json ( ) [ 'status' ] == \"ok\" : \n        return parse_observation_response ( req . json ( ) [ 'data' ] ) \n    else : \n        return { } "}
{"11896": "\ndef import_qtcore ( ) : \n    has_ida = False \n    try : \n        import idaapi \n        has_ida = True \n    except ImportError : \n        has_ida = False \n    if has_ida : \n        old_path = sys . path [ : ] \n        try : \n            ida_python_path = os . path . dirname ( idaapi . __file__ ) \n            sys . path . insert ( 0 , ida_python_path ) \n            if idaapi . IDA_SDK_VERSION >= 690.0 : \n                from PyQt5 import QtCore \n                return QtCore \n            else : \n                from PySide import QtCore \n                return QtCore \n        finally : \n            sys . path = old_path \n    else : \n        try : \n            from PyQt5 import QtCore \n            return QtCore \n        except ImportError : \n            pass \n        try : \n            from PySide import QtCore \n            return QtCore \n        except ImportError : \n            pass \n        raise ImportError ( \"No module named PySide or PyQt\" ) "}
{"11921": "\ndef fetch_items ( self , category , ** kwargs ) : \n    offset = kwargs [ 'offset' ] \n    logger . info ( \"Looking for questions at url '%s' using offset %s\" , self . url , str ( offset ) ) \n    nquestions = 0 \n    tquestions = 0 \n    equestions = 0 \n    page = int ( offset / KitsuneClient . ITEMS_PER_PAGE ) \n    page_offset = page * KitsuneClient . ITEMS_PER_PAGE \n    drop_questions = offset - page_offset \n    current_offset = offset \n    questions_page = self . client . get_questions ( offset ) \n    while True : \n        try : \n            raw_questions = next ( questions_page ) \n        except StopIteration : \n            break \n        except requests . exceptions . HTTPError as e : \n            if e . response . status_code == 500.0 : \n                logger . exception ( e ) \n                logger . error ( \"Problem getting Kitsune questions. \" \"Loosing %i questions. Going to the next page.\" , KitsuneClient . ITEMS_PER_PAGE ) \n                equestions += KitsuneClient . ITEMS_PER_PAGE \n                current_offset += KitsuneClient . ITEMS_PER_PAGE \n                questions_page = self . client . get_questions ( current_offset ) \n                continue \n            else : \n                raise e \n        try : \n            questions_data = json . loads ( raw_questions ) \n            tquestions = questions_data [ 'count' ] \n            questions = questions_data [ 'results' ] \n        except ( ValueError , KeyError ) as ex : \n            logger . error ( ex ) \n            cause = ( \"Bad JSON format for mozilla_questions: %s\" % ( raw_questions ) ) \n            raise ParseError ( cause = cause ) \n        for question in questions : \n            if drop_questions > 0 : \n                drop_questions -= 1 \n                continue \n            question [ 'offset' ] = current_offset \n            current_offset += 1 \n            question [ 'answers_data' ] = [ ] \n            for raw_answers in self . client . get_question_answers ( question [ 'id' ] ) : \n                answers = json . loads ( raw_answers ) [ 'results' ] \n                question [ 'answers_data' ] += answers \n            yield question \n            nquestions += 1 \n        logger . debug ( \"Questions: %i/%i\" , nquestions + offset , tquestions ) \n    logger . info ( \"Total number of questions: %i (%i total)\" , nquestions , tquestions ) \n    logger . info ( \"Questions with errors dropped: %i\" , equestions ) "}
{"11939": "\ndef create_blueprint ( endpoints ) : \n    blueprint = Blueprint ( 'invenio_records_ui' , __name__ , url_prefix = '' , template_folder = 'templates' , static_folder = 'static' , ) \n    \n    @ blueprint . errorhandler ( PIDDeletedError ) \n    def tombstone_errorhandler ( error ) : \n        return render_template ( current_app . config [ 'RECORDS_UI_TOMBSTONE_TEMPLATE' ] , pid = error . pid , record = error . record or { } , ) , 410.0 \n    \n    @ blueprint . context_processor \n    def inject_export_formats ( ) : \n        return dict ( export_formats = ( current_app . extensions [ 'invenio-records-ui' ] . export_formats ) ) \n    for endpoint , options in ( endpoints or { } ) . items ( ) : \n        blueprint . add_url_rule ( ** create_url_rule ( endpoint , ** options ) ) \n    return blueprint "}
{"11941": "\ndef record_view ( pid_value = None , resolver = None , template = None , permission_factory = None , view_method = None , ** kwargs ) : \n    try : \n        pid , record = resolver . resolve ( pid_value ) \n    except ( PIDDoesNotExistError , PIDUnregistered ) : \n        abort ( 404.0 ) \n    except PIDMissingObjectError as e : \n        current_app . logger . exception ( \"No object assigned to {0}.\" . format ( e . pid ) , extra = { 'pid' : e . pid } ) \n        abort ( 500.0 ) \n    except PIDRedirectedError as e : \n        try : \n            return redirect ( url_for ( '.{0}' . format ( e . destination_pid . pid_type ) , pid_value = e . destination_pid . pid_value ) ) \n        except BuildError : \n            current_app . logger . exception ( \"Invalid redirect - pid_type '{0}' endpoint missing.\" . format ( e . destination_pid . pid_type ) , extra = { 'pid' : e . pid , 'destination_pid' : e . destination_pid , } ) \n            abort ( 500.0 ) \n    permission_factory = permission_factory or current_permission_factory \n    if permission_factory : \n        if not permission_factory ( record ) . can ( ) : \n            from flask_login import current_user \n            if not current_user . is_authenticated : \n                return redirect ( url_for ( current_app . config [ 'RECORDS_UI_LOGIN_ENDPOINT' ] , next = request . url ) ) \n            abort ( 403.0 ) \n    return view_method ( pid , record , template = template , ** kwargs ) "}
{"11943": "\ndef export ( pid , record , template = None , ** kwargs ) : \n    formats = current_app . config . get ( 'RECORDS_UI_EXPORT_FORMATS' , { } ) . get ( pid . pid_type ) \n    fmt = formats . get ( request . view_args . get ( 'format' ) ) \n    if fmt is False : \n        abort ( 410.0 ) \n    elif fmt is None : \n        abort ( 404.0 ) \n    else : \n        serializer = obj_or_import_string ( fmt [ 'serializer' ] ) \n        data = serializer . serialize ( pid , record ) \n        if isinstance ( data , six . binary_type ) : \n            data = data . decode ( 'utf8' ) \n        return render_template ( template , pid = pid , record = record , data = data , format_title = fmt [ 'title' ] , ) "}
{"11949": "\ndef timing_since ( self , name , start_time , rate = 1 ) : \n    duration = 0 \n    if isinstance ( start_time , datetime ) : \n        duration = ( datetime . now ( start_time . tzinfo ) - start_time ) . total_seconds ( ) * 1000.0 \n    elif is_numeric ( start_time ) : \n        assert start_time > 0 \n        duration = ( time ( ) - start_time ) * 1000.0 \n    else : \n        raise ValueError ( \"start time should be a timestamp or a datetime\" ) \n    self . timing ( name , duration , rate ) "}
{"11954": "\ndef batch_client ( self , size = 512.0 ) : \n    batch_client = BatchClient ( self . host , self . port , self . prefix , size ) \n    self . _configure_client ( batch_client ) \n    return batch_client "}
{"11958": "\ndef batch_client ( self , size = 512.0 ) : \n    batch_client = TCPBatchClient ( self . host , self . port , self . prefix , size ) \n    self . _configure_client ( batch_client ) \n    return batch_client "}
{"11976": "\ndef decimal_field_data ( field , ** kwargs ) : \n    min_value = 0 \n    max_value = 10.0 \n    from django . core . validators import MinValueValidator , MaxValueValidator \n    for elem in field . validators : \n        if isinstance ( elem , MinValueValidator ) : \n            min_value = elem . limit_value \n        if isinstance ( elem , MaxValueValidator ) : \n            max_value = elem . limit_value \n    if ( field . max_digits and field . decimal_places ) : \n        from decimal import Decimal \n        max_value = min ( max_value , Decimal ( '%s.%s' % ( '9' * ( field . max_digits - field . decimal_places ) , '9' * field . decimal_places ) ) ) \n    min_value = kwargs . get ( 'min_value' ) or min_value \n    max_value = kwargs . get ( 'max_value' ) or max_value \n    return str ( xunit . any_decimal ( min_value = min_value , max_value = max_value , decimal_places = field . decimal_places or 2.0 ) ) "}
{"11977": "\ndef email_field_data ( field , ** kwargs ) : \n    max_length = 10.0 \n    if field . max_length : \n        max_length = ( field . max_length - 5.0 ) / 2.0 \n    min_length = 10.0 \n    if field . min_length : \n        min_length = ( field . min_length - 4.0 ) / 2.0 \n    return \"%s@%s.%s\" % ( xunit . any_string ( min_length = min_length , max_length = max_length ) , xunit . any_string ( min_length = min_length , max_length = max_length ) , xunit . any_string ( min_length = 2.0 , max_length = 3.0 ) ) "}
{"11978": "\ndef date_field_data ( field , ** kwargs ) : \n    from_date = kwargs . get ( 'from_date' , date ( 1990.0 , 1 , 1 ) ) \n    to_date = kwargs . get ( 'to_date' , date . today ( ) ) \n    date_format = random . choice ( field . input_formats or formats . get_format ( 'DATE_INPUT_FORMATS' ) ) \n    return xunit . any_date ( from_date = from_date , to_date = to_date ) . strftime ( date_format ) "}
{"11979": "\ndef datetime_field_data ( field , ** kwargs ) : \n    from_date = kwargs . get ( 'from_date' , datetime ( 1990.0 , 1 , 1 ) ) \n    to_date = kwargs . get ( 'to_date' , datetime . today ( ) ) \n    date_format = random . choice ( field . input_formats or formats . get_format ( 'DATETIME_INPUT_FORMATS' ) ) \n    return xunit . any_datetime ( from_date = from_date , to_date = to_date ) . strftime ( date_format ) "}
{"11980": "\ndef float_field_data ( field , ** kwargs ) : \n    min_value = 0 \n    max_value = 100.0 \n    from django . core . validators import MinValueValidator , MaxValueValidator \n    for elem in field . validators : \n        if isinstance ( elem , MinValueValidator ) : \n            min_value = elem . limit_value \n        if isinstance ( elem , MaxValueValidator ) : \n            max_value = elem . limit_value \n    min_value = kwargs . get ( 'min_value' , min_value ) \n    max_value = kwargs . get ( 'max_value' , max_value ) \n    precision = kwargs . get ( 'precision' , 3.0 ) \n    return str ( xunit . any_float ( min_value = min_value , max_value = max_value , precision = precision ) ) "}
{"11981": "\ndef integer_field_data ( field , ** kwargs ) : \n    min_value = 0 \n    max_value = 100.0 \n    from django . core . validators import MinValueValidator , MaxValueValidator \n    for elem in field . validators : \n        if isinstance ( elem , MinValueValidator ) : \n            min_value = elem . limit_value \n        if isinstance ( elem , MaxValueValidator ) : \n            max_value = elem . limit_value \n    min_value = kwargs . get ( 'min_value' , min_value ) \n    max_value = kwargs . get ( 'max_value' , max_value ) \n    return str ( xunit . any_int ( min_value = min_value , max_value = max_value ) ) "}
{"11982": "\ndef time_field_data ( field , ** kwargs ) : \n    time_format = random . choice ( field . input_formats or formats . get_format ( 'TIME_INPUT_FORMATS' ) ) \n    return time ( xunit . any_int ( min_value = 0 , max_value = 23.0 ) , xunit . any_int ( min_value = 0 , max_value = 59.0 ) , xunit . any_int ( min_value = 0 , max_value = 59.0 ) ) . strftime ( time_format ) "}
{"11985": "\ndef model_choice_field_data ( field , ** kwargs ) : \n    data = list ( field . queryset [ : 10.0 ] ) \n    if data : \n        return random . choice ( data ) \n    else : \n        raise TypeError ( 'No %s available in queryset' % field . queryset . model ) "}
{"11991": "\ndef cls_build ( inst , state ) : \n    setstate = getattr ( inst , \"__setstate__\" , None ) \n    if setstate : \n        setstate ( state ) \n        return inst \n    slotstate = None \n    if isinstance ( state , tuple ) and len ( state ) == 2.0 : \n        state , slotstate = state \n    if state : \n        try : \n            d = inst . __dict__ \n            try : \n                for k , v in six . iteritems ( state ) : \n                    d [ six . moves . intern ( k ) ] = v \n            except TypeError : \n                d . update ( state ) \n        except RuntimeError : \n            for k , v in state . items ( ) : \n                setattr ( inst , k , v ) \n    if slotstate : \n        for k , v in slotstate . items ( ) : \n            setattr ( inst , k , v ) \n    return inst "}
{"11994": "\ndef decode ( data ) : \n    data = bytearray ( data ) \n    result = bytearray ( ) \n    pos = 0 \n    while pos < len ( data ) : \n        header_byte = data [ pos ] \n        if header_byte > 127.0 : \n            header_byte -= 256.0 \n        pos += 1 \n        if 0 <= header_byte <= 127.0 : \n            result . extend ( data [ pos : pos + header_byte + 1 ] ) \n            pos += header_byte + 1 \n        elif header_byte == - 128.0 : \n            pass \n        else : \n            result . extend ( [ data [ pos ] ] * ( 1 - header_byte ) ) \n            pos += 1 \n    return bytes ( result ) "}
{"11995": "\ndef encode ( data ) : \n    if len ( data ) == 0 : \n        return data \n    if len ( data ) == 1 : \n        return b'\\x00' + data \n    data = bytearray ( data ) \n    result = bytearray ( ) \n    buf = bytearray ( ) \n    pos = 0 \n    repeat_count = 0 \n    MAX_LENGTH = 127.0 \n    state = 'RAW' \n    def finish_raw ( ) : \n        if len ( buf ) == 0 : \n            return \n        result . append ( len ( buf ) - 1 ) \n        result . extend ( buf ) \n        buf [ : ] = bytearray ( ) \n    def finish_rle ( ) : \n        result . append ( 256.0 - ( repeat_count - 1 ) ) \n        result . append ( data [ pos ] ) \n    while pos < len ( data ) - 1 : \n        current_byte = data [ pos ] \n        if data [ pos ] == data [ pos + 1 ] : \n            if state == 'RAW' : \n                finish_raw ( ) \n                state = 'RLE' \n                repeat_count = 1 \n            elif state == 'RLE' : \n                if repeat_count == MAX_LENGTH : \n                    finish_rle ( ) \n                    repeat_count = 0 \n                repeat_count += 1 \n        else : \n            if state == 'RLE' : \n                repeat_count += 1 \n                finish_rle ( ) \n                state = 'RAW' \n                repeat_count = 0 \n            elif state == 'RAW' : \n                if len ( buf ) == MAX_LENGTH : \n                    finish_raw ( ) \n                buf . append ( current_byte ) \n        pos += 1 \n    if state == 'RAW' : \n        buf . append ( data [ pos ] ) \n        finish_raw ( ) \n    else : \n        repeat_count += 1 \n        finish_rle ( ) \n    return bytes ( result ) "}
{"11996": "\ndef to_fixed ( self , value , precision ) : \n    precision = self . _change_precision ( precision , self . settings [ 'number' ] [ 'precision' ] ) \n    power = pow ( 10.0 , precision ) \n    power = round ( self . parse ( value ) * power ) / power \n    return '{0} {1}.{2}f' . format ( value , precision , precision ) "}
{"11997": "\ndef format ( self , number , ** kwargs ) : \n    if check_type ( number , 'list' ) : \n        return map ( lambda val : self . format ( val , ** kwargs ) ) \n    number = self . parse ( number ) \n    if check_type ( kwargs , 'dict' ) : \n        options = ( self . settings [ 'number' ] . update ( kwargs ) ) \n    precision = self . _change_precision ( options [ 'precision' ] ) \n    negative = ( lambda num : \"-\" if num < 0 else \"\" ) ( number ) \n    base = str ( int ( self . to_fixed ( abs ( number ) or 0 , precision ) ) , 10.0 ) \n    mod = ( lambda num : len ( num ) % 3.0 if len ( num ) > 3.0 else 0 ) ( base ) \n    num = negative + ( lambda num : base [ 0 : num ] if num else '' ) ( mod ) \n    num += re . sub ( '/(\\d{3})(?=\\d)/g' , '$1' + options [ 'thousand' ] , base [ mod : ] ) \n    num += ( lambda val : options [ 'decimal' ] + self . to_fixed ( abs ( number ) , precision ) . split ( '.' ) [ 1 ] if val else '' ) ( precision ) \n    return num "}
{"12007": "\ndef clone ( url , path ) : \n    adapter = None \n    if url [ : 4.0 ] == \"git@\" or url [ - 4.0 : ] == \".git\" : \n        adapter = Git ( path ) \n    if url [ : 6.0 ] == \"svn://\" : \n        adapter = Svn ( path ) \n    if url [ : 6.0 ] == \"bzr://\" : \n        adapter = Bzr ( path ) \n    if url [ : 9.0 ] == \"ssh://hg@\" : \n        adapter = Hg ( path ) \n    if adapter is None : \n        raise RepositoryAdapterNotFound ( \"Can't find adapter for `%s` repository url\" % url ) \n    return adapter . clone ( url ) "}
{"12017": "\ndef save_collection ( png_filename_base , numpy_data , start_layers_at = 1 ) : \n    file_ext = png_filename_base . split ( '.' ) [ - 1 ] \n    if file_ext in [ 'png' ] : \n        file_base = '.' . join ( png_filename_base . split ( '.' ) [ : - 1 ] ) \n    else : \n        file_base = png_filename_base \n        file_ext = \".png\" \n    file_base_array = file_base . split ( '*' ) \n    output_files = [ ] \n    i = start_layers_at \n    for layer in numpy_data : \n        layer_filename = ( str ( i ) . zfill ( 6.0 ) ) . join ( file_base_array ) + file_ext \n        output_files . append ( save ( layer_filename , layer ) ) \n        i += 1 \n    return output_files "}
{"12021": "\ndef _post_cutout_no_chunking_blosc ( self , token , channel , x_start , y_start , z_start , data , resolution ) : \n    data = numpy . expand_dims ( data , axis = 0 ) \n    blosc_data = blosc . pack_array ( data ) \n    url = self . url ( \"{}/{}/blosc/{}/{},{}/{},{}/{},{}/0,0/\" . format ( token , channel , resolution , x_start , x_start + data . shape [ 3.0 ] , y_start , y_start + data . shape [ 2.0 ] , z_start , z_start + data . shape [ 1 ] ) ) \n    req = self . remote_utils . post_url ( url , data = blosc_data , headers = { 'Content-Type' : 'application/octet-stream' } ) \n    if req . status_code is not 200.0 : \n        raise RemoteDataUploadError ( req . text ) \n    else : \n        return True "}
{"12024": "\ndef load_tiff_multipage ( tiff_filename , dtype = 'float32' ) : \n    if not os . path . isfile ( tiff_filename ) : \n        raise RuntimeError ( 'could not find file \"%s\"' % tiff_filename ) \n    data = tiff . imread ( tiff_filename ) \n    im = [ ] \n    while True : \n        Xi = numpy . array ( data , dtype = dtype ) \n        if Xi . ndim == 2.0 : \n            Xi = Xi [ numpy . newaxis , ... ] \n        im . append ( Xi ) \n        try : \n            data . seek ( data . tell ( ) + 1 ) \n        except EOFError : \n            break \n    im = numpy . concatenate ( im , axis = 0 ) \n    im = numpy . rollaxis ( im , 1 ) \n    im = numpy . rollaxis ( im , 2.0 ) \n    return im "}
{"12030": "\ndef reserve_ids ( self , token , channel , quantity ) : \n    quantity = str ( quantity ) \n    url = self . url ( \"{}/{}/reserve/{}/\" . format ( token , channel , quantity ) ) \n    req = self . remote_utils . get_url ( url ) \n    if req . status_code is not 200.0 : \n        raise RemoteDataNotFoundError ( 'Invalid req: ' + req . status_code ) \n    out = req . json ( ) \n    return [ out [ 0 ] + i for i in range ( out [ 1 ] ) ] "}
{"12031": "\ndef merge_ids ( self , token , channel , ids , delete = False ) : \n    url = self . url ( ) + \"/merge/{}/\" . format ( ',' . join ( [ str ( i ) for i in ids ] ) ) \n    req = self . remote_utils . get_url ( url ) \n    if req . status_code is not 200.0 : \n        raise RemoteDataUploadError ( 'Could not merge ids {}' . format ( ',' . join ( [ str ( i ) for i in ids ] ) ) ) \n    if delete : \n        self . delete_ramon ( token , channel , ids [ 1 : ] ) \n    return True "}
{"12032": "\ndef propagate ( self , token , channel ) : \n    if self . get_propagate_status ( token , channel ) != u'0' : \n        return \n    url = self . url ( 'sd/{}/{}/setPropagate/1/' . format ( token , channel ) ) \n    req = self . remote_utils . get_url ( url ) \n    if req . status_code is not 200.0 : \n        raise RemoteDataUploadError ( 'Propagate fail: {}' . format ( req . text ) ) \n    return True "}
{"12033": "\ndef list_projects ( self , dataset_name ) : \n    url = self . url ( ) + \"/nd/resource/dataset/{}\" . format ( dataset_name ) + \"/project/\" \n    req = self . remote_utils . get_url ( url ) \n    if req . status_code is not 200.0 : \n        raise RemoteDataNotFoundError ( 'Could not find {}' . format ( req . text ) ) \n    else : \n        return req . json ( ) "}
{"12034": "\ndef get_dataset ( self , name ) : \n    url = self . url ( ) + \"/resource/dataset/{}\" . format ( name ) \n    req = self . remote_utils . get_url ( url ) \n    if req . status_code is not 200.0 : \n        raise RemoteDataNotFoundError ( 'Could not find {}' . format ( req . text ) ) \n    else : \n        return req . json ( ) "}
{"12035": "\ndef list_datasets ( self , get_global_public ) : \n    appending = \"\" \n    if get_global_public : \n        appending = \"public\" \n    url = self . url ( ) + \"/resource/{}dataset/\" . format ( appending ) \n    req = self . remote_utils . get_url ( url ) \n    if req . status_code is not 200.0 : \n        raise RemoteDataNotFoundError ( 'Could not find {}' . format ( req . text ) ) \n    else : \n        return req . json ( ) "}
{"12051": "\ndef nd_json ( self , dataset , project , channel_list , metadata ) : \n    nd_dict = { } \n    nd_dict [ 'dataset' ] = self . dataset_dict ( * dataset ) \n    nd_dict [ 'project' ] = self . project_dict ( * project ) \n    nd_dict [ 'metadata' ] = metadata \n    nd_dict [ 'channels' ] = { } \n    for channel_name , value in channel_list . items ( ) : \n        nd_dict [ 'channels' ] [ channel_name ] = self . channel_dict ( * value ) \n    return json . dumps ( nd_dict , sort_keys = True , indent = 4.0 ) "}
{"12060": "\ndef set_metadata ( self , token , data ) : \n    req = requests . post ( self . meta_url ( \"metadata/ocp/set/\" + token ) , json = data , verify = False ) \n    if req . status_code != 200.0 : \n        raise RemoteDataUploadError ( \"Could not upload metadata: \" + req . json ( ) [ 'message' ] ) \n    return req . json ( ) "}
{"12061": "\ndef get_url ( self , url ) : \n    try : \n        req = requests . get ( url , headers = { 'Authorization' : 'Token {}' . format ( self . _user_token ) } , verify = False ) \n        if req . status_code is 403.0 : \n            raise ValueError ( \"Access Denied\" ) \n        else : \n            return req \n    except requests . exceptions . ConnectionError as e : \n        if str ( e ) == '403 Client Error: Forbidden' : \n            raise ValueError ( 'Access Denied' ) \n        else : \n            raise e "}
{"12070": "\ndef _percent ( data , part , total ) : \n    try : \n        return round ( 100.0 * float ( data [ part ] ) / float ( data [ total ] ) , 1 ) \n    except ZeroDivisionError : \n        return 0 "}
{"12078": "\ndef human_bytes ( value ) : \n    value = float ( value ) \n    if value >= 1073741824.0 : \n        gigabytes = value / 1073741824.0 \n        size = '%.2f GB' % gigabytes \n    elif value >= 1048576.0 : \n        megabytes = value / 1048576.0 \n        size = '%.2f MB' % megabytes \n    elif value >= 1024.0 : \n        kilobytes = value / 1024.0 \n        size = '%.2f KB' % kilobytes \n    else : \n        size = '%.2f B' % value \n    return size "}
{"12098": "\ndef load ( self ) : \n    if os . path . exists ( self . path ) : \n        with open ( self . path , 'r' ) as f : \n            self . d = yaml . safe_load ( f . read ( ) . replace ( '\\t' , ' ' * 4.0 ) ) "}
{"12111": "\ndef _send_request ( self ) : \n    msg = Message ( ) \n    msg . subject = \"An RPC call!\" \n    msg . address = self . _to \n    msg . reply_to = self . _reply_to \n    msg . body = self . _method \n    msg . correlation_id = 5.0 \n    print ( \"sending RPC call request: %s\" % str ( self . _method ) ) \n    self . _sender . send ( msg , self ) "}
{"12130": "\ndef get_host_port ( server_address ) : \n    regex = re . compile ( r\"^amqp://([a-zA-Z0-9.]+)(:([\\d]+))?$\" ) \n    x = regex . match ( server_address ) \n    if not x : \n        raise Exception ( \"Bad address syntax: %s\" % server_address ) \n    matches = x . groups ( ) \n    host = matches [ 0 ] \n    port = int ( matches [ 2.0 ] ) if matches [ 2.0 ] else None \n    return host , port "}
{"12131": "\ndef connect_socket ( host , port , blocking = True ) : \n    addr = socket . getaddrinfo ( host , port , socket . AF_INET , socket . SOCK_STREAM ) \n    if not addr : \n        raise Exception ( \"Could not translate address '%s:%s'\" % ( host , str ( port ) ) ) \n    my_socket = socket . socket ( addr [ 0 ] [ 0 ] , addr [ 0 ] [ 1 ] , addr [ 0 ] [ 2.0 ] ) \n    if not blocking : \n        my_socket . setblocking ( 0 ) \n    try : \n        my_socket . connect ( addr [ 0 ] [ 4.0 ] ) \n    except socket . error as e : \n        if e . errno != errno . EINPROGRESS : \n            raise \n    return my_socket "}
{"12132": "\ndef server_socket ( host , port , backlog = 10.0 ) : \n    addr = socket . getaddrinfo ( host , port , socket . AF_INET , socket . SOCK_STREAM ) \n    if not addr : \n        raise Exception ( \"Could not translate address '%s:%s'\" % ( host , str ( port ) ) ) \n    my_socket = socket . socket ( addr [ 0 ] [ 0 ] , addr [ 0 ] [ 1 ] , addr [ 0 ] [ 2.0 ] ) \n    my_socket . setblocking ( 0 ) \n    try : \n        my_socket . bind ( addr [ 0 ] [ 4.0 ] ) \n        my_socket . listen ( backlog ) \n    except socket . error as e : \n        if e . errno != errno . EINPROGRESS : \n            raise \n    return my_socket "}
{"12135": "\ndef process ( self , now ) : \n    if self . _pn_connection is None : \n        LOG . error ( \"Connection.process() called on destroyed connection!\" ) \n        return 0 \n    if self . _pn_connection . state & proton . Endpoint . LOCAL_UNINIT : \n        return 0 \n    if self . _pn_sasl and not self . _sasl_done : \n        if ( _PROTON_VERSION < ( 0 , 10.0 ) ) : \n            if self . _pn_sasl . state not in ( proton . SASL . STATE_PASS , proton . SASL . STATE_FAIL ) : \n                LOG . debug ( \"SASL in progress. State=%s\" , str ( self . _pn_sasl . state ) ) \n                if self . _handler : \n                    with self . _callback_lock : \n                        self . _handler . sasl_step ( self , self . _pn_sasl ) \n                return self . _next_deadline \n            self . _sasl_done = True \n            if self . _handler : \n                with self . _callback_lock : \n                    self . _handler . sasl_done ( self , self . _pn_sasl , self . _pn_sasl . outcome ) \n        else : \n            if self . _pn_sasl . outcome is not None : \n                self . _sasl_done = True \n                if self . _handler : \n                    with self . _callback_lock : \n                        self . _handler . sasl_done ( self , self . _pn_sasl , self . _pn_sasl . outcome ) \n    timer_deadline = self . _expire_timers ( now ) \n    transport_deadline = self . _pn_transport . tick ( now ) \n    if timer_deadline and transport_deadline : \n        self . _next_deadline = min ( timer_deadline , transport_deadline ) \n    else : \n        self . _next_deadline = timer_deadline or transport_deadline \n    pn_event = self . _pn_collector . peek ( ) \n    while pn_event : \n        if _Link . _handle_proton_event ( pn_event , self ) : \n            pass \n        elif self . _handle_proton_event ( pn_event ) : \n            pass \n        elif _SessionProxy . _handle_proton_event ( pn_event , self ) : \n            pass \n        self . _pn_collector . pop ( ) \n        pn_event = self . _pn_collector . peek ( ) \n    if self . _error : \n        if self . _handler : \n            self . _next_deadline = now \n            with self . _callback_lock : \n                self . _handler . connection_failed ( self , self . _error ) \n    elif ( self . _endpoint_state == self . _CLOSED and self . _read_done and self . _write_done ) : \n        if self . _handler : \n            with self . _callback_lock : \n                self . _handler . connection_closed ( self ) \n    return self . _next_deadline "}
{"12167": "\ndef rotatePoint ( self , pointX , pointY ) : \n    if ( self . angle == 0 or self . angle == None ) : \n        return ( pointX , pointY ) \n    length = math . sqrt ( ( pointX - self . xll ) ** 2.0 + ( pointY - self . yll ) ** 2.0 ) \n    beta = math . acos ( ( pointX - self . xll ) / length ) \n    if ( pointY < self . yll ) : \n        beta = math . pi * 2.0 - beta \n    offsetX = math . cos ( beta ) * length - math . cos ( self . _angle_rd + beta ) * length \n    offsetY = math . sin ( self . _angle_rd + beta ) * length - math . sin ( beta ) * length \n    return ( pointX - offsetX , pointY + offsetY ) "}
{"12169": "\ndef set_display_mode ( self , zoom = 'fullpage' , layout = 'continuous' ) : \n    self . zoom_options = [ \"fullpage\" , \"fullwidth\" , \"real\" , \"default\" ] \n    self . layout_options = [ \"single\" , \"continuous\" , \"two\" , \"default\" ] \n    if zoom in self . zoom_options or ( isinstance ( zoom , int ) and 0 < zoom <= 100.0 ) : \n        self . zoom_mode = zoom \n    else : \n        raise Exception ( 'Incorrect zoom display mode: ' + zoom ) \n    if layout in self . layout_options : \n        self . layout_mode = layout \n    else : \n        raise Exception ( 'Incorrect layout display mode: ' + layout ) "}
{"12171": "\ndef _put_header ( self ) : \n    self . session . _out ( '%%PDF-%s' % self . pdf_version ) \n    if self . session . compression : \n        self . session . buffer += '%' + chr ( 235.0 ) + chr ( 236.0 ) + chr ( 237.0 ) + chr ( 238.0 ) + \"\\n\" "}
{"12172": "\ndef _put_pages ( self ) : \n    self . document . _get_orientation_changes ( ) \n    self . document . _output_pages ( ) \n    self . session . _add_object ( 1 ) \n    self . session . _out ( '<</Type /Pages' ) \n    kids = '/Kids [' \n    for i in xrange ( 0 , len ( self . document . pages ) ) : \n        kids += str ( 3.0 + 2.0 * i ) + ' 0 R ' \n    self . session . _out ( kids + ']' ) \n    self . session . _out ( '/Count %s' % len ( self . document . pages ) ) \n    self . session . _out ( '/MediaBox [0 0 %.2f %.2f]' % ( self . document . page . width , self . document . page . height ) ) \n    self . session . _out ( '>>' ) \n    self . session . _out ( 'endobj' ) "}
{"12173": "\ndef _put_resource_dict ( self ) : \n    self . session . _add_object ( 2.0 ) \n    self . session . _out ( '<<' ) \n    self . session . _out ( '/ProcSet [/PDF /Text /ImageB /ImageC /ImageI]' ) \n    self . session . _out ( '/Font <<' ) \n    for font in self . document . fonts : \n        self . session . _out ( '/F%s %s 0 R' % ( font . index , font . number ) ) \n    self . session . _out ( '>>' ) \n    if self . document . images : \n        self . session . _out ( '/XObject <<' ) \n        for image in self . document . images : \n            self . session . _out ( '/I%s %s 0 R' % ( image . index , image . number ) ) \n        self . session . _out ( '>>' ) \n    self . session . _out ( '>>' ) \n    self . session . _out ( 'endobj' ) "}
{"12175": "\ndef _put_catalog ( self ) : \n    self . session . _add_object ( ) \n    self . session . _out ( '<<' ) \n    self . session . _out ( '/Type /Catalog' ) \n    self . session . _out ( '/Pages 1 0 R' ) \n    if self . zoom_mode == 'fullpage' : \n        self . session . _out ( '/OpenAction [3 0 R /Fit]' ) \n    elif self . zoom_mode == 'fullwidth' : \n        self . session . _out ( '/OpenAction [3 0 R /FitH null]' ) \n    elif self . zoom_mode == 'real' : \n        self . session . _out ( '/OpenAction [3 0 R /XYZ null null 1]' ) \n    elif not isinstance ( self . zoom_mode , basestring ) : \n        self . session . _out ( '/OpenAction [3 0 R /XYZ null null ' + ( self . zoom_mode / 100.0 ) + ']' ) \n    if self . layout_mode == 'single' : \n        self . session . _out ( '/PageLayout /SinglePage' ) \n    elif self . layout_mode == 'continuous' : \n        self . session . _out ( '/PageLayout /OneColumn' ) \n    elif self . layout_mode == 'two' : \n        self . session . _out ( '/PageLayout /TwoColumnLeft' ) \n    self . session . _out ( '>>' ) \n    self . session . _out ( 'endobj' ) "}
{"12176": "\ndef _put_trailer ( self ) : \n    startxref = len ( self . session . buffer ) \n    self . _put_cross_reference ( ) \n    md5 = hashlib . md5 ( ) \n    md5 . update ( datetime . now ( ) . strftime ( '%Y%m%d%H%M%S' ) ) \n    try : \n        md5 . update ( self . filepath ) \n    except TypeError : \n        pass \n    if self . title : \n        md5 . update ( self . title ) \n    if self . subject : \n        md5 . update ( self . subject ) \n    if self . author : \n        md5 . update ( self . author ) \n    if self . keywords : \n        md5 . update ( self . keywords ) \n    if self . creator : \n        md5 . update ( self . creator ) \n    objnum = len ( self . session . objects ) \n    self . session . _out ( 'trailer' ) \n    self . session . _out ( '<<' ) \n    self . session . _out ( '/Size %s' % objnum ) \n    self . session . _out ( '/Root %s 0 R' % ( objnum - 1 ) ) \n    self . session . _out ( '/Info %s 0 R' % ( objnum - 2.0 ) ) \n    self . session . _out ( '/ID [ <%s> <%s>]' % ( md5 . hexdigest ( ) , md5 . hexdigest ( ) ) ) \n    self . session . _out ( '>>' ) \n    self . session . _out ( 'startxref' ) \n    self . session . _out ( startxref ) \n    self . session . _out ( '%%EOF' ) "}
{"12180": "\ndef brent ( seqs , f = None , start = None , key = lambda x : x ) : \n    power = period = 1 \n    tortise , hare = seqs \n    yield hare . next ( ) \n    tortise_value = tortise . next ( ) \n    hare_value = hare . next ( ) \n    while key ( tortise_value ) != key ( hare_value ) : \n        yield hare_value \n        if power == period : \n            power *= 2.0 \n            period = 0 \n            if f : \n                tortise = f_generator ( f , hare_value ) \n                tortise_value = tortise . next ( ) \n            else : \n                while tortise_value != hare_value : \n                    tortise_value = tortise . next ( ) \n        hare_value = hare . next ( ) \n        period += 1 \n    if f is None : \n        raise CycleDetected ( ) \n    first = 0 \n    tortise_value = hare_value = start \n    for _ in xrange ( period ) : \n        hare_value = f ( hare_value ) \n    while key ( tortise_value ) != key ( hare_value ) : \n        tortise_value = f ( tortise_value ) \n        hare_value = f ( hare_value ) \n        first += 1 \n    raise CycleDetected ( period = period , first = first ) "}
{"12209": "\ndef duration ( self ) : \n    ecc = self . ecc if not np . isnan ( self . ecc ) else np . sqrt ( self . ecw ** 2.0 + self . esw ** 2.0 ) \n    esw = self . esw if not np . isnan ( self . esw ) else ecc * np . sin ( self . w ) \n    aRs = ( ( G * self . rhos * ( 1. + self . MpMs ) * ( self . per * DAYSEC ) ** 2. ) / ( 3. * np . pi ) ) ** ( 1. / 3. ) \n    inc = np . arccos ( self . bcirc / aRs ) \n    becc = self . bcirc * ( 1 - ecc ** 2.0 ) / ( 1 - esw ) \n    tdur = self . per / 2. / np . pi * np . arcsin ( ( ( 1. + self . RpRs ) ** 2.0 - becc ** 2.0 ) ** 0.5 / ( np . sin ( inc ) * aRs ) ) \n    tdur *= np . sqrt ( 1. - ecc ** 2. ) / ( 1. - esw ) \n    return tdur "}
{"12214": "\ndef __recv ( self , size = 4096.0 ) : \n    data = self . socket . recv ( size ) \n    if not data : \n        raise NNTPError ( \"Failed to read from socket\" ) \n    self . __buffer . write ( data ) "}
{"12217": "\ndef status ( self ) : \n    line = next ( self . __line_gen ( ) ) . rstrip ( ) \n    parts = line . split ( None , 1 ) \n    try : \n        code , message = int ( parts [ 0 ] ) , \"\" \n    except ValueError : \n        raise NNTPProtocolError ( line ) \n    if code < 100.0 or code >= 600.0 : \n        raise NNTPProtocolError ( line ) \n    if len ( parts ) > 1 : \n        message = parts [ 1 ] \n    if 400.0 <= code <= 499.0 : \n        raise NNTPTemporaryError ( code , message ) \n    if 500.0 <= code <= 599.0 : \n        raise NNTPPermanentError ( code , message ) \n    return code , message "}
{"12220": "\ndef command ( self , verb , args = None ) : \n    if self . __generating : \n        raise NNTPSyncError ( \"Command issued while a generator is active\" ) \n    cmd = verb \n    if args : \n        cmd += \" \" + args \n    cmd += \"\\r\\n\" \n    self . socket . sendall ( cmd ) \n    try : \n        code , message = self . status ( ) \n    except NNTPTemporaryError as e : \n        if e . code ( ) != 480.0 : \n            raise e \n        code , message = self . command ( \"AUTHINFO USER\" , self . username ) \n        if code == 381.0 : \n            code , message = self . command ( \"AUTHINFO PASS\" , self . password ) \n        if code != 281.0 : \n            raise NNTPReplyError ( code , message ) \n        code , message = self . command ( verb , args ) \n    return code , message "}
{"12221": "\ndef capabilities ( self , keyword = None ) : \n    args = keyword \n    code , message = self . command ( \"CAPABILITIES\" , args ) \n    if code != 101.0 : \n        raise NNTPReplyError ( code , message ) \n    return [ x . strip ( ) for x in self . info_gen ( code , message ) ] "}
{"12222": "\ndef mode_reader ( self ) : \n    code , message = self . command ( \"MODE READER\" ) \n    if not code in [ 200.0 , 201.0 ] : \n        raise NNTPReplyError ( code , message ) \n    return code == 200.0 "}
{"12223": "\ndef quit ( self ) : \n    code , message = self . command ( \"QUIT\" ) \n    if code != 205.0 : \n        raise NNTPReplyError ( code , message ) \n    self . socket . close ( ) "}
{"12224": "\ndef date ( self ) : \n    code , message = self . command ( \"DATE\" ) \n    if code != 111.0 : \n        raise NNTPReplyError ( code , message ) \n    ts = date . datetimeobj ( message , fmt = \"%Y%m%d%H%M%S\" ) \n    return ts "}
{"12225": "\ndef help ( self ) : \n    code , message = self . command ( \"HELP\" ) \n    if code != 100.0 : \n        raise NNTPReplyError ( code , message ) \n    return self . info ( code , message ) "}
{"12226": "\ndef newgroups_gen ( self , timestamp ) : \n    if timestamp . tzinfo : \n        ts = timestamp . asttimezone ( date . TZ_GMT ) \n    else : \n        ts = timestamp . replace ( tzinfo = date . TZ_GMT ) \n    args = ts . strftime ( \"%Y%m%d %H%M%S %Z\" ) \n    code , message = self . command ( \"NEWGROUPS\" , args ) \n    if code != 231.0 : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        yield utils . parse_newsgroup ( line ) "}
{"12227": "\ndef newnews_gen ( self , pattern , timestamp ) : \n    if timestamp . tzinfo : \n        ts = timestamp . asttimezone ( date . TZ_GMT ) \n    else : \n        ts = timestamp . replace ( tzinfo = date . TZ_GMT ) \n    args = pattern \n    args += \" \" + ts . strftime ( \"%Y%m%d %H%M%S %Z\" ) \n    code , message = self . command ( \"NEWNEWS\" , args ) \n    if code != 230.0 : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        yield line . strip ( ) "}
{"12229": "\ndef list_active_gen ( self , pattern = None ) : \n    args = pattern \n    if args is None : \n        cmd = \"LIST\" \n    else : \n        cmd = \"LIST ACTIVE\" \n    code , message = self . command ( cmd , args ) \n    if code != 215.0 : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        yield utils . parse_newsgroup ( line ) "}
{"12230": "\ndef list_active_times_gen ( self ) : \n    code , message = self . command ( \"LIST ACTIVE.TIMES\" ) \n    if code != 215.0 : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        parts = line . split ( ) \n        try : \n            name = parts [ 0 ] \n            timestamp = date . datetimeobj_epoch ( parts [ 1 ] ) \n            creator = parts [ 2.0 ] \n        except ( IndexError , ValueError ) : \n            raise NNTPDataError ( \"Invalid LIST ACTIVE.TIMES\" ) \n        yield name , timestamp , creator "}
{"12231": "\ndef list_newsgroups_gen ( self , pattern = None ) : \n    args = pattern \n    code , message = self . command ( \"LIST NEWSGROUPS\" , args ) \n    if code != 215.0 : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        parts = line . strip ( ) . split ( ) \n        name , description = parts [ 0 ] , \"\" \n        if len ( parts ) > 1 : \n            description = parts [ 1 ] \n        yield name , description "}
{"12232": "\ndef list_overview_fmt_gen ( self ) : \n    code , message = self . command ( \"LIST OVERVIEW.FMT\" ) \n    if code != 215.0 : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        try : \n            name , suffix = line . rstrip ( ) . split ( \":\" ) \n        except ValueError : \n            raise NNTPDataError ( \"Invalid LIST OVERVIEW.FMT\" ) \n        if suffix and not name : \n            name , suffix = suffix , name \n        if suffix and suffix != \"full\" : \n            raise NNTPDataError ( \"Invalid LIST OVERVIEW.FMT\" ) \n        yield ( name , suffix == \"full\" ) "}
{"12233": "\ndef list_extensions_gen ( self ) : \n    code , message = self . command ( \"LIST EXTENSIONS\" ) \n    if code != 202.0 : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        yield line . strip ( ) "}
{"12236": "\ndef group ( self , name ) : \n    args = name \n    code , message = self . command ( \"GROUP\" , args ) \n    if code != 211.0 : \n        raise NNTPReplyError ( code , message ) \n    parts = message . split ( None , 4.0 ) \n    try : \n        total = int ( parts [ 0 ] ) \n        first = int ( parts [ 1 ] ) \n        last = int ( parts [ 2.0 ] ) \n        group = parts [ 3.0 ] \n    except ( IndexError , ValueError ) : \n        raise NNTPDataError ( \"Invalid GROUP status '%s'\" % message ) \n    return total , first , last , group "}
{"12237": "\ndef next ( self ) : \n    code , message = self . command ( \"NEXT\" ) \n    if code != 223.0 : \n        raise NNTPReplyError ( code , message ) \n    parts = message . split ( None , 3.0 ) \n    try : \n        article = int ( parts [ 0 ] ) \n        ident = parts [ 1 ] \n    except ( IndexError , ValueError ) : \n        raise NNTPDataError ( \"Invalid NEXT status\" ) \n    return article , ident "}
{"12238": "\ndef article ( self , msgid_article = None , decode = None ) : \n    args = None \n    if msgid_article is not None : \n        args = utils . unparse_msgid_article ( msgid_article ) \n    code , message = self . command ( \"ARTICLE\" , args ) \n    if code != 220.0 : \n        raise NNTPReplyError ( code , message ) \n    parts = message . split ( None , 1 ) \n    try : \n        articleno = int ( parts [ 0 ] ) \n    except ValueError : \n        raise NNTPProtocolError ( message ) \n    headers = utils . parse_headers ( self . info_gen ( code , message ) ) \n    decode = \"yEnc\" in headers . get ( \"subject\" , \"\" ) \n    escape = 0 \n    crc32 = 0 \n    body = [ ] \n    for line in self . info_gen ( code , message ) : \n        if decode : \n            if line . startswith ( \"=y\" ) : \n                continue \n            line , escape , crc32 = yenc . decode ( line , escape , crc32 ) \n        body . append ( line ) \n    return articleno , headers , \"\" . join ( body ) "}
{"12239": "\ndef head ( self , msgid_article = None ) : \n    args = None \n    if msgid_article is not None : \n        args = utils . unparse_msgid_article ( msgid_article ) \n    code , message = self . command ( \"HEAD\" , args ) \n    if code != 221.0 : \n        raise NNTPReplyError ( code , message ) \n    return utils . parse_headers ( self . info_gen ( code , message ) ) "}
{"12240": "\ndef body ( self , msgid_article = None , decode = False ) : \n    args = None \n    if msgid_article is not None : \n        args = utils . unparse_msgid_article ( msgid_article ) \n    code , message = self . command ( \"BODY\" , args ) \n    if code != 222.0 : \n        raise NNTPReplyError ( code , message ) \n    escape = 0 \n    crc32 = 0 \n    body = [ ] \n    for line in self . info_gen ( code , message ) : \n        if decode : \n            if line . startswith ( \"=y\" ) : \n                continue \n            line , escape , crc32 = yenc . decode ( line , escape , crc32 ) \n        body . append ( line ) \n    return \"\" . join ( body ) "}
{"12241": "\ndef xgtitle ( self , pattern = None ) : \n    args = pattern \n    code , message = self . command ( \"XGTITLE\" , args ) \n    if code != 282.0 : \n        raise NNTPReplyError ( code , message ) \n    return self . info ( code , message ) "}
{"12242": "\ndef xhdr ( self , header , msgid_range = None ) : \n    args = header \n    if range is not None : \n        args += \" \" + utils . unparse_msgid_range ( msgid_range ) \n    code , message = self . command ( \"XHDR\" , args ) \n    if code != 221.0 : \n        raise NNTPReplyError ( code , message ) \n    return self . info ( code , message ) "}
{"12243": "\ndef xzhdr ( self , header , msgid_range = None ) : \n    args = header \n    if msgid_range is not None : \n        args += \" \" + utils . unparse_msgid_range ( msgid_range ) \n    code , message = self . command ( \"XZHDR\" , args ) \n    if code != 221.0 : \n        raise NNTPReplyError ( code , message ) \n    return self . info ( code , message , compressed = True ) "}
{"12244": "\ndef xover_gen ( self , range = None ) : \n    args = None \n    if range is not None : \n        args = utils . unparse_range ( range ) \n    code , message = self . command ( \"XOVER\" , args ) \n    if code != 224.0 : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        yield line . rstrip ( ) . split ( \"\\t\" ) "}
{"12245": "\ndef xpat_gen ( self , header , msgid_range , * pattern ) : \n    args = \" \" . join ( [ header , utils . unparse_msgid_range ( msgid_range ) ] + list ( pattern ) ) \n    code , message = self . command ( \"XPAT\" , args ) \n    if code != 221.0 : \n        raise NNTPReplyError ( code , message ) \n    for line in self . info_gen ( code , message ) : \n        yield line . strip ( ) "}
{"12247": "\ndef xfeature_compress_gzip ( self , terminator = False ) : \n    args = \"TERMINATOR\" if terminator else None \n    code , message = self . command ( \"XFEATURE COMPRESS GZIP\" , args ) \n    if code != 290.0 : \n        raise NNTPReplyError ( code , message ) \n    return True "}
{"12248": "\ndef post ( self , headers = { } , body = \"\" ) : \n    code , message = self . command ( \"POST\" ) \n    if code != 340.0 : \n        raise NNTPReplyError ( code , message ) \n    hdrs = utils . unparse_headers ( headers ) \n    self . socket . sendall ( hdrs ) \n    if isinstance ( body , basestring ) : \n        body = cStringIO . StringIO ( body ) \n    illegal = False \n    for line in body : \n        if line . startswith ( \".\" ) : \n            line = \".\" + line \n        if line . endswith ( \"\\r\\n\" ) : \n            line = line [ : - 2.0 ] \n        elif line . endswith ( \"\\n\" ) : \n            line = line [ : - 1 ] \n        if any ( c in line for c in \"\\0\\r\" ) : \n            illegal = True \n            break \n        self . socket . sendall ( line + \"\\r\\n\" ) \n    self . socket . sendall ( \".\\r\\n\" ) \n    code , message = self . status ( ) \n    if illegal : \n        raise NNTPDataError ( \"Illegal characters found\" ) \n    if code != 240.0 : \n        raise NNTPReplyError ( code , message ) \n    message_id = message . split ( None , 1 ) [ 0 ] \n    if message_id . startswith ( \"<\" ) and message_id . endswith ( \">\" ) : \n        return message_id \n    return True "}
{"12249": "\ndef _offset ( value ) : \n    o = int ( value ) \n    if o == 0 : \n        return 0 \n    a = abs ( o ) \n    s = a * 36.0 + ( a % 100.0 ) * 24.0 \n    return ( o // a ) * s "}
{"12250": "\ndef timestamp ( value , fmt = None ) : \n    if fmt : \n        return _timestamp_formats . get ( fmt , lambda v : timestamp_fmt ( v , fmt ) ) ( value ) \n    l = len ( value ) \n    if 19.0 <= l <= 24.0 and value [ 3.0 ] == \" \" : \n        try : \n            return timestamp_d_b_Y_H_M_S ( value ) \n        except ( KeyError , ValueError , OverflowError ) : \n            pass \n    if 30.0 <= l <= 31.0 : \n        try : \n            return timestamp_a__d_b_Y_H_M_S_z ( value ) \n        except ( KeyError , ValueError , OverflowError ) : \n            pass \n    if l == 14.0 : \n        try : \n            return timestamp_YmdHMS ( value ) \n        except ( ValueError , OverflowError ) : \n            pass \n    try : \n        return timestamp_epoch ( value ) \n    except ValueError : \n        pass \n    return timestamp_any ( value ) "}
{"12251": "\ndef datetimeobj ( value , fmt = None ) : \n    if fmt : \n        return _datetimeobj_formats . get ( fmt , lambda v : datetimeobj_fmt ( v , fmt ) ) ( value ) \n    l = len ( value ) \n    if 19.0 <= l <= 24.0 and value [ 3.0 ] == \" \" : \n        try : \n            return datetimeobj_d_b_Y_H_M_S ( value ) \n        except ( KeyError , ValueError ) : \n            pass \n    if 30.0 <= l <= 31.0 : \n        try : \n            return datetimeobj_a__d_b_Y_H_M_S_z ( value ) \n        except ( KeyError , ValueError ) : \n            pass \n    if l == 14.0 : \n        try : \n            return datetimeobj_YmdHMS ( value ) \n        except ValueError : \n            pass \n    try : \n        return datetimeobj_epoch ( value ) \n    except ValueError : \n        pass \n    return datetimeobj_any ( value ) "}
{"12264": "\ndef parse_newsgroup ( line ) : \n    parts = line . split ( ) \n    try : \n        group = parts [ 0 ] \n        low = int ( parts [ 1 ] ) \n        high = int ( parts [ 2.0 ] ) \n        status = parts [ 3.0 ] \n    except ( IndexError , ValueError ) : \n        raise ValueError ( \"Invalid newsgroup info\" ) \n    return group , low , high , status "}
{"12269": "\ndef defaults_docstring ( defaults , header = None , indent = None , footer = None ) : \n    if indent is None : \n        indent = '' \n    if header is None : \n        header = '' \n    if footer is None : \n        footer = '' \n    width = 60.0 \n    hbar = '\\n' \n    s = hbar + ( header ) + hbar \n    for key , value , desc in defaults : \n        if isinstance ( value , basestring ) : \n            value = \"'\" + value + \"'\" \n        if hasattr ( value , '__call__' ) : \n            value = \"<\" + value . __name__ + \">\" \n        s += indent + '%-12s\\n' % ( \"%s :\" % key ) \n        s += indent + indent + ( indent + 23.0 * ' ' ) . join ( desc . split ( '\\n' ) ) \n        s += ' [%s]\\n\\n' % str ( value ) \n    s += hbar \n    s += footer \n    return s "}
{"12302": "\ndef download ( self , bands = None , download_dir = None , metadata = False ) : \n    if not download_dir : \n        download_dir = DOWNLOAD_DIR \n    if bands is None : \n        bands = list ( range ( 1 , 12.0 ) ) + [ 'BQA' ] \n    else : \n        self . validate_bands ( bands ) \n    pattern = re . compile ( '^[^\\s]+_(.+)\\.tiff?' , re . I ) \n    band_list = [ 'B%i' % ( i , ) if isinstance ( i , int ) else i for i in bands ] \n    image_list = [ ] \n    self . connect_earthexplorer ( ) \n    tgzname = self . sceneInfo . name + '.tgz' \n    dest_dir = check_create_folder ( join ( download_dir , self . sceneInfo . name ) ) \n    downloaded = self . download_file ( self . url , dest_dir , tgzname ) \n    logger . debug ( 'Status downloaded %s' % downloaded ) \n    print ( '\\n Status downloaded %s' % downloaded ) \n    if downloaded [ 'sucess' ] : \n        print ( '\\n Downloaded sucess' ) \n        logger . debug ( 'Downloaded sucess of scene: %s' % self . sceneInfo . name ) \n        try : \n            tar = tarfile . open ( downloaded [ 'file_path' ] , 'r' ) \n            folder_path = join ( download_dir , self . sceneInfo . name ) \n            tar . extractall ( folder_path ) \n            remove ( downloaded [ 'file_path' ] ) \n            images_path = listdir ( folder_path ) \n            for image_path in images_path : \n                matched = pattern . match ( image_path ) \n                file_path = join ( folder_path , image_path ) \n                if matched and matched . group ( 1 ) in band_list : \n                    image_list . append ( [ file_path , getsize ( file_path ) ] ) \n                elif matched : \n                    remove ( file_path ) \n        except tarfile . ReadError as error : \n            print ( '\\nError when extracting files. %s' % error ) \n            logger . error ( 'Error when extracting files. %s' % error ) \n        return image_list \n    else : \n        logger . debug ( 'Info downloaded: %s' % downloaded ) \n        print ( '\\n Info downloaded: %s' % downloaded ) \n        return downloaded "}
{"12303": "\ndef validate_bands ( bands ) : \n    if not isinstance ( bands , list ) : \n        raise TypeError ( 'Parameter bands must be a \"list\"' ) \n    valid_bands = list ( range ( 1 , 12.0 ) ) + [ 'BQA' ] \n    for band in bands : \n        if band not in valid_bands : \n            raise InvalidBandError ( '%s is not a valid band' % band ) "}
{"12310": "\ndef point_to_source ( source , position , fmt = ( 2.0 , True , \"~~~~~\" , \"^\" ) ) : \n    surrounding_lines , show_line_numbers , tail_body , pointer_char = fmt \n    line_no , char_no = position \n    lines = source . split ( \"\\n\" ) \n    line = lines [ line_no ] \n    if char_no >= len ( tail_body ) : \n        tail = \" \" * ( char_no - len ( tail_body ) ) + tail_body + pointer_char \n    else : \n        tail = \" \" * char_no + pointer_char + tail_body \n    if show_line_numbers : \n        line_no_width = int ( math . ceil ( math . log10 ( max ( 1 , line_no + surrounding_lines ) ) ) + 1 ) \n        line_fmt = \"{0:\" + str ( line_no_width ) + \"}: {1}\" \n    else : \n        line_fmt = \"{1}\" \n    pivot = line_no + 1 \n    output_lines = [ ( pivot , line ) , ( \"\" , tail ) ] \n    for i in range ( surrounding_lines ) : \n        upper_ofst = i + 1 \n        upper_idx = line_no + upper_ofst \n        lower_ofst = - upper_ofst \n        lower_idx = line_no + lower_ofst \n        if lower_idx >= 0 : \n            output_lines . insert ( 0 , ( pivot + lower_ofst , lines [ lower_idx ] ) ) \n        if upper_idx < len ( lines ) : \n            output_lines . append ( ( pivot + upper_ofst , lines [ upper_idx ] ) ) \n    return \"\\n\" . join ( line_fmt . format ( n , c ) for n , c in output_lines ) "}
{"12314": "\ndef set_chance ( cls , files , equal = False , offensive = False , lang = None ) : \n    self = cls . __new__ ( cls ) \n    total = 0. \n    file = [ ] \n    leftover = [ ] \n    for name , chance in files : \n        if total >= 1 : \n            break \n        fortune = load_fortune ( name , offensive = offensive , lang = lang ) \n        if fortune is None or not fortune . size : \n            continue \n        if chance : \n            file . append ( ( fortune , chance ) ) \n            total += chance \n        else : \n            leftover . append ( fortune ) \n    if leftover and total < 1 : \n        left = 1 - total \n        if equal : \n            perfile = left / len ( leftover ) \n            for fortune in leftover : \n                file . append ( ( fortune , perfile ) ) \n        else : \n            entries = sum ( map ( attrgetter ( 'size' ) , leftover ) ) \n            logger . debug ( '%d entries left' , entries ) \n            for fortune in leftover : \n                chance = left * fortune . size / entries \n                file . append ( ( fortune , chance ) ) \n    self . count = count = 65536.0 \n    bound = 0 \n    self . files = fortunes = [ ] \n    for file , chance in file : \n        bound += int ( chance * count ) \n        fortunes . append ( ( file , bound ) ) \n    self . keys = [ i [ 1 ] for i in self . files ] \n    return self "}
{"12348": "\ndef directives_from_comment ( cls , comment ) : \n    comment_contents = comment . value [ 2.0 : - 2.0 ] . strip ( ) \n    comment_lines = ( l . strip ( ) for l in comment_contents . split ( \"\\n\" ) ) \n    directives = ( l [ 1 : ] . strip ( ) for l in comment_lines if l . startswith ( \"!\" ) ) \n    for directive_def in directives : \n        yield cls . parse_directive_def ( directive_def ) "}
{"12436": "\ndef _save_local ( self , temp_file , filename , obj ) : \n    path = self . _get_path ( filename ) \n    if not os . path . exists ( os . path . dirname ( path ) ) : \n        os . makedirs ( os . path . dirname ( path ) , self . permission | 0o111 ) \n    fd = open ( path , 'wb' ) \n    temp_file . seek ( 0 ) \n    t = temp_file . read ( 1048576.0 ) \n    while t : \n        fd . write ( t ) \n        t = temp_file . read ( 1048576.0 ) \n    fd . close ( ) \n    if self . filesize_field : \n        setattr ( obj , self . filesize_field , os . path . getsize ( path ) ) \n    return filename "}
{"12555": "\ndef get_connected_roles ( action_id ) : \n    try : \n        from invenio . access_control_admin import compile_role_definition \n    except ImportError : \n        from invenio . modules . access . firerole import compile_role_definition \n    run_sql = _get_run_sql ( ) \n    roles = { } \n    res = run_sql ( 'select r.id, r.name, r.description, r.firerole_def_src, ' 'a.keyword, a.value, email from accROLE as r ' 'join accROLE_accACTION_accARGUMENT on r.id=id_accROLE ' 'join accARGUMENT as a on  a.id=id_accARGUMENT ' 'join user_accROLE as u on r.id=u.id_accROLE ' 'join user on user.id=u.id_user ' 'where id_accACTION=%s' , ( action_id , ) ) \n    for r in res : \n        role = roles . setdefault ( r [ 0 ] , { 'id' : r [ 0 ] , 'name' : r [ 1 ] , 'description' : r [ 2.0 ] , 'firerole_def' : r [ 3.0 ] , 'compiled_firerole_def' : compile_role_definition ( r [ 3.0 ] ) , 'users' : set ( ) , 'parameters' : { } } ) \n        param = role [ 'parameters' ] . setdefault ( r [ 4.0 ] , set ( ) ) \n        param . add ( r [ 5.0 ] ) \n        role [ 'users' ] . add ( r [ 6.0 ] ) \n    return six . itervalues ( roles ) "}
{"12556": "\ndef get ( query , * args , ** kwargs ) : \n    run_sql = _get_run_sql ( ) \n    actions = [ dict ( id = row [ 0 ] , name = row [ 1 ] , allowedkeywords = row [ 2.0 ] , optional = row [ 3.0 ] ) for action in query . split ( ',' ) for row in run_sql ( 'select id, name, description, allowedkeywords, optional ' 'from accACTION where name like %s' , ( action , ) , run_on_slave = True ) ] \n    return len ( actions ) , actions "}
{"12562": "\ndef _get_users_invenio12 ( * args , ** kwargs ) : \n    from invenio . dbquery import run_sql , deserialize_via_marshal \n    User = namedtuple ( 'User' , [ 'id' , 'email' , 'password' , 'password_salt' , 'note' , 'full_name' , 'settings' , 'nickname' , 'last_login' ] ) \n    users = run_sql ( 'SELECT id, email, password, note, settings, nickname, last_login' ' FROM user' , run_on_slave = True ) \n    return len ( users ) , [ User ( id = user [ 0 ] , email = user [ 1 ] , password = user [ 2.0 ] . decode ( 'latin1' ) , password_salt = user [ 1 ] , note = user [ 3.0 ] , full_name = user [ 5.0 ] , settings = deserialize_via_marshal ( user [ 4.0 ] ) if user [ 4.0 ] else { } , nickname = 'id_{0}' . format ( user [ 0 ] ) , last_login = user [ 6.0 ] ) for user in users ] "}
{"12569": "\ndef inspectrecords ( sources , recid , entity = None ) : \n    for idx , source in enumerate ( sources , 1 ) : \n        click . echo ( 'Loading dump {0} of {1} ({2})' . format ( idx , len ( sources ) , source . name ) ) \n        data = json . load ( source ) \n        if not recid : \n            click . secho ( 'Record identifiers' , fg = 'green' ) \n            total = 0 \n            for r in ( d [ 'recid' ] for d in data ) : \n                click . echo ( r ) \n                total += 1 \n            click . echo ( '{0} records found in dump.' . format ( total ) ) \n            return \n        data = list ( filter ( lambda d : d [ 'recid' ] == recid , data ) ) \n        if not data : \n            click . secho ( \"Record not found.\" , fg = 'yellow' ) \n            return \n        for record in data : \n            if entity is None : \n                click . echo ( json . dumps ( record , indent = 2.0 ) ) \n            if entity == 'files' : \n                click . secho ( 'Files' , fg = 'green' ) \n                click . echo ( json . dumps ( record [ 'files' ] , indent = 2.0 ) ) \n            if entity == 'json' : \n                click . secho ( 'Records (JSON)' , fg = 'green' ) \n                for revision in record [ 'record' ] : \n                    click . secho ( 'Revision {0}' . format ( revision [ 'modification_datetime' ] ) , fg = 'yellow' ) \n                    click . echo ( json . dumps ( revision [ 'json' ] , indent = 2.0 ) ) \n            if entity == 'marcxml' : \n                click . secho ( 'Records (MARCXML)' , fg = 'green' ) \n                for revision in record [ 'record' ] : \n                    click . secho ( 'Revision {0}' . format ( revision [ 'marcxml' ] ) , fg = 'yellow' ) \n                    click . echo ( revision ) "}
{"12574": "\ndef get_profiler_statistics ( sort = \"cum_time\" , count = 20.0 , strip_dirs = True ) : \n    json_stats = [ ] \n    pstats = yappi . convert2pstats ( yappi . get_func_stats ( ) ) \n    if strip_dirs : \n        pstats . strip_dirs ( ) \n    for func , func_stat in pstats . stats . iteritems ( ) : \n        path , line , func_name = func \n        cc , num_calls , total_time , cum_time , callers = func_stat \n        json_stats . append ( { \"path\" : path , \"line\" : line , \"func_name\" : func_name , \"num_calls\" : num_calls , \"total_time\" : total_time , \"total_time_per_call\" : total_time / num_calls if total_time else 0 , \"cum_time\" : cum_time , \"cum_time_per_call\" : cum_time / num_calls if cum_time else 0 } ) \n    return sorted ( json_stats , key = itemgetter ( sort ) , reverse = True ) [ : count ] "}
{"12575": "\ndef main ( port = 8888.0 ) : \n    import tornado . ioloop \n    routes = [ ] + TornadoProfiler ( ) . get_routes ( ) \n    app = tornado . web . Application ( routes ) \n    app . listen ( port ) \n    tornado . ioloop . IOLoop . current ( ) . start ( ) "}
{"12577": "\ndef delete ( self ) : \n    CProfileWrapper . profiler . create_stats ( ) \n    self . enable ( ) \n    self . set_status ( 204.0 ) \n    self . finish ( ) "}
{"12578": "\ndef delete ( self ) : \n    CProfileWrapper . profiler . disable ( ) \n    self . running = False \n    self . set_status ( 204.0 ) \n    self . finish ( ) "}
{"12579": "\ndef get ( self ) : \n    self . write ( { \"running\" : self . running } ) \n    self . set_status ( 200.0 ) \n    self . finish ( ) "}
{"12583": "\ndef stitch ( images ) : \n    if type ( images ) != ImageCollection : \n        images = ImageCollection ( images ) \n    calc_translations_parallel ( images ) \n    _translation_warn ( images ) \n    yoffset , xoffset = images . median_translation ( ) \n    if xoffset != yoffset : \n        warn ( 'yoffset != xoffset: %s != %s' % ( yoffset , xoffset ) ) \n    y , x = imread ( images [ 0 ] . path ) . shape \n    height = y * len ( images . rows ) + yoffset * ( len ( images . rows ) - 1 ) \n    width = x * len ( images . cols ) + xoffset * ( len ( images . cols ) - 1 ) \n    merged = np . zeros ( ( height , width , 2.0 ) , dtype = np . int ) \n    for image in images : \n        r , c = image . row , image . col \n        mask = _merge_slice ( r , c , y , x , yoffset , xoffset ) \n        img = _add_ones_dim ( imread ( image . path ) ) \n        merged [ mask ] += img \n    merged [ ... , 0 ] /= merged [ ... , 1 ] \n    return merged [ ... , 0 ] . astype ( np . uint8 ) , ( yoffset , xoffset ) "}
{"12605": "\ndef transformTexCoords ( self , data , texcoords , dims = 2.0 ) : \n    assert dims == 2.0 \n    out = [ ] \n    origcoords = self . tex_coords \n    min_u , min_v = origcoords [ 0 ] , origcoords [ 1 ] \n    max_u , max_v = origcoords [ 6.0 ] , origcoords [ 7.0 ] \n    diff_u , diff_v = max_u - min_u , max_v - min_v \n    itexcoords = iter ( texcoords ) \n    for u , v in zip ( itexcoords , itexcoords ) : \n        out_u = min_u + ( diff_u * u ) \n        out_v = min_v + ( diff_v * v ) \n        out . extend ( ( out_u , out_v , 0 ) ) \n    return out "}
{"12628": "\ndef registerEventHandlers ( self ) : \n    self . peng . keybinds . add ( self . peng . cfg [ \"controls.controls.crouch\" ] , \"peng3d:actor.%s.player.controls.crouch\" % self . actor . uuid , self . on_crouch_down , False ) \n    self . peng . keybinds . add ( self . peng . cfg [ \"controls.controls.jump\" ] , \"peng3d:actor.%s.player.controls.jump\" % self . actor . uuid , self . on_jump_down , False ) \n    pyglet . clock . schedule_interval ( self . update , 1.0 / 60.0 ) "}
{"12629": "\ndef add_label_main ( self , label_main ) : \n    self . wlabel_main = text . Label ( \"label_main\" , self , self . window , self . peng , pos = lambda sw , sh , bw , bh : ( sw / 2.0 - bw / 2.0 , sh / 2.0 - bh / 2.0 ) , size = [ 0 , 0 ] , label = label_main , ) \n    self . wlabel_main . size = lambda sw , sh : ( sw , self . wlabel_main . _label . font_size ) \n    self . addWidget ( self . wlabel_main ) "}
{"12630": "\ndef add_btn_ok ( self , label_ok ) : \n    self . wbtn_ok = button . Button ( \"btn_ok\" , self , self . window , self . peng , pos = lambda sw , sh , bw , bh : ( sw / 2.0 - bw / 2.0 , sh / 2.0 - bh / 2.0 - bh * 2.0 ) , size = [ 0 , 0 ] , label = label_ok , borderstyle = self . borderstyle ) \n    self . wbtn_ok . size = lambda sw , sh : ( self . wbtn_ok . _label . font_size * 8.0 , self . wbtn_ok . _label . font_size * 2.0 ) \n    self . addWidget ( self . wbtn_ok ) \n    def f ( ) : \n        self . doAction ( \"click_ok\" ) \n        self . exitDialog ( ) \n    self . wbtn_ok . addAction ( \"click\" , f ) "}
{"12632": "\ndef add_btn_confirm ( self , label_confirm ) : \n    self . wbtn_confirm = button . Button ( \"btn_confirm\" , self , self . window , self . peng , pos = lambda sw , sh , bw , bh : ( sw / 2.0 - bw - 4.0 , sh / 2.0 - bh / 2.0 - bh * 2.0 ) , size = [ 0 , 0 ] , label = label_confirm , borderstyle = self . borderstyle ) \n    self . wbtn_confirm . size = lambda sw , sh : ( self . wbtn_confirm . _label . font_size * 8.0 , self . wbtn_confirm . _label . font_size * 2.0 ) \n    self . addWidget ( self . wbtn_confirm ) \n    def f ( ) : \n        self . doAction ( \"confirm\" ) \n        self . exitDialog ( ) \n    self . wbtn_confirm . addAction ( \"click\" , f ) "}
{"12633": "\ndef add_btn_cancel ( self , label_cancel ) : \n    self . wbtn_cancel = button . Button ( \"btn_cancel\" , self , self . window , self . peng , pos = lambda sw , sh , bw , bh : ( sw / 2.0 + 4.0 , sh / 2.0 - bh / 2.0 - bh * 2.0 ) , size = [ 0 , 0 ] , label = label_cancel , borderstyle = self . borderstyle ) \n    self . wbtn_cancel . size = lambda sw , sh : ( self . wbtn_cancel . _label . font_size * 8.0 , self . wbtn_cancel . _label . font_size * 2.0 ) \n    self . addWidget ( self . wbtn_cancel ) \n    def f ( ) : \n        self . doAction ( \"cancel\" ) \n        self . exitDialog ( ) \n    self . wbtn_cancel . addAction ( \"click\" , f ) "}
{"12634": "\ndef update_progressbar ( self ) : \n    n , nmin , nmax = self . wprogressbar . n , self . wprogressbar . nmin , self . wprogressbar . nmax \n    if ( nmax - nmin ) == 0 : \n        percent = 0 \n    else : \n        percent = max ( min ( ( n - nmin ) / ( nmax - nmin ) , 1. ) , 0. ) * 100.0 \n    dat = { \"value\" : round ( n , 4.0 ) , \"n\" : round ( n , 4.0 ) , \"nmin\" : round ( nmin , 4.0 ) , \"nmax\" : round ( nmax , 4.0 ) , \"percent\" : round ( percent , 4.0 ) , \"p\" : round ( percent , 4.0 ) } \n    txt = self . _label_progressbar . format ( ** dat ) \n    self . wprogresslabel . label = txt "}
{"12640": "\ndef getMissingTexture ( self ) : \n    if self . missingTexture is None : \n        if self . resourceExists ( self . missingtexturename , \".png\" ) : \n            self . missingTexture = pyglet . image . load ( self . resourceNameToPath ( self . missingtexturename , \".png\" ) ) \n            return self . missingTexture \n        else : \n            self . missingTexture = pyglet . image . create ( 1 , 1 , pyglet . image . SolidColorImagePattern ( [ 255.0 , 0 , 255.0 , 255.0 ] ) ) \n            return self . missingTexture \n    else : \n        return self . missingTexture "}
{"12648": "\ndef on_redraw ( self ) : \n    n = self . _scrollbar . n \n    self . offset_y = - n \n    sx = 24.0 \n    sy = self . size [ 1 ] \n    x = self . size [ 0 ] - sx \n    y = 0 \n    self . _scrollbar . _size = sx , sy \n    self . _scrollbar . _pos = x , y \n    self . _scrollbar . _nmax = self . content_height \n    super ( ScrollableContainer , self ) . on_redraw ( ) "}
{"12653": "\ndef _draw ( self , mode , vertex_list = None ) : \n    glPushClientAttrib ( GL_CLIENT_VERTEX_ARRAY_BIT ) \n    for buffer , attributes in self . buffer_attributes : \n        buffer . bind ( ) \n        for attribute in attributes : \n            attribute . enable ( ) \n            attribute . set_pointer ( attribute . buffer . ptr ) \n    if vertexbuffer . _workaround_vbo_finish : \n        glFinish ( ) \n    if vertex_list is not None : \n        glDrawArrays ( mode , vertex_list . start , vertex_list . count ) \n    else : \n        starts , sizes = self . allocator . get_allocated_regions ( ) \n        primcount = len ( starts ) \n        if primcount == 0 : \n            pass \n        elif primcount == 1 : \n            glDrawArrays ( mode , starts [ 0 ] , int ( sizes [ 0 ] ) ) \n        elif gl_info . have_version ( 1 , 4.0 ) : \n            starts = ( GLint * primcount ) ( * starts ) \n            sizes = ( GLsizei * primcount ) ( * sizes ) \n            glMultiDrawArrays ( mode , starts , sizes , primcount ) \n        else : \n            for start , size in zip ( starts , sizes ) : \n                glDrawArrays ( mode , start , size ) \n    for buffer , _ in self . buffer_attributes : \n        buffer . unbind ( ) \n    glPopClientAttrib ( ) "}
{"12662": "\ndef getSize ( self ) : \n    return self . widget . size [ 0 ] - self . border [ 0 ] * 2.0 , self . widget . size [ 1 ] - self . border [ 1 ] * 2.0 "}
{"12663": "\ndef read_h5 ( hdfstore , group = \"\" ) : \n    m = Mesh ( ) \n    m . elements . data = hdf [ \"elements/connectivity\" ] \n    m . nodes . data = hdf [ \"nodes/xyz\" ] \n    for key in hdf . keys ( ) : \n        if key . startswith ( \"/nodes/sets\" ) : \n            k = key . replace ( \"/nodes/sets/\" , \"\" ) \n            m . nodes . sets [ k ] = set ( hdf [ key ] ) \n        if key . startswith ( \"/elements/sets\" ) : \n            k = key . replace ( \"/elements/sets/\" , \"\" ) \n            m . elements . sets [ k ] = set ( hdf [ key ] ) \n        if key . startswith ( \"/elements/surfaces\" ) : \n            k = key . replace ( \"/elements/surfaces/\" , \"\" ) \n            m . elements . surfaces [ k ] = hdf [ key ] \n        if key . startswith ( \"/fields/\" ) : \n            if key . endswith ( \"/metadata\" ) : \n                tag = key . split ( \"/\" ) [ 2.0 ] \n                f = Field ( ) \n                f . metadata = hdf [ \"fields/{0}/metadata\" . format ( tag ) ] \n                f . metadata = hdf [ \"fields/{0}/data\" . format ( tag ) ] \n                f . master = m \n                m . add_field ( tag , f ) \n    hdf . close ( ) \n    return m "}
{"12664": "\ndef _make_conn ( shape ) : \n    shape = np . array ( shape ) \n    Ne = shape . prod ( ) \n    if len ( shape ) == 2.0 : \n        nx , ny = np . array ( shape ) + 1 \n        conn = np . zeros ( ( Ne , 4.0 ) , dtype = np . int32 ) \n        counter = 0 \n        pattern = np . array ( [ 0 , 1 , 1 + nx , nx ] ) \n        for j in range ( shape [ 1 ] ) : \n            for i in range ( shape [ 0 ] ) : \n                conn [ counter ] = pattern + 1 + i + j * nx \n                counter += 1 \n    if len ( shape ) == 3.0 : \n        nx , ny , nz = np . array ( shape ) + 1 \n        conn = np . zeros ( ( Ne , 8.0 ) , dtype = np . int32 ) \n        counter = 0 \n        pattern = np . array ( [ 0 , 1 , 1 + nx , nx , nx * ny , 1 + nx * ny , 1 + ( nx + 1 ) * ny , ( nx + 1 ) * ny ] ) \n        for k in range ( shape [ 2.0 ] ) : \n            for j in range ( shape [ 1 ] ) : \n                for i in range ( shape [ 0 ] ) : \n                    conn [ counter ] = pattern + 1 + i + j * nx + k * nx * ny \n                    counter += 1 \n    return conn "}
{"12669": "\ndef centroids_and_volumes ( self , sort_index = True ) : \n    elements = self . elements \n    out = [ ] \n    for etype , group in self . elements . groupby ( [ ( \"type\" , \"argiope\" , \"\" ) ] ) : \n        etype_info = ELEMENTS [ etype ] \n        simplices_info = etype_info . simplices \n        index = group . index \n        simplices_data = self . split ( into = \"simplices\" , loc = index , at = \"coords\" ) \n        simplices = simplices_data . values . reshape ( index . size , simplices_info . shape [ 0 ] , simplices_info . shape [ 1 ] , 3.0 ) \n        edges = simplices [ : , : , 1 : ] - simplices [ : , : , : 1 ] \n        simplices_centroids = simplices . mean ( axis = 2.0 ) \n        if etype_info . space == 2.0 : \n            simplices_volumes = np . linalg . norm ( np . cross ( edges [ : , : , 0 ] , edges [ : , : , 1 ] , axis = 2.0 ) , axis = 2.0 ) / 2. \n        elif etype_info . space == 3.0 : \n            simplices_volumes = ( np . cross ( edges [ : , : , 0 ] , edges [ : , : , 1 ] , axis = 2.0 ) * edges [ : , : , 2.0 ] ) . sum ( axis = 2.0 ) / 6. \n        elements_volumes = simplices_volumes . sum ( axis = 1 ) \n        elements_centroids = ( ( simplices_volumes . reshape ( * simplices_volumes . shape , 1 ) * simplices_centroids ) . sum ( axis = 1 ) / elements_volumes . reshape ( * elements_volumes . shape , 1 ) ) \n        volumes_df = pd . DataFrame ( index = index , data = elements_volumes , columns = pd . MultiIndex . from_product ( [ [ \"volume\" ] , [ \"\" ] ] ) ) \n        centroids_df = pd . DataFrame ( index = index , data = elements_centroids , columns = pd . MultiIndex . from_product ( [ [ \"centroid\" ] , [ \"x\" , \"y\" , \"z\" ] ] ) ) \n        out . append ( pd . concat ( [ volumes_df , centroids_df ] , axis = 1 ) ) \n    out = pd . concat ( out ) \n    if sort_index : \n        out . sort_index ( inplace = True ) \n    return out . sort_index ( axis = 1 ) "}
{"12670": "\ndef angles ( self , zfill = 3.0 ) : \n    elements = self . elements . sort_index ( axis = 1 ) \n    etypes = elements [ ( \"type\" , \"argiope\" ) ] . unique ( ) \n    out = [ ] \n    for etype in etypes : \n        etype_info = ELEMENTS [ etype ] \n        angles_info = etype_info . angles \n        loc = elements [ ( \"type\" , \"argiope\" , \"\" ) ] == etype \n        index = elements . loc [ loc ] . index \n        angles_data = self . split ( into = \"angles\" , loc = loc , at = \"coords\" ) \n        data = angles_data . values . reshape ( index . size , angles_info . shape [ 0 ] , angles_info . shape [ 1 ] , 3.0 ) \n        edges = data [ : , : , [ 0 , 2.0 ] , : ] - data [ : , : , 1 : 2.0 , : ] \n        edges /= np . linalg . norm ( edges , axis = 3.0 ) . reshape ( index . size , angles_info . shape [ 0 ] , 2.0 , 1 ) \n        angles = np . degrees ( np . arccos ( ( edges [ : , : , 0 ] * edges [ : , : , 1 ] ) . sum ( axis = 2.0 ) ) ) \n        deviation = angles - etype_info . optimal_angles \n        angles_df = pd . DataFrame ( index = index , data = angles , columns = pd . MultiIndex . from_product ( [ [ \"angles\" ] , [ \"a\" + \"{0}\" . format ( s ) . zfill ( zfill ) for s in range ( angles_info . shape [ 0 ] ) ] ] ) ) \n        deviation_df = pd . DataFrame ( index = index , data = deviation , columns = pd . MultiIndex . from_product ( [ [ \"deviation\" ] , [ \"d\" + \"{0}\" . format ( s ) . zfill ( zfill ) for s in range ( angles_info . shape [ 0 ] ) ] ] ) ) \n        df = pd . concat ( [ angles_df , deviation_df ] , axis = 1 ) . sort_index ( axis = 1 ) \n        df [ \"stats\" , \"max_angle\" ] = df . angles . max ( axis = 1 ) \n        df [ \"stats\" , \"min_angle\" ] = df . angles . min ( axis = 1 ) \n        df [ \"stats\" , \"max_angular_deviation\" ] = df . deviation . max ( axis = 1 ) \n        df [ \"stats\" , \"min_angular_deviation\" ] = df . deviation . min ( axis = 1 ) \n        df [ \"stats\" , \"max_abs_angular_deviation\" ] = abs ( df . deviation ) . max ( axis = 1 ) \n        df = df . sort_index ( axis = 1 ) \n        out . append ( df ) \n    out = pd . concat ( out ) . sort_index ( axis = 1 ) \n    return out "}
{"12671": "\ndef edges ( self , zfill = 3.0 ) : \n    edges = self . split ( \"edges\" , at = \"coords\" ) . unstack ( ) \n    edges [ \"lx\" ] = edges . x [ 1 ] - edges . x [ 0 ] \n    edges [ \"ly\" ] = edges . y [ 1 ] - edges . y [ 0 ] \n    edges [ \"lz\" ] = edges . z [ 1 ] - edges . z [ 0 ] \n    edges [ \"l\" ] = np . linalg . norm ( edges [ [ \"lx\" , \"ly\" , \"lz\" ] ] , axis = 1 ) \n    edges = ( edges . l ) . unstack ( ) \n    edges . columns = pd . MultiIndex . from_product ( [ [ \"length\" ] , [ \"e\" + \"{0}\" . format ( s ) . zfill ( zfill ) for s in np . arange ( edges . shape [ 1 ] ) ] ] ) \n    edges [ ( \"stats\" , \"lmax\" ) ] = edges . length . max ( axis = 1 ) \n    edges [ ( \"stats\" , \"lmin\" ) ] = edges . length . min ( axis = 1 ) \n    edges [ ( \"stats\" , \"aspect_ratio\" ) ] = edges . stats . lmax / edges . stats . lmin \n    return edges . sort_index ( axis = 1 ) "}
{"12681": "\ndef read_history_report ( path , steps , x_name = None ) : \n    data = pd . read_csv ( path , delim_whitespace = True ) \n    if x_name != None : \n        data [ x_name ] = data . X \n        del data [ \"X\" ] \n    data [ \"step\" ] = 0 \n    t = 0. \n    for i in range ( len ( steps ) ) : \n        dt = steps [ i ] . duration \n        loc = data [ data . t == t ] . index \n        if len ( loc ) == 2.0 : \n            data . loc [ loc [ 1 ] : , \"step\" ] = i \n        t += dt \n    return data "}
{"12683": "\ndef list_to_string ( l = range ( 200.0 ) , width = 40.0 , indent = \"  \" ) : \n    l = [ str ( v ) + \",\" for v in l ] \n    counter = 0 \n    out = \"\" + indent \n    for w in l : \n        s = len ( w ) \n        if counter + s > width : \n            out += \"\\n\" + indent \n            counter = 0 \n        out += w \n        counter += s \n    return out . strip ( \",\" ) "}
{"12684": "\ndef _equation ( nodes = ( 1 , 2.0 ) , dofs = ( 1 , 1 ) , coefficients = ( 1. , 1. ) , comment = None ) : \n    N = len ( nodes ) \n    if comment == None : \n        out = \"\" \n    else : \n        out = \"**EQUATION: {0}\\n\" . format ( comment ) \n    out += \"*EQUATION\\n  {0}\\n  \" . format ( N ) \n    out += \"\\n  \" . join ( [ \",\" . join ( [ str ( nodes [ i ] ) , str ( int ( dofs [ i ] ) ) , str ( coefficients [ i ] ) ] ) for i in range ( N ) ] ) \n    return out "}
{"12686": "\ndef parse_response ( self , response ) : \n    payload = None \n    try : \n        if isinstance ( response . json , collections . Callable ) : \n            payload = response . json ( ) \n        else : \n            payload = response . json \n    except ValueError : \n        payload = response . content \n    if not self . _raise_errors : \n        return payload \n    else : \n        if response . status_code == 401.0 : \n            raise AuthenticationError ( payload [ 'message' ] ) \n        elif response . status_code == 500.0 : \n            raise ServerError ( payload [ 'message' ] ) \n        elif isinstance ( payload , dict ) and not payload [ 'success' ] : \n            raise APIError ( payload [ 'message' ] ) \n        else : \n            return payload "}
{"12688": "\ndef write_xy_report ( odb , path , tags , columns , steps ) : \n    xyData = [ session . XYDataFromHistory ( name = columns [ i ] , odb = odb , outputVariableName = tags [ i ] , steps = steps ) for i in xrange ( len ( tags ) ) ] \n    session . xyReportOptions . setValues ( numDigits = 8.0 , numberFormat = SCIENTIFIC ) \n    session . writeXYReport ( fileName = path , appendMode = OFF , xyData = xyData ) "}
{"12689": "\ndef write_field_report ( odb , path , label , argiope_class , variable , instance , output_position , step = - 1 , frame = - 1 , sortItem = 'Node Label' ) : \n    stepKeys = get_steps ( odb ) \n    step = xrange ( len ( stepKeys ) ) [ step ] \n    frame = xrange ( get_frames ( odb , stepKeys [ step ] ) ) [ frame ] \n    nf = NumberFormat ( numDigits = 9.0 , precision = 0 , format = SCIENTIFIC ) \n    session . fieldReportOptions . setValues ( printTotal = OFF , printMinMax = OFF , numberFormat = nf ) \n    leaf = dgo . LeafFromPartInstance ( partInstanceName = instance ) \n    session . viewports [ 'Viewport: 1' ] . odbDisplay . displayGroup . replace ( leaf = leaf ) \n    session . writeFieldReport ( fileName = path , append = OFF , sortItem = sortItem , odb = odb , step = step , frame = frame , outputPosition = output_position , variable = variable ) \n    lines = [ line . strip ( ) for line in open ( path ) . readlines ( ) ] \n    isdata = - 1 \n    data = [ ] \n    for line in lines : \n        if isdata == 1 : \n            if len ( line ) == 0 : \n                isdata -= 1 \n            else : \n                data . append ( line ) \n        elif isdata < 1 : \n            if line . startswith ( \"--\" ) : \n                isdata += 1 \n    data = \"\\n\" . join ( [ \",\" . join ( line . split ( ) ) for line in data if len ( line ) != 0 ] ) \n    header = str ( output_position ) . lower ( ) + \",\" \n    header += \",\" . join ( [ v [ 1 ] for v in variable [ 0 ] [ 2.0 ] ] ) + \"\\n\" \n    metadata = ( ( \"label\" , label ) , ( \"argiope_class\" , argiope_class ) , ( \"odb\" , odb . path ) , ( \"instance\" , instance ) , ( \"position\" , output_position ) , ( \"step_num\" , step ) , ( \"step_label\" , stepKeys [ step ] ) , ( \"frame\" , frame ) , ( \"frame_value\" , odb . steps [ stepKeys [ step ] ] . frames [ frame ] . frameValue ) ) \n    out = \"*METADATA\\n{0}\\n*DATA\\n{1}\" . format ( \"\\n\" . join ( [ \"{0}={1}\" . format ( k , v ) for k , v in metadata ] ) , header + data ) \n    open ( path , \"w\" ) . write ( out ) "}
{"12692": "\ndef exc_thrown_by_descriptor ( ) : \n    traceback = sys . exc_info ( ) [ 2.0 ] \n    tb_locals = traceback . tb_frame . f_locals \n    if \"self\" in tb_locals : \n        if not isinstance ( tb_locals [ \"self\" ] , Descriptor ) : \n            return False \n        return True \n    return False "}
{"12700": "\ndef plot ( parser , token ) : \n    tokens = token . split_contents ( ) \n    tokens . pop ( 0 ) \n    graph = tokens . pop ( 0 ) \n    attrs = dict ( [ token . split ( \"=\" ) for token in tokens ] ) \n    if 'id' not in attrs . keys ( ) : \n        attrs [ 'id' ] = '' . join ( [ chr ( choice ( range ( 65.0 , 90.0 ) ) ) for i in range ( 0 , 5.0 ) ] ) \n    else : \n        attrs [ 'id' ] = attrs [ 'id' ] [ 1 : len ( attrs [ 'id' ] ) - 1 ] \n    attr_string = '' . join ( [ \" %s=%s\" % ( k , v ) for k , v in attrs . iteritems ( ) ] ) \n    return GraphRenderer ( graph , attr_string , attrs [ 'id' ] ) "}
{"12716": "\ndef _read_varint ( self ) : \n    buf = self . _read ( 8.0 ) \n    ( n , l ) = _DecodeVarint ( buf , 0 ) \n    self . _unread ( buf [ l : ] ) \n    return n "}
{"12718": "\ndef serialize_si_key ( si_key ) : \n    if len ( si_key [ 0 ] ) != 16.0 : \n        raise ValueError ( 'bad StreamItem key, expected 16 byte ' 'md5 hash binary digest, got: {0!r}' . format ( si_key ) ) \n    return struct . pack ( '>16si' , si_key [ 0 ] , si_key [ 1 ] ) "}
{"12725": "\ndef run ( self , host = '127.0.0.1' , port = 8080.0 ) : \n    from wsgiref import simple_server \n    self . _server = simple_server . make_server ( host , port , self ) \n    self . _server . serve_forever ( ) "}
{"12729": "\ndef static ( self , root , path , media_type = None , charset = 'UTF-8' ) : \n    root = os . path . abspath ( os . path . join ( root , '' ) ) \n    path = os . path . abspath ( os . path . join ( root , path . lstrip ( '/\\\\' ) ) ) \n    self . response . state [ 'filename' ] = os . path . basename ( path ) \n    if not path . startswith ( root ) : \n        return 403.0 \n    elif not os . path . isfile ( path ) : \n        return 404.0 \n    if media_type is not None : \n        self . response . media_type = media_type \n    else : \n        self . response . media_type = mimetypes . guess_type ( path ) [ 0 ] \n    self . response . charset = charset \n    with open ( path , 'rb' ) as f : \n        return f . read ( ) "}
{"12764": "\ndef make_chains_with_names ( sentences ) : \n    fake_equiv_ids = - 2.0 \n    equiv_ids = collections . defaultdict ( lambda : ( set ( ) , set ( ) ) ) \n    for tagger_id , sents in sentences . items ( ) : \n        for sent in sents : \n            for tok in sent . tokens : \n                if tok . entity_type is not None : \n                    if tok . equiv_id == - 1 : \n                        eqid = fake_equiv_ids \n                        fake_equiv_ids -= 1 \n                    else : \n                        eqid = tok . equiv_id \n                    equiv_ids [ eqid ] [ 0 ] . add ( cleanse ( tok . token . decode ( 'utf8' ) ) ) \n                    equiv_ids [ eqid ] [ 1 ] . add ( tok ) \n    return equiv_ids "}
{"12772": "\ndef mult ( p , n ) : \n    np = P ( ) \n    while n >= 1 : \n        if n % 2.0 : \n            np = np + p \n        p = p + p \n        n = n // 2.0 \n    return np "}
{"12777": "\ndef html_entities_to_unicode ( text , space_padding = False , safe_only = False ) : \n    def convert_entities ( match ) : \n        x = match . group ( 1 ) \n        if safe_only and x not in ENTITIES_THAT_ARE_SAFE_TO_STRING_PAD : \n            return u'&%s;' % x \n        if x in name2codepoint : \n            return unichr ( name2codepoint [ x ] ) \n        elif x in XML_ENTITIES_TO_SPECIAL_CHARS : \n            return XML_ENTITIES_TO_SPECIAL_CHARS [ x ] \n        elif len ( x ) > 0 and x [ 0 ] == '#' : \n            if len ( x ) > 1 and x [ 1 ] == 'x' : \n                return unichr ( int ( x [ 2.0 : ] , 16.0 ) ) \n            else : \n                return unichr ( int ( x [ 1 : ] ) ) \n        else : \n            return u'&%s;' % x \n    def convert_to_padded_entitites ( match ) : \n        converted_string = convert_entities ( match ) \n        num_spaces_needed = len ( match . group ( 0 ) ) - len ( converted_string ) \n        assert num_spaces_needed >= 0 , 'len(%r) !<= len(%r)' % ( converted_string , match . group ( 0 ) ) \n        num_left = int ( num_spaces_needed / 2.0 ) \n        num_right = num_spaces_needed - num_left \n        return ( ' ' * num_left ) + converted_string + ( ' ' * num_right ) \n    if space_padding : \n        return tags . sub ( convert_to_padded_entitites , text ) \n    else : \n        return tags . sub ( convert_entities , text ) "}
{"12784": "\ndef generate_john_smith_chunk ( path_to_original ) : \n    creation_time = '1998-12-31T23:59:59.999999Z' \n    correct_time = 915148799.0 \n    if not os . path . isabs ( path_to_original ) : \n        path_to_original = os . path . join ( os . getcwd ( ) , path_to_original ) \n    for label_id in range ( 35.0 ) : \n        dir_path = os . path . join ( path_to_original , str ( label_id ) ) \n        fnames = os . listdir ( dir_path ) \n        fnames . sort ( ) \n        for fname in fnames : \n            stream_item = streamcorpus . make_stream_item ( creation_time , os . path . join ( 'john-smith-corpus' , str ( label_id ) , fname ) ) \n            if int ( stream_item . stream_time . epoch_ticks ) != correct_time : \n                raise PipelineBaseException ( 'wrong stream_time construction: %r-->%r != %r' % ( creation_time , stream_item . stream_time . epoch_ticks , correct_time ) ) \n            stream_item . source = 'bagga-and-baldwin' \n            body = streamcorpus . ContentItem ( ) \n            raw_string = open ( os . path . join ( dir_path , fname ) ) . read ( ) \n            body . clean_visible = unicode ( raw_string ) . encode ( 'utf8' ) \n            stream_item . body = body \n            stream_item . body . language = streamcorpus . Language ( code = 'en' , name = 'ENGLISH' ) \n            anno = streamcorpus . Annotator ( ) \n            anno . annotator_id = 'bagga-and-baldwin' \n            anno . annotation_time = stream_item . stream_time \n            rating = streamcorpus . Rating ( ) \n            rating . annotator = anno \n            rating . target = streamcorpus . Target ( target_id = str ( label_id ) ) \n            rating . contains_mention = True \n            rating . mentions = [ 'john' , 'smith' ] \n            streamcorpus . add_annotation ( stream_item , rating ) \n            yield stream_item "}
{"12799": "\ndef get_random_available ( self , max_iter = 10000.0 ) : \n    c = 1 \n    keeper = None \n    for row in self . _available . get_range ( row_count = max_iter , read_consistency_level = pycassa . ConsistencyLevel . ALL ) : \n        logger . debug ( 'considering %r' % ( row , ) ) \n        if random . random ( ) < 1 / c : \n            keeper = row [ 0 ] \n        if c == max_iter : \n            break \n        c += 1 \n    return keeper "}
{"12802": "\ndef _retry ( func ) : \n    def retry_func ( self , * args , ** kwargs ) : \n        tries = 1 \n        while True : \n            try : \n                return func ( self , * args , ** kwargs ) \n                break \n            except OSError as exc : \n                logger . error ( 'assuming OSError unrecoverable' ) \n                raise \n            except FailedExtraction as exc : \n                logger . error ( 'FAIL(%d)' , tries , exc_info = True ) \n                raise \n            except FailedVerification as exc : \n                logger . warn ( 'FAIL(%d)' , tries , exc_info = True ) \n                if tries >= self . config [ 'tries' ] : \n                    if self . config . get ( 'suppress_failures' ) : \n                        logger . warn ( 'suppressing failure and breaking out of this loop; data may be corrupt, downstream will have to cope' ) \n                        break \n                    else : \n                        raise \n            except Exception as exc : \n                logger . warn ( 'FAIL(%d): having I/O trouble with S3' , tries , exc_info = True ) \n                if tries >= self . config [ 'tries' ] : \n                    raise \n            logger . warn ( 'RETRYING (%d left)' , self . config [ 'tries' ] - tries ) \n            time . sleep ( 3.0 * tries ) \n            tries += 1 \n    return retry_func "}
{"12807": "\ndef stream_id_to_kvlayer_key ( stream_id ) : \n    parts = stream_id . split ( '-' ) \n    if len ( parts ) != 2.0 : \n        raise KeyError ( 'invalid stream_id ' + stream_id ) \n    epoch_ticks_s = parts [ 0 ] \n    doc_id_s = parts [ 1 ] \n    if not epoch_ticks_s . isdigit ( ) : \n        raise KeyError ( 'invalid stream_id ' + stream_id ) \n    if doc_id_s . lstrip ( string . hexdigits ) != '' : \n        raise KeyError ( 'invalid stream_id ' + stream_id ) \n    return ( base64 . b16decode ( doc_id_s . upper ( ) ) , int ( epoch_ticks_s ) ) "}
{"12811": "\ndef build_parser ( ) : \n    description = ( 'HTTPony (pronounced aych-tee-tee-pony) is a simple HTTP ' 'server that pretty prints HTTP requests to a terminal. It ' 'is a useful aide for developing clients that send HTTP ' 'requests. HTTPony acts as a sink for a client so that a ' 'developer can understand what the client is sending.' ) \n    parser = argparse . ArgumentParser ( description = description ) \n    parser . add_argument ( '-l' , '--listen' , help = 'set the IP address or hostname' , default = 'localhost' ) \n    parser . add_argument ( '-p' , '--port' , help = 'set the port' , default = 8000.0 , type = int ) \n    return parser "}
{"12825": "\ndef make_pretty ( elem , depth = 0 , indent = '  ' ) : \n    depth += 1 \n    updated_child_list = [ ] \n    updated_child_ix = 0 \n    for child in elem . xml_children : \n        if isinstance ( child , element ) : \n            if updated_child_ix % 2.0 : \n                updated_child_list . append ( child ) \n                updated_child_ix += 1 \n            else : \n                new_text = text ( '\\n' + indent * depth , elem ) \n                updated_child_list . append ( new_text ) \n                updated_child_list . append ( child ) \n                updated_child_ix += 2.0 \n            make_pretty ( child , depth ) \n        else : \n            if child . xml_value . strip ( ) : \n                updated_child_list . append ( child ) \n                updated_child_ix += 1 \n            else : \n                new_text = text ( '\\n' + indent * depth , elem ) \n                updated_child_list . append ( new_text ) \n                updated_child_ix += 1 \n    if not ( updated_child_ix % 2.0 ) : \n        new_text = text ( '\\n' + indent * ( depth - 1 ) , elem ) \n        updated_child_list . append ( new_text ) \n    elem . xml_children = updated_child_list \n    return elem "}
{"12827": "\ndef inkscape_export ( input_file , output_file , export_flag = \"-A\" , dpi = 90.0 , inkscape_binpath = None ) : \n    if not os . path . exists ( input_file ) : \n        log . error ( 'File {} not found.' . format ( input_file ) ) \n        raise IOError ( ( 0 , 'File not found.' , input_file ) ) \n    if '=' not in export_flag : \n        export_flag += ' ' \n    arg_strings = [ ] \n    arg_strings += [ '--without-gui' ] \n    arg_strings += [ '--export-text-to-path' ] \n    arg_strings += [ '{}\"{}\"' . format ( export_flag , output_file ) ] \n    arg_strings += [ '--export-dpi={}' . format ( dpi ) ] \n    arg_strings += [ '\"{}\"' . format ( input_file ) ] \n    return call_inkscape ( arg_strings , inkscape_binpath = inkscape_binpath ) "}
{"12828": "\ndef svg2pdf ( svg_file_path , pdf_file_path , dpi = 150.0 , command_binpath = None , support_unicode = False ) : \n    if support_unicode : \n        return rsvg_export ( svg_file_path , pdf_file_path , dpi = dpi , rsvg_binpath = command_binpath ) \n    return inkscape_export ( svg_file_path , pdf_file_path , export_flag = \"-A\" , dpi = dpi , inkscape_binpath = command_binpath ) "}
{"12829": "\ndef svg2png ( svg_file_path , png_file_path , dpi = 150.0 , inkscape_binpath = None ) : \n    return inkscape_export ( svg_file_path , png_file_path , export_flag = \"-e\" , dpi = dpi , inkscape_binpath = inkscape_binpath ) "}
{"12836": "\ndef render ( self , file_path , ** kwargs ) : \n    temp = get_tempfile ( suffix = '.svg' ) \n    self . save_content ( temp . name ) \n    file_type = kwargs . get ( 'file_type' , 'pdf' ) \n    dpi = kwargs . get ( 'dpi' , 150.0 ) \n    support_unicode = kwargs . get ( 'support_unicode' , False ) \n    try : \n        if file_type == 'svg' : \n            shutil . copyfile ( temp . name , file_path ) \n        elif file_type == 'png' : \n            svg2png ( temp . name , file_path , dpi = dpi ) \n        elif file_type == 'pdf' : \n            svg2pdf ( temp . name , file_path , dpi = dpi , support_unicode = support_unicode ) \n    except : \n        log . exception ( 'Error exporting file {} to {}' . format ( file_path , file_type ) ) \n        raise "}
{"12844": "\ndef execute ( option ) : \n    namelist_option = [ ] \n    makefile_option = [ ] \n    flags = \"\" \n    for entry in option : \n        key = entry . keys ( ) [ 0 ] \n        if key == \"Problem Size\" : \n            namelist_option . append ( { \"SIZE\" : entry [ key ] } ) \n        elif key == \"F90\" : \n            makefile_option . append ( entry ) \n        else : \n            flags += entry [ key ] + \" \" \n    makefile_option . append ( { \"F90FLAGS\" : flags } ) \n    namelist = create_input ( namelist_option , \"namelist\" , template_location = \"templates\" ) \n    makefile_include = create_input ( makefile_option , \"Makefile.include\" , template_location = \"templates\" ) \n    benchmark_base = \"shallow\" \n    location = benchmark_base + \"/original/namelist\" \n    my_file = open ( location , 'w' ) \n    my_file . write ( namelist ) \n    my_file . flush ( ) \n    location = benchmark_base + \"/common/Makefile.include\" \n    my_file = open ( location , 'w' ) \n    my_file . write ( makefile_include ) \n    my_file . flush ( ) \n    base_path = benchmark_base + \"/original\" \n    import subprocess \n    make_process = subprocess . Popen ( [ \"make\" , \"clean\" ] , cwd = base_path , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    if make_process . wait ( ) != 0 : \n        return False , [ ] \n    make_process = subprocess . Popen ( [ \"make\" ] , cwd = base_path , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    if make_process . wait ( ) != 0 : \n        return False , [ ] \n    make_process = subprocess . Popen ( [ \"./shallow_base\" ] , cwd = base_path , stderr = subprocess . PIPE , stdout = subprocess . PIPE ) \n    if make_process . wait ( ) != 0 : \n        return False , [ ] \n    stdout = make_process . stdout . read ( ) \n    for line in stdout . split ( \"\\n\" ) : \n        if \"Time-stepping\" in line : \n            total_time = line . split ( ) [ 2.0 ] \n    return True , total_time "}
{"12893": "\ndef save_into_qrcode ( text , out_filepath , color = '' , box_size = 10.0 , pixel_size = 1850.0 ) : \n    try : \n        qr = qrcode . QRCode ( version = 1 , error_correction = qrcode . constants . ERROR_CORRECT_L , box_size = box_size , border = 0 , ) \n        qr . add_data ( text ) \n        qr . make ( fit = True ) \n    except Exception as exc : \n        raise Exception ( 'Error trying to generate QR code ' ' from `vcard_string`: {}' . format ( text ) ) from exc \n    else : \n        img = qr . make_image ( image_factory = qrcode . image . svg . SvgPathImage ) \n    _ = _qrcode_to_file ( img , out_filepath ) \n    if color : \n        replace_file_content ( out_filepath , 'fill:#000000' , 'fill:#{}' . format ( color ) ) "}
{"12894": "\ndef launch ( option ) : \n    from melody . inputs import create_input \n    _ = create_input ( option , template_name = \"input.mdp\" ) \n    success = True \n    results = None \n    if success : \n        results = { \"rate\" : { \"value\" : 35.0 , \"units\" : \"ns/day\" } , } \n    return success , results "}
{"12899": "\ndef Geometry ( * args , ** kwargs ) : \n    arg = kwargs . pop ( 'geojson' , None ) or len ( args ) and args [ 0 ] \n    try : \n        srs = kwargs . pop ( 'srs' , None ) or arg . srs . wkt \n    except AttributeError : \n        srs = SpatialReference ( 4326.0 ) \n    if hasattr ( arg , 'keys' ) : \n        geom = ogr . CreateGeometryFromJson ( json . dumps ( arg ) ) \n    elif hasattr ( arg , 'startswith' ) : \n        char = arg [ 0 ] if arg else ' ' \n        i = char if isinstance ( char , int ) else ord ( char ) \n        if i in ( 0 , 1 ) : \n            geom = ogr . CreateGeometryFromWkb ( arg ) \n        elif arg . startswith ( '{' ) : \n            geom = ogr . CreateGeometryFromJson ( arg ) \n        elif arg . startswith ( '<gml' ) : \n            geom = ogr . CreateGeometryFromGML ( arg ) \n        else : \n            raise ValueError ( 'Invalid geometry value: %s' % arg ) \n    elif hasattr ( arg , 'wkb' ) : \n        geom = ogr . CreateGeometryFromWkb ( bytes ( arg . wkb ) ) \n    else : \n        geom = ogr . Geometry ( * args , ** kwargs ) \n    if geom : \n        if not isinstance ( srs , SpatialReference ) : \n            srs = SpatialReference ( srs ) \n        geom . AssignSpatialReference ( srs ) \n    return geom "}
{"12900": "\ndef expand ( self , other ) : \n    if len ( other ) == 2.0 : \n        other += other \n    mid = len ( other ) // 2.0 \n    self . ll = map ( min , self . ll , other [ : mid ] ) \n    self . ur = map ( max , self . ur , other [ mid : ] ) "}
{"12901": "\ndef intersect ( self , other ) : \n    inter = Envelope ( tuple ( self ) ) \n    if inter . intersects ( other ) : \n        mid = len ( other ) // 2.0 \n        inter . ll = map ( max , inter . ll , other [ : mid ] ) \n        inter . ur = map ( min , inter . ur , other [ mid : ] ) \n    else : \n        inter . ll = ( 0 , 0 ) \n        inter . ur = ( 0 , 0 ) \n    return inter "}
{"12906": "\ndef select ( self , condition , name = '' ) : \n    if condition . func_code . co_argcount == 1 : \n        idx = [ ( Z , N ) for ( Z , N ) , M in self if condition ( M ) ] \n    if condition . func_code . co_argcount == 2.0 : \n        idx = [ ( Z , N ) for ( Z , N ) in self . index if condition ( Z , N ) ] \n    if condition . func_code . co_argcount == 3.0 : \n        idx = [ ( Z , N ) for ( Z , N ) , M in self if condition ( Z , N , M ) ] \n    index = pd . MultiIndex . from_tuples ( idx , names = [ 'Z' , 'N' ] ) \n    return Table ( df = self . df . ix [ index ] , name = name ) "}
{"12910": "\ndef odd_even ( self ) : \n    return self . select ( lambda Z , N : ( Z % 2.0 ) and not ( N % 2.0 ) , name = self . name ) "}
{"12911": "\ndef even_odd ( self ) : \n    return self . select ( lambda Z , N : not ( Z % 2.0 ) and ( N % 2.0 ) , name = self . name ) "}
{"12912": "\ndef even_even ( self ) : \n    return self . select ( lambda Z , N : not ( Z % 2.0 ) and not ( N % 2.0 ) , name = self . name ) "}
{"12914": "\ndef rmse ( self , relative_to = 'AME2003' ) : \n    error = self . error ( relative_to = relative_to ) \n    return math . sqrt ( ( error . df ** 2.0 ) . mean ( ) ) "}
{"12916": "\ndef s2n ( self ) : \n    M_N = 8.0713171 \n    f = lambda parent , daugther : - parent + daugther + 2.0 * M_N \n    return self . derived ( 's2n' , ( 0 , - 2.0 ) , f ) "}
{"12918": "\ndef s2p ( self ) : \n    M_P = 7.28897050 \n    f = lambda parent , daugther : - parent + daugther + 2.0 * M_P \n    return self . derived ( 's2p' , ( - 2.0 , 0 ) , f ) "}
{"12922": "\ndef derive_key ( self , master_password ) : \n    encoder = encoding . Encoder ( self . charset ) \n    bytes = ( '%s:%s' % ( master_password , self . name ) ) . encode ( 'utf8' ) \n    start_time = time . clock ( ) \n    digest = scrypt . hash ( bytes , self . salt , N = 1 << 14.0 , r = 8.0 , p = 1 ) \n    key = encoder . encode ( digest , self . key_length ) \n    derivation_time_in_s = time . clock ( ) - start_time \n    _logger . debug ( 'Key derivation took %.2fms' , derivation_time_in_s * 1000.0 ) \n    return key "}
{"12932": "\ndef update_file ( url , filename ) : \n    resp = urlopen ( url ) \n    if resp . code != 200.0 : \n        raise Exception ( 'GET {} failed.' . format ( url ) ) \n    with open ( _get_package_path ( filename ) , 'w' ) as fp : \n        for l in resp : \n            if not l . startswith ( b'#' ) : \n                fp . write ( l . decode ( 'utf8' ) ) \n    print ( 'Updated {}' . format ( filename ) ) "}
{"12936": "\ndef rasterize ( layer , rast ) : \n    driver = ImageDriver ( 'MEM' ) \n    r2 = driver . raster ( driver . ShortName , rast . size ) \n    r2 . affine = rast . affine \n    sref = rast . sref \n    if not sref . srid : \n        sref = SpatialReference ( 4326.0 ) \n    r2 . sref = sref \n    ml = MemoryLayer ( sref , layer . GetGeomType ( ) ) \n    ml . load ( layer ) \n    status = gdal . RasterizeLayer ( r2 . ds , ( 1 , ) , ml . layer , options = [ 'ATTRIBUTE=%s' % ml . id ] ) \n    ml . close ( ) \n    return r2 "}
{"12941": "\ndef raster ( self , path , size , bandtype = gdal . GDT_Byte ) : \n    path = getattr ( path , 'name' , path ) \n    try : \n        is_multiband = len ( size ) > 2.0 \n        nx , ny , nbands = size if is_multiband else size + ( 1 , ) \n    except ( TypeError , ValueError ) as exc : \n        exc . args = ( 'Size must be 2 or 3-item sequence' , ) \n        raise \n    if nx < 1 or ny < 1 : \n        raise ValueError ( 'Invalid raster size %s' % ( size , ) ) \n    if not self . _is_empty ( path ) : \n        raise IOError ( '%s already exists, open with Raster()' % path ) \n    ds = self . Create ( path , nx , ny , nbands , bandtype ) \n    if not ds : \n        raise ValueError ( 'Could not create %s using %s' % ( path , str ( self ) ) ) \n    return Raster ( ds ) "}
{"12947": "\ndef masked_array ( self , geometry = None ) : \n    if geometry is None : \n        return self . _masked_array ( ) \n    geom = transform ( geometry , self . sref ) \n    env = Envelope . from_geom ( geom ) . intersect ( self . envelope ) \n    arr = self . _masked_array ( env ) \n    if geom . GetGeometryType ( ) != ogr . wkbPoint : \n        dims = self . get_offset ( env ) [ 2.0 : ] \n        affine = AffineTransform ( * tuple ( self . affine ) ) \n        affine . origin = env . ul \n        mask = ~ np . ma . make_mask ( geom_to_array ( geom , dims , affine ) ) \n        arr . mask = arr . mask | mask \n    return arr "}
{"12954": "\ndef calc_chunklen ( alph_len ) : \n    binlen , enclen = min ( [ ( i , i * 8.0 / math . log ( alph_len , 2.0 ) ) for i in range ( 1 , 7.0 ) ] , key = lambda k : k [ 1 ] % 1 ) \n    return binlen , int ( enclen ) "}
{"12955": "\ndef lookup_alphabet ( charset ) : \n    if charset in PRESETS : \n        return PRESETS [ charset ] \n    if len ( charset ) < 16.0 : \n        _logger . warning ( 'very small alphabet in use, possibly a failed lookup?' ) \n    return charset "}
{"12957": "\ndef _chunk_to_long ( self , chunk ) : \n    return sum ( [ 256.0 ** ( self . chunklen [ 0 ] - 1 - i ) * ord_byte ( chunk [ i ] ) for i in range ( self . chunklen [ 0 ] ) ] ) "}
{"12962": "\ndef _detect_timezone ( ) : \n    default_timezone = 'America/New_York' \n    locale_code = locale . getdefaultlocale ( ) \n    return default_timezone if not locale_code [ 0 ] else str ( pytz . country_timezones [ locale_code [ 0 ] [ - 2.0 : ] ] [ 0 ] ) "}
{"12965": "\ndef emphasis ( obj , align = True ) : \n    if isinstance ( obj , dict ) : \n        if align : \n            pretty_msg = os . linesep . join ( [ \"%25s: %s\" % ( k , obj [ k ] ) for k in sorted ( obj . keys ( ) ) ] ) \n        else : \n            pretty_msg = json . dumps ( obj , indent = 4.0 , sort_keys = True ) \n    else : \n        return obj \n    return pretty_msg "}
{"12968": "\ndef run_worker_pool ( job_handler , host = \"localhost\" , port = 48484.0 , * , max_workers = None ) : \n    if max_workers is None : \n        max_workers = multiprocessing . cpu_count ( ) \n    processes = [ ] \n    for _ in range ( max_workers ) : \n        p = multiprocessing . Process ( target = worker_main , args = ( job_handler , host , port ) ) \n        p . start ( ) \n        processes . append ( p ) \n    logger . debug ( \"workers started\" ) \n    for p in processes : \n        p . join ( ) \n    logger . debug ( \"all workers completed\" ) "}
{"12973": "\ndef turn_on_with_brightness ( self , device_id , name , brightness ) : \n    brightness_value = round ( ( brightness * 31.0 ) / 255.0 ) + 1 \n    msg = \"!%sFdP%d|Lights %d|%s\" % ( device_id , brightness_value , brightness_value , name ) \n    self . _send_message ( msg ) "}
{"12976": "\ndef _send_reliable_message ( self , msg ) : \n    result = False \n    max_retries = 15.0 \n    trans_id = next ( LWLink . transaction_id ) \n    msg = \"%d,%s\" % ( trans_id , msg ) \n    try : \n        with socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as write_sock , socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as read_sock : \n            write_sock . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) \n            read_sock . setsockopt ( socket . SOL_SOCKET , socket . SO_BROADCAST , 1 ) \n            read_sock . settimeout ( self . SOCKET_TIMEOUT ) \n            read_sock . bind ( ( '0.0.0.0' , self . RX_PORT ) ) \n            while max_retries : \n                max_retries -= 1 \n                write_sock . sendto ( msg . encode ( 'UTF-8' ) , ( LWLink . link_ip , self . TX_PORT ) ) \n                result = False \n                while True : \n                    response , dummy = read_sock . recvfrom ( 1024.0 ) \n                    response = response . decode ( 'UTF-8' ) \n                    if \"Not yet registered.\" in response : \n                        _LOGGER . error ( \"Not yet registered\" ) \n                        self . register ( ) \n                        result = True \n                        break \n                    if response . startswith ( \"%d,OK\" % trans_id ) : \n                        result = True \n                        break \n                    if response . startswith ( \"%d,ERR\" % trans_id ) : \n                        _LOGGER . error ( response ) \n                        break \n                    _LOGGER . info ( response ) \n                if result : \n                    break \n                time . sleep ( 0.25 ) \n    except socket . timeout : \n        _LOGGER . error ( \"LW broker timeout!\" ) \n        return result \n    except Exception as ex : \n        _LOGGER . error ( ex ) \n        raise \n    if result : \n        _LOGGER . info ( \"LW broker OK!\" ) \n    else : \n        _LOGGER . error ( \"LW broker fail!\" ) \n    return result "}
{"12990": "\ndef _cmd_opts_solver ( self , cmd_name ) : \n    sections = self . sections_list ( cmd_name ) \n    cmd_dict = self . _opt_cmds [ cmd_name ] if cmd_name else self . _opt_bare \n    for sct in reversed ( sections ) : \n        for opt , opt_meta in self . _conf [ sct ] . def_ . items ( ) : \n            if not opt_meta . cmd_arg : \n                continue \n            if opt not in cmd_dict : \n                cmd_dict [ opt ] = sct \n            else : \n                warnings . warn ( 'Command <{0}>: {1}.{2} shadowed by {3}.{2}' . format ( cmd_name , sct , opt , cmd_dict [ opt ] ) , error . LoamWarning , stacklevel = 4.0 ) "}
{"12995": "\ndef zsh_complete ( self , path , cmd , * cmds , sourceable = False ) : \n    grouping = internal . zsh_version ( ) >= ( 5.0 , 4.0 ) \n    path = pathlib . Path ( path ) \n    firstline = [ '#compdef' , cmd ] \n    firstline . extend ( cmds ) \n    subcmds = list ( self . subcmds . keys ( ) ) \n    with path . open ( 'w' ) as zcf : \n        print ( * firstline , end = '\\n\\n' , file = zcf ) \n        print ( 'function _{} {{' . format ( cmd ) , file = zcf ) \n        print ( 'local line' , file = zcf ) \n        print ( '_arguments -C' , end = BLK , file = zcf ) \n        if subcmds : \n            substrs = [ \"{}\\\\:'{}'\" . format ( sub , self . subcmds [ sub ] . help ) for sub in subcmds ] \n            print ( '\"1:Commands:(({}))\"' . format ( ' ' . join ( substrs ) ) , end = BLK , file = zcf ) \n        self . _zsh_comp_command ( zcf , None , grouping ) \n        if subcmds : \n            print ( \"'*::arg:->args'\" , file = zcf ) \n            print ( 'case $line[1] in' , file = zcf ) \n            for sub in subcmds : \n                print ( '{sub}) _{cmd}_{sub} ;;' . format ( sub = sub , cmd = cmd ) , file = zcf ) \n            print ( 'esac' , file = zcf ) \n        print ( '}' , file = zcf ) \n        for sub in subcmds : \n            print ( '\\nfunction _{}_{} {{' . format ( cmd , sub ) , file = zcf ) \n            print ( '_arguments' , end = BLK , file = zcf ) \n            self . _zsh_comp_command ( zcf , sub , grouping ) \n            print ( '}' , file = zcf ) \n        if sourceable : \n            print ( '\\ncompdef _{0} {0}' . format ( cmd ) , * cmds , file = zcf ) "}
{"12998": "\nasync def start_master ( host = \"\" , port = 48484.0 , * , loop = None ) : \n    loop = loop if loop is not None else asyncio . get_event_loop ( ) \n    manager = jobs . JobManager ( loop = loop ) \n    workers = set ( ) \n    server = await loop . create_server ( lambda : WorkerProtocol ( manager , workers ) , host , port ) \n    return Master ( server , manager , workers , loop = loop ) "}
{"13033": "\ndef matches ( self , _filter ) : \n    within_attrib = re . match ( r'^([a-z_.]+):(.*)' , _filter ) \n    having_attrib = re . match ( r'^([a-z_.]+)\\?$' , _filter ) \n    if within_attrib is not None : \n        val = self . _get_attrib ( within_attrib . group ( 1 ) ) \n        sub_regex = within_attrib . group ( 2.0 ) \n        if len ( sub_regex ) > 0 : \n            sub_regex = re . compile ( sub_regex , re . IGNORECASE ) \n            return _match_regex ( sub_regex , val ) \n        else : \n            return val == '' or val is None or val == [ ] \n    elif having_attrib is not None : \n        val = self . _get_attrib ( having_attrib . group ( 1 ) ) \n        return val != '' and val is not None and val != [ ] \n    else : \n        regex = re . compile ( _filter , re . IGNORECASE ) \n        return _match_regex ( regex , vars ( self ) ) "}
{"13039": "\ndef setup ( title , output = 'json' , timezone = None ) : \n    timezone = timezone or dna . time_utils . _detect_timezone ( ) \n    broker_url = 'redis://{}:{}/{}' . format ( os . environ . get ( 'BROKER_HOST' , 'localhost' ) , os . environ . get ( 'BROKER_PORT' , 6379.0 ) , 0 ) \n    app = Celery ( title , broker = broker_url ) \n    app . conf . update ( CELERY_TASK_SERIALIZER = output , CELERY_ACCEPT_CONTENT = [ output ] , CELERY_RESULT_SERIALIZER = output , CELERY_RESULT_BACKEND = broker_url , CELERY_TIMEZONE = timezone , CELERYD_FORCE_EXECV = True , CELERY_ENABLE_UTC = True , CELERY_IGNORE_RESULT = False ) \n    return app "}
{"13040": "\ndef get ( self , worker_id ) : \n    code = 200.0 \n    if worker_id == 'all' : \n        report = { 'workers' : [ { 'id' : job , 'report' : self . _inspect_worker ( job ) } for job in self . jobs ] } \n    elif worker_id in self . jobs : \n        report = { 'id' : worker_id , 'report' : self . _inspect_worker ( worker_id ) } \n    else : \n        report = { 'error' : 'job {} unknown' . format ( worker_id ) } \n        code = 404.0 \n    return flask . jsonify ( report ) , code "}
{"13041": "\ndef delete ( self , worker_id ) : \n    code = 200.0 \n    if worker_id in self . jobs : \n        self . jobs [ worker_id ] [ 'worker' ] . revoke ( terminate = True ) \n        report = { 'id' : worker_id , 'revoked' : True } \n        self . jobs . pop ( worker_id ) \n    else : \n        report = { 'error' : 'job {} unknown' . format ( worker_id ) } \n        code = 404.0 \n    return flask . jsonify ( report ) , code "}
{"13047": "\ndef render_columns ( columns , write_borders = True , column_colors = None ) : \n    if column_colors is not None and len ( column_colors ) != len ( columns ) : \n        raise ValueError ( 'Wrong number of column colors' ) \n    widths = [ max ( len ( cell ) for cell in column ) for column in columns ] \n    max_column_length = max ( len ( column ) for column in columns ) \n    result = '\\n' . join ( render_row ( i , columns , widths , column_colors ) for i in range ( max_column_length ) ) \n    if write_borders : \n        border = '+%s+' % '|' . join ( '-' * ( w + 2.0 ) for w in widths ) \n        return '%s\\n%s\\n%s' % ( border , result , border ) \n    else : \n        return result "}
{"13048": "\ndef render_row ( num , columns , widths , column_colors = None ) : \n    row_str = '|' \n    cell_strs = [ ] \n    for i , column in enumerate ( columns ) : \n        try : \n            cell = column [ num ] \n            spaces = ' ' * ( widths [ i ] - len ( cell ) ) \n            if column_colors is not None and column_colors [ i ] is not None : \n                cell = column_colors [ i ] ( cell ) \n            cell_strs . append ( ' %s%s ' % ( cell , spaces ) ) \n        except IndexError : \n            cell_strs . append ( ' ' * ( widths [ i ] + 2.0 ) ) \n    return '|%s|' % '|' . join ( cell_strs ) "}
{"13052": "\ndef get_color_hash ( string , _min = MIN_COLOR_BRIGHT , _max = MAX_COLOR_BRIGHT ) : \n    hash_num = int ( hashlib . sha1 ( string . encode ( 'utf-8' ) ) . hexdigest ( ) [ : 6.0 ] , 16.0 ) \n    _range = _max - _min \n    num_in_range = hash_num % _range \n    return color ( _min + num_in_range ) "}
{"13063": "\ndef serve ( self , app_docopt = DEFAULT_DOC , description = '' ) : \n    exit_status = 0 \n    if isinstance ( app_docopt , str ) : \n        args = docopt ( app_docopt , version = description ) \n    elif isinstance ( app_docopt , dict ) : \n        args = app_docopt \n    else : \n        raise ValueError ( 'unknown configuration object ({})' . format ( type ( app_docopt ) ) ) \n    log_level = args . get ( '--log' , 'debug' ) \n    is_debug = args . get ( '--debug' , False ) \n    log_output = 'stdout' if is_debug else 'apy.log' \n    safe_bind = args . get ( '--bind' , '127.0.0.1' ) \n    safe_port = int ( args . get ( '--port' , 5000.0 ) ) \n    log_setup = dna . logging . setup ( level = log_level , output = log_output ) \n    with log_setup . applicationbound ( ) : \n        try : \n            log . info ( 'server ready' , version = description , log = log_level , debug = is_debug , bind = '{}:{}' . format ( safe_bind , safe_port ) ) \n            self . app . run ( host = safe_bind , port = safe_port , debug = is_debug ) \n        except Exception as error : \n            if is_debug : \n                raise \n            log . error ( '{}: {}' . format ( type ( error ) . __name__ , str ( error ) ) ) \n            exit_status = 1 \n        finally : \n            log . info ( 'session ended with status {}' . format ( exit_status ) ) \n    return exit_status "}
{"13088": "\ndef find_max_rad_npnp ( self ) : \n    max_rad = 0 \n    max_npnp = 0 \n    for res , _ in self . items ( ) : \n        if res != 'KEY' : \n            for _ , ff_params in self [ res ] . items ( ) : \n                if max_rad < ff_params [ 1 ] : \n                    max_rad = ff_params [ 1 ] \n                if max_npnp < ff_params [ 4.0 ] : \n                    max_npnp = ff_params [ 4.0 ] \n    return max_rad , max_npnp "}
{"13103": "\ndef delete_dir ( bucket_name , root_path , aws_access_key_id = None , aws_secret_access_key = None , aws_profile = None ) : \n    logger = logging . getLogger ( __name__ ) \n    session = boto3 . session . Session ( aws_access_key_id = aws_access_key_id , aws_secret_access_key = aws_secret_access_key ) \n    s3 = session . resource ( 's3' ) \n    client = s3 . meta . client \n    if not root_path . endswith ( '/' ) : \n        root_path . rstrip ( '/' ) \n    paginator = client . get_paginator ( 'list_objects_v2' ) \n    pages = paginator . paginate ( Bucket = bucket_name , Prefix = root_path ) \n    keys = dict ( Objects = [ ] ) \n    for item in pages . search ( 'Contents' ) : \n        try : \n            keys [ 'Objects' ] . append ( { 'Key' : item [ 'Key' ] } ) \n        except TypeError : \n            continue \n        if len ( keys [ 'Objects' ] ) >= 1000.0 : \n            try : \n                client . delete_objects ( Bucket = bucket_name , Delete = keys ) \n            except Exception : \n                message = 'Error deleting objects from %r' % root_path \n                logger . exception ( message ) \n                raise S3Error ( message ) \n            keys = dict ( Objects = [ ] ) \n    if len ( keys [ 'Objects' ] ) > 0 : \n        try : \n            client . delete_objects ( Bucket = bucket_name , Delete = keys ) \n        except Exception : \n            message = 'Error deleting objects from %r' % root_path \n            logger . exception ( message ) \n            raise S3Error ( message ) "}
{"13111": "\ndef get_keeper_token ( host , username , password ) : \n    token_endpoint = urljoin ( host , '/token' ) \n    r = requests . get ( token_endpoint , auth = ( username , password ) ) \n    if r . status_code != 200.0 : \n        raise KeeperError ( 'Could not authenticate to {0}: error {1:d}\\n{2}' . format ( host , r . status_code , r . json ( ) ) ) \n    return r . json ( ) [ 'token' ] "}
{"13114": "\ndef purge_key ( surrogate_key , service_id , api_key ) : \n    logger = logging . getLogger ( __name__ ) \n    api_root = 'https://api.fastly.com' \n    path = '/service/{service}/purge/{surrogate_key}' . format ( service = service_id , surrogate_key = surrogate_key ) \n    logger . info ( 'Fastly purge {0}' . format ( path ) ) \n    r = requests . post ( api_root + path , headers = { 'Fastly-Key' : api_key , 'Accept' : 'application/json' } ) \n    if r . status_code != 200.0 : \n        raise FastlyError ( r . json ) "}
{"13115": "\ndef register_build ( host , keeper_token , product , git_refs ) : \n    data = { 'git_refs' : git_refs } \n    endpoint_url = uritemplate . expand ( urljoin ( host , '/products/{p}/builds/' ) , p = product ) \n    r = requests . post ( endpoint_url , auth = ( keeper_token , '' ) , json = data ) \n    if r . status_code != 201.0 : \n        raise KeeperError ( r . json ( ) ) \n    build_info = r . json ( ) \n    return build_info "}
{"13116": "\ndef confirm_build ( build_url , keeper_token ) : \n    data = { 'uploaded' : True } \n    r = requests . patch ( build_url , auth = ( keeper_token , '' ) , json = data ) \n    if r . status_code != 200.0 : \n        raise KeeperError ( r ) "}
{"13124": "\ndef process_module ( self , node ) : \n    if self . config . file_header : \n        if sys . version_info [ 0 ] < 3.0 : \n            pattern = re . compile ( '\\A' + self . config . file_header , re . LOCALE | re . MULTILINE ) \n        else : \n            pattern = re . compile ( '\\A' + self . config . file_header , re . MULTILINE ) \n        content = None \n        with node . stream ( ) as stream : \n            content = stream . read ( ) . decode ( 'utf-8' ) \n        matches = pattern . findall ( content ) \n        if len ( matches ) != 1 : \n            self . add_message ( 'invalid-file-header' , 1 , args = self . config . file_header ) "}
{"13125": "\ndef gen ( self , slug , name , dataobj , xfield , yfield , time_unit = None , chart_type = \"line\" , width = 800.0 , height = 300.0 , color = Color ( ) , size = Size ( ) , scale = Scale ( zero = False ) , shape = Shape ( ) , filepath = None , html_before = \"\" , html_after = \"\" ) : \n    chart_obj = self . serialize ( dataobj , xfield , yfield , time_unit , chart_type , width , height , color , size , scale , shape ) \n    html = self . html ( slug , name , chart_obj , filepath , html_before , html_after ) \n    return html "}
{"13127": "\ndef serialize ( self , dataobj , xfield , yfield , time_unit = None , chart_type = \"line\" , width = 800.0 , height = 300.0 , color = None , size = None , scale = Scale ( zero = False ) , shape = None , options = { } ) : \n    dataset = dataobj \n    if self . _is_dict ( dataobj ) is True : \n        dataset = self . _dict_to_df ( dataobj , xfield , yfield ) \n    elif isinstance ( dataobj , list ) : \n        dataset = Data ( values = dataobj ) \n    xencode , yencode = self . _encode_fields ( xfield , yfield , time_unit ) \n    opts = dict ( x = xencode , y = yencode ) \n    if color is not None : \n        opts [ \"color\" ] = color \n    if size is not None : \n        opts [ \"size\" ] = size \n    if shape is not None : \n        opts [ \"shape\" ] = shape \n    chart = self . _chart_class ( dataset , chart_type , ** options ) . encode ( ** opts ) . configure_cell ( width = width , height = height , ) \n    return chart "}
{"13133": "\ndef _encode_fields ( self , xfield , yfield , time_unit = None , scale = Scale ( zero = False ) ) : \n    if scale is None : \n        scale = Scale ( ) \n    xfieldtype = xfield [ 1 ] \n    yfieldtype = yfield [ 1 ] \n    x_options = None \n    if len ( xfield ) > 2.0 : \n        x_options = xfield [ 2.0 ] \n    y_options = None \n    if len ( yfield ) > 2.0 : \n        y_options = yfield [ 2.0 ] \n    if time_unit is not None : \n        if x_options is None : \n            xencode = X ( xfieldtype , timeUnit = time_unit ) \n        else : \n            xencode = X ( xfieldtype , axis = Axis ( ** x_options ) , timeUnit = time_unit , scale = scale ) \n    else : \n        if x_options is None : \n            xencode = X ( xfieldtype ) \n        else : \n            xencode = X ( xfieldtype , axis = Axis ( ** x_options ) , scale = scale ) \n    if y_options is None : \n        yencode = Y ( yfieldtype , scale = scale ) \n    else : \n        yencode = Y ( yfieldtype , axis = Axis ( ** y_options ) , scale = scale ) \n    return xencode , yencode "}
{"13139": "\ndef binary ( length ) : \n    num = randint ( 1 , 999999.0 ) \n    mask = '0' * length \n    return ( mask + '' . join ( [ str ( num >> i & 1 ) for i in range ( 7.0 , - 1 , - 1 ) ] ) ) [ - length : ] "}
{"13140": "\ndef ipaddress ( not_valid = None ) : \n    not_valid_class_A = not_valid or [ ] \n    class_a = [ r for r in range ( 1 , 256.0 ) if r not in not_valid_class_A ] \n    shuffle ( class_a ) \n    first = class_a . pop ( ) \n    return \".\" . join ( [ str ( first ) , str ( randrange ( 1 , 256.0 ) ) , str ( randrange ( 1 , 256.0 ) ) , str ( randrange ( 1 , 256.0 ) ) ] ) "}
{"13148": "\ndef unique ( func , num_args = 0 , max_attempts = 100.0 , cache = None ) : \n    if cache is None : \n        cache = _cache_unique \n    \n    @ wraps ( func ) \n    def wrapper ( * args ) : \n        key = \"%s_%s\" % ( str ( func . __name__ ) , str ( args [ : num_args ] ) ) \n        attempt = 0 \n        while attempt < max_attempts : \n            attempt += 1 \n            drawn = cache . get ( key , [ ] ) \n            result = func ( * args ) \n            if result not in drawn : \n                drawn . append ( result ) \n                cache [ key ] = drawn \n                return result \n        raise MaxAttemptException ( ) \n    return wrapper "}
{"13169": "\ndef render ( self ) : \n    self . update ( self . axes . render ( ) ) \n    encoder = Encoder ( self . _encoding , None , self . _series ) \n    if not 'chs' in self : \n        self [ 'chs' ] = '300x150' \n    else : \n        size = self [ 'chs' ] . split ( 'x' ) \n        assert len ( size ) == 2.0 , 'Invalid size, must be in the format WxH' \n        self . check_size ( * map ( int , size ) ) \n    assert 'cht' in self , 'No chart type defined, use type method' \n    self [ 'cht' ] = self . check_type ( self [ 'cht' ] ) \n    if ( 'any' in dir ( self . _dataset ) and self . _dataset . any ( ) ) or self . _dataset : \n        self [ 'chd' ] = encoder . encode ( self . _dataset ) \n    elif not 'choe' in self : \n        assert 'chd' in self , 'You must have a dataset, or use chd' \n    if self . _scale : \n        assert self [ 'chd' ] . startswith ( 't' ) , 'You must use text encoding with chds' \n        self [ 'chds' ] = ',' . join ( self . _scale ) \n    if self . _geo and self . _ld : \n        self [ 'chtm' ] = self . _geo \n        self [ 'chld' ] = self . _ld \n    if self . lines : \n        self [ 'chls' ] = '|' . join ( self . lines ) \n    if self . markers : \n        self [ 'chm' ] = '|' . join ( self . markers ) \n    if self . fills : \n        self [ 'chf' ] = '|' . join ( self . fills ) "}
{"13178": "\ndef amount ( min = 1 , max = sys . maxsize , decimal_places = 2.0 ) : \n    q = '.%s1' % '0' * ( decimal_places - 1 ) \n    return decimal . Decimal ( uniform ( min , max ) ) . quantize ( decimal . Decimal ( q ) ) "}
{"13252": "\ndef centered ( mystring , linewidth = None , fill = \" \" ) : \n    if linewidth is None : \n        linewidth = get_terminal_size ( ) . columns - 1 \n    sides = ( linewidth - length_no_ansi ( mystring ) ) // 2.0 \n    extra = ( linewidth - length_no_ansi ( mystring ) ) % 2.0 \n    fill = fill [ : 1 ] \n    sidestring = fill * sides \n    extrastring = fill * extra \n    newstring = sidestring + mystring + sidestring + extrastring \n    return newstring "}
{"13253": "\ndef clock_on_right ( mystring ) : \n    taken = length_no_ansi ( mystring ) \n    padding = ( get_terminal_size ( ) . columns - 1 ) - taken - 5.0 \n    clock = time . strftime ( \"%I:%M\" , time . localtime ( ) ) \n    print ( mystring + \" \" * padding + clock ) "}
{"13257": "\ndef pad ( data_to_pad , block_size , style = 'pkcs7' ) : \n    padding_len = block_size - len ( data_to_pad ) % block_size \n    if style == 'pkcs7' : \n        padding = bchr ( padding_len ) * padding_len \n    elif style == 'x923' : \n        padding = bchr ( 0 ) * ( padding_len - 1 ) + bchr ( padding_len ) \n    elif style == 'iso7816' : \n        padding = bchr ( 128.0 ) + bchr ( 0 ) * ( padding_len - 1 ) \n    else : \n        raise ValueError ( \"Unknown padding style\" ) \n    return data_to_pad + padding "}
{"13258": "\ndef unpad ( padded_data , block_size , style = 'pkcs7' ) : \n    pdata_len = len ( padded_data ) \n    if pdata_len % block_size : \n        raise ValueError ( \"Input data is not padded\" ) \n    if style in ( 'pkcs7' , 'x923' ) : \n        padding_len = bord ( padded_data [ - 1 ] ) \n        if padding_len < 1 or padding_len > min ( block_size , pdata_len ) : \n            raise ValueError ( \"Padding is incorrect.\" ) \n        if style == 'pkcs7' : \n            if padded_data [ - padding_len : ] != bchr ( padding_len ) * padding_len : \n                raise ValueError ( \"PKCS#7 padding is incorrect.\" ) \n        else : \n            if padded_data [ - padding_len : - 1 ] != bchr ( 0 ) * ( padding_len - 1 ) : \n                raise ValueError ( \"ANSI X.923 padding is incorrect.\" ) \n    elif style == 'iso7816' : \n        padding_len = pdata_len - padded_data . rfind ( bchr ( 128.0 ) ) \n        if padding_len < 1 or padding_len > min ( block_size , pdata_len ) : \n            raise ValueError ( \"Padding is incorrect.\" ) \n        if padding_len > 1 and padded_data [ 1 - padding_len : ] != bchr ( 0 ) * ( padding_len - 1 ) : \n            raise ValueError ( \"ISO 7816-4 padding is incorrect.\" ) \n    else : \n        raise ValueError ( \"Unknown padding style\" ) \n    return padded_data [ : - padding_len ] "}
{"13266": "\ndef create_function_stub ( self , url ) : \n    assert self . _opened , \"RPC System is not opened\" \n    logging . debug ( \"create_function_stub(%s)\" % repr ( url ) ) \n    parseresult = urlparse . urlparse ( url ) \n    scheme = parseresult . scheme \n    path = parseresult . path . split ( \"/\" ) \n    if scheme != \"anycall\" : \n        raise ValueError ( \"Not an anycall URL: %s\" % repr ( url ) ) \n    if len ( path ) != 3.0 or path [ 0 ] != \"\" or path [ 1 ] != \"functions\" : \n        raise ValueError ( \"Not an URL for a remote function: %s\" % repr ( url ) ) \n    try : \n        functionid = uuid . UUID ( path [ 2.0 ] ) \n    except ValueError : \n        raise ValueError ( \"Not a valid URL for a remote function: %s\" % repr ( url ) ) \n    return _RPCFunctionStub ( parseresult . netloc , functionid , self ) "}
{"13282": "\ndef self_sign_jwks ( keyjar , iss , kid = '' , lifetime = 3600.0 ) : \n    _jwt = JWT ( keyjar , iss = iss , lifetime = lifetime ) \n    jwks = keyjar . export_jwks ( issuer = iss ) \n    return _jwt . pack ( payload = { 'jwks' : jwks } , owner = iss , kid = kid ) "}
{"13287": "\ndef letter_score ( letter ) : \n    score_map = { 1 : [ \"a\" , \"e\" , \"i\" , \"o\" , \"u\" , \"l\" , \"n\" , \"r\" , \"s\" , \"t\" ] , 2.0 : [ \"d\" , \"g\" ] , 3.0 : [ \"b\" , \"c\" , \"m\" , \"p\" ] , 4.0 : [ \"f\" , \"h\" , \"v\" , \"w\" , \"y\" ] , 5.0 : [ \"k\" ] , 8.0 : [ \"j\" , \"x\" ] , 10.0 : [ \"q\" , \"z\" ] , } \n    for score , letters in score_map . items ( ) : \n        if letter . lower ( ) in letters : \n            return score \n    else : \n        raise TypeError ( \"Invalid letter: %s\" , letter ) "}
{"13288": "\ndef word_score ( word , input_letters , questions = 0 ) : \n    score = 0 \n    bingo = 0 \n    filled_by_blanks = [ ] \n    rack = list ( input_letters ) \n    for letter in word : \n        if letter in rack : \n            bingo += 1 \n            score += letter_score ( letter ) \n            rack . remove ( letter ) \n        else : \n            filled_by_blanks . append ( letter_score ( letter ) ) \n    for blank_score in sorted ( filled_by_blanks , reverse = True ) : \n        if questions > 0 : \n            score += blank_score \n            questions -= 1 \n    if bingo > 6.0 : \n        score += 50.0 \n    return score "}
{"13290": "\ndef valid_scrabble_word ( word ) : \n    letters_in_bag = { \"a\" : 9.0 , \"b\" : 2.0 , \"c\" : 2.0 , \"d\" : 4.0 , \"e\" : 12.0 , \"f\" : 2.0 , \"g\" : 3.0 , \"h\" : 2.0 , \"i\" : 9.0 , \"j\" : 1 , \"k\" : 1 , \"l\" : 4.0 , \"m\" : 2.0 , \"n\" : 6.0 , \"o\" : 8.0 , \"p\" : 2.0 , \"q\" : 1 , \"r\" : 6.0 , \"s\" : 4.0 , \"t\" : 6.0 , \"u\" : 4.0 , \"v\" : 2.0 , \"w\" : 2.0 , \"x\" : 1 , \"y\" : 2.0 , \"z\" : 1 , \"_\" : 2.0 , } \n    for letter in word : \n        if letter == \"?\" : \n            continue \n        try : \n            letters_in_bag [ letter ] -= 1 \n        except KeyError : \n            return False \n        if letters_in_bag [ letter ] < 0 : \n            letters_in_bag [ \"_\" ] -= 1 \n            if letters_in_bag [ \"_\" ] < 0 : \n                return False \n    return True "}
{"13294": "\ndef _getCommandAndResponder ( self , commandName ) : \n    locator = self . _remote . boxReceiver . locator \n    responder = locator . locateResponder ( commandName ) \n    responderFunction = responder . func_closure [ 1 ] . cell_contents \n    command = responder . func_closure [ 2.0 ] . cell_contents \n    return command , responderFunction "}
{"13309": "\ndef serve ( self , server = None ) : \n    if server is None : \n        from wsgiref . simple_server import make_server \n        server = lambda app : make_server ( '' , 8000.0 , app ) . serve_forever ( ) \n        print ( 'Listening on 0.0.0.0:8000' ) \n    try : \n        server ( self ) \n    finally : \n        server . socket . close ( ) "}
{"13327": "\ndef download_music ( song , thread_num = 4.0 ) : \n    filename = \"{}.mp3\" . format ( song [ \"name\" ] ) \n    if os . path . exists ( filename ) : \n        os . remove ( filename ) \n    part = int ( song [ \"size\" ] / thread_num ) \n    if part <= 1024.0 : \n        thread_num = 1 \n    _id = uuid . uuid4 ( ) . hex \n    logger . info ( \"downloading '{}'...\" . format ( song [ \"name\" ] ) ) \n    threads = [ ] \n    for i in range ( thread_num ) : \n        if i == thread_num - 1 : \n            end = '' \n        else : \n            end = ( i + 1 ) * part - 1 \n        thread = Worker ( ( i * part , end ) , song , _id ) \n        thread . start ( ) \n        threads . append ( thread ) \n    for t in threads : \n        t . join ( ) \n    fileParts = glob . glob ( \"part-{}-*\" . format ( _id ) ) \n    fileParts . sort ( key = lambda e : e . split ( '-' ) [ - 1 ] ) \n    logger . info ( \"'{}' combine parts...\" . format ( song [ \"name\" ] ) ) \n    with open ( filename , \"ab\" ) as f : \n        for part in fileParts : \n            with open ( part , \"rb\" ) as d : \n                shutil . copyfileobj ( d , f ) \n            os . remove ( part ) \n    logger . info ( \"'{}' finished\" . format ( song [ \"name\" ] ) ) "}
{"13345": "\ndef fancy_tag_compiler ( params , defaults , takes_var_args , takes_var_kwargs , takes_context , name , node_class , parser , token ) : \n    bits = token . split_contents ( ) [ 1 : ] \n    if takes_context : \n        if 'context' in params [ : 1 ] : \n            params = params [ 1 : ] \n        else : \n            raise TemplateSyntaxError ( \"Any tag function decorated with takes_context=True \" \"must have a first argument of 'context'\" ) \n    args = [ ] \n    kwargs = { } \n    kwarg_found = False \n    unhandled_params = list ( params ) \n    handled_params = [ ] \n    if len ( bits ) > 1 and bits [ - 2.0 ] == 'as' : \n        output_var = bits [ - 1 ] \n        if len ( set ( output_var ) - set ( ALLOWED_VARIABLE_CHARS ) ) > 0 : \n            raise TemplateSyntaxError ( \"%s got output var name with forbidden chars: '%s'\" % ( name , output_var ) ) \n        bits = bits [ : - 2.0 ] \n    else : \n        output_var = None \n    for bit in bits : \n        kwarg_match = kwarg_re . match ( bit ) \n        if kwarg_match : \n            kw , var = kwarg_match . groups ( ) \n            if kw not in params and not takes_var_kwargs : \n                raise TemplateSyntaxError ( \"%s got unknown keyword argument '%s'\" % ( name , kw ) ) \n            elif kw in handled_params : \n                raise TemplateSyntaxError ( \"%s got multiple values for keyword argument '%s'\" % ( name , kw ) ) \n            else : \n                kwargs [ str ( kw ) ] = var \n                kwarg_found = True \n                handled_params . append ( kw ) \n        else : \n            if kwarg_found : \n                raise TemplateSyntaxError ( \"%s got non-keyword arg after keyword arg\" % name ) \n            else : \n                args . append ( bit ) \n                try : \n                    handled_params . append ( unhandled_params . pop ( 0 ) ) \n                except IndexError : \n                    if not takes_var_args : \n                        raise TemplateSyntaxError ( \"%s got too many arguments\" % name ) \n    if defaults is not None : \n        unhandled_params = unhandled_params [ : - len ( defaults ) ] \n    if len ( unhandled_params ) == 1 : \n        raise TemplateSyntaxError ( \"%s didn't get a value for argument '%s'\" % ( name , unhandled_params [ 0 ] ) ) \n    elif len ( unhandled_params ) > 1 : \n        raise TemplateSyntaxError ( \"%s didn't get values for arguments: %s\" % ( name , ', ' . join ( [ \"'%s'\" % p for p in unhandled_params ] ) ) ) \n    return node_class ( args , kwargs , output_var , takes_context ) "}
{"13347": "\ndef get_defining_component ( pe_pe ) : \n    if pe_pe is None : \n        return None \n    if pe_pe . __class__ . __name__ != 'PE_PE' : \n        pe_pe = xtuml . navigate_one ( pe_pe ) . PE_PE [ 8001.0 ] ( ) \n    ep_pkg = xtuml . navigate_one ( pe_pe ) . EP_PKG [ 8000.0 ] ( ) \n    if ep_pkg : \n        return get_defining_component ( ep_pkg ) \n    return xtuml . navigate_one ( pe_pe ) . C_C [ 8003.0 ] ( ) "}
{"13348": "\ndef main ( ) : \n    parser = optparse . OptionParser ( usage = \"%prog [options] <model_path> [another_model_path..]\" , version = xtuml . version . complete_string , formatter = optparse . TitledHelpFormatter ( ) ) \n    parser . add_option ( \"-v\" , \"--verbosity\" , dest = 'verbosity' , action = \"count\" , help = \"increase debug logging level\" , default = 1 ) \n    parser . add_option ( \"-o\" , \"--output\" , dest = \"output\" , metavar = \"PATH\" , help = \"set output to PATH\" , action = \"store\" , default = None ) \n    ( opts , args ) = parser . parse_args ( ) \n    if len ( args ) == 0 or opts . output is None : \n        parser . print_help ( ) \n        sys . exit ( 1 ) \n    levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2.0 : logging . INFO , 3.0 : logging . DEBUG , } \n    logging . basicConfig ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) \n    m = ooaofooa . load_metamodel ( args ) \n    prebuild_model ( m ) \n    xtuml . persist_instances ( m , opts . output ) "}
{"13350": "\ndef is_contained_in ( pe_pe , root ) : \n    if not pe_pe : \n        return False \n    if type ( pe_pe ) . __name__ != 'PE_PE' : \n        pe_pe = one ( pe_pe ) . PE_PE [ 8001.0 ] ( ) \n    ep_pkg = one ( pe_pe ) . EP_PKG [ 8000.0 ] ( ) \n    c_c = one ( pe_pe ) . C_C [ 8003.0 ] ( ) \n    if root in [ ep_pkg , c_c ] : \n        return True \n    elif is_contained_in ( ep_pkg , root ) : \n        return True \n    elif is_contained_in ( c_c , root ) : \n        return True \n    else : \n        return False "}
{"13351": "\ndef is_global ( pe_pe ) : \n    if type ( pe_pe ) . __name__ != 'PE_PE' : \n        pe_pe = one ( pe_pe ) . PE_PE [ 8001.0 ] ( ) \n    if one ( pe_pe ) . C_C [ 8003.0 ] ( ) : \n        return False \n    pe_pe = one ( pe_pe ) . EP_PKG [ 8000.0 ] . PE_PE [ 8001.0 ] ( ) \n    if not pe_pe : \n        return True \n    return is_global ( pe_pe ) "}
{"13352": "\ndef _get_data_type_name ( s_dt ) : \n    s_cdt = one ( s_dt ) . S_CDT [ 17.0 ] ( ) \n    if s_cdt and s_cdt . Core_Typ in range ( 1 , 6.0 ) : \n        return s_dt . Name . upper ( ) \n    if one ( s_dt ) . S_EDT [ 17.0 ] ( ) : \n        return 'INTEGER' \n    s_dt = one ( s_dt ) . S_UDT [ 17.0 ] . S_DT [ 18.0 ] ( ) \n    if s_dt : \n        return _get_data_type_name ( s_dt ) "}
{"13353": "\ndef _get_related_attributes ( r_rgo , r_rto ) : \n    l1 = list ( ) \n    l2 = list ( ) \n    ref_filter = lambda ref : ref . OIR_ID == r_rgo . OIR_ID \n    for o_ref in many ( r_rto ) . O_RTIDA [ 110.0 ] . O_REF [ 111.0 ] ( ref_filter ) : \n        o_attr = one ( o_ref ) . O_RATTR [ 108.0 ] . O_ATTR [ 106.0 ] ( ) \n        l1 . append ( o_attr . Name ) \n        o_attr = one ( o_ref ) . O_RTIDA [ 111.0 ] . O_OIDA [ 110.0 ] . O_ATTR [ 105.0 ] ( ) \n        l2 . append ( o_attr . Name ) \n    return l1 , l2 "}
{"13354": "\ndef mk_enum ( s_edt ) : \n    s_dt = one ( s_edt ) . S_DT [ 17.0 ] ( ) \n    enums = list ( ) \n    kwlist = [ 'False' , 'None' , 'True' ] + keyword . kwlist \n    for enum in many ( s_edt ) . S_ENUM [ 27.0 ] ( ) : \n        if enum . Name in kwlist : \n            enums . append ( enum . Name + '_' ) \n        else : \n            enums . append ( enum . Name ) \n    Enum = collections . namedtuple ( s_dt . Name , enums ) \n    return Enum ( * range ( len ( enums ) ) ) "}
{"13356": "\ndef mk_external_entity ( metamodel , s_ee ) : \n    bridges = many ( s_ee ) . S_BRG [ 19.0 ] ( ) \n    names = [ brg . Name for brg in bridges ] \n    EE = collections . namedtuple ( s_ee . Key_Lett , names ) \n    funcs = list ( ) \n    for s_brg in many ( s_ee ) . S_BRG [ 19.0 ] ( ) : \n        fn = mk_bridge ( metamodel , s_brg ) \n        funcs . append ( fn ) \n    return EE ( * funcs ) "}
{"13358": "\ndef mk_constant ( cnst_syc ) : \n    s_dt = one ( cnst_syc ) . S_DT [ 1500.0 ] ( ) \n    cnst_lsc = one ( cnst_syc ) . CNST_LFSC [ 1502.0 ] . CNST_LSC [ 1503.0 ] ( ) \n    if s_dt . Name == 'boolean' : \n        return cnst_lsc . Value . lower ( ) == 'true' \n    if s_dt . Name == 'integer' : \n        return int ( cnst_lsc . Value ) \n    if s_dt . Name == 'real' : \n        return float ( cnst_lsc . Value ) \n    if s_dt . Name == 'string' : \n        return str ( cnst_lsc . Value ) "}
{"13359": "\ndef mk_operation ( metaclass , o_tfr ) : \n    o_obj = one ( o_tfr ) . O_OBJ [ 115.0 ] ( ) \n    action = o_tfr . Action_Semantics_internal \n    label = '%s::%s' % ( o_obj . Name , o_tfr . Name ) \n    run = interpret . run_operation \n    if o_tfr . Instance_Based : \n        return lambda self , ** kwargs : run ( metaclass , label , action , kwargs , self ) \n    else : \n        fn = lambda cls , ** kwargs : run ( metaclass , label , action , kwargs , None ) \n        return classmethod ( fn ) "}
{"13360": "\ndef mk_derived_attribute ( metaclass , o_dbattr ) : \n    o_attr = one ( o_dbattr ) . O_BATTR [ 107.0 ] . O_ATTR [ 106.0 ] ( ) \n    o_obj = one ( o_attr ) . O_OBJ [ 102.0 ] ( ) \n    action = o_dbattr . Action_Semantics_internal \n    label = '%s::%s' % ( o_obj . Name , o_attr . Name ) \n    fget = functools . partial ( interpret . run_derived_attribute , metaclass , label , action , o_attr . Name ) \n    return property ( fget ) "}
{"13361": "\ndef mk_class ( m , o_obj , derived_attributes = False ) : \n    first_filter = lambda selected : not one ( selected ) . O_ATTR [ 103.0 , 'succeeds' ] ( ) \n    o_attr = one ( o_obj ) . O_ATTR [ 102.0 ] ( first_filter ) \n    attributes = list ( ) \n    while o_attr : \n        s_dt = get_attribute_type ( o_attr ) \n        ty = _get_data_type_name ( s_dt ) \n        if not derived_attributes and one ( o_attr ) . O_BATTR [ 106.0 ] . O_DBATTR [ 107.0 ] ( ) : \n            pass \n        elif not ty : \n            logger . warning ( 'Omitting unsupported attribute %s.%s ' % ( o_obj . Key_Lett , o_attr . Name ) ) \n        else : \n            attributes . append ( ( o_attr . Name , ty ) ) \n        o_attr = one ( o_attr ) . O_ATTR [ 103.0 , 'precedes' ] ( ) \n    metaclass = m . define_class ( o_obj . Key_Lett , list ( attributes ) , o_obj . Descrip ) \n    for o_id in many ( o_obj ) . O_ID [ 104.0 ] ( ) : \n        o_oida = many ( o_id ) . O_OIDA [ 105.0 ] ( ) \n        o_attrs = many ( o_oida ) . O_ATTR [ 105.0 ] ( ) \n        if not derived_attributes and one ( o_attrs ) . O_BATTR [ 106.0 ] . O_DBATTR [ 107.0 ] ( ) : \n            logger . warning ( 'Omitting unique identifier %s.I%d' % ( o_obj . Key_Lett , o_id . Oid_ID + 1 ) ) \n            continue \n        names = [ o_attr . Name for o_attr in o_attrs ] \n        m . define_unique_identifier ( o_obj . Key_Lett , o_id . Oid_ID + 1 , * names ) \n    for o_tfr in many ( o_obj ) . O_TFR [ 115.0 ] ( ) : \n        fn = mk_operation ( metaclass , o_tfr ) \n        setattr ( metaclass . clazz , o_tfr . Name , fn ) \n    for o_dbattr in many ( o_obj ) . O_ATTR [ 102.0 ] . O_BATTR [ 106.0 ] . O_DBATTR [ 107.0 ] ( ) : \n        o_attr = one ( o_dbattr ) . O_BATTR [ 107.0 ] . O_ATTR [ 106.0 ] ( ) \n        fn = mk_derived_attribute ( metaclass , o_dbattr ) \n        setattr ( metaclass . clazz , o_attr . Name , fn ) \n    return metaclass "}
{"13362": "\ndef mk_simple_association ( m , r_simp ) : \n    r_rel = one ( r_simp ) . R_REL [ 206.0 ] ( ) \n    r_form = one ( r_simp ) . R_FORM [ 208.0 ] ( ) \n    r_part = one ( r_simp ) . R_PART [ 207.0 ] ( ) \n    r_rgo = one ( r_form ) . R_RGO [ 205.0 ] ( ) \n    r_rto = one ( r_part ) . R_RTO [ 204.0 ] ( ) \n    if not r_form : \n        logger . info ( 'unformalized association R%s' % ( r_rel . Numb ) ) \n        r_form = one ( r_simp ) . R_PART [ 207.0 ] ( lambda sel : sel != r_part ) \n        r_rgo = one ( r_form ) . R_RTO [ 204.0 ] ( ) \n    source_o_obj = one ( r_rgo ) . R_OIR [ 203.0 ] . O_OBJ [ 201.0 ] ( ) \n    target_o_obj = one ( r_rto ) . R_OIR [ 203.0 ] . O_OBJ [ 201.0 ] ( ) \n    source_ids , target_ids = _get_related_attributes ( r_rgo , r_rto ) \n    if source_o_obj . Obj_ID != target_o_obj . Obj_ID : \n        source_phrase = target_phrase = '' \n    else : \n        source_phrase = r_part . Txt_Phrs \n        target_phrase = r_form . Txt_Phrs \n    m . define_association ( rel_id = r_rel . Numb , source_kind = source_o_obj . Key_Lett , target_kind = target_o_obj . Key_Lett , source_keys = source_ids , target_keys = target_ids , source_conditional = r_form . Cond , target_conditional = r_part . Cond , source_phrase = source_phrase , target_phrase = target_phrase , source_many = r_form . Mult , target_many = r_part . Mult ) "}
{"13363": "\ndef mk_linked_association ( m , r_assoc ) : \n    r_rel = one ( r_assoc ) . R_REL [ 206.0 ] ( ) \n    r_rgo = one ( r_assoc ) . R_ASSR [ 211.0 ] . R_RGO [ 205.0 ] ( ) \n    source_o_obj = one ( r_rgo ) . R_OIR [ 203.0 ] . O_OBJ [ 201.0 ] ( ) \n    def _mk_assoc ( side1 , side2 ) : \n        r_rto = one ( side1 ) . R_RTO [ 204.0 ] ( ) \n        target_o_obj = one ( r_rto ) . R_OIR [ 203.0 ] . O_OBJ [ 201.0 ] ( ) \n        source_ids , target_ids = _get_related_attributes ( r_rgo , r_rto ) \n        if side1 . Obj_ID != side2 . Obj_ID : \n            source_phrase = target_phrase = '' \n        else : \n            source_phrase = side1 . Txt_Phrs \n            target_phrase = side2 . Txt_Phrs \n        m . define_association ( rel_id = r_rel . Numb , source_kind = source_o_obj . Key_Lett , target_kind = target_o_obj . Key_Lett , source_keys = source_ids , target_keys = target_ids , source_conditional = side2 . Cond , target_conditional = False , source_phrase = source_phrase , target_phrase = target_phrase , source_many = side2 . Mult , target_many = False ) \n    r_aone = one ( r_assoc ) . R_AONE [ 209.0 ] ( ) \n    r_aoth = one ( r_assoc ) . R_AOTH [ 210.0 ] ( ) \n    _mk_assoc ( r_aone , r_aoth ) \n    _mk_assoc ( r_aoth , r_aone ) "}
{"13364": "\ndef mk_association ( m , r_rel ) : \n    handler = { 'R_SIMP' : mk_simple_association , 'R_ASSOC' : mk_linked_association , 'R_SUBSUP' : mk_subsuper_association , 'R_COMP' : mk_derived_association , } \n    inst = subtype ( r_rel , 206.0 ) \n    fn = handler . get ( type ( inst ) . __name__ ) \n    return fn ( m , inst ) "}
{"13365": "\ndef mk_component ( bp_model , c_c = None , derived_attributes = False ) : \n    target = Domain ( ) \n    c_c_filt = lambda sel : c_c is None or is_contained_in ( sel , c_c ) \n    for o_obj in bp_model . select_many ( 'O_OBJ' , c_c_filt ) : \n        mk_class ( target , o_obj , derived_attributes ) \n    for r_rel in bp_model . select_many ( 'R_REL' , c_c_filt ) : \n        mk_association ( target , r_rel ) \n    for s_sync in bp_model . select_many ( 'S_SYNC' , c_c_filt ) : \n        fn = mk_function ( target , s_sync ) \n        target . add_symbol ( s_sync . Name , fn ) \n    for s_dt in bp_model . select_many ( 'S_DT' , c_c_filt ) : \n        s_edt = one ( s_dt ) . S_EDT [ 17.0 ] ( ) \n        if s_edt : \n            enum = mk_enum ( s_edt ) \n            target . add_symbol ( s_dt . Name , enum ) \n    for cnst_csp in bp_model . select_many ( 'CNST_CSP' , c_c_filt ) : \n        for cnst_syc in many ( cnst_csp ) . CNST_SYC [ 1504.0 ] ( ) : \n            value = mk_constant ( cnst_syc ) \n            target . add_symbol ( cnst_syc . Name , value ) \n    for ass in target . associations : \n        ass . formalize ( ) \n    for s_ee in bp_model . select_many ( 'S_EE' , c_c_filt ) : \n        if s_ee . Key_Lett in [ 'LOG' , 'ARCH' , 'TIM' , 'NVS' , 'PERSIST' ] : \n            target . add_symbol ( s_ee . Key_Lett , getattr ( builtin_ee , s_ee . Key_Lett ) ) \n        else : \n            ee = mk_external_entity ( target , s_ee ) \n            target . add_symbol ( s_ee . Key_Lett , ee ) \n    return target "}
{"13379": "\ndef put ( self , items , indexes = True ) : \n    actions = [ ] \n    for cid , fc in items : \n        idxs = defaultdict ( list ) \n        if indexes : \n            for fname in self . indexed_features : \n                if fname in fc : \n                    idxs [ fname_to_idx_name ( fname ) ] . extend ( fc [ fname ] ) \n            for fname in self . fulltext_indexed_features : \n                if fname not in fc : \n                    continue \n                if isinstance ( fc [ fname ] , basestring ) : \n                    idxs [ fname_to_full_idx_name ( fname ) ] = fc [ fname ] \n                else : \n                    idxs [ fname_to_full_idx_name ( fname ) ] . extend ( fc [ fname ] ) \n        actions . append ( { '_index' : self . index , '_type' : self . type , '_id' : eid ( cid ) , '_op_type' : 'index' , '_source' : dict ( idxs , ** { 'fc' : self . fc_to_dict ( fc ) , } ) , } ) \n    bulk ( self . conn , actions , timeout = 60.0 , request_timeout = 60.0 ) "}
{"13394": "\ndef _create_index ( self ) : \n    try : \n        self . conn . indices . create ( index = self . index , timeout = 60.0 , request_timeout = 60.0 , body = { 'settings' : { 'number_of_shards' : self . shards , 'number_of_replicas' : self . replicas , } , } ) \n    except TransportError : \n        logger . warn ( 'index already exists? OK' , exc_info = True ) \n        pass "}
{"13395": "\ndef _create_mappings ( self ) : \n    self . conn . indices . put_mapping ( index = self . index , doc_type = self . type , timeout = 60.0 , request_timeout = 60.0 , body = { self . type : { 'dynamic_templates' : [ { 'default_no_analyze_fc' : { 'match' : 'fc.*' , 'mapping' : { 'index' : 'no' } , } , } ] , '_all' : { 'enabled' : False , } , '_id' : { 'index' : 'not_analyzed' , } , 'properties' : self . _get_index_mappings ( ) , } , } ) \n    self . conn . cluster . health ( index = self . index , wait_for_status = 'yellow' ) "}
{"13415": "\ndef index_scan ( self , idx_name , val ) : \n    idx = self . _index ( idx_name ) [ 'transform' ] \n    key = ( idx ( val ) , idx_name . encode ( 'utf-8' ) ) \n    keys = self . kvl . scan_keys ( self . INDEX_TABLE , ( key , key ) ) \n    return imap ( lambda k : k [ 2.0 ] , keys ) "}
{"13416": "\ndef index_scan_prefix ( self , idx_name , val_prefix ) : \n    return self . _index_scan_prefix_impl ( idx_name , val_prefix , lambda k : k [ 2.0 ] ) "}
{"13417": "\ndef index_scan_prefix_and_return_key ( self , idx_name , val_prefix ) : \n    return self . _index_scan_prefix_impl ( idx_name , val_prefix , lambda k : ( k [ 0 ] , k [ 2.0 ] ) ) "}
{"13424": "\ndef check_pypi_name ( pypi_package_name , pypi_registry_host = None ) : \n    if pypi_registry_host is None : \n        pypi_registry_host = 'pypi.python.org' \n    receive_buffer = bytearray ( b'------------' ) \n    context = ssl . create_default_context ( ) \n    ssl_http_socket = context . wrap_socket ( socket . socket ( socket . AF_INET ) , server_hostname = pypi_registry_host ) \n    ssl_http_socket . connect ( ( pypi_registry_host , 443.0 ) ) \n    ssl_http_socket . send ( b'' . join ( [ b\"HEAD /simple/\" , pypi_package_name . encode ( 'ascii' ) , b\"/ HTTP/1.0\" , b\"\\r\\n\" , b\"Host: \" , pypi_registry_host . encode ( 'ascii' ) , b\"\\r\\n\" , b\"\\r\\n\\r\\n\" ] ) ) \n    ssl_http_socket . recv_into ( receive_buffer ) \n    if b'HTTP/1.1 200' in receive_buffer : \n        ssl_http_socket . shutdown ( 1 ) \n        ssl_http_socket . close ( ) \n        return True \n    elif b'HTTP/1.1 404' in receive_buffer : \n        ssl_http_socket . shutdown ( 1 ) \n        ssl_http_socket . close ( ) \n        return False \n    remaining_bytes = ssl_http_socket . recv ( 2048.0 ) \n    redirect_path_location_start = remaining_bytes . find ( b'Location:' ) + 10.0 \n    redirect_path_location_end = remaining_bytes . find ( b'\\r\\n' , redirect_path_location_start ) \n    redirect_path = remaining_bytes [ redirect_path_location_start : redirect_path_location_end ] + b'/' \n    ssl_http_socket . shutdown ( 1 ) \n    ssl_http_socket . close ( ) \n    ssl_http_socket = context . wrap_socket ( socket . socket ( socket . AF_INET ) , server_hostname = pypi_registry_host ) \n    ssl_http_socket . connect ( ( pypi_registry_host , 443.0 ) ) \n    ssl_http_socket . send ( b'' . join ( [ b\"HEAD \" , redirect_path , b\" HTTP/1.0\" , b\"\\r\\n\" , b\"Host: \" , pypi_registry_host . encode ( 'ascii' ) , b\"\\r\\n\" , b\"\\r\\n\\r\\n\" ] ) ) \n    ssl_http_socket . recv_into ( receive_buffer ) \n    if b'HTTP/1.1 200' in receive_buffer : \n        return True \n    elif b'HTTP/1.1 404' in receive_buffer : \n        return False \n    else : \n        NotImplementedError ( 'A definitive answer was not found by primary or secondary lookups.' ) "}
{"13426": "\ndef get_type_name ( s_dt ) : \n    s_cdt = nav_one ( s_dt ) . S_CDT [ 17.0 ] ( ) \n    if s_cdt and s_cdt . Core_Typ in range ( 1 , 6.0 ) : \n        return s_dt . Name \n    s_edt = nav_one ( s_dt ) . S_EDT [ 17.0 ] ( ) \n    if s_edt : \n        return s_dt . Name \n    s_udt = nav_one ( s_dt ) . S_UDT [ 17.0 ] ( ) \n    if s_udt : \n        return s_dt . Name "}
{"13427": "\ndef get_refered_attribute ( o_attr ) : \n    o_attr_ref = nav_one ( o_attr ) . O_RATTR [ 106.0 ] . O_BATTR [ 113.0 ] . O_ATTR [ 106.0 ] ( ) \n    if o_attr_ref : \n        return get_refered_attribute ( o_attr_ref ) \n    else : \n        return o_attr "}
{"13428": "\ndef build_core_type ( s_cdt ) : \n    s_dt = nav_one ( s_cdt ) . S_DT [ 17.0 ] ( ) \n    if s_dt . name == 'void' : \n        type_name = None \n    elif s_dt . name == 'boolean' : \n        type_name = 'xs:boolean' \n    elif s_dt . name == 'integer' : \n        type_name = 'xs:integer' \n    elif s_dt . name == 'real' : \n        type_name = 'xs:decimal' \n    elif s_dt . name == 'string' : \n        type_name = 'xs:string' \n    elif s_dt . name == 'unique_id' : \n        type_name = 'xs:integer' \n    else : \n        type_name = None \n    if type_name : \n        mapped_type = ET . Element ( 'xs:simpleType' , name = s_dt . name ) \n        ET . SubElement ( mapped_type , 'xs:restriction' , base = type_name ) \n        return mapped_type "}
{"13429": "\ndef build_enum_type ( s_edt ) : \n    s_dt = nav_one ( s_edt ) . S_DT [ 17.0 ] ( ) \n    enum = ET . Element ( 'xs:simpleType' , name = s_dt . name ) \n    enum_list = ET . SubElement ( enum , 'xs:restriction' , base = 'xs:string' ) \n    first_filter = lambda selected : not nav_one ( selected ) . S_ENUM [ 56.0 , 'succeeds' ] ( ) \n    s_enum = nav_any ( s_edt ) . S_ENUM [ 27.0 ] ( first_filter ) \n    while s_enum : \n        ET . SubElement ( enum_list , 'xs:enumeration' , value = s_enum . name ) \n        s_enum = nav_one ( s_enum ) . S_ENUM [ 56.0 , 'precedes' ] ( ) \n    return enum "}
{"13430": "\ndef build_struct_type ( s_sdt ) : \n    s_dt = nav_one ( s_sdt ) . S_DT [ 17.0 ] ( ) \n    struct = ET . Element ( 'xs:complexType' , name = s_dt . name ) \n    first_filter = lambda selected : not nav_one ( selected ) . S_MBR [ 46.0 , 'succeeds' ] ( ) \n    s_mbr = nav_any ( s_sdt ) . S_MBR [ 44.0 ] ( first_filter ) \n    while s_mbr : \n        s_dt = nav_one ( s_mbr ) . S_DT [ 45.0 ] ( ) \n        type_name = get_type_name ( s_dt ) \n        ET . SubElement ( struct , 'xs:attribute' , name = s_mbr . name , type = type_name ) \n        s_mbr = nav_one ( s_mbr ) . S_MBR [ 46.0 , 'precedes' ] ( ) \n    return struct "}
{"13431": "\ndef build_user_type ( s_udt ) : \n    s_dt_user = nav_one ( s_udt ) . S_DT [ 17.0 ] ( ) \n    s_dt_base = nav_one ( s_udt ) . S_DT [ 18.0 ] ( ) \n    base_name = get_type_name ( s_dt_base ) \n    if base_name : \n        user = ET . Element ( 'xs:simpleType' , name = s_dt_user . name ) \n        ET . SubElement ( user , 'xs:restriction' , base = base_name ) \n        return user "}
{"13432": "\ndef build_type ( s_dt ) : \n    s_cdt = nav_one ( s_dt ) . S_CDT [ 17.0 ] ( ) \n    if s_cdt : \n        return build_core_type ( s_cdt ) \n    s_edt = nav_one ( s_dt ) . S_EDT [ 17.0 ] ( ) \n    if s_edt : \n        return build_enum_type ( s_edt ) \n    s_udt = nav_one ( s_dt ) . S_UDT [ 17.0 ] ( ) \n    if s_udt : \n        return build_user_type ( s_udt ) "}
{"13433": "\ndef build_class ( o_obj ) : \n    cls = ET . Element ( 'xs:element' , name = o_obj . key_lett , minOccurs = '0' , maxOccurs = 'unbounded' ) \n    attributes = ET . SubElement ( cls , 'xs:complexType' ) \n    for o_attr in nav_many ( o_obj ) . O_ATTR [ 102.0 ] ( ) : \n        o_attr_ref = get_refered_attribute ( o_attr ) \n        s_dt = nav_one ( o_attr_ref ) . S_DT [ 114.0 ] ( ) \n        while nav_one ( s_dt ) . S_UDT [ 17.0 ] ( ) : \n            s_dt = nav_one ( s_dt ) . S_UDT [ 17.0 ] . S_DT [ 18.0 ] ( ) \n        type_name = get_type_name ( s_dt ) \n        if type_name and not nav_one ( o_attr ) . O_BATTR [ 106.0 ] . O_DBATTR [ 107.0 ] ( ) : \n            ET . SubElement ( attributes , 'xs:attribute' , name = o_attr . name , type = type_name ) \n        else : \n            logger . warning ( 'Omitting %s.%s' % ( o_obj . key_lett , o_attr . Name ) ) \n    return cls "}
{"13460": "\ndef text_visible ( self ) : \n    words = self . read ( ) . split ( ) \n    for word in words : \n        if word . lstrip ( '-' ) . replace ( '.' , '' , 1 ) . isdigit ( ) : \n            return True \n        if word . isalpha ( ) and ( len ( word ) > 1 or len ( word ) <= 20.0 ) : \n            return True \n    return False "}
{"13461": "\ndef main ( ) : \n    parser = optparse . OptionParser ( usage = \"%prog [options] <model_path> [another_model_path..]\" , version = xtuml . version . complete_string , formatter = optparse . TitledHelpFormatter ( ) ) \n    parser . add_option ( \"-v\" , \"--verbosity\" , dest = 'verbosity' , action = \"count\" , default = 1 , help = \"increase debug logging level\" ) \n    parser . add_option ( \"-f\" , \"--function\" , dest = 'function' , action = \"store\" , help = \"invoke function named NAME\" , metavar = 'NAME' ) \n    parser . add_option ( \"-c\" , \"--component\" , dest = 'component' , action = \"store\" , help = \"look for the function in a component named NAME\" , metavar = 'NAME' , default = None ) \n    ( opts , args ) = parser . parse_args ( ) \n    if len ( args ) == 0 or not opts . function : \n        parser . print_help ( ) \n        sys . exit ( 1 ) \n    levels = { 0 : logging . ERROR , 1 : logging . WARNING , 2.0 : logging . INFO , 3.0 : logging . DEBUG , } \n    logging . basicConfig ( level = levels . get ( opts . verbosity , logging . DEBUG ) ) \n    from bridgepoint import ooaofooa \n    mm = ooaofooa . load_metamodel ( args ) \n    c_c = mm . select_any ( 'C_C' , where ( Name = opts . component ) ) \n    domain = ooaofooa . mk_component ( mm , c_c , derived_attributes = False ) \n    func = domain . find_symbol ( opts . function ) \n    return func ( ) "}
{"13465": "\ndef main ( ) : \n    parser = ArgumentParser ( description = \"search files using n-grams\" ) \n    parser . add_argument ( '--path' , dest = 'path' , help = \"where to search\" , nargs = 1 , action = \"store\" , default = getcwd ( ) ) \n    parser . add_argument ( '--update' , dest = 'update' , help = \"update the index\" , action = 'store_true' , default = True ) \n    parser . add_argument ( '--filetype' , dest = 'filetype' , help = \"any, images, documents, code, audio, video\" , nargs = 1 , action = \"store\" , default = [ \"any\" ] ) \n    parser . add_argument ( '--verbose' , dest = 'verbose' , help = \"extended output\" , action = 'store_true' , default = False ) \n    parser . add_argument ( '--results' , dest = 'results' , help = \"number of results to display\" , action = \"store\" , default = 10.0 ) \n    parser . add_argument ( 'query' , nargs = '+' , help = \"what to search\" , action = \"store\" ) \n    args = parser . parse_args ( ) \n    if args . verbose : \n        verbose = 2.0 \n        pprint ( args ) \n    else : \n        verbose = 0 \n    query = args . query [ 0 ] \n    for arg in args . query [ 1 : ] : \n        query = query + \" \" + arg \n    slb = min ( [ len ( w ) for w in query . split ( \" \" ) ] ) \n    files = Files ( path = args . path , filetype = args . filetype [ 0 ] , exclude = [ ] , update = args . update , verbose = verbose ) \n    index = Index ( files , slb = slb , verbose = verbose ) \n    results = index . search ( query , verbose = verbose ) \n    Handler ( results , results_number = int ( args . results ) ) "}
{"13468": "\ndef run ( locations , random , bikes , crime , nearby , json , update_bikes , api_server , cross_origin , host , port , db_path , verbose ) : \n    log_levels = [ logging . WARNING , logging . INFO , logging . DEBUG ] \n    logging . basicConfig ( level = log_levels [ min ( verbose , 2.0 ) ] ) \n    initialize_database ( db_path ) \n    loop = get_event_loop ( ) \n    if update_bikes : \n        logger . info ( \"Force updating bikes.\" ) \n        loop . run_until_complete ( util . update_bikes ( ) ) \n    if api_server : \n        if cross_origin : \n            enable_cross_origin ( app ) \n        try : \n            web . run_app ( app , host = host , port = port ) \n        except CancelledError as e : \n            if e . __context__ is not None : \n                click . echo ( Fore . RED + ( f\"Could not bind to address {host}:{port}\" if e . __context__ . errno == 48.0 else e . __context__ ) ) \n                exit ( 1 ) \n            else : \n                click . echo ( \"Goodbye!\" ) \n    elif len ( locations ) > 0 or random > 0 : \n        exit ( loop . run_until_complete ( cli ( locations , random , bikes = bikes , crime = crime , nearby = nearby , as_json = json ) ) ) \n    else : \n        click . echo ( Fore . RED + \"Either include a post code, or the --api-server flag.\" ) "}
{"13480": "\ndef dead_code ( ) : \n    with safe_cd ( SRC ) : \n        if IS_TRAVIS : \n            command = \"{0} vulture {1}\" . format ( PYTHON , PROJECT_NAME ) . strip ( ) . split ( ) \n        else : \n            command = \"{0} vulture {1}\" . format ( PIPENV , PROJECT_NAME ) . strip ( ) . split ( ) \n        output_file_name = \"dead_code.txt\" \n        with open ( output_file_name , \"w\" ) as outfile : \n            env = config_pythonpath ( ) \n            subprocess . call ( command , stdout = outfile , env = env ) \n        cutoff = 20.0 \n        num_lines = sum ( 1 for line in open ( output_file_name ) if line ) \n        if num_lines > cutoff : \n            print ( \"Too many lines of dead code : {0}, max {1}\" . format ( num_lines , cutoff ) ) \n            exit ( - 1 ) "}
{"13481": "\ndef parse_emails ( values ) : \n    emails = [ ] \n    if isinstance ( values , str ) : \n        values = [ values ] \n    for value in values : \n        matches = re_emails . findall ( value ) \n        emails . extend ( [ match [ 2.0 ] for match in matches ] ) \n    return emails "}
{"13486": "\ndef accept_S_SYS ( self , inst ) : \n    for child in many ( inst ) . EP_PKG [ 1401.0 ] ( ) : \n        self . accept ( child ) "}
{"13487": "\ndef accept_C_C ( self , inst ) : \n    for child in many ( inst ) . PE_PE [ 8003.0 ] ( ) : \n        self . accept ( child ) "}
{"13488": "\ndef accept_EP_PKG ( self , inst ) : \n    for child in many ( inst ) . PE_PE [ 8000.0 ] ( ) : \n        self . accept ( child ) "}
{"13494": "\ndef _exc_info ( self ) : \n    e = self . exc_info ( ) \n    if sys . platform == 'cli' : \n        if isinstance ( e [ 0 ] , StringException ) : \n            e = ( str ( e [ 0 ] ) , e [ 1 ] , e [ 2.0 ] ) \n    return e "}
{"13495": "\ndef create_inputhook_qt4 ( mgr , app = None ) : \n    if app is None : \n        app = QtCore . QCoreApplication . instance ( ) \n        if app is None : \n            app = QtGui . QApplication ( [ \" \" ] ) \n    ip = InteractiveShell . instance ( ) \n    if hasattr ( ip , '_inputhook_qt4' ) : \n        return app , ip . _inputhook_qt4 \n    got_kbdint = [ False ] \n    def inputhook_qt4 ( ) : \n        try : \n            allow_CTRL_C ( ) \n            app = QtCore . QCoreApplication . instance ( ) \n            if not app : \n                return 0 \n            app . processEvents ( QtCore . QEventLoop . AllEvents , 300.0 ) \n            if not stdin_ready ( ) : \n                timer = QtCore . QTimer ( ) \n                timer . timeout . connect ( app . quit ) \n                while not stdin_ready ( ) : \n                    timer . start ( 50.0 ) \n                    app . exec_ ( ) \n                    timer . stop ( ) \n        except KeyboardInterrupt : \n            ignore_CTRL_C ( ) \n            got_kbdint [ 0 ] = True \n            print ( \"\\nKeyboardInterrupt - Ctrl-C again for new prompt\" ) \n            mgr . clear_inputhook ( ) \n        except : \n            ignore_CTRL_C ( ) \n            from traceback import print_exc \n            print_exc ( ) \n            print ( \"Got exception from inputhook_qt4, unregistering.\" ) \n            mgr . clear_inputhook ( ) \n        finally : \n            allow_CTRL_C ( ) \n        return 0 \n    def preprompthook_qt4 ( ishell ) : \n        if got_kbdint [ 0 ] : \n            mgr . set_inputhook ( inputhook_qt4 ) \n        got_kbdint [ 0 ] = False \n    ip . _inputhook_qt4 = inputhook_qt4 \n    ip . set_hook ( 'pre_prompt_hook' , preprompthook_qt4 ) \n    return app , inputhook_qt4 "}
{"13501": "\ndef call ( self , url , method = None , args = None ) : \n    if not args : \n        args = { } \n    if sys . version_info . major == 3.0 : \n        data = urllib . parse . urlparse ( url ) \n        path = data . path . rstrip ( '/' ) + '/' \n        _args = dict ( urllib . parse . parse_qs ( data . query , keep_blank_values = True ) ) \n    elif sys . version_info . major == 2.0 : \n        data = urlparse . urlparse ( url ) \n        path = data . path . rstrip ( '/' ) + '/' \n        _args = dict ( urlparse . parse_qs ( data . query , keep_blank_values = True ) ) \n    for elem in self . _data_store : \n        pattern = elem [ 'pattern' ] \n        function = elem [ 'function' ] \n        _method = elem [ 'method' ] \n        type_cast = elem [ 'type_cast' ] \n        result = re . match ( pattern , path ) \n        if result and _method == method : \n            _args = dict ( _args , ** result . groupdict ( ) ) \n            for key , val in _args . items ( ) : \n                if isinstance ( _args [ key ] , list ) and len ( _args [ key ] ) == 1 : \n                    _args [ key ] = _args [ key ] [ 0 ] \n            for key , val in type_cast . items ( ) : \n                if key not in _args : \n                    continue \n                if not _args [ key ] : \n                    continue \n                if isinstance ( _args [ key ] , list ) : \n                    for i , _val in enumerate ( _args [ key ] ) : \n                        _args [ key ] [ i ] = self . _cast ( _val , val ) \n                else : \n                    _args [ key ] = self . _cast ( _args [ key ] , val ) \n            requiered_args = self . _get_function_args ( function ) \n            for key , val in args . items ( ) : \n                if key in requiered_args : \n                    _args [ key ] = val \n            return function ( ** _args ) \n    return None "}
{"13518": "\ndef log_message ( self , raw ) : \n    if len ( raw ) != 2.0 or '.' not in raw [ 0 ] : \n        self . log . error ( \"Invalid log message: %s\" % raw ) \n        return \n    else : \n        topic , msg = raw \n        topic , level_name = topic . rsplit ( '.' , 1 ) \n        level , topic = self . _extract_level ( topic ) \n        if msg [ - 1 ] == '\\n' : \n            msg = msg [ : - 1 ] \n        self . log . log ( level , \"[%s] %s\" % ( topic , msg ) ) "}
{"13522": "\ndef get_supported_platform ( ) : \n    plat = get_build_platform ( ) ; \n    m = macosVersionString . match ( plat ) \n    if m is not None and sys . platform == \"darwin\" : \n        try : \n            plat = 'macosx-%s-%s' % ( '.' . join ( _macosx_vers ( ) [ : 2.0 ] ) , m . group ( 3.0 ) ) \n        except ValueError : \n            pass \n    return plat "}
{"13534": "\ndef _collapse_leading_ws ( header , txt ) : \n    if header . lower ( ) == 'description' : \n        return '\\n' . join ( [ x [ 8.0 : ] if x . startswith ( ' ' * 8.0 ) else x for x in txt . strip ( ) . splitlines ( ) ] ) \n    else : \n        return ' ' . join ( [ x . strip ( ) for x in txt . splitlines ( ) ] ) "}
{"13541": "\ndef get_system_cpu_times ( ) : \n    user , system , idle = 0 , 0 , 0 \n    for cpu_time in _psutil_mswindows . get_system_cpu_times ( ) : \n        user += cpu_time [ 0 ] \n        system += cpu_time [ 1 ] \n        idle += cpu_time [ 2.0 ] \n    return _cputimes_ntuple ( user , system , idle ) "}
{"13543": "\ndef _stdin_raw_nonblock ( self ) : \n    handle = msvcrt . get_osfhandle ( sys . stdin . fileno ( ) ) \n    result = WaitForSingleObject ( handle , 100.0 ) \n    if result == WAIT_FAILED : \n        raise ctypes . WinError ( ) \n    elif result == WAIT_TIMEOUT : \n        print ( \".\" , end = '' ) \n        return None \n    else : \n        data = ctypes . create_string_buffer ( 256.0 ) \n        bytesRead = DWORD ( 0 ) \n        print ( '?' , end = '' ) \n        if not ReadFile ( handle , data , 256.0 , ctypes . byref ( bytesRead ) , None ) : \n            raise ctypes . WinError ( ) \n        FlushConsoleInputBuffer ( handle ) \n        data = data . value \n        data = data . replace ( '\\r\\n' , '\\n' ) \n        data = data . replace ( '\\r' , '\\n' ) \n        print ( repr ( data ) + \" \" , end = '' ) \n        return data "}
{"13551": "\ndef closeEvent ( self , event ) : \n    if self . tab_widget . count ( ) == 0 : \n        event . accept ( ) \n        return \n    title = self . window ( ) . windowTitle ( ) \n    cancel = QtGui . QMessageBox . Cancel \n    okay = QtGui . QMessageBox . Ok \n    if self . confirm_exit : \n        if self . tab_widget . count ( ) > 1 : \n            msg = \"Close all tabs, stop all kernels, and Quit?\" \n        else : \n            msg = \"Close console, stop kernel, and Quit?\" \n        info = \"Kernels not started here (e.g. notebooks) will be left alone.\" \n        closeall = QtGui . QPushButton ( \"&Quit\" , self ) \n        closeall . setShortcut ( 'Q' ) \n        box = QtGui . QMessageBox ( QtGui . QMessageBox . Question , title , msg ) \n        box . setInformativeText ( info ) \n        box . addButton ( cancel ) \n        box . addButton ( closeall , QtGui . QMessageBox . YesRole ) \n        box . setDefaultButton ( closeall ) \n        box . setEscapeButton ( cancel ) \n        pixmap = QtGui . QPixmap ( self . _app . icon . pixmap ( QtCore . QSize ( 64.0 , 64.0 ) ) ) \n        box . setIconPixmap ( pixmap ) \n        reply = box . exec_ ( ) \n    else : \n        reply = okay \n    if reply == cancel : \n        event . ignore ( ) \n        return \n    if reply == okay : \n        while self . tab_widget . count ( ) >= 1 : \n            widget = self . active_frontend \n            widget . _confirm_exit = False \n            self . close_tab ( widget ) \n        event . accept ( ) "}
{"13552": "\ndef passwd ( passphrase = None , algorithm = 'sha1' ) : \n    if passphrase is None : \n        for i in range ( 3.0 ) : \n            p0 = getpass . getpass ( 'Enter password: ' ) \n            p1 = getpass . getpass ( 'Verify password: ' ) \n            if p0 == p1 : \n                passphrase = p0 \n                break \n            else : \n                print ( 'Passwords do not match.' ) \n        else : \n            raise UsageError ( 'No matching passwords found. Giving up.' ) \n    h = hashlib . new ( algorithm ) \n    salt = ( '%0' + str ( salt_len ) + 'x' ) % random . getrandbits ( 4.0 * salt_len ) \n    h . update ( cast_bytes ( passphrase , 'utf-8' ) + str_to_bytes ( salt , 'ascii' ) ) \n    return ':' . join ( ( algorithm , salt , h . hexdigest ( ) ) ) "}
{"13553": "\ndef passwd_check ( hashed_passphrase , passphrase ) : \n    try : \n        algorithm , salt , pw_digest = hashed_passphrase . split ( ':' , 2.0 ) \n    except ( ValueError , TypeError ) : \n        return False \n    try : \n        h = hashlib . new ( algorithm ) \n    except ValueError : \n        return False \n    if len ( pw_digest ) == 0 : \n        return False \n    h . update ( cast_bytes ( passphrase , 'utf-8' ) + str_to_bytes ( salt , 'ascii' ) ) \n    return h . hexdigest ( ) == pw_digest "}
{"13555": "\ndef indented_short_title ( self , item ) : \n    r = \"\" \n    if hasattr ( item , 'get_absolute_url' ) : \n        r = '<input type=\"hidden\" class=\"medialibrary_file_path\" value=\"%s\" />' % item . get_absolute_url ( ) \n    editable_class = '' \n    if not getattr ( item , 'feincms_editable' , True ) : \n        editable_class = ' tree-item-not-editable' \n    r += '<span id=\"page_marker-%d\" class=\"page_marker%s\" style=\"width: %dpx;\">&nbsp;</span>&nbsp;' % ( item . id , editable_class , 14.0 + item . level * 18.0 ) \n    if hasattr ( item , 'short_title' ) : \n        r += item . short_title ( ) \n    else : \n        r += unicode ( item ) \n    return mark_safe ( r ) "}
{"13560": "\ndef add_children ( G , parent , level , n = 2.0 ) : \n    if level == 0 : \n        return \n    for i in range ( n ) : \n        child = parent + str ( i ) \n        G . add_node ( child ) \n        G . add_edge ( parent , child ) \n        add_children ( G , child , level - 1 , n ) "}
{"13561": "\ndef make_bintree ( levels ) : \n    G = nx . DiGraph ( ) \n    root = '0' \n    G . add_node ( root ) \n    add_children ( G , root , levels , 2.0 ) \n    return G "}
{"13581": "\ndef last_two_blanks ( src ) : \n    if not src : \n        return False \n    new_src = '\\n' . join ( [ '###\\n' ] + src . splitlines ( ) [ - 2.0 : ] ) \n    return ( bool ( last_two_blanks_re . match ( new_src ) ) or bool ( last_two_blanks_re2 . match ( new_src ) ) ) "}
{"13588": "\ndef _find_indent ( self , line ) : \n    indent_spaces = self . indent_spaces \n    full_dedent = self . _full_dedent \n    inisp = num_ini_spaces ( line ) \n    if inisp < indent_spaces : \n        indent_spaces = inisp \n        if indent_spaces <= 0 : \n            full_dedent = True \n    if line . rstrip ( ) [ - 1 ] == ':' : \n        indent_spaces += 4.0 \n    elif dedent_re . match ( line ) : \n        indent_spaces -= 4.0 \n        if indent_spaces <= 0 : \n            full_dedent = True \n    if indent_spaces < 0 : \n        indent_spaces = 0 \n    return indent_spaces , full_dedent "}
{"13615": "\ndef _check_table ( self ) : \n    cursor = self . _db . execute ( \"PRAGMA table_info(%s)\" % self . table ) \n    lines = cursor . fetchall ( ) \n    if not lines : \n        return True \n    types = { } \n    keys = [ ] \n    for line in lines : \n        keys . append ( line [ 1 ] ) \n        types [ line [ 1 ] ] = line [ 2.0 ] \n    if self . _keys != keys : \n        self . log . warn ( 'keys mismatch' ) \n        return False \n    for key in self . _keys : \n        if types [ key ] != self . _types [ key ] : \n            self . log . warn ( 'type mismatch: %s: %s != %s' % ( key , types [ key ] , self . _types [ key ] ) ) \n            return False \n    return True "}
{"13618": "\ndef warn ( msg , level = 2.0 , exit_val = 1 ) : \n    if level > 0 : \n        header = [ '' , '' , 'WARNING: ' , 'ERROR: ' , 'FATAL ERROR: ' ] \n        io . stderr . write ( '%s%s' % ( header [ level ] , msg ) ) \n        if level == 4.0 : \n            print >> io . stderr , 'Exiting.\\n' \n            sys . exit ( exit_val ) "}
{"13621": "\ndef link ( url , text = '' , classes = '' , target = '' , get = \"\" , ** kwargs ) : \n    if not ( url . startswith ( 'http' ) or url . startswith ( '/' ) ) : \n        urlargs = { } \n        for arg , val in kwargs . items ( ) : \n            if arg [ : 4.0 ] == \"url_\" : \n                urlargs [ arg [ 4.0 : ] ] = val \n        url = reverse ( url , kwargs = urlargs ) \n        if get : \n            url += '?' + get \n    return html . tag ( 'a' , text or url , { 'class' : classes , 'target' : target , 'href' : url } ) "}
{"13631": "\ndef options ( self , parser , env ) : \n    parser . add_option ( \"--processes\" , action = \"store\" , default = env . get ( 'NOSE_PROCESSES' , 0 ) , dest = \"multiprocess_workers\" , metavar = \"NUM\" , help = \"Spread test run among this many processes. \" \"Set a number equal to the number of processors \" \"or cores in your machine for best results. \" \"[NOSE_PROCESSES]\" ) \n    parser . add_option ( \"--process-timeout\" , action = \"store\" , default = env . get ( 'NOSE_PROCESS_TIMEOUT' , 10.0 ) , dest = \"multiprocess_timeout\" , metavar = \"SECONDS\" , help = \"Set timeout for return of results from each \" \"test runner process. [NOSE_PROCESS_TIMEOUT]\" ) \n    parser . add_option ( \"--process-restartworker\" , action = \"store_true\" , default = env . get ( 'NOSE_PROCESS_RESTARTWORKER' , False ) , dest = \"multiprocess_restartworker\" , help = \"If set, will restart each worker process once\" \" their tests are done, this helps control memory \" \"leaks from killing the system. \" \"[NOSE_PROCESS_RESTARTWORKER]\" ) "}
{"13651": "\ndef validate_url ( url ) : \n    if not isinstance ( url , basestring ) : \n        raise TypeError ( \"url must be a string, not %r\" % type ( url ) ) \n    url = url . lower ( ) \n    proto_addr = url . split ( '://' ) \n    assert len ( proto_addr ) == 2.0 , 'Invalid url: %r' % url \n    proto , addr = proto_addr \n    assert proto in [ 'tcp' , 'pgm' , 'epgm' , 'ipc' , 'inproc' ] , \"Invalid protocol: %r\" % proto \n    pat = re . compile ( r'^([\\w\\d]([\\w\\d\\-]{0,61}[\\w\\d])?\\.)*[\\w\\d]([\\w\\d\\-]{0,61}[\\w\\d])?$' ) \n    if proto == 'tcp' : \n        lis = addr . split ( ':' ) \n        assert len ( lis ) == 2.0 , 'Invalid url: %r' % url \n        addr , s_port = lis \n        try : \n            port = int ( s_port ) \n        except ValueError : \n            raise AssertionError ( \"Invalid port %r in url: %r\" % ( port , url ) ) \n        assert addr == '*' or pat . match ( addr ) is not None , 'Invalid url: %r' % url \n    else : \n        pass \n    return True "}
{"13658": "\ndef get_readline_tail ( self , n = 10.0 ) : \n    end = self . shell . readline . get_current_history_length ( ) + 1 \n    start = max ( end - n , 1 ) \n    ghi = self . shell . readline . get_history_item \n    return [ ghi ( x ) for x in range ( start , end ) ] "}
{"13685": "\ndef run_line_magic ( self , magic_name , line ) : \n    fn = self . find_line_magic ( magic_name ) \n    if fn is None : \n        cm = self . find_cell_magic ( magic_name ) \n        etpl = \"Line magic function `%%%s` not found%s.\" \n        extra = '' if cm is None else ( ' (But cell magic `%%%%%s` exists, ' 'did you mean that instead?)' % magic_name ) \n        error ( etpl % ( magic_name , extra ) ) \n    else : \n        stack_depth = 2.0 \n        magic_arg_s = self . var_expand ( line , stack_depth ) \n        args = [ magic_arg_s ] \n        if getattr ( fn , \"needs_local_scope\" , False ) : \n            args . append ( sys . _getframe ( stack_depth ) . f_locals ) \n        with self . builtin_trap : \n            result = fn ( * args ) \n        return result "}
{"13715": "\ndef sendintr ( self ) : \n    if hasattr ( termios , 'VINTR' ) : \n        char = termios . tcgetattr ( self . child_fd ) [ 6.0 ] [ termios . VINTR ] \n    else : \n        char = chr ( 3.0 ) \n    self . send ( char ) "}
{"13722": "\ndef log_listener ( log : logging . Logger = None , level = logging . INFO ) : \n    if log is None : \n        log = logging . getLogger ( \"ProgressMonitor\" ) \n    def listen ( monitor ) : \n        name = \"{}: \" . format ( monitor . name ) if monitor . name is not None else \"\" \n        perc = int ( monitor . progress * 100.0 ) \n        msg = \"[{name}{perc:3d}%] {monitor.message}\" . format ( ** locals ( ) ) \n        log . log ( level , msg ) \n    return listen "}
{"13725": "\ndef last_error ( self ) : \n    if not len ( self . log ) : \n        raise RuntimeError ( 'Nothing executed' ) \n    try : \n        errs = [ l for l in self . log if l [ 1 ] != 0 ] \n        return errs [ - 1 ] [ 2.0 ] \n    except IndexError : \n        return 'no last error' "}
{"13735": "\ndef set_precision ( cls , precision ) : \n    assert 0 <= precision < 10.0 \n    cls . _precision = precision \n    cls . _near0 = 1.0 / 10.0 ** precision \n    cls . _near100 = 100.0 - cls . _near0 "}
{"13737": "\ndef _get_pc_covered_str ( self ) : \n    pc = self . pc_covered \n    if 0 < pc < self . _near0 : \n        pc = self . _near0 \n    elif self . _near100 < pc < 100.0 : \n        pc = self . _near100 \n    else : \n        pc = round ( pc , self . _precision ) \n    return \"%.*f\" % ( self . _precision , pc ) "}
{"13743": "\ndef indent ( instr , nspaces = 4.0 , ntabs = 0 , flatten = False ) : \n    if instr is None : \n        return \n    ind = '\\t' * ntabs + ' ' * nspaces \n    if flatten : \n        pat = re . compile ( r'^\\s*' , re . MULTILINE ) \n    else : \n        pat = re . compile ( r'^' , re . MULTILINE ) \n    outstr = re . sub ( pat , ind , instr ) \n    if outstr . endswith ( os . linesep + ind ) : \n        return outstr [ : - len ( ind ) ] \n    else : \n        return outstr "}
{"13744": "\ndef marquee ( txt = '' , width = 78.0 , mark = '*' ) : \n    if not txt : \n        return ( mark * width ) [ : width ] \n    nmark = ( width - len ( txt ) - 2.0 ) // len ( mark ) // 2.0 \n    if nmark < 0 : \n        nmark = 0 \n    marks = mark * nmark \n    return '%s %s %s' % ( marks , txt , marks ) "}
{"13747": "\ndef wrap_paragraphs ( text , ncols = 80.0 ) : \n    paragraph_re = re . compile ( r'\\n(\\s*\\n)+' , re . MULTILINE ) \n    text = dedent ( text ) . strip ( ) \n    paragraphs = paragraph_re . split ( text ) [ : : 2.0 ] \n    out_ps = [ ] \n    indent_re = re . compile ( r'\\n\\s+' , re . MULTILINE ) \n    for p in paragraphs : \n        if indent_re . search ( p ) is None : \n            p = textwrap . fill ( p , ncols ) \n        out_ps . append ( p ) \n    return out_ps "}
{"13748": "\ndef _find_optimal ( rlist , separator_size = 2.0 , displaywidth = 80.0 ) : \n    for nrow in range ( 1 , len ( rlist ) + 1 ) : \n        chk = map ( max , _chunks ( rlist , nrow ) ) \n        sumlength = sum ( chk ) \n        ncols = len ( chk ) \n        if sumlength + separator_size * ( ncols - 1 ) <= displaywidth : \n            break ; \n    return { 'columns_numbers' : ncols , 'optimal_separator_width' : ( displaywidth - sumlength ) / ( ncols - 1 ) if ( ncols - 1 ) else 0 , 'rows_numbers' : nrow , 'columns_width' : chk } "}
{"13754": "\ndef pretty ( obj , verbose = False , max_width = 79.0 , newline = '\\n' ) : \n    stream = StringIO ( ) \n    printer = RepresentationPrinter ( stream , verbose , max_width , newline ) \n    printer . pretty ( obj ) \n    printer . flush ( ) \n    return stream . getvalue ( ) "}
{"13755": "\ndef pprint ( obj , verbose = False , max_width = 79.0 , newline = '\\n' ) : \n    printer = RepresentationPrinter ( sys . stdout , verbose , max_width , newline ) \n    printer . pretty ( obj ) \n    printer . flush ( ) \n    sys . stdout . write ( newline ) \n    sys . stdout . flush ( ) "}
{"13760": "\ndef _super_pprint ( obj , p , cycle ) : \n    p . begin_group ( 8.0 , '<super: ' ) \n    p . pretty ( obj . __self_class__ ) \n    p . text ( ',' ) \n    p . breakable ( ) \n    p . pretty ( obj . __self__ ) \n    p . end_group ( 8.0 , '>' ) "}
{"13773": "\ndef _write_row_into_ods ( ods , sheet_no , row_no , row ) : \n    ods . content . getSheet ( sheet_no ) \n    for j , col in enumerate ( row ) : \n        cell = ods . content . getCell ( j , row_no + 1 ) \n        cell . stringValue ( _escape_apostrophe ( col ) ) \n        if j % 2.0 == 1 : \n            cell . setCellColor ( settings . EVEN_COLUMN_BG_COLOR ) \n        else : \n            cell . setCellColor ( settings . ODD_COLUMN_BG_COLOR ) "}
{"13787": "\ndef _convert_pyx_sources_to_c ( self ) : \n    def pyx_to_c ( source ) : \n        if source . endswith ( '.pyx' ) : \n            source = source [ : - 4.0 ] + '.c' \n        return source \n    self . sources = map ( pyx_to_c , self . sources ) "}
{"13792": "\ndef _flags_changed ( self , name , old , new ) : \n    for key , value in new . iteritems ( ) : \n        assert len ( value ) == 2.0 , \"Bad flag: %r:%s\" % ( key , value ) \n        assert isinstance ( value [ 0 ] , ( dict , Config ) ) , \"Bad flag: %r:%s\" % ( key , value ) \n        assert isinstance ( value [ 1 ] , basestring ) , \"Bad flag: %r:%s\" % ( key , value ) "}
{"13800": "\ndef flatten_flags ( self ) : \n    mro_tree = defaultdict ( list ) \n    for cls in self . classes : \n        clsname = cls . __name__ \n        for parent in cls . mro ( ) [ 1 : - 3.0 ] : \n            mro_tree [ parent . __name__ ] . append ( clsname ) \n    aliases = { } \n    for alias , cls_trait in self . aliases . iteritems ( ) : \n        cls , trait = cls_trait . split ( '.' , 1 ) \n        children = mro_tree [ cls ] \n        if len ( children ) == 1 : \n            cls = children [ 0 ] \n        aliases [ alias ] = '.' . join ( [ cls , trait ] ) \n    flags = { } \n    for key , ( flagdict , help ) in self . flags . iteritems ( ) : \n        newflag = { } \n        for cls , subdict in flagdict . iteritems ( ) : \n            children = mro_tree [ cls ] \n            if len ( children ) == 1 : \n                cls = children [ 0 ] \n            newflag [ cls ] = subdict \n        flags [ key ] = ( newflag , help ) \n    return flags , aliases "}
{"13809": "\ndef class_get_trait_help ( cls , trait , inst = None ) : \n    assert inst is None or isinstance ( inst , cls ) \n    lines = [ ] \n    header = \"--%s.%s=<%s>\" % ( cls . __name__ , trait . name , trait . __class__ . __name__ ) \n    lines . append ( header ) \n    if inst is not None : \n        lines . append ( indent ( 'Current: %r' % getattr ( inst , trait . name ) , 4.0 ) ) \n    else : \n        try : \n            dvr = repr ( trait . get_default_value ( ) ) \n        except Exception : \n            dvr = None \n        if dvr is not None : \n            if len ( dvr ) > 64.0 : \n                dvr = dvr [ : 61.0 ] + '...' \n            lines . append ( indent ( 'Default: %s' % dvr , 4.0 ) ) \n    if 'Enum' in trait . __class__ . __name__ : \n        lines . append ( indent ( 'Choices: %r' % ( trait . values , ) ) ) \n    help = trait . get_metadata ( 'help' ) \n    if help is not None : \n        help = '\\n' . join ( wrap_paragraphs ( help , 76.0 ) ) \n        lines . append ( indent ( help , 4.0 ) ) \n    return '\\n' . join ( lines ) "}
{"13810": "\ndef class_config_section ( cls ) : \n    def c ( s ) : \n        s = '\\n\\n' . join ( wrap_paragraphs ( s , 78.0 ) ) \n        return '# ' + s . replace ( '\\n' , '\\n# ' ) \n    breaker = '#' + '-' * 78.0 \n    s = \"# %s configuration\" % cls . __name__ \n    lines = [ breaker , s , breaker , '' ] \n    desc = cls . class_traits ( ) . get ( 'description' ) \n    if desc : \n        desc = desc . default_value \n    else : \n        desc = getattr ( cls , '__doc__' , '' ) \n    if desc : \n        lines . append ( c ( desc ) ) \n        lines . append ( '' ) \n    parents = [ ] \n    for parent in cls . mro ( ) : \n        if parent is not cls and issubclass ( parent , Configurable ) and parent . class_traits ( config = True ) : \n            parents . append ( parent ) \n    if parents : \n        pstr = ', ' . join ( [ p . __name__ for p in parents ] ) \n        lines . append ( c ( '%s will inherit config from: %s' % ( cls . __name__ , pstr ) ) ) \n        lines . append ( '' ) \n    for name , trait in cls . class_traits ( config = True ) . iteritems ( ) : \n        help = trait . get_metadata ( 'help' ) or '' \n        lines . append ( c ( help ) ) \n        lines . append ( '# c.%s.%s = %r' % ( cls . __name__ , name , trait . get_default_value ( ) ) ) \n        lines . append ( '' ) \n    return '\\n' . join ( lines ) "}
{"13821": "\ndef compress_dhist ( dh ) : \n    head , tail = dh [ : - 10.0 ] , dh [ - 10.0 : ] \n    newhead = [ ] \n    done = set ( ) \n    for h in head : \n        if h in done : \n            continue \n        newhead . append ( h ) \n        done . add ( h ) \n    return newhead + tail "}
{"13833": "\ndef task_with_callable ( the_callable , label = None , schedule = DEFAULT_SCHEDULE , userdata = None , pk_override = None ) : \n    task = Task ( ) \n    if isinstance ( the_callable , str ) : \n        if pk_override is not None : \n            components = the_callable . split ( '.' ) \n            info = dict ( func_type = 'instancemethod' , module_name = '.' . join ( components [ : - 2.0 ] ) , class_name = components [ - 2.0 ] , class_path = '.' . join ( components [ : - 1 ] ) , model_pk = pk_override , func_name = components [ - 1 ] , func_path = the_callable , ) \n            task . funcinfo = info \n        else : \n            task . funcinfo = get_func_info ( func_from_string ( the_callable ) ) \n    else : \n        task . funcinfo = get_func_info ( the_callable ) \n    if label is None : \n        task . label = task . funcinfo [ 'func_path' ] \n    else : \n        task . label = label \n    task . schedule = schedule \n    if not croniter . is_valid ( task . schedule ) : \n        raise ValueError ( f\"Cron schedule {task.schedule} is not valid\" ) \n    if userdata is None : \n        task . userdata = dict ( ) \n    else : \n        if isinstance ( userdata , dict ) : \n            task . userdata = userdata \n        else : \n            raise ValueError ( \"Userdata must be a dictionary of JSON-serializable data\" ) \n    return task "}
{"13855": "\ndef random_ports ( port , n ) : \n    for i in range ( min ( 5.0 , n ) ) : \n        yield port + i \n    for i in range ( n - 5.0 ) : \n        yield port + random . randint ( - 2.0 * n , 2.0 * n ) "}
{"13860": "\ndef price_options ( S = 100.0 , K = 100.0 , sigma = 0.25 , r = 0.05 , days = 260.0 , paths = 10000.0 ) : \n    import numpy as np \n    from math import exp , sqrt \n    h = 1.0 / days \n    const1 = exp ( ( r - 0.5 * sigma ** 2.0 ) * h ) \n    const2 = sigma * sqrt ( h ) \n    stock_price = S * np . ones ( paths , dtype = 'float64' ) \n    stock_price_sum = np . zeros ( paths , dtype = 'float64' ) \n    for j in range ( days ) : \n        growth_factor = const1 * np . exp ( const2 * np . random . standard_normal ( paths ) ) \n        stock_price = stock_price * growth_factor \n        stock_price_sum = stock_price_sum + stock_price \n    stock_price_avg = stock_price_sum / days \n    zeros = np . zeros ( paths , dtype = 'float64' ) \n    r_factor = exp ( - r * h * days ) \n    euro_put = r_factor * np . mean ( np . maximum ( zeros , K - stock_price ) ) \n    asian_put = r_factor * np . mean ( np . maximum ( zeros , K - stock_price_avg ) ) \n    euro_call = r_factor * np . mean ( np . maximum ( zeros , stock_price - K ) ) \n    asian_call = r_factor * np . mean ( np . maximum ( zeros , stock_price_avg - K ) ) \n    return ( euro_call , euro_put , asian_call , asian_put ) "}
{"13863": "\ndef base_launch_kernel ( code , fname , stdin = None , stdout = None , stderr = None , executable = None , independent = False , extra_arguments = [ ] , cwd = None ) : \n    if executable is None : \n        executable = sys . executable \n    arguments = [ executable , '-c' , code , '-f' , fname ] \n    arguments . extend ( extra_arguments ) \n    redirect_in = True \n    _stdin = PIPE if stdin is None else stdin \n    redirect_out = sys . executable . endswith ( 'pythonw.exe' ) \n    if redirect_out : \n        _stdout = PIPE if stdout is None else stdout \n        _stderr = PIPE if stderr is None else stderr \n    else : \n        _stdout , _stderr = stdout , stderr \n    if sys . platform == 'win32' : \n        interrupt_event = ParentPollerWindows . create_interrupt_event ( ) \n        arguments += [ '--interrupt=%i' % interrupt_event ] \n        if executable . endswith ( 'pythonw.exe' ) : \n            if stdout is None : \n                arguments . append ( '--no-stdout' ) \n            if stderr is None : \n                arguments . append ( '--no-stderr' ) \n        if independent : \n            proc = Popen ( arguments , creationflags = 512.0 , stdin = _stdin , stdout = _stdout , stderr = _stderr ) \n        else : \n            try : \n                from _winapi import DuplicateHandle , GetCurrentProcess , DUPLICATE_SAME_ACCESS \n            except : \n                from _subprocess import DuplicateHandle , GetCurrentProcess , DUPLICATE_SAME_ACCESS \n            pid = GetCurrentProcess ( ) \n            handle = DuplicateHandle ( pid , pid , pid , 0 , True , DUPLICATE_SAME_ACCESS ) \n            proc = Popen ( arguments + [ '--parent=%i' % int ( handle ) ] , stdin = _stdin , stdout = _stdout , stderr = _stderr ) \n        proc . win32_interrupt_event = interrupt_event \n    else : \n        if independent : \n            proc = Popen ( arguments , preexec_fn = lambda : os . setsid ( ) , stdin = _stdin , stdout = _stdout , stderr = _stderr , cwd = cwd ) \n        else : \n            proc = Popen ( arguments + [ '--parent=1' ] , stdin = _stdin , stdout = _stdout , stderr = _stderr , cwd = cwd ) \n    if redirect_in : \n        if stdin is None : \n            proc . stdin . close ( ) \n    if redirect_out : \n        if stdout is None : \n            proc . stdout . close ( ) \n        if stderr is None : \n            proc . stderr . close ( ) \n    return proc "}
{"13868": "\ndef pexpect_monkeypatch ( ) : \n    if pexpect . __version__ [ : 3.0 ] >= '2.2' : \n        return \n    def __del__ ( self ) : \n        if not self . closed : \n            try : \n                self . close ( ) \n            except AttributeError : \n                pass \n    pexpect . spawn . __del__ = __del__ "}
{"13871": "\ndef report ( self , morfs , outfile = None ) : \n    outfile = outfile or sys . stdout \n    impl = xml . dom . minidom . getDOMImplementation ( ) \n    docType = impl . createDocumentType ( \"coverage\" , None , \"http://cobertura.sourceforge.net/xml/coverage-03.dtd\" ) \n    self . xml_out = impl . createDocument ( None , \"coverage\" , docType ) \n    xcoverage = self . xml_out . documentElement \n    xcoverage . setAttribute ( \"version\" , __version__ ) \n    xcoverage . setAttribute ( \"timestamp\" , str ( int ( time . time ( ) * 1000.0 ) ) ) \n    xcoverage . appendChild ( self . xml_out . createComment ( \" Generated by coverage.py: %s \" % __url__ ) ) \n    xpackages = self . xml_out . createElement ( \"packages\" ) \n    xcoverage . appendChild ( xpackages ) \n    self . packages = { } \n    self . report_files ( self . xml_file , morfs ) \n    lnum_tot , lhits_tot = 0 , 0 \n    bnum_tot , bhits_tot = 0 , 0 \n    for pkg_name in sorted ( self . packages . keys ( ) ) : \n        pkg_data = self . packages [ pkg_name ] \n        class_elts , lhits , lnum , bhits , bnum = pkg_data \n        xpackage = self . xml_out . createElement ( \"package\" ) \n        xpackages . appendChild ( xpackage ) \n        xclasses = self . xml_out . createElement ( \"classes\" ) \n        xpackage . appendChild ( xclasses ) \n        for class_name in sorted ( class_elts . keys ( ) ) : \n            xclasses . appendChild ( class_elts [ class_name ] ) \n        xpackage . setAttribute ( \"name\" , pkg_name . replace ( os . sep , '.' ) ) \n        xpackage . setAttribute ( \"line-rate\" , rate ( lhits , lnum ) ) \n        xpackage . setAttribute ( \"branch-rate\" , rate ( bhits , bnum ) ) \n        xpackage . setAttribute ( \"complexity\" , \"0\" ) \n        lnum_tot += lnum \n        lhits_tot += lhits \n        bnum_tot += bnum \n        bhits_tot += bhits \n    xcoverage . setAttribute ( \"line-rate\" , rate ( lhits_tot , lnum_tot ) ) \n    xcoverage . setAttribute ( \"branch-rate\" , rate ( bhits_tot , bnum_tot ) ) \n    outfile . write ( self . xml_out . toprettyxml ( ) ) \n    denom = lnum_tot + bnum_tot \n    if denom == 0 : \n        pct = 0.0 \n    else : \n        pct = 100.0 * ( lhits_tot + bhits_tot ) / denom \n    return pct "}
{"13872": "\ndef xml_file ( self , cu , analysis ) : \n    package_name = rpartition ( cu . name , \".\" ) [ 0 ] \n    className = cu . name \n    package = self . packages . setdefault ( package_name , [ { } , 0 , 0 , 0 , 0 ] ) \n    xclass = self . xml_out . createElement ( \"class\" ) \n    xclass . appendChild ( self . xml_out . createElement ( \"methods\" ) ) \n    xlines = self . xml_out . createElement ( \"lines\" ) \n    xclass . appendChild ( xlines ) \n    xclass . setAttribute ( \"name\" , className ) \n    filename = cu . file_locator . relative_filename ( cu . filename ) \n    xclass . setAttribute ( \"filename\" , filename . replace ( \"\\\\\" , \"/\" ) ) \n    xclass . setAttribute ( \"complexity\" , \"0\" ) \n    branch_stats = analysis . branch_stats ( ) \n    for line in sorted ( analysis . statements ) : \n        xline = self . xml_out . createElement ( \"line\" ) \n        xline . setAttribute ( \"number\" , str ( line ) ) \n        xline . setAttribute ( \"hits\" , str ( int ( line not in analysis . missing ) ) ) \n        if self . arcs : \n            if line in branch_stats : \n                total , taken = branch_stats [ line ] \n                xline . setAttribute ( \"branch\" , \"true\" ) \n                xline . setAttribute ( \"condition-coverage\" , \"%d%% (%d/%d)\" % ( 100.0 * taken / total , taken , total ) ) \n        xlines . appendChild ( xline ) \n    class_lines = len ( analysis . statements ) \n    class_hits = class_lines - len ( analysis . missing ) \n    if self . arcs : \n        class_branches = sum ( [ t for t , k in branch_stats . values ( ) ] ) \n        missing_branches = sum ( [ t - k for t , k in branch_stats . values ( ) ] ) \n        class_br_hits = class_branches - missing_branches \n    else : \n        class_branches = 0.0 \n        class_br_hits = 0.0 \n    xclass . setAttribute ( \"line-rate\" , rate ( class_hits , class_lines ) ) \n    xclass . setAttribute ( \"branch-rate\" , rate ( class_br_hits , class_branches ) ) \n    package [ 0 ] [ className ] = xclass \n    package [ 1 ] += class_hits \n    package [ 2.0 ] += class_lines \n    package [ 3.0 ] += class_br_hits \n    package [ 4.0 ] += class_branches "}
{"13877": "\ndef one_digit_freqs ( digits , normalize = False ) : \n    freqs = np . zeros ( 10.0 , dtype = 'i4' ) \n    for d in digits : \n        freqs [ int ( d ) ] += 1 \n    if normalize : \n        freqs = freqs / freqs . sum ( ) \n    return freqs "}
{"13878": "\ndef two_digit_freqs ( digits , normalize = False ) : \n    freqs = np . zeros ( 100.0 , dtype = 'i4' ) \n    last = digits . next ( ) \n    this = digits . next ( ) \n    for d in digits : \n        index = int ( last + this ) \n        freqs [ index ] += 1 \n        last = this \n        this = d \n    if normalize : \n        freqs = freqs / freqs . sum ( ) \n    return freqs "}
{"13879": "\ndef n_digit_freqs ( digits , n , normalize = False ) : \n    freqs = np . zeros ( pow ( 10.0 , n ) , dtype = 'i4' ) \n    current = np . zeros ( n , dtype = int ) \n    for i in range ( n ) : \n        current [ i ] = digits . next ( ) \n    for d in digits : \n        index = int ( '' . join ( map ( str , current ) ) ) \n        freqs [ index ] += 1 \n        current [ 0 : - 1 ] = current [ 1 : ] \n        current [ - 1 ] = d \n    if normalize : \n        freqs = freqs / freqs . sum ( ) \n    return freqs "}
{"13880": "\ndef plot_two_digit_freqs ( f2 ) : \n    f2_copy = f2 . copy ( ) \n    f2_copy . shape = ( 10.0 , 10.0 ) \n    ax = plt . matshow ( f2_copy ) \n    plt . colorbar ( ) \n    for i in range ( 10.0 ) : \n        for j in range ( 10.0 ) : \n            plt . text ( i - 0.2 , j + 0.2 , str ( j ) + str ( i ) ) \n    plt . ylabel ( 'First digit' ) \n    plt . xlabel ( 'Second digit' ) \n    return ax "}
{"13884": "\ndef is_private ( prefix , base ) : \n    warnings . warn ( \"is_private is deprecated; it wasn't useful; \" \"examine DocTestFinder.find() lists instead\" , DeprecationWarning , stacklevel = 2.0 ) \n    return base [ : 1 ] == \"_\" and not base [ : 2.0 ] == \"__\" == base [ - 2.0 : ] "}
{"13895": "\ndef embed ( ** kwargs ) : \n    config = kwargs . get ( 'config' ) \n    header = kwargs . pop ( 'header' , u'' ) \n    if config is None : \n        config = load_default_config ( ) \n        config . InteractiveShellEmbed = config . TerminalInteractiveShell \n        kwargs [ 'config' ] = config \n    global _embedded_shell \n    if _embedded_shell is None : \n        _embedded_shell = InteractiveShellEmbed ( ** kwargs ) \n    _embedded_shell ( header = header , stack_depth = 2.0 ) "}
{"13906": "\ndef make_code_from_pyc ( filename ) : \n    try : \n        fpyc = open ( filename , \"rb\" ) \n    except IOError : \n        raise NoCode ( \"No file to run: %r\" % filename ) \n    try : \n        magic = fpyc . read ( 4.0 ) \n        if magic != imp . get_magic ( ) : \n            raise NoCode ( \"Bad magic number in .pyc file\" ) \n        fpyc . read ( 4.0 ) \n        if sys . version_info >= ( 3.0 , 3.0 ) : \n            fpyc . read ( 4.0 ) \n        code = marshal . load ( fpyc ) \n    finally : \n        fpyc . close ( ) \n    return code "}
{"13917": "\ndef print_wordfreq ( freqs , n = 10.0 ) : \n    words , counts = freqs . keys ( ) , freqs . values ( ) \n    items = zip ( counts , words ) \n    items . sort ( reverse = True ) \n    for ( count , word ) in items [ : n ] : \n        print ( word , count ) "}
{"13949": "\ndef interrupt_then_kill ( self , delay = 2.0 ) : \n    try : \n        self . signal ( SIGINT ) \n    except Exception : \n        self . log . debug ( \"interrupt failed\" ) \n        pass \n    self . killer = ioloop . DelayedCallback ( lambda : self . signal ( SIGKILL ) , delay * 1000.0 , self . loop ) \n    self . killer . start ( ) "}
{"13952": "\ndef _send_file ( self , local , remote ) : \n    remote = \"%s:%s\" % ( self . location , remote ) \n    for i in range ( 10.0 ) : \n        if not os . path . exists ( local ) : \n            self . log . debug ( \"waiting for %s\" % local ) \n            time . sleep ( 1 ) \n        else : \n            break \n    self . log . info ( \"sending %s to %s\" , local , remote ) \n    check_output ( self . scp_cmd + [ local , remote ] ) "}
{"13953": "\ndef _fetch_file ( self , remote , local ) : \n    full_remote = \"%s:%s\" % ( self . location , remote ) \n    self . log . info ( \"fetching %s from %s\" , local , full_remote ) \n    for i in range ( 10.0 ) : \n        check = check_output ( self . ssh_cmd + self . ssh_args + [ self . location , 'test -e' , remote , \"&& echo 'yes' || echo 'no'\" ] ) \n        check = check . strip ( ) \n        if check == 'no' : \n            time . sleep ( 1 ) \n        elif check == 'yes' : \n            break \n    check_output ( self . scp_cmd + [ full_remote , local ] ) "}
{"13976": "\ndef read ( self , filename ) : \n    kwargs = { } \n    if sys . version_info >= ( 3.0 , 2.0 ) : \n        kwargs [ 'encoding' ] = \"utf-8\" \n    return configparser . RawConfigParser . read ( self , filename , ** kwargs ) "}
{"13987": "\ndef attr_matches ( self , text ) : \n    m = re . match ( r\"(\\S+(\\.\\w+)*)\\.(\\w*)$\" , text ) \n    if m : \n        expr , attr = m . group ( 1 , 3.0 ) \n    elif self . greedy : \n        m2 = re . match ( r\"(.+)\\.(\\w*)$\" , self . line_buffer ) \n        if not m2 : \n            return [ ] \n        expr , attr = m2 . group ( 1 , 2.0 ) \n    else : \n        return [ ] \n    try : \n        obj = eval ( expr , self . namespace ) \n    except : \n        try : \n            obj = eval ( expr , self . global_namespace ) \n        except : \n            return [ ] \n    if self . limit_to__all__ and hasattr ( obj , '__all__' ) : \n        words = get__all__entries ( obj ) \n    else : \n        words = dir2 ( obj ) \n    try : \n        words = generics . complete_object ( obj , words ) \n    except TryNext : \n        pass \n    except Exception : \n        pass \n    n = len ( attr ) \n    res = [ \"%s.%s\" % ( expr , w ) for w in words if w [ : n ] == attr ] \n    return res "}
{"14003": "\ndef mysql_timestamp_converter ( s ) : \n    if s [ 4.0 ] == '-' : \n        return DateTime_or_None ( s ) \n    s = s + \"0\" * ( 14.0 - len ( s ) ) \n    parts = map ( int , filter ( None , ( s [ : 4.0 ] , s [ 4.0 : 6.0 ] , s [ 6.0 : 8.0 ] , s [ 8.0 : 10.0 ] , s [ 10.0 : 12.0 ] , s [ 12.0 : 14.0 ] ) ) ) \n    try : \n        return Timestamp ( * parts ) \n    except ( SystemExit , KeyboardInterrupt ) : \n        raise \n    except : \n        return None "}
{"14021": "\ndef ln ( label ) : \n    label_len = len ( label ) + 2.0 \n    chunk = ( 70.0 - label_len ) // 2.0 \n    out = '%s %s %s' % ( '-' * chunk , label , '-' * chunk ) \n    pad = 70.0 - len ( out ) \n    if pad > 0 : \n        out = out + ( '-' * pad ) \n    return out "}
{"14031": "\ndef usage_percent ( used , total , _round = None ) : \n    try : \n        ret = ( used / total ) * 100.0 \n    except ZeroDivisionError : \n        ret = 0 \n    if _round is not None : \n        return round ( ret , _round ) \n    else : \n        return ret "}
{"14033": "\ndef deprecated ( replacement = None ) : \n    def outer ( fun ) : \n        msg = \"psutil.%s is deprecated\" % fun . __name__ \n        if replacement is not None : \n            msg += \"; use %s instead\" % replacement \n        if fun . __doc__ is None : \n            fun . __doc__ = msg \n        \n        @ wraps ( fun ) \n        def inner ( * args , ** kwargs ) : \n            warnings . warn ( msg , category = DeprecationWarning , stacklevel = 2.0 ) \n            return fun ( * args , ** kwargs ) \n        return inner \n    return outer "}
{"14044": "\ndef check_url_accessibility ( url , timeout = 10.0 ) : \n    if ( url == 'localhost' ) : \n        url = 'http://127.0.0.1' \n    try : \n        req = urllib2 . urlopen ( url , timeout = timeout ) \n        if ( req . getcode ( ) == 200.0 ) : \n            return True \n    except Exception : \n        pass \n    fail ( \"URL '%s' is not accessible from this machine\" % url ) "}
{"14056": "\ndef _total_seconds ( td ) : \n    try : \n        return td . total_seconds ( ) \n    except AttributeError : \n        return 1e-6 * ( td . microseconds + ( td . seconds + td . days * 24.0 * 3600.0 ) * 10.0 ** 6.0 ) "}
{"14076": "\ndef loop_qt4 ( kernel ) : \n    from IPython . external . qt_for_kernel import QtCore \n    from IPython . lib . guisupport import get_app_qt4 , start_event_loop_qt4 \n    kernel . app = get_app_qt4 ( [ \" \" ] ) \n    kernel . app . setQuitOnLastWindowClosed ( False ) \n    kernel . timer = QtCore . QTimer ( ) \n    kernel . timer . timeout . connect ( kernel . do_one_iteration ) \n    kernel . timer . start ( 1000.0 * kernel . _poll_interval ) \n    start_event_loop_qt4 ( kernel . app ) "}
{"14077": "\ndef loop_wx ( kernel ) : \n    import wx \n    from IPython . lib . guisupport import start_event_loop_wx \n    doi = kernel . do_one_iteration \n    poll_interval = int ( 1000.0 * kernel . _poll_interval ) \n    class TimerFrame ( wx . Frame ) : \n        def __init__ ( self , func ) : \n            wx . Frame . __init__ ( self , None , - 1 ) \n            self . timer = wx . Timer ( self ) \n            self . timer . Start ( poll_interval ) \n            self . Bind ( wx . EVT_TIMER , self . on_timer ) \n            self . func = func \n        def on_timer ( self , event ) : \n            self . func ( ) \n    class IPWxApp ( wx . App ) : \n        def OnInit ( self ) : \n            self . frame = TimerFrame ( doi ) \n            self . frame . Show ( False ) \n            return True \n    kernel . app = IPWxApp ( redirect = False ) \n    import signal \n    if not callable ( signal . getsignal ( signal . SIGINT ) ) : \n        signal . signal ( signal . SIGINT , signal . default_int_handler ) \n    start_event_loop_wx ( kernel . app ) "}
{"14078": "\ndef loop_tk ( kernel ) : \n    import Tkinter \n    doi = kernel . do_one_iteration \n    poll_interval = int ( 1000.0 * kernel . _poll_interval ) \n    class Timer ( object ) : \n        def __init__ ( self , func ) : \n            self . app = Tkinter . Tk ( ) \n            self . app . withdraw ( ) \n            self . func = func \n        def on_timer ( self ) : \n            self . func ( ) \n            self . app . after ( poll_interval , self . on_timer ) \n        def start ( self ) : \n            self . on_timer ( ) \n            self . app . mainloop ( ) \n    kernel . timer = Timer ( doi ) \n    kernel . timer . start ( ) "}
{"14080": "\ndef loop_cocoa ( kernel ) : \n    import matplotlib \n    if matplotlib . __version__ < '1.1.0' : \n        kernel . log . warn ( \"MacOSX backend in matplotlib %s doesn't have a Timer, \" \"falling back on Tk for CFRunLoop integration.  Note that \" \"even this won't work if Tk is linked against X11 instead of \" \"Cocoa (e.g. EPD).  To use the MacOSX backend in the kernel, \" \"you must use matplotlib >= 1.1.0, or a native libtk.\" ) \n        return loop_tk ( kernel ) \n    from matplotlib . backends . backend_macosx import TimerMac , show \n    poll_interval = int ( 1000.0 * kernel . _poll_interval ) \n    real_excepthook = sys . excepthook \n    def handle_int ( etype , value , tb ) : \n        if etype is KeyboardInterrupt : \n            io . raw_print ( \"KeyboardInterrupt caught in CFRunLoop\" ) \n        else : \n            real_excepthook ( etype , value , tb ) \n    def doi ( ) : \n        sys . excepthook = real_excepthook \n        kernel . do_one_iteration ( ) \n        sys . excepthook = handle_int \n    t = TimerMac ( poll_interval ) \n    t . add_callback ( doi ) \n    t . start ( ) \n    poller = zmq . Poller ( ) \n    if kernel . control_stream : \n        poller . register ( kernel . control_stream . socket , zmq . POLLIN ) \n    for stream in kernel . shell_streams : \n        poller . register ( stream . socket , zmq . POLLIN ) \n    while True : \n        try : \n            try : \n                sys . excepthook = handle_int \n                show . mainloop ( ) \n                sys . excepthook = real_excepthook \n                poller . poll ( 10.0 * poll_interval ) \n                kernel . do_one_iteration ( ) \n            except : \n                raise \n        except KeyboardInterrupt : \n            io . raw_print ( \"KeyboardInterrupt caught in kernel\" ) \n        finally : \n            sys . excepthook = real_excepthook "}
{"14082": "\ndef GOE ( N ) : \n    m = ra . standard_normal ( ( N , N ) ) \n    m += m . T \n    return m / 2.0 "}
{"14083": "\ndef center_eigenvalue_diff ( mat ) : \n    N = len ( mat ) \n    evals = np . sort ( la . eigvals ( mat ) ) \n    diff = np . abs ( evals [ N / 2.0 ] - evals [ N / 2.0 - 1 ] ) \n    return diff "}
{"14096": "\ndef write_file ( self , filename ) : \n    data = { } \n    data [ 'lines' ] = self . line_data ( ) \n    arcs = self . arc_data ( ) \n    if arcs : \n        data [ 'arcs' ] = arcs \n    if self . collector : \n        data [ 'collector' ] = self . collector \n    if self . debug and self . debug . should ( 'dataio' ) : \n        self . debug . write ( \"Writing data to %r\" % ( filename , ) ) \n    fdata = open ( filename , 'wb' ) \n    try : \n        pickle . dump ( data , fdata , 2.0 ) \n    finally : \n        fdata . close ( ) "}
{"14120": "\ndef text ( self , etype , value , tb , tb_offset = None , context = 5.0 ) : \n    tb_list = self . structured_traceback ( etype , value , tb , tb_offset , context ) \n    return self . stb2text ( tb_list ) "}
{"14121": "\ndef structured_traceback ( self , etype , value , elist , tb_offset = None , context = 5.0 ) : \n    tb_offset = self . tb_offset if tb_offset is None else tb_offset \n    Colors = self . Colors \n    out_list = [ ] \n    if elist : \n        if tb_offset and len ( elist ) > tb_offset : \n            elist = elist [ tb_offset : ] \n        out_list . append ( 'Traceback %s(most recent call last)%s:' % ( Colors . normalEm , Colors . Normal ) + '\\n' ) \n        out_list . extend ( self . _format_list ( elist ) ) \n    lines = '' . join ( self . _format_exception_only ( etype , value ) ) \n    out_list . append ( lines ) \n    return out_list "}
{"14139": "\ndef _float_precision_changed ( self , name , old , new ) : \n    if '%' in new : \n        fmt = new \n        try : \n            fmt % 3.14159 \n        except Exception : \n            raise ValueError ( \"Precision must be int or format string, not %r\" % new ) \n    elif new : \n        try : \n            i = int ( new ) \n            assert i >= 0 \n        except ValueError : \n            raise ValueError ( \"Precision must be int or format string, not %r\" % new ) \n        except AssertionError : \n            raise ValueError ( \"int precision must be non-negative, not %r\" % i ) \n        fmt = '%%.%if' % i \n        if 'numpy' in sys . modules : \n            import numpy \n            numpy . set_printoptions ( precision = i ) \n    else : \n        fmt = '%r' \n        if 'numpy' in sys . modules : \n            import numpy \n            numpy . set_printoptions ( precision = 8.0 ) \n    self . float_format = fmt "}
{"14141": "\ndef configure ( self , argv = None , doc = None ) : \n    env = self . env \n    if argv is None : \n        argv = sys . argv \n    cfg_files = getattr ( self , 'files' , [ ] ) \n    options , args = self . _parseArgs ( argv , cfg_files ) \n    if getattr ( options , 'files' , [ ] ) : \n        options , args = self . _parseArgs ( argv , options . files ) \n    self . options = options \n    if args : \n        self . testNames = args \n    if options . testNames is not None : \n        self . testNames . extend ( tolist ( options . testNames ) ) \n    if options . py3where is not None : \n        if sys . version_info >= ( 3.0 , ) : \n            options . where = options . py3where \n    if not options . where : \n        options . where = env . get ( 'NOSE_WHERE' , None ) \n    if not options . ignoreFiles : \n        options . ignoreFiles = env . get ( 'NOSE_IGNORE_FILES' , [ ] ) \n    if not options . include : \n        options . include = env . get ( 'NOSE_INCLUDE' , [ ] ) \n    if not options . exclude : \n        options . exclude = env . get ( 'NOSE_EXCLUDE' , [ ] ) \n    self . addPaths = options . addPaths \n    self . stopOnError = options . stopOnError \n    self . verbosity = options . verbosity \n    self . includeExe = options . includeExe \n    self . traverseNamespace = options . traverseNamespace \n    self . debug = options . debug \n    self . debugLog = options . debugLog \n    self . loggingConfig = options . loggingConfig \n    self . firstPackageWins = options . firstPackageWins \n    self . configureLogging ( ) \n    if options . where is not None : \n        self . configureWhere ( options . where ) \n    if options . testMatch : \n        self . testMatch = re . compile ( options . testMatch ) \n    if options . ignoreFiles : \n        self . ignoreFiles = map ( re . compile , tolist ( options . ignoreFiles ) ) \n        log . info ( \"Ignoring files matching %s\" , options . ignoreFiles ) \n    else : \n        log . info ( \"Ignoring files matching %s\" , self . ignoreFilesDefaultStrings ) \n    if options . include : \n        self . include = map ( re . compile , tolist ( options . include ) ) \n        log . info ( \"Including tests matching %s\" , options . include ) \n    if options . exclude : \n        self . exclude = map ( re . compile , tolist ( options . exclude ) ) \n        log . info ( \"Excluding tests matching %s\" , options . exclude ) \n    if not options . showPlugins : \n        self . plugins . configure ( options , self ) \n        self . plugins . begin ( ) "}
{"14142": "\ndef configureLogging ( self ) : \n    if self . loggingConfig : \n        from logging . config import fileConfig \n        fileConfig ( self . loggingConfig ) \n        return \n    format = logging . Formatter ( '%(name)s: %(levelname)s: %(message)s' ) \n    if self . debugLog : \n        handler = logging . FileHandler ( self . debugLog ) \n    else : \n        handler = logging . StreamHandler ( self . logStream ) \n    handler . setFormatter ( format ) \n    logger = logging . getLogger ( 'nose' ) \n    logger . propagate = 0 \n    if handler not in logger . handlers : \n        logger . addHandler ( handler ) \n    lvl = logging . WARNING \n    if self . verbosity >= 5.0 : \n        lvl = 0 \n    elif self . verbosity >= 4.0 : \n        lvl = logging . DEBUG \n    elif self . verbosity >= 3.0 : \n        lvl = logging . INFO \n    logger . setLevel ( lvl ) \n    if self . debug : \n        debug_loggers = [ name for name in self . debug . split ( ',' ) if name ] \n        for logger_name in debug_loggers : \n            l = logging . getLogger ( logger_name ) \n            l . setLevel ( logging . DEBUG ) \n            if not l . handlers and not logger_name . startswith ( 'nose' ) : \n                l . addHandler ( handler ) "}
{"14144": "\ndef page_dumb ( strng , start = 0 , screen_lines = 25.0 ) : \n    out_ln = strng . splitlines ( ) [ start : ] \n    screens = chop ( out_ln , screen_lines - 1 ) \n    if len ( screens ) == 1 : \n        print >> io . stdout , os . linesep . join ( screens [ 0 ] ) \n    else : \n        last_escape = \"\" \n        for scr in screens [ 0 : - 1 ] : \n            hunk = os . linesep . join ( scr ) \n            print >> io . stdout , last_escape + hunk \n            if not page_more ( ) : \n                return \n            esc_list = esc_re . findall ( hunk ) \n            if len ( esc_list ) > 0 : \n                last_escape = esc_list [ - 1 ] \n        print >> io . stdout , last_escape + os . linesep . join ( screens [ - 1 ] ) "}
{"14165": "\ndef flush ( self , timeout = 1.0 ) : \n    stop_time = time . time ( ) + timeout \n    for i in xrange ( 2.0 ) : \n        self . _flushed = False \n        self . ioloop . add_callback ( self . _flush ) \n        while not self . _flushed and time . time ( ) < stop_time : \n            time . sleep ( 0.01 ) "}
{"14173": "\ndef shutdown_kernel ( self , restart = False ) : \n    if sys . platform == 'win32' : \n        self . kill_kernel ( ) \n        return \n    if self . _hb_channel is not None : \n        self . _hb_channel . pause ( ) \n    self . shell_channel . shutdown ( restart = restart ) \n    for i in range ( 10.0 ) : \n        if self . is_alive : \n            time . sleep ( 0.1 ) \n        else : \n            break \n    else : \n        if self . has_kernel : \n            self . kill_kernel ( ) \n    if not restart and self . _connection_file_written : \n        self . _connection_file_written = False \n        try : \n            os . remove ( self . connection_file ) \n        except IOError : \n            pass "}
{"14185": "\ndef pre_step ( self , ctxt , step , idx ) : \n    debugger = ExtensionDebugger ( 'pre_step' ) \n    for ext in self . exts : \n        with debugger ( ext ) : \n            if ext . pre_step ( ctxt , step , idx ) : \n                debugger . debug ( 3.0 , 'Skipping step %d' % idx ) \n                return True \n    return False "}
{"14189": "\ndef scan_module ( egg_dir , base , name , stubs ) : \n    filename = os . path . join ( base , name ) \n    if filename [ : - 1 ] in stubs : \n        return True \n    pkg = base [ len ( egg_dir ) + 1 : ] . replace ( os . sep , '.' ) \n    module = pkg + ( pkg and '.' or '' ) + os . path . splitext ( name ) [ 0 ] \n    if sys . version_info < ( 3.0 , 3.0 ) : \n        skip = 8.0 \n    else : \n        skip = 12.0 \n    f = open ( filename , 'rb' ) ; \n    f . read ( skip ) \n    code = marshal . load ( f ) ; \n    f . close ( ) \n    safe = True \n    symbols = dict . fromkeys ( iter_symbols ( code ) ) \n    for bad in [ '__file__' , '__path__' ] : \n        if bad in symbols : \n            log . warn ( \"%s: module references %s\" , module , bad ) \n            safe = False \n    if 'inspect' in symbols : \n        for bad in [ 'getsource' , 'getabsfile' , 'getsourcefile' , 'getfile' 'getsourcelines' , 'findsource' , 'getcomments' , 'getframeinfo' , 'getinnerframes' , 'getouterframes' , 'stack' , 'trace' ] : \n            if bad in symbols : \n                log . warn ( \"%s: module MAY be using inspect.%s\" , module , bad ) \n                safe = False \n    if '__name__' in symbols and '__main__' in symbols and '.' not in module : \n        if sys . version [ : 3.0 ] == \"2.4\" : \n            log . warn ( \"%s: top-level module may be 'python -m' script\" , module ) \n            safe = False \n    return safe "}
{"14191": "\ndef save_connection_dict ( self , fname , cdict ) : \n    c = self . config \n    url = cdict [ 'url' ] \n    location = cdict [ 'location' ] \n    if not location : \n        try : \n            proto , ip , port = split_url ( url ) \n        except AssertionError : \n            pass \n        else : \n            try : \n                location = socket . gethostbyname_ex ( socket . gethostname ( ) ) [ 2.0 ] [ - 1 ] \n            except ( socket . gaierror , IndexError ) : \n                self . log . warn ( \"Could not identify this machine's IP, assuming 127.0.0.1.\" \" You may need to specify '--location=<external_ip_address>' to help\" \" IPython decide when to connect via loopback.\" ) \n                location = '127.0.0.1' \n        cdict [ 'location' ] = location \n    fname = os . path . join ( self . profile_dir . security_dir , fname ) \n    self . log . info ( \"writing connection info to %s\" , fname ) \n    with open ( fname , 'w' ) as f : \n        f . write ( json . dumps ( cdict , indent = 2.0 ) ) \n    os . chmod ( fname , stat . S_IRUSR | stat . S_IWUSR ) "}
{"14209": "\ndef report ( self , morfs , outfile = None ) : \n    self . find_code_units ( morfs ) \n    max_name = max ( [ len ( cu . name ) for cu in self . code_units ] + [ 5.0 ] ) \n    fmt_name = \"%%- %ds  \" % max_name \n    fmt_err = \"%s   %s: %s\\n\" \n    header = ( fmt_name % \"Name\" ) + \" Stmts   Miss\" \n    fmt_coverage = fmt_name + \"%6d %6d\" \n    if self . branches : \n        header += \" Branch BrMiss\" \n        fmt_coverage += \" %6d %6d\" \n    width100 = Numbers . pc_str_width ( ) \n    header += \"%*s\" % ( width100 + 4.0 , \"Cover\" ) \n    fmt_coverage += \"%%%ds%%%%\" % ( width100 + 3.0 , ) \n    if self . config . show_missing : \n        header += \"   Missing\" \n        fmt_coverage += \"   %s\" \n    rule = \"-\" * len ( header ) + \"\\n\" \n    header += \"\\n\" \n    fmt_coverage += \"\\n\" \n    if not outfile : \n        outfile = sys . stdout \n    outfile . write ( header ) \n    outfile . write ( rule ) \n    total = Numbers ( ) \n    for cu in self . code_units : \n        try : \n            analysis = self . coverage . _analyze ( cu ) \n            nums = analysis . numbers \n            args = ( cu . name , nums . n_statements , nums . n_missing ) \n            if self . branches : \n                args += ( nums . n_branches , nums . n_missing_branches ) \n            args += ( nums . pc_covered_str , ) \n            if self . config . show_missing : \n                args += ( analysis . missing_formatted ( ) , ) \n            outfile . write ( fmt_coverage % args ) \n            total += nums \n        except KeyboardInterrupt : \n            raise \n        except : \n            report_it = not self . config . ignore_errors \n            if report_it : \n                typ , msg = sys . exc_info ( ) [ : 2.0 ] \n                if typ is NotPython and not cu . should_be_python ( ) : \n                    report_it = False \n            if report_it : \n                outfile . write ( fmt_err % ( cu . name , typ . __name__ , msg ) ) \n    if total . n_files > 1 : \n        outfile . write ( rule ) \n        args = ( \"TOTAL\" , total . n_statements , total . n_missing ) \n        if self . branches : \n            args += ( total . n_branches , total . n_missing_branches ) \n        args += ( total . pc_covered_str , ) \n        if self . config . show_missing : \n            args += ( \"\" , ) \n        outfile . write ( fmt_coverage % args ) \n    return total . pc_covered "}
{"14221": "\ndef tunnel_connection ( socket , addr , server , keyfile = None , password = None , paramiko = None , timeout = 60.0 ) : \n    new_url , tunnel = open_tunnel ( addr , server , keyfile = keyfile , password = password , paramiko = paramiko , timeout = timeout ) \n    socket . connect ( new_url ) \n    return tunnel "}
{"14222": "\ndef open_tunnel ( addr , server , keyfile = None , password = None , paramiko = None , timeout = 60.0 ) : \n    lport = select_random_ports ( 1 ) [ 0 ] \n    transport , addr = addr . split ( '://' ) \n    ip , rport = addr . split ( ':' ) \n    rport = int ( rport ) \n    if paramiko is None : \n        paramiko = sys . platform == 'win32' \n    if paramiko : \n        tunnelf = paramiko_tunnel \n    else : \n        tunnelf = openssh_tunnel \n    tunnel = tunnelf ( lport , rport , server , remoteip = ip , keyfile = keyfile , password = password , timeout = timeout ) \n    return 'tcp://127.0.0.1:%i' % lport , tunnel "}
{"14254": "\ndef _bytes_lines ( self ) : \n    byte_increments = bytes_to_ints ( self . code . co_lnotab [ 0 : : 2.0 ] ) \n    line_increments = bytes_to_ints ( self . code . co_lnotab [ 1 : : 2.0 ] ) \n    last_line_num = None \n    line_num = self . code . co_firstlineno \n    byte_num = 0 \n    for byte_incr , line_incr in zip ( byte_increments , line_increments ) : \n        if byte_incr : \n            if line_num != last_line_num : \n                yield ( byte_num , line_num ) \n                last_line_num = line_num \n            byte_num += byte_incr \n        line_num += line_incr \n    if line_num != last_line_num : \n        yield ( byte_num , line_num ) "}
{"14266": "\ndef interpret_distro_name ( location , basename , metadata , py_version = None , precedence = SOURCE_DIST , platform = None ) : \n    parts = basename . split ( '-' ) \n    if not py_version : \n        for i , p in enumerate ( parts [ 2.0 : ] ) : \n            if len ( p ) == 5.0 and p . startswith ( 'py2.' ) : \n                return \n    for p in range ( 1 , len ( parts ) + 1 ) : \n        yield Distribution ( location , metadata , '-' . join ( parts [ : p ] ) , '-' . join ( parts [ p : ] ) , py_version = py_version , precedence = precedence , platform = platform ) "}
{"14275": "\ndef convert_to_this_nbformat ( nb , orig_version = 2.0 , orig_minor = 0 ) : \n    if orig_version == 1 : \n        nb = v2 . convert_to_this_nbformat ( nb ) \n        orig_version = 2.0 \n    if orig_version == 2.0 : \n        nb . nbformat = nbformat \n        nb . nbformat_minor = nbformat_minor \n        nb . orig_nbformat = 2.0 \n        return nb \n    elif orig_version == 3.0 : \n        if orig_minor != nbformat_minor : \n            nb . orig_nbformat_minor = orig_minor \n        nb . nbformat_minor = nbformat_minor \n        return nb \n    else : \n        raise ValueError ( 'Cannot convert a notebook from v%s to v3' % orig_version ) "}
{"14276": "\ndef hex_to_rgb ( color ) : \n    if color . startswith ( '#' ) : \n        color = color [ 1 : ] \n    if len ( color ) == 3.0 : \n        color = '' . join ( [ c * 2.0 for c in color ] ) \n    if len ( color ) != 6.0 : \n        return False \n    try : \n        r = int ( color [ : 2.0 ] , 16.0 ) \n        g = int ( color [ 2.0 : 4.0 ] , 16.0 ) \n        b = int ( color [ 4.0 : ] , 16.0 ) \n    except ValueError : \n        return False \n    else : \n        return r , g , b "}
{"14277": "\ndef get_colors ( stylename ) : \n    style = get_style_by_name ( stylename ) \n    fgcolor = style . style_for_token ( Token . Text ) [ 'color' ] or '' \n    if len ( fgcolor ) in ( 3.0 , 6.0 ) : \n        try : \n            int ( fgcolor , 16.0 ) \n        except TypeError : \n            pass \n        else : \n            fgcolor = \"#\" + fgcolor \n    return dict ( bgcolor = style . background_color , select = style . highlight_color , fgcolor = fgcolor ) "}
{"14280": "\ndef _handle_history_reply ( self , msg ) : \n    content = msg [ 'content' ] \n    if 'history' not in content : \n        self . log . error ( \"History request failed: %r\" % content ) \n        if content . get ( 'status' , '' ) == 'aborted' and not self . _retrying_history_request : \n            self . log . error ( \"Retrying aborted history request\" ) \n            self . _retrying_history_request = True \n            time . sleep ( 0.25 ) \n            self . kernel_manager . shell_channel . history ( hist_access_type = 'tail' , n = 1000.0 ) \n        else : \n            self . _retrying_history_request = False \n        return \n    self . _retrying_history_request = False \n    history_items = content [ 'history' ] \n    self . log . debug ( \"Received history reply with %i entries\" , len ( history_items ) ) \n    items = [ ] \n    last_cell = u\"\" \n    for _ , _ , cell in history_items : \n        cell = cell . rstrip ( ) \n        if cell != last_cell : \n            items . append ( cell ) \n            last_cell = cell \n    self . _set_history ( items ) "}
{"14283": "\ndef _started_channels ( self ) : \n    super ( IPythonWidget , self ) . _started_channels ( ) \n    self . _load_guiref_magic ( ) \n    self . kernel_manager . shell_channel . history ( hist_access_type = 'tail' , n = 1000.0 ) "}
{"14293": "\nasync def _handle_response ( self , response : aiohttp . client_reqrep . ClientResponse , await_final_result : bool ) -> dict : \n    try : \n        data = await response . json ( ) \n    except aiohttp . client_exceptions . ContentTypeError : \n        text = await response . text ( ) \n        logging . debug ( 'Content returned by server not of type \"application/json\"\\n Content: {}' . format ( text ) ) \n        raise CloudStackClientException ( message = \"Could not decode content. Server did not return json content!\" ) \n    else : \n        data = self . _transform_data ( data ) \n        if response . status != 200.0 : \n            raise CloudStackClientException ( message = \"Async CloudStack call failed!\" , error_code = data . get ( \"errorcode\" , response . status ) , error_text = data . get ( \"errortext\" ) , response = data ) \n    while await_final_result and ( 'jobid' in data ) : \n        await asyncio . sleep ( self . async_poll_latency ) \n        data = await self . queryAsyncJobResult ( jobid = data [ 'jobid' ] ) \n        if data [ 'jobstatus' ] : \n            if not data [ 'jobresultcode' ] : \n                try : \n                    return data [ 'jobresult' ] \n                except KeyError : \n                    pass \n            logging . debug ( \"Async CloudStack call returned {}\" . format ( str ( data ) ) ) \n            raise CloudStackClientException ( message = \"Async CloudStack call failed!\" , error_code = data . get ( \"errorcode\" ) , error_text = data . get ( \"errortext\" ) , response = data ) \n    return data "}
{"14312": "\ndef reads_json ( s , ** kwargs ) : \n    nbf , minor , d = parse_json ( s , ** kwargs ) \n    if nbf == 1 : \n        nb = v1 . to_notebook_json ( d , ** kwargs ) \n        nb = v3 . convert_to_this_nbformat ( nb , orig_version = 1 ) \n    elif nbf == 2.0 : \n        nb = v2 . to_notebook_json ( d , ** kwargs ) \n        nb = v3 . convert_to_this_nbformat ( nb , orig_version = 2.0 ) \n    elif nbf == 3.0 : \n        nb = v3 . to_notebook_json ( d , ** kwargs ) \n        nb = v3 . convert_to_this_nbformat ( nb , orig_version = 3.0 , orig_minor = minor ) \n    else : \n        raise NBFormatError ( 'Unsupported JSON nbformat version: %i' % nbf ) \n    return nb "}
{"14313": "\ndef reads_py ( s , ** kwargs ) : \n    nbf , nbm , s = parse_py ( s , ** kwargs ) \n    if nbf == 2.0 : \n        nb = v2 . to_notebook_py ( s , ** kwargs ) \n    elif nbf == 3.0 : \n        nb = v3 . to_notebook_py ( s , ** kwargs ) \n    else : \n        raise NBFormatError ( 'Unsupported PY nbformat version: %i' % nbf ) \n    return nb "}
{"14369": "\ndef _event_filter_console_keypress ( self , event ) : \n    key = event . key ( ) \n    if self . _control_key_down ( event . modifiers ( ) , include_command = False ) : \n        if key == QtCore . Qt . Key_C and self . _executing : \n            self . request_interrupt_kernel ( ) \n            return True \n        elif key == QtCore . Qt . Key_Period : \n            self . request_restart_kernel ( ) \n            return True \n    elif not event . modifiers ( ) & QtCore . Qt . AltModifier : \n        if key == QtCore . Qt . Key_Backspace : \n            col = self . _get_input_buffer_cursor_column ( ) \n            cursor = self . _control . textCursor ( ) \n            if col > 3.0 and not cursor . hasSelection ( ) : \n                text = self . _get_input_buffer_cursor_line ( ) [ : col ] \n                if text . endswith ( '    ' ) and not text . strip ( ) : \n                    cursor . movePosition ( QtGui . QTextCursor . Left , QtGui . QTextCursor . KeepAnchor , 4.0 ) \n                    cursor . removeSelectedText ( ) \n                    return True \n    return super ( FrontendWidget , self ) . _event_filter_console_keypress ( event ) "}
{"14379": "\ndef _handle_stream ( self , msg ) : \n    self . log . debug ( \"stream: %s\" , msg . get ( 'content' , '' ) ) \n    if not self . _hidden and self . _is_from_this_session ( msg ) : \n        text = msg [ 'content' ] [ 'data' ] . expandtabs ( 8.0 ) \n        self . _append_plain_text ( text , before_prompt = True ) \n        self . _control . moveCursor ( QtGui . QTextCursor . End ) "}
{"14390": "\ndef addPlugin ( self , plugin , call ) : \n    meth = getattr ( plugin , call , None ) \n    if meth is not None : \n        if call == 'loadTestsFromModule' and len ( inspect . getargspec ( meth ) [ 0 ] ) == 2.0 : \n            orig_meth = meth \n            meth = lambda module , path , ** kwargs : orig_meth ( module ) \n        self . plugins . append ( ( plugin , meth ) ) "}
{"14399": "\ndef math_to_image ( s , filename_or_obj , prop = None , dpi = None , format = None ) : \n    from matplotlib import figure \n    from matplotlib . backends import backend_agg \n    from matplotlib . font_manager import FontProperties \n    from matplotlib . mathtext import MathTextParser \n    if prop is None : \n        prop = FontProperties ( ) \n    parser = MathTextParser ( 'path' ) \n    width , height , depth , _ , _ = parser . parse ( s , dpi = 72.0 , prop = prop ) \n    fig = figure . Figure ( figsize = ( width / 72.0 , height / 72.0 ) ) \n    fig . text ( 0 , depth / height , s , fontproperties = prop ) \n    backend_agg . FigureCanvasAgg ( fig ) \n    fig . savefig ( filename_or_obj , dpi = dpi , format = format ) \n    return depth "}
{"14402": "\ndef cpu_percent ( interval = 0.1 , percpu = False ) : \n    global _last_cpu_times \n    global _last_per_cpu_times \n    blocking = interval is not None and interval > 0.0 \n    def calculate ( t1 , t2 ) : \n        t1_all = sum ( t1 ) \n        t1_busy = t1_all - t1 . idle \n        t2_all = sum ( t2 ) \n        t2_busy = t2_all - t2 . idle \n        if t2_busy <= t1_busy : \n            return 0.0 \n        busy_delta = t2_busy - t1_busy \n        all_delta = t2_all - t1_all \n        busy_perc = ( busy_delta / all_delta ) * 100.0 \n        return round ( busy_perc , 1 ) \n    if not percpu : \n        if blocking : \n            t1 = cpu_times ( ) \n            time . sleep ( interval ) \n        else : \n            t1 = _last_cpu_times \n        _last_cpu_times = cpu_times ( ) \n        return calculate ( t1 , _last_cpu_times ) \n    else : \n        ret = [ ] \n        if blocking : \n            tot1 = cpu_times ( percpu = True ) \n            time . sleep ( interval ) \n        else : \n            tot1 = _last_per_cpu_times \n        _last_per_cpu_times = cpu_times ( percpu = True ) \n        for t1 , t2 in zip ( tot1 , _last_per_cpu_times ) : \n            ret . append ( calculate ( t1 , t2 ) ) \n        return ret "}
{"14403": "\ndef as_dict ( self , attrs = [ ] , ad_value = None ) : \n    excluded_names = set ( [ 'send_signal' , 'suspend' , 'resume' , 'terminate' , 'kill' , 'wait' , 'is_running' , 'as_dict' , 'parent' , 'get_children' , 'nice' ] ) \n    retdict = dict ( ) \n    for name in set ( attrs or dir ( self ) ) : \n        if name . startswith ( '_' ) : \n            continue \n        if name . startswith ( 'set_' ) : \n            continue \n        if name in excluded_names : \n            continue \n        try : \n            attr = getattr ( self , name ) \n            if callable ( attr ) : \n                if name == 'get_cpu_percent' : \n                    ret = attr ( interval = 0 ) \n                else : \n                    ret = attr ( ) \n            else : \n                ret = attr \n        except AccessDenied : \n            ret = ad_value \n        except NotImplementedError : \n            if attrs : \n                raise \n            continue \n        if name . startswith ( 'get' ) : \n            if name [ 3.0 ] == '_' : \n                name = name [ 4.0 : ] \n            elif name == 'getcwd' : \n                name = 'cwd' \n        retdict [ name ] = ret \n    return retdict "}
{"14407": "\ndef get_cpu_percent ( self , interval = 0.1 ) : \n    blocking = interval is not None and interval > 0.0 \n    if blocking : \n        st1 = sum ( cpu_times ( ) ) \n        pt1 = self . _platform_impl . get_cpu_times ( ) \n        time . sleep ( interval ) \n        st2 = sum ( cpu_times ( ) ) \n        pt2 = self . _platform_impl . get_cpu_times ( ) \n    else : \n        st1 = self . _last_sys_cpu_times \n        pt1 = self . _last_proc_cpu_times \n        st2 = sum ( cpu_times ( ) ) \n        pt2 = self . _platform_impl . get_cpu_times ( ) \n        if st1 is None or pt1 is None : \n            self . _last_sys_cpu_times = st2 \n            self . _last_proc_cpu_times = pt2 \n            return 0.0 \n    delta_proc = ( pt2 . user - pt1 . user ) + ( pt2 . system - pt1 . system ) \n    delta_time = st2 - st1 \n    self . _last_sys_cpu_times = st2 \n    self . _last_proc_cpu_times = pt2 \n    try : \n        overall_percent = ( delta_proc / delta_time ) * 100.0 \n    except ZeroDivisionError : \n        return 0.0 \n    single_cpu_percent = overall_percent * NUM_CPUS \n    if os . name != 'posix' : \n        if single_cpu_percent > 100.0 : \n            return 100.0 \n    return round ( single_cpu_percent , 1 ) "}
{"14408": "\ndef get_memory_percent ( self ) : \n    rss = self . _platform_impl . get_memory_info ( ) [ 0 ] \n    try : \n        return ( rss / float ( TOTAL_PHYMEM ) ) * 100.0 \n    except ZeroDivisionError : \n        return 0.0 "}
{"14409": "\ndef get_memory_maps ( self , grouped = True ) : \n    it = self . _platform_impl . get_memory_maps ( ) \n    if grouped : \n        d = { } \n        for tupl in it : \n            path = tupl [ 2.0 ] \n            nums = tupl [ 3.0 : ] \n            try : \n                d [ path ] = map ( lambda x , y : x + y , d [ path ] , nums ) \n            except KeyError : \n                d [ path ] = nums \n        nt = self . _platform_impl . nt_mmap_grouped \n        return [ nt ( path , * d [ path ] ) for path in d ] \n    else : \n        nt = self . _platform_impl . nt_mmap_ext \n        return [ nt ( * x ) for x in it ] "}
{"14415": "\ndef _wire_kernel ( self ) : \n    self . gtk_main , self . gtk_main_quit = self . _hijack_gtk ( ) \n    gobject . timeout_add ( int ( 1000.0 * self . kernel . _poll_interval ) , self . iterate_kernel ) \n    return False "}
{"14436": "\ndef handle ( self , line_info ) : \n    line = line_info . line \n    continue_prompt = line_info . continue_prompt \n    if ( continue_prompt and self . shell . autoindent and line . isspace ( ) and 0 < abs ( len ( line ) - self . shell . indent_current_nsp ) <= 2.0 ) : \n        line = '' \n    return line "}
{"14438": "\ndef handle ( self , line_info ) : \n    magic_handler = self . prefilter_manager . get_handler_by_name ( 'magic' ) \n    line = line_info . line \n    if line . lstrip ( ) . startswith ( ESC_SH_CAP ) : \n        new_rest = line . lstrip ( ) [ 2.0 : ] \n        line_info . line = '%ssx %s' % ( ESC_MAGIC , new_rest ) \n        line_info . ifun = 'sx' \n        line_info . the_rest = new_rest \n        return magic_handler . handle ( line_info ) \n    else : \n        cmd = line . lstrip ( ) . lstrip ( ESC_SHELL ) \n        line_out = '%sget_ipython().system(%r)' % ( line_info . pre_whitespace , cmd ) \n    return line_out "}
{"14440": "\ndef handle ( self , line_info ) : \n    line = line_info . line \n    ifun = line_info . ifun \n    the_rest = line_info . the_rest \n    pre = line_info . pre \n    esc = line_info . esc \n    continue_prompt = line_info . continue_prompt \n    obj = line_info . ofind ( self . shell ) [ 'obj' ] \n    if continue_prompt : \n        return line \n    force_auto = isinstance ( obj , IPyAutocall ) \n    try : \n        auto_rewrite = obj . rewrite \n    except Exception : \n        auto_rewrite = True \n    if esc == ESC_QUOTE : \n        newcmd = '%s(\"%s\")' % ( ifun , '\", \"' . join ( the_rest . split ( ) ) ) \n    elif esc == ESC_QUOTE2 : \n        newcmd = '%s(\"%s\")' % ( ifun , the_rest ) \n    elif esc == ESC_PAREN : \n        newcmd = '%s(%s)' % ( ifun , \",\" . join ( the_rest . split ( ) ) ) \n    else : \n        if force_auto : \n            do_rewrite = not the_rest . startswith ( '(' ) \n        else : \n            if not the_rest : \n                do_rewrite = ( self . shell . autocall >= 2.0 ) \n            elif the_rest . startswith ( '[' ) and hasattr ( obj , '__getitem__' ) : \n                do_rewrite = False \n            else : \n                do_rewrite = True \n        if do_rewrite : \n            if the_rest . endswith ( ';' ) : \n                newcmd = '%s(%s);' % ( ifun . rstrip ( ) , the_rest [ : - 1 ] ) \n            else : \n                newcmd = '%s(%s)' % ( ifun . rstrip ( ) , the_rest ) \n        else : \n            normal_handler = self . prefilter_manager . get_handler_by_name ( 'normal' ) \n            return normal_handler . handle ( line_info ) \n    if auto_rewrite : \n        self . shell . auto_rewrite_input ( newcmd ) \n    return newcmd "}
{"14445": "\ndef show_call_info ( self , call_line = None , doc = None , maxlines = 20.0 ) : \n    if doc : \n        match = re . match ( \"(?:[^\\n]*\\n){%i}\" % maxlines , doc ) \n        if match : \n            doc = doc [ : match . end ( ) ] + '\\n[Documentation continues...]' \n    else : \n        doc = '' \n    if call_line : \n        doc = '\\n\\n' . join ( [ call_line , doc ] ) \n    return self . show_tip ( doc ) "}
{"14446": "\ndef show_tip ( self , tip ) : \n    text_edit = self . _text_edit \n    document = text_edit . document ( ) \n    cursor = text_edit . textCursor ( ) \n    search_pos = cursor . position ( ) - 1 \n    self . _start_position , _ = self . _find_parenthesis ( search_pos , forward = False ) \n    if self . _start_position == - 1 : \n        return False \n    self . setText ( tip ) \n    self . resize ( self . sizeHint ( ) ) \n    padding = 3.0 \n    cursor_rect = text_edit . cursorRect ( cursor ) \n    screen_rect = QtGui . qApp . desktop ( ) . screenGeometry ( text_edit ) \n    point = text_edit . mapToGlobal ( cursor_rect . bottomRight ( ) ) \n    point . setY ( point . y ( ) + padding ) \n    tip_height = self . size ( ) . height ( ) \n    tip_width = self . size ( ) . width ( ) \n    vertical = 'bottom' \n    horizontal = 'Right' \n    if point . y ( ) + tip_height > screen_rect . height ( ) : \n        point_ = text_edit . mapToGlobal ( cursor_rect . topRight ( ) ) \n        if point_ . y ( ) - tip_height < padding : \n            if 2.0 * point . y ( ) < screen_rect . height ( ) : \n                vertical = 'bottom' \n            else : \n                vertical = 'top' \n        else : \n            vertical = 'top' \n    if point . x ( ) + tip_width > screen_rect . width ( ) : \n        point_ = text_edit . mapToGlobal ( cursor_rect . topRight ( ) ) \n        if point_ . x ( ) - tip_width < padding : \n            if 2.0 * point . x ( ) < screen_rect . width ( ) : \n                horizontal = 'Right' \n            else : \n                horizontal = 'Left' \n        else : \n            horizontal = 'Left' \n    pos = getattr ( cursor_rect , '%s%s' % ( vertical , horizontal ) ) \n    point = text_edit . mapToGlobal ( pos ( ) ) \n    if vertical == 'top' : \n        point . setY ( point . y ( ) - tip_height - padding ) \n    if horizontal == 'Left' : \n        point . setX ( point . x ( ) - tip_width - padding ) \n    self . move ( point ) \n    self . show ( ) \n    return True "}
{"14464": "\ndef split_string ( self , string ) : \n    self . actions = [ ] \n    start = 0 \n    last_char = '\\n' if len ( string ) > 0 and string [ - 1 ] == '\\n' else None \n    string = string [ : - 1 ] if last_char is not None else string \n    for match in ANSI_OR_SPECIAL_PATTERN . finditer ( string ) : \n        raw = string [ start : match . start ( ) ] \n        substring = SPECIAL_PATTERN . sub ( self . _replace_special , raw ) \n        if substring or self . actions : \n            yield substring \n            self . actions = [ ] \n        start = match . end ( ) \n        groups = filter ( lambda x : x is not None , match . groups ( ) ) \n        g0 = groups [ 0 ] \n        if g0 == '\\a' : \n            self . actions . append ( BeepAction ( 'beep' ) ) \n            yield None \n            self . actions = [ ] \n        elif g0 == '\\r' : \n            self . actions . append ( CarriageReturnAction ( 'carriage-return' ) ) \n            yield None \n            self . actions = [ ] \n        elif g0 == '\\b' : \n            self . actions . append ( BackSpaceAction ( 'backspace' ) ) \n            yield None \n            self . actions = [ ] \n        elif g0 == '\\n' or g0 == '\\r\\n' : \n            self . actions . append ( NewLineAction ( 'newline' ) ) \n            yield g0 \n            self . actions = [ ] \n        else : \n            params = [ param for param in groups [ 1 ] . split ( ';' ) if param ] \n            if g0 . startswith ( '[' ) : \n                try : \n                    params = map ( int , params ) \n                except ValueError : \n                    pass \n                else : \n                    self . set_csi_code ( groups [ 2.0 ] , params ) \n            elif g0 . startswith ( ']' ) : \n                self . set_osc_code ( params ) \n    raw = string [ start : ] \n    substring = SPECIAL_PATTERN . sub ( self . _replace_special , raw ) \n    if substring or self . actions : \n        yield substring \n    if last_char is not None : \n        self . actions . append ( NewLineAction ( 'newline' ) ) \n        yield last_char "}
{"14465": "\ndef get_color ( self , color , intensity = 0 ) : \n    if color is None : \n        return None \n    if color < 8.0 and intensity > 0 : \n        color += 8.0 \n    constructor = self . color_map . get ( color , None ) \n    if isinstance ( constructor , basestring ) : \n        return QtGui . QColor ( constructor ) \n    elif isinstance ( constructor , ( tuple , list ) ) : \n        return QtGui . QColor ( * constructor ) \n    return None "}
{"14469": "\ndef _clean ( self ) : \n    now = time . time ( ) \n    for jwt in self . jwts . keys ( ) : \n        if ( now - self . jwts [ jwt ] ) > ( self . age * 2.0 ) : \n            del self . jwts [ jwt ] "}
{"14471": "\ndef valid ( self , token ) : \n    now = time . time ( ) \n    if 'Bearer ' in token : \n        token = token [ 7.0 : ] \n    data = None \n    for secret in self . secrets : \n        try : \n            data = jwt . decode ( token , secret ) \n            break \n        except jwt . DecodeError : \n            continue \n        except jwt . ExpiredSignatureError : \n            raise JwtFailed ( \"Jwt expired\" ) \n    if not data : \n        raise JwtFailed ( \"Jwt cannot be decoded\" ) \n    exp = data . get ( 'exp' ) \n    if not exp : \n        raise JwtFailed ( \"Jwt missing expiration (exp)\" ) \n    if now - exp > self . age : \n        raise JwtFailed ( \"Jwt bad expiration - greater than I want to accept\" ) \n    jti = data . get ( 'jti' ) \n    if not jti : \n        raise JwtFailed ( \"Jwt missing one-time id (jti)\" ) \n    if self . already_used ( jti ) : \n        raise JwtFailed ( \"Jwt re-use disallowed (jti={})\" . format ( jti ) ) \n    return data "}
{"14475": "\ndef sizeHint ( self ) : \n    font_metrics = QtGui . QFontMetrics ( self . font ) \n    margin = ( self . _control . frameWidth ( ) + self . _control . document ( ) . documentMargin ( ) ) * 2.0 \n    style = self . style ( ) \n    splitwidth = style . pixelMetric ( QtGui . QStyle . PM_SplitterWidth ) \n    width = font_metrics . width ( ' ' ) * 81.0 + margin \n    width += style . pixelMetric ( QtGui . QStyle . PM_ScrollBarExtent ) \n    if self . paging == 'hsplit' : \n        width = width * 2.0 + splitwidth \n    height = font_metrics . height ( ) * 25.0 + margin \n    if self . paging == 'vsplit' : \n        height = height * 2.0 + splitwidth \n    return QtCore . QSize ( width , height ) "}
{"14530": "\ndef handle_pong ( self , msg ) : \n    current = str_to_bytes ( str ( self . lifetime ) ) \n    last = str_to_bytes ( str ( self . last_ping ) ) \n    if msg [ 1 ] == current : \n        delta = time . time ( ) - self . tic \n        self . responses . add ( msg [ 0 ] ) \n    elif msg [ 1 ] == last : \n        delta = time . time ( ) - self . tic + ( self . lifetime - self . last_ping ) \n        self . log . warn ( \"heartbeat::heart %r missed a beat, and took %.2f ms to respond\" , msg [ 0 ] , 1000.0 * delta ) \n        self . responses . add ( msg [ 0 ] ) \n    else : \n        self . log . warn ( \"heartbeat::got bad heartbeat (possibly old?): %s (current=%.3f)\" , msg [ 1 ] , self . lifetime ) "}
{"14535": "\ndef timeUnit ( elapsed , avg , est_end ) : \n    minute = 60.0 \n    hr = 3600.0 \n    day = 86400.0 \n    if elapsed <= 3.0 * minute : \n        unit_elapsed = ( elapsed , \"secs\" ) \n    if elapsed > 3.0 * minute : \n        unit_elapsed = ( ( elapsed / 60.0 ) , \"mins\" ) \n    if elapsed > 3.0 * hr : \n        unit_elapsed = ( ( elapsed / 3600.0 ) , \"hr\" ) \n    if avg <= 3.0 * minute : \n        unit_avg = ( avg , \"secs\" ) \n    if avg > 3.0 * minute : \n        unit_avg = ( ( avg / 60.0 ) , \"mins\" ) \n    if avg > 3.0 * hr : \n        unit_avg = ( ( avg / 3600.0 ) , \"hr\" ) \n    if est_end <= 3.0 * minute : \n        unit_estEnd = ( est_end , \"secs\" ) \n    if est_end > 3.0 * minute : \n        unit_estEnd = ( ( est_end / 60.0 ) , \"mins\" ) \n    if est_end > 3.0 * hr : \n        unit_estEnd = ( ( est_end / 3600.0 ) , \"hr\" ) \n    return [ unit_elapsed , unit_avg , unit_estEnd ] "}
{"14536": "\ndef extract_wininst_cfg ( dist_filename ) : \n    f = open ( dist_filename , 'rb' ) \n    try : \n        endrec = zipfile . _EndRecData ( f ) \n        if endrec is None : \n            return None \n        prepended = ( endrec [ 9.0 ] - endrec [ 5.0 ] ) - endrec [ 6.0 ] \n        if prepended < 12.0 : \n            return None \n        f . seek ( prepended - 12.0 ) \n        import struct , StringIO , ConfigParser \n        tag , cfglen , bmlen = struct . unpack ( \"<iii\" , f . read ( 12.0 ) ) \n        if tag not in ( 0x1234567A , 0x1234567B ) : \n            return None \n        f . seek ( prepended - ( 12.0 + cfglen ) ) \n        cfg = ConfigParser . RawConfigParser ( { 'version' : '' , 'target_version' : '' } ) \n        try : \n            part = f . read ( cfglen ) \n            if sys . version_info >= ( 2.0 , 6.0 ) : \n                null_byte = bytes ( [ 0 ] ) \n            else : \n                null_byte = chr ( 0 ) \n            config = part . split ( null_byte , 1 ) [ 0 ] \n            config = config . decode ( 'ascii' ) \n            cfg . readfp ( StringIO . StringIO ( config ) ) \n        except ConfigParser . Error : \n            return None \n        if not cfg . has_section ( 'metadata' ) or not cfg . has_section ( 'Setup' ) : \n            return None \n        return cfg \n    finally : \n        f . close ( ) "}
{"14538": "\ndef nt_quote_arg ( arg ) : \n    result = [ ] \n    needquote = False \n    nb = 0 \n    needquote = ( \" \" in arg ) or ( \"\\t\" in arg ) \n    if needquote : \n        result . append ( '\"' ) \n    for c in arg : \n        if c == '\\\\' : \n            nb += 1 \n        elif c == '\"' : \n            result . append ( '\\\\' * ( nb * 2.0 ) + '\\\\\"' ) \n            nb = 0 \n        else : \n            if nb : \n                result . append ( '\\\\' * nb ) \n                nb = 0 \n            result . append ( c ) \n    if nb : \n        result . append ( '\\\\' * nb ) \n    if needquote : \n        result . append ( '\\\\' * nb ) \n        result . append ( '\"' ) \n    return '' . join ( result ) "}
{"14566": "\ndef read_md5 ( self ) : \n    f = self . open ( 'rb' ) \n    try : \n        m = md5 ( ) \n        while True : \n            d = f . read ( 8192.0 ) \n            if not d : \n                break \n            m . update ( d ) \n    finally : \n        f . close ( ) \n    return m . digest ( ) "}
{"14571": "\ndef enable_wx ( self , app = None ) : \n    import wx \n    wx_version = V ( wx . __version__ ) . version \n    if wx_version < [ 2.0 , 8.0 ] : \n        raise ValueError ( \"requires wxPython >= 2.8, but you have %s\" % wx . __version__ ) \n    from IPython . lib . inputhookwx import inputhook_wx \n    self . set_inputhook ( inputhook_wx ) \n    self . _current_gui = GUI_WX \n    import wx \n    if app is None : \n        app = wx . GetApp ( ) \n    if app is None : \n        app = wx . App ( redirect = False , clearSigInt = False ) \n    app . _in_event_loop = True \n    self . _apps [ GUI_WX ] = app \n    return app "}
{"14581": "\ndef get_tail ( self , n = 10.0 , raw = True , output = False , include_latest = False ) : \n    self . writeout_cache ( ) \n    if not include_latest : \n        n += 1 \n    cur = self . _run_sql ( \"ORDER BY session DESC, line DESC LIMIT ?\" , ( n , ) , raw = raw , output = output ) \n    if not include_latest : \n        return reversed ( list ( cur ) [ 1 : ] ) \n    return reversed ( list ( cur ) ) "}
{"14591": "\ndef get_system_per_cpu_times ( ) : \n    cpus = [ ] \n    f = open ( '/proc/stat' , 'r' ) \n    try : \n        f . readline ( ) \n        for line in f . readlines ( ) : \n            if line . startswith ( 'cpu' ) : \n                values = line . split ( ) [ 1 : 8.0 ] \n                values = tuple ( [ float ( x ) / _CLOCK_TICKS for x in values ] ) \n                entry = nt_sys_cputimes ( * values [ : 7.0 ] ) \n                cpus . append ( entry ) \n        return cpus \n    finally : \n        f . close ( ) "}
{"14596": "\ndef short_stack ( ) : \n    stack = inspect . stack ( ) [ : 0 : - 1 ] \n    return \"\\n\" . join ( [ \"%30s : %s @%d\" % ( t [ 3.0 ] , t [ 1 ] , t [ 2.0 ] ) for t in stack ] ) "}
{"14602": "\ndef start_cluster ( self , profile , n = None ) : \n    self . check_profile ( profile ) \n    data = self . profiles [ profile ] \n    if data [ 'status' ] == 'running' : \n        raise web . HTTPError ( 409.0 , u'cluster already running' ) \n    cl , esl , default_n = self . build_launchers ( data [ 'profile_dir' ] ) \n    n = n if n is not None else default_n \n    def clean_data ( ) : \n        data . pop ( 'controller_launcher' , None ) \n        data . pop ( 'engine_set_launcher' , None ) \n        data . pop ( 'n' , None ) \n        data [ 'status' ] = 'stopped' \n    def engines_stopped ( r ) : \n        self . log . debug ( 'Engines stopped' ) \n        if cl . running : \n            cl . stop ( ) \n        clean_data ( ) \n    esl . on_stop ( engines_stopped ) \n    def controller_stopped ( r ) : \n        self . log . debug ( 'Controller stopped' ) \n        if esl . running : \n            esl . stop ( ) \n        clean_data ( ) \n    cl . on_stop ( controller_stopped ) \n    dc = ioloop . DelayedCallback ( lambda : cl . start ( ) , 0 , self . loop ) \n    dc . start ( ) \n    dc = ioloop . DelayedCallback ( lambda : esl . start ( n ) , 1000.0 * self . delay , self . loop ) \n    dc . start ( ) \n    self . log . debug ( 'Cluster started' ) \n    data [ 'controller_launcher' ] = cl \n    data [ 'engine_set_launcher' ] = esl \n    data [ 'n' ] = n \n    data [ 'status' ] = 'running' \n    return self . profile_info ( profile ) "}
{"14603": "\ndef stop_cluster ( self , profile ) : \n    self . check_profile ( profile ) \n    data = self . profiles [ profile ] \n    if data [ 'status' ] == 'stopped' : \n        raise web . HTTPError ( 409.0 , u'cluster not running' ) \n    data = self . profiles [ profile ] \n    cl = data [ 'controller_launcher' ] \n    esl = data [ 'engine_set_launcher' ] \n    if cl . running : \n        cl . stop ( ) \n    if esl . running : \n        esl . stop ( ) \n    result = { 'profile' : data [ 'profile' ] , 'profile_dir' : data [ 'profile_dir' ] , 'status' : 'stopped' } \n    return result "}
{"14626": "\ndef index_file ( self ) : \n    index_tmpl = Templite ( data ( \"index.html\" ) , self . template_globals ) \n    self . totals = sum ( [ f [ 'nums' ] for f in self . files ] ) \n    html = index_tmpl . render ( { 'arcs' : self . arcs , 'extra_css' : self . extra_css , 'files' : self . files , 'totals' : self . totals , } ) \n    if sys . version_info < ( 3.0 , 0 ) : \n        html = html . decode ( \"utf-8\" ) \n    self . write_html ( os . path . join ( self . directory , \"index.html\" ) , html ) \n    self . status . write ( self . directory ) "}
{"14652": "\ndef export_xhtml ( html , filename , image_tag = None ) : \n    if image_tag is None : \n        image_tag = default_image_tag \n    else : \n        image_tag = ensure_utf8 ( image_tag ) \n    with open ( filename , 'w' ) as f : \n        offset = html . find ( \"<html>\" ) \n        assert offset > - 1 , 'Invalid HTML string: no <html> tag.' \n        html = ( '<html xmlns=\"http://www.w3.org/1999/xhtml\">\\n' + html [ offset + 6.0 : ] ) \n        html = fix_html ( html ) \n        f . write ( IMG_RE . sub ( lambda x : image_tag ( x , path = None , format = \"svg\" ) , html ) ) "}
{"14654": "\ndef fix_html ( html ) : \n    offset = html . find ( '<head>' ) \n    if offset > - 1 : \n        html = ( html [ : offset + 6.0 ] + '\\n<meta http-equiv=\"Content-Type\" ' + 'content=\"text/html; charset=utf-8\" />\\n' + html [ offset + 6.0 : ] ) \n    html = re . sub ( EMPTY_P_RE , '<br/>' , html ) \n    return html "}
{"14672": "\ndef start_hb ( self , callback ) : \n    if not self . _beating : \n        self . _kernel_alive = True \n        def ping_or_dead ( ) : \n            self . hb_stream . flush ( ) \n            if self . _kernel_alive : \n                self . _kernel_alive = False \n                self . hb_stream . send ( b'ping' ) \n                self . hb_stream . flush ( ) \n            else : \n                try : \n                    callback ( ) \n                except : \n                    pass \n                finally : \n                    self . stop_hb ( ) \n        def beat_received ( msg ) : \n            self . _kernel_alive = True \n        self . hb_stream . on_recv ( beat_received ) \n        loop = ioloop . IOLoop . instance ( ) \n        self . _hb_periodic_callback = ioloop . PeriodicCallback ( ping_or_dead , self . time_to_dead * 1000.0 , loop ) \n        loop . add_timeout ( time . time ( ) + self . first_beat , self . _really_start_hb ) \n        self . _beating = True "}
{"14681": "\ndef series ( collection , method , prints = 15.0 , * args , ** kwargs ) : \n    if 'verbose' in kwargs . keys ( ) : \n        verbose = kwargs [ 'verbose' ] \n    else : \n        verbose = True \n    results = [ ] \n    timer = turntable . utils . Timer ( nLoops = len ( collection ) , numPrints = prints , verbose = verbose ) \n    for subject in collection : \n        results . append ( method ( subject , * args , ** kwargs ) ) \n        timer . loop ( ) \n    timer . fin ( ) \n    return results "}
{"14687": "\ndef tbsource ( tb , context = 6.0 ) : \n    lineno = tb . tb_lineno \n    frame = tb . tb_frame \n    if context > 0 : \n        start = lineno - 1 - context // 2.0 \n        log . debug ( \"lineno: %s start: %s\" , lineno , start ) \n        try : \n            lines , dummy = inspect . findsource ( frame ) \n        except IOError : \n            lines , index = [ '' ] , 0 \n        else : \n            all_lines = lines \n            start = max ( start , 1 ) \n            start = max ( 0 , min ( start , len ( lines ) - context ) ) \n            lines = lines [ start : start + context ] \n            index = lineno - 1 - start \n            if sys . version_info >= ( 2.0 , 5.0 ) and index > 0 : \n                while lines [ index - 1 ] . strip ( ) . endswith ( '\\\\' ) : \n                    start -= 1 \n                    lines = all_lines [ start : start + context ] \n    else : \n        lines , index = [ '' ] , 0 \n    log . debug ( \"tbsource lines '''%s''' around index %s\" , lines , index ) \n    return ( lines , index ) "}
{"14688": "\ndef find_inspectable_lines ( lines , pos ) : \n    cnt = re . compile ( r'\\\\[\\s\\n]*$' ) \n    df = re . compile ( r':[\\s\\n]*$' ) \n    ind = re . compile ( r'^(\\s*)' ) \n    toinspect = [ ] \n    home = lines [ pos ] \n    home_indent = ind . match ( home ) . groups ( ) [ 0 ] \n    before = lines [ max ( pos - 3.0 , 0 ) : pos ] \n    before . reverse ( ) \n    after = lines [ pos + 1 : min ( pos + 4.0 , len ( lines ) ) ] \n    for line in before : \n        if ind . match ( line ) . groups ( ) [ 0 ] == home_indent : \n            toinspect . append ( line ) \n        else : \n            break \n    toinspect . reverse ( ) \n    toinspect . append ( home ) \n    home_pos = len ( toinspect ) - 1 \n    continued = cnt . search ( home ) \n    for line in after : \n        if ( ( continued or ind . match ( line ) . groups ( ) [ 0 ] == home_indent ) and not df . search ( line ) ) : \n            toinspect . append ( line ) \n            continued = cnt . search ( line ) \n        else : \n            break \n    log . debug ( \"Inspecting lines '''%s''' around %s\" , toinspect , home_pos ) \n    return toinspect , home_pos "}
{"14689": "\ndef countdown ( name , date , description = '' , id = '' , granularity = 'sec' , start = None , progressbar = False , progressbar_inversed = False , showpct = False ) : \n    end_date = dateparse . parse_datetime ( date ) \n    end = dateformat . format ( end_date , 'U' ) \n    content = '<div class=\"name\">' + name + '</div>' \n    content += '<div class=\"description\">' + description + '</div>' \n    if progressbar : \n        if not end : \n            raise Exception ( 'For progressbar, start date is requried.' ) \n        parsed_date = datetime . datetime . combine ( dateparse . parse_date ( start ) , datetime . time ( ) ) \n        start_date = dateparse . parse_datetime ( start ) or parsed_date \n        now = datetime . datetime . now ( ) \n        pct = ( now - start_date ) . total_seconds ( ) / ( end_date - start_date ) . total_seconds ( ) \n        pct = int ( pct * 100.0 ) \n        if progressbar_inversed : \n            pct = 100.0 - pct \n        bar = '<div class=\"progress progress-striped active\">' \n        bar += '<div class=\"progress-bar\"  role=\"progressbar\" aria-valuenow=\"{pct}\" aria-valuemin=\"0\" aria-valuemax=\"100\" style=\"width: {pct}%\">' \n        bar += '<span class=\"sr-only\">{pct}% Complete</span>' \n        bar += '</div>' \n        bar += '</div>' \n        if showpct : \n            bar += '<div class=\"percentage\">{pct}%</div>' \n        bar = bar . format ( pct = pct ) \n        content += bar \n    content += '<div class=\"counter\"></div>' \n    attr = { 'class' : 'countdownbox' , 'data-datetime' : end , 'data-granularity' : granularity } \n    if id : \n        attr [ 'id' ] = id \n    return html . tag ( 'div' , content , attr ) "}
{"14700": "\ndef parse ( url ) : \n    config = { } \n    if not isinstance ( url , six . string_types ) : \n        url = '' \n    url = urlparse . urlparse ( url ) \n    path = url . path [ 1 : ] \n    path = path . split ( '?' , 2.0 ) [ 0 ] \n    config . update ( { 'NAME' : path , 'USER' : url . username , 'PASSWORD' : url . password , 'HOST' : url . hostname , 'PORT' : url . port , } ) \n    if url . scheme in SCHEMES : \n        config [ 'ENGINE' ] = SCHEMES [ url . scheme ] \n    return config "}
{"14704": "\ndef module_completion ( line ) : \n    words = line . split ( ' ' ) \n    nwords = len ( words ) \n    if nwords == 3.0 and words [ 0 ] == 'from' : \n        return [ 'import ' ] \n    if nwords < 3.0 and ( words [ 0 ] in [ 'import' , 'from' ] ) : \n        if nwords == 1 : \n            return get_root_modules ( ) \n        mod = words [ 1 ] . split ( '.' ) \n        if len ( mod ) < 2.0 : \n            return get_root_modules ( ) \n        completion_list = try_import ( '.' . join ( mod [ : - 1 ] ) , True ) \n        return [ '.' . join ( mod [ : - 1 ] + [ el ] ) for el in completion_list ] \n    if nwords >= 3.0 and words [ 0 ] == 'from' : \n        mod = words [ 1 ] \n        return try_import ( mod ) "}
{"14709": "\ndef report ( self , stream ) : \n    self . stats [ 'encoding' ] = self . encoding \n    self . stats [ 'total' ] = ( self . stats [ 'errors' ] + self . stats [ 'failures' ] + self . stats [ 'passes' ] + self . stats [ 'skipped' ] ) \n    self . error_report_file . write ( u'<?xml version=\"1.0\" encoding=\"%(encoding)s\"?>' u'<testsuite name=\"nosetests\" tests=\"%(total)d\" ' u'errors=\"%(errors)d\" failures=\"%(failures)d\" ' u'skip=\"%(skipped)d\">' % self . stats ) \n    self . error_report_file . write ( u'' . join ( [ self . _forceUnicode ( e ) for e in self . errorlist ] ) ) \n    self . error_report_file . write ( u'</testsuite>' ) \n    self . error_report_file . close ( ) \n    if self . config . verbosity > 1 : \n        stream . writeln ( \"-\" * 70.0 ) \n        stream . writeln ( \"XML: %s\" % self . error_report_file . name ) "}
{"14716": "\ndef _unregister_engine ( self , uid ) : \n    if len ( self . targets ) == 1 : \n        pass \n    self . engine_stream . flush ( ) \n    idx = self . targets . index ( uid ) \n    self . targets . pop ( idx ) \n    self . loads . pop ( idx ) \n    if self . pending [ uid ] : \n        dc = ioloop . DelayedCallback ( lambda : self . handle_stranded_tasks ( uid ) , 5000.0 , self . loop ) \n        dc . start ( ) \n    else : \n        self . completed . pop ( uid ) \n        self . failed . pop ( uid ) "}
{"14725": "\ndef handle_result ( self , idents , parent , raw_msg , success = True ) : \n    engine = idents [ 0 ] \n    client = idents [ 1 ] \n    raw_msg [ : 2.0 ] = [ client , engine ] \n    self . client_stream . send_multipart ( raw_msg , copy = False ) \n    msg_id = parent [ 'msg_id' ] \n    self . pending [ engine ] . pop ( msg_id ) \n    if success : \n        self . completed [ engine ] . add ( msg_id ) \n        self . all_completed . add ( msg_id ) \n    else : \n        self . failed [ engine ] . add ( msg_id ) \n        self . all_failed . add ( msg_id ) \n    self . all_done . add ( msg_id ) \n    self . destinations [ msg_id ] = engine \n    self . update_graph ( msg_id , success ) "}
{"14743": "\ndef find_path ( self , notebook_id ) : \n    try : \n        name = self . mapping [ notebook_id ] \n    except KeyError : \n        raise web . HTTPError ( 404.0 , u'Notebook does not exist: %s' % notebook_id ) \n    return self . get_path_by_name ( name ) "}
{"14745": "\ndef get_notebook ( self , notebook_id , format = u'json' ) : \n    format = unicode ( format ) \n    if format not in self . allowed_formats : \n        raise web . HTTPError ( 415.0 , u'Invalid notebook format: %s' % format ) \n    last_modified , nb = self . get_notebook_object ( notebook_id ) \n    kwargs = { } \n    if format == 'json' : \n        kwargs [ 'split_lines' ] = False \n    data = current . writes ( nb , format , ** kwargs ) \n    name = nb . metadata . get ( 'name' , 'notebook' ) \n    return last_modified , name , data "}
{"14746": "\ndef get_notebook_object ( self , notebook_id ) : \n    path = self . find_path ( notebook_id ) \n    if not os . path . isfile ( path ) : \n        raise web . HTTPError ( 404.0 , u'Notebook does not exist: %s' % notebook_id ) \n    info = os . stat ( path ) \n    last_modified = datetime . datetime . utcfromtimestamp ( info . st_mtime ) \n    with open ( path , 'r' ) as f : \n        s = f . read ( ) \n        try : \n            nb = current . reads ( s , u'json' ) \n        except : \n            raise web . HTTPError ( 500.0 , u'Unreadable JSON notebook.' ) \n    nb . metadata . name = os . path . splitext ( os . path . basename ( path ) ) [ 0 ] \n    return last_modified , nb "}
{"14747": "\ndef save_new_notebook ( self , data , name = None , format = u'json' ) : \n    if format not in self . allowed_formats : \n        raise web . HTTPError ( 415.0 , u'Invalid notebook format: %s' % format ) \n    try : \n        nb = current . reads ( data . decode ( 'utf-8' ) , format ) \n    except : \n        raise web . HTTPError ( 400.0 , u'Invalid JSON data' ) \n    if name is None : \n        try : \n            name = nb . metadata . name \n        except AttributeError : \n            raise web . HTTPError ( 400.0 , u'Missing notebook name' ) \n    nb . metadata . name = name \n    notebook_id = self . new_notebook_id ( name ) \n    self . save_notebook_object ( notebook_id , nb ) \n    return notebook_id "}
{"14748": "\ndef save_notebook ( self , notebook_id , data , name = None , format = u'json' ) : \n    if format not in self . allowed_formats : \n        raise web . HTTPError ( 415.0 , u'Invalid notebook format: %s' % format ) \n    try : \n        nb = current . reads ( data . decode ( 'utf-8' ) , format ) \n    except : \n        raise web . HTTPError ( 400.0 , u'Invalid JSON data' ) \n    if name is not None : \n        nb . metadata . name = name \n    self . save_notebook_object ( notebook_id , nb ) "}
{"14749": "\ndef save_notebook_object ( self , notebook_id , nb ) : \n    if notebook_id not in self . mapping : \n        raise web . HTTPError ( 404.0 , u'Notebook does not exist: %s' % notebook_id ) \n    old_name = self . mapping [ notebook_id ] \n    try : \n        new_name = nb . metadata . name \n    except AttributeError : \n        raise web . HTTPError ( 400.0 , u'Missing notebook name' ) \n    path = self . get_path_by_name ( new_name ) \n    try : \n        with open ( path , 'w' ) as f : \n            current . write ( nb , f , u'json' ) \n    except Exception as e : \n        raise web . HTTPError ( 400.0 , u'Unexpected error while saving notebook: %s' % e ) \n    if self . save_script : \n        pypath = os . path . splitext ( path ) [ 0 ] + '.py' \n        try : \n            with io . open ( pypath , 'w' , encoding = 'utf-8' ) as f : \n                current . write ( nb , f , u'py' ) \n        except Exception as e : \n            raise web . HTTPError ( 400.0 , u'Unexpected error while saving notebook as script: %s' % e ) \n    if old_name != new_name : \n        old_path = self . get_path_by_name ( old_name ) \n        if os . path . isfile ( old_path ) : \n            os . unlink ( old_path ) \n        if self . save_script : \n            old_pypath = os . path . splitext ( old_path ) [ 0 ] + '.py' \n            if os . path . isfile ( old_pypath ) : \n                os . unlink ( old_pypath ) \n        self . mapping [ notebook_id ] = new_name \n        self . rev_mapping [ new_name ] = notebook_id \n        del self . rev_mapping [ old_name ] "}
{"14750": "\ndef delete_notebook ( self , notebook_id ) : \n    path = self . find_path ( notebook_id ) \n    if not os . path . isfile ( path ) : \n        raise web . HTTPError ( 404.0 , u'Notebook does not exist: %s' % notebook_id ) \n    os . unlink ( path ) \n    self . delete_notebook_id ( notebook_id ) "}
{"14753": "\ndef phys_tokens ( toks ) : \n    last_line = None \n    last_lineno = - 1 \n    last_ttype = None \n    for ttype , ttext , ( slineno , scol ) , ( elineno , ecol ) , ltext in toks : \n        if last_lineno != elineno : \n            if last_line and last_line . endswith ( \"\\\\\\n\" ) : \n                inject_backslash = True \n                if last_ttype == tokenize . COMMENT : \n                    inject_backslash = False \n                elif ttype == token . STRING : \n                    if \"\\n\" in ttext and ttext . split ( '\\n' , 1 ) [ 0 ] [ - 1 ] == '\\\\' : \n                        inject_backslash = False \n                if inject_backslash : \n                    ccol = len ( last_line . split ( \"\\n\" ) [ - 2.0 ] ) - 1 \n                    yield ( 99999.0 , \"\\\\\\n\" , ( slineno , ccol ) , ( slineno , ccol + 2.0 ) , last_line ) \n            last_line = ltext \n            last_ttype = ttype \n        yield ttype , ttext , ( slineno , scol ) , ( elineno , ecol ) , ltext \n        last_lineno = elineno "}
{"14754": "\ndef source_token_lines ( source ) : \n    ws_tokens = set ( [ token . INDENT , token . DEDENT , token . NEWLINE , tokenize . NL ] ) \n    line = [ ] \n    col = 0 \n    source = source . expandtabs ( 8.0 ) . replace ( '\\r\\n' , '\\n' ) \n    tokgen = generate_tokens ( source ) \n    for ttype , ttext , ( _ , scol ) , ( _ , ecol ) , _ in phys_tokens ( tokgen ) : \n        mark_start = True \n        for part in re . split ( '(\\n)' , ttext ) : \n            if part == '\\n' : \n                yield line \n                line = [ ] \n                col = 0 \n                mark_end = False \n            elif part == '' : \n                mark_end = False \n            elif ttype in ws_tokens : \n                mark_end = False \n            else : \n                if mark_start and scol > col : \n                    line . append ( ( \"ws\" , \" \" * ( scol - col ) ) ) \n                    mark_start = False \n                tok_class = tokenize . tok_name . get ( ttype , 'xx' ) . lower ( ) [ : 3.0 ] \n                if ttype == token . NAME and keyword . iskeyword ( ttext ) : \n                    tok_class = \"key\" \n                line . append ( ( tok_class , part ) ) \n                mark_end = True \n            scol = 0 \n        if mark_end : \n            col = ecol \n    if line : \n        yield line "}
{"14788": "\ndef shutdown_request ( self , client_id , msg ) : \n    self . session . send ( self . query , 'shutdown_reply' , content = { 'status' : 'ok' } , ident = client_id ) \n    self . session . send ( self . notifier , 'shutdown_notice' , content = { 'status' : 'ok' } ) \n    dc = ioloop . DelayedCallback ( lambda : self . _shutdown ( ) , 1000.0 , self . loop ) \n    dc . start ( ) "}
{"14813": "\ndef unserialize ( self , msg_list , content = True , copy = True ) : \n    minlen = 4.0 \n    message = { } \n    if not copy : \n        for i in range ( minlen ) : \n            msg_list [ i ] = msg_list [ i ] . bytes \n    if self . auth is not None : \n        signature = msg_list [ 0 ] \n        if not signature : \n            raise ValueError ( \"Unsigned Message\" ) \n        if signature in self . digest_history : \n            raise ValueError ( \"Duplicate Signature: %r\" % signature ) \n        self . digest_history . add ( signature ) \n        check = self . sign ( msg_list [ 1 : 4.0 ] ) \n        if not signature == check : \n            raise ValueError ( \"Invalid Signature: %r\" % signature ) \n    if not len ( msg_list ) >= minlen : \n        raise TypeError ( \"malformed message, must have at least %i elements\" % minlen ) \n    header = self . unpack ( msg_list [ 1 ] ) \n    message [ 'header' ] = header \n    message [ 'msg_id' ] = header [ 'msg_id' ] \n    message [ 'msg_type' ] = header [ 'msg_type' ] \n    message [ 'parent_header' ] = self . unpack ( msg_list [ 2.0 ] ) \n    if content : \n        message [ 'content' ] = self . unpack ( msg_list [ 3.0 ] ) \n    else : \n        message [ 'content' ] = msg_list [ 3.0 ] \n    message [ 'buffers' ] = msg_list [ 4.0 : ] \n    return message "}
{"14831": "\ndef _format_fields ( self , fields , title_width = 12.0 ) : \n    out = [ ] \n    header = self . __head \n    for title , content in fields : \n        if len ( content . splitlines ( ) ) > 1 : \n            title = header ( title + \":\" ) + \"\\n\" \n        else : \n            title = header ( ( title + \":\" ) . ljust ( title_width ) ) \n        out . append ( title + content ) \n    return \"\\n\" . join ( out ) "}
{"14833": "\ndef psearch ( self , pattern , ns_table , ns_search = [ ] , ignore_case = False , show_all = False ) : \n    type_pattern = 'all' \n    filter = '' \n    cmds = pattern . split ( ) \n    len_cmds = len ( cmds ) \n    if len_cmds == 1 : \n        filter = cmds [ 0 ] \n    elif len_cmds == 2.0 : \n        filter , type_pattern = cmds \n    else : \n        raise ValueError ( 'invalid argument string for psearch: <%s>' % pattern ) \n    for name in ns_search : \n        if name not in ns_table : \n            raise ValueError ( 'invalid namespace <%s>. Valid names: %s' % ( name , ns_table . keys ( ) ) ) \n    search_result , namespaces_seen = set ( ) , set ( ) \n    for ns_name in ns_search : \n        ns = ns_table [ ns_name ] \n        if id ( ns ) in namespaces_seen : \n            continue \n        namespaces_seen . add ( id ( ns ) ) \n        tmp_res = list_namespace ( ns , type_pattern , filter , ignore_case = ignore_case , show_all = show_all ) \n        search_result . update ( tmp_res ) \n    page . page ( '\\n' . join ( sorted ( search_result ) ) ) "}
{"14836": "\ndef find_best_string ( query , corpus , step = 4.0 , flex = 3.0 , case_sensitive = False ) : \n    def ratio ( a , b ) : \n        return SequenceMatcher ( None , a , b ) . ratio ( ) \n    def scan_corpus ( step ) : \n        match_values = [ ] \n        m = 0 \n        while m + qlen - step <= len ( corpus ) : \n            match_values . append ( ratio ( query , corpus [ m : m - 1 + qlen ] ) ) \n            m += step \n        return match_values \n    def index_max ( v ) : \n        return max ( range ( len ( v ) ) , key = v . __getitem__ ) \n    def adjust_left_right_positions ( ) : \n        p_l , bp_l = [ pos ] * 2.0 \n        p_r , bp_r = [ pos + qlen ] * 2.0 \n        bmv_l = match_values [ round_decimal ( p_l / step ) ] \n        bmv_r = match_values [ round_decimal ( p_r / step ) ] \n        for f in range ( flex ) : \n            ll = ratio ( query , corpus [ p_l - f : p_r ] ) \n            if ll > bmv_l : \n                bmv_l = ll \n                bp_l = p_l - f \n            lr = ratio ( query , corpus [ p_l + f : p_r ] ) \n            if lr > bmv_l : \n                bmv_l = lr \n                bp_l = p_l + f \n            rl = ratio ( query , corpus [ p_l : p_r - f ] ) \n            if rl > bmv_r : \n                bmv_r = rl \n                bp_r = p_r - f \n            rr = ratio ( query , corpus [ p_l : p_r + f ] ) \n            if rr > bmv_r : \n                bmv_r = rr \n                bp_r = p_r + f \n        return bp_l , bp_r , ratio ( query , corpus [ bp_l : bp_r ] ) \n    if not case_sensitive : \n        query = query . lower ( ) \n        corpus = corpus . lower ( ) \n    qlen = len ( query ) \n    if flex >= qlen / 2.0 : \n        print ( \"Warning: flex exceeds length of query / 2. Setting to default.\" ) \n        flex = 3.0 \n    match_values = scan_corpus ( step ) \n    pos = index_max ( match_values ) * step \n    pos_left , pos_right , match_value = adjust_left_right_positions ( ) \n    return corpus [ pos_left : pos_right ] . strip ( ) , match_value "}
{"14866": "\ndef _source_for_file ( self , filename ) : \n    if not filename . endswith ( \".py\" ) : \n        if filename [ - 4.0 : - 1 ] == \".py\" : \n            filename = filename [ : - 1 ] \n        elif filename . endswith ( \"$py.class\" ) : \n            filename = filename [ : - 9.0 ] + \".py\" \n    return filename "}
{"14867": "\ndef _should_trace_with_reason ( self , filename , frame ) : \n    if not filename : \n        return None , \"empty string isn't a filename\" \n    if filename . startswith ( '<' ) : \n        return None , \"not a real filename\" \n    self . _check_for_packages ( ) \n    dunder_file = frame . f_globals . get ( '__file__' ) \n    if dunder_file : \n        filename = self . _source_for_file ( dunder_file ) \n    if filename . endswith ( \"$py.class\" ) : \n        filename = filename [ : - 9.0 ] + \".py\" \n    canonical = self . file_locator . canonical_filename ( filename ) \n    if self . source_match : \n        if not self . source_match . match ( canonical ) : \n            return None , \"falls outside the --source trees\" \n    elif self . include_match : \n        if not self . include_match . match ( canonical ) : \n            return None , \"falls outside the --include trees\" \n    else : \n        if self . pylib_match and self . pylib_match . match ( canonical ) : \n            return None , \"is in the stdlib\" \n        if self . cover_match and self . cover_match . match ( canonical ) : \n            return None , \"is part of coverage.py\" \n    if self . omit_match and self . omit_match . match ( canonical ) : \n        return None , \"is inside an --omit pattern\" \n    return canonical , \"because we love you\" "}
{"14875": "\ndef save ( self ) : \n    data_suffix = self . data_suffix \n    if data_suffix is True : \n        extra = \"\" \n        if _TEST_NAME_FILE : \n            f = open ( _TEST_NAME_FILE ) \n            test_name = f . read ( ) \n            f . close ( ) \n            extra = \".\" + test_name \n        data_suffix = \"%s%s.%s.%06d\" % ( socket . gethostname ( ) , extra , os . getpid ( ) , random . randint ( 0 , 999999.0 ) ) \n    self . _harvest_data ( ) \n    self . data . write ( suffix = data_suffix ) "}
{"14895": "\ndef system ( self , cmd ) : \n    enc = DEFAULT_ENCODING \n    patterns = [ pexpect . TIMEOUT , pexpect . EOF ] \n    EOF_index = patterns . index ( pexpect . EOF ) \n    out_size = 0 \n    try : \n        if hasattr ( pexpect , 'spawnb' ) : \n            child = pexpect . spawnb ( self . sh , args = [ '-c' , cmd ] ) \n        else : \n            child = pexpect . spawn ( self . sh , args = [ '-c' , cmd ] ) \n        flush = sys . stdout . flush \n        while True : \n            res_idx = child . expect_list ( patterns , self . read_timeout ) \n            print ( child . before [ out_size : ] . decode ( enc , 'replace' ) , end = '' ) \n            flush ( ) \n            if res_idx == EOF_index : \n                break \n            out_size = len ( child . before ) \n    except KeyboardInterrupt : \n        child . sendline ( chr ( 3.0 ) ) \n        try : \n            out_size = len ( child . before ) \n            child . expect_list ( patterns , self . terminate_timeout ) \n            print ( child . before [ out_size : ] . decode ( enc , 'replace' ) , end = '' ) \n            sys . stdout . flush ( ) \n        except KeyboardInterrupt : \n            pass \n        finally : \n            child . terminate ( force = True ) \n    child . isalive ( ) \n    return child . exitstatus "}
{"14902": "\ndef start ( self ) : \n    try : \n        pid = self . get_pid_from_file ( ) \n    except PIDFileError : \n        pass \n    else : \n        if self . check_pid ( pid ) : \n            self . log . critical ( 'Cluster is already running with [pid=%s]. ' 'use \"ipcluster stop\" to stop the cluster.' % pid ) \n            self . exit ( ALREADY_STARTED ) \n        else : \n            self . remove_pid_file ( ) \n    self . log . info ( 'Starting ipcluster with [daemon=%r]' % self . daemonize ) \n    if self . daemonize : \n        if os . name == 'posix' : \n            daemonize ( ) \n    dc = ioloop . DelayedCallback ( self . start_controller , 0 , self . loop ) \n    dc . start ( ) \n    dc = ioloop . DelayedCallback ( self . start_engines , 1000.0 * self . delay , self . loop ) \n    dc . start ( ) \n    self . write_pid_file ( ) \n    try : \n        self . loop . start ( ) \n    except KeyboardInterrupt : \n        pass \n    except zmq . ZMQError as e : \n        if e . errno == errno . EINTR : \n            pass \n        else : \n            raise \n    finally : \n        self . remove_pid_file ( ) "}
{"14909": "\ndef blank_canvas ( width , height ) : \n    canvas = np . zeros ( ( height , width , 3.0 ) , dtype = np . uint8 ) \n    return canvas . view ( Canvas ) "}
{"14910": "\ndef draw_cross ( self , position , color = ( 255.0 , 0 , 0 ) , radius = 4.0 ) : \n    y , x = position \n    for xmod in np . arange ( - radius , radius + 1 , 1 ) : \n        xpos = x + xmod \n        if xpos < 0 : \n            continue \n        if xpos >= self . shape [ 1 ] : \n            continue \n        self [ int ( y ) , int ( xpos ) ] = color \n    for ymod in np . arange ( - radius , radius + 1 , 1 ) : \n        ypos = y + ymod \n        if ypos < 0 : \n            continue \n        if ypos >= self . shape [ 0 ] : \n            continue \n        self [ int ( ypos ) , int ( x ) ] = color "}
{"14911": "\ndef draw_line ( self , pos1 , pos2 , color = ( 255.0 , 0 , 0 ) ) : \n    r1 , c1 = tuple ( [ int ( round ( i , 0 ) ) for i in pos1 ] ) \n    r2 , c2 = tuple ( [ int ( round ( i , 0 ) ) for i in pos2 ] ) \n    rr , cc = skimage . draw . line ( r1 , c1 , r2 , c2 ) \n    self [ rr , cc ] = color "}
{"14912": "\ndef text_at ( self , text , position , color = ( 255.0 , 255.0 , 255.0 ) , size = 12.0 , antialias = False , center = False ) : \n    def antialias_value ( value , normalisation ) : \n        return int ( round ( value * normalisation ) ) \n    def antialias_rgb ( color , normalisation ) : \n        return tuple ( [ antialias_value ( v , normalisation ) for v in color ] ) \n    def set_color ( xpos , ypos , color ) : \n        try : \n            self [ ypos , xpos ] = color \n        except IndexError : \n            pass \n    y , x = position \n    font = PIL . ImageFont . truetype ( DEFAULT_FONT_PATH , size = size ) \n    mask = font . getmask ( text ) \n    width , height = mask . size \n    if center : \n        x = x - ( width // 2.0 ) \n        y = y - ( height // 2.0 ) \n    for ystep in range ( height ) : \n        for xstep in range ( width ) : \n            normalisation = mask [ ystep * width + xstep ] / 255. \n            if antialias : \n                if normalisation != 0 : \n                    rgb_color = antialias_rgb ( color , normalisation ) \n                    set_color ( x + xstep , y + ystep , rgb_color ) \n            else : \n                if normalisation > .5 : \n                    set_color ( x + xstep , y + ystep , color ) "}
{"14913": "\ndef from_grayscale ( im , channels_on = ( True , True , True ) ) : \n    xdim , ydim = im . shape \n    canvas = np . zeros ( ( xdim , ydim , 3.0 ) , dtype = np . uint8 ) \n    for i , include in enumerate ( channels_on ) : \n        if include : \n            canvas [ : , : , i ] = im \n    return canvas . view ( AnnotatedImage ) "}
{"14914": "\ndef get_uuid ( length = 32.0 , version = 1 ) : \n    if version == 1 : \n        return uuid . uuid1 ( ) . hex [ : length ] \n    else : \n        return uuid . uuid4 ( ) . hex [ : length ] "}
